{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054d583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#导入mindspore框架\n",
    "import mindspore as ms\n",
    "#导入mindspore中Neural Networks(nn)模块，包含预先定义的构建块或计算单元来构建神经网络。\n",
    "from mindspore import nn\n",
    "#导入mindspore中context模块，用于配置当前执行环境，包括执行模式等特性。\n",
    "from mindspore import context\n",
    "#导入参数初始化模块\n",
    "from mindspore.common.initializer import Normal\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "# 设置MindSpore的执行模式和设备，需要更改为CPU\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target='CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7afae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 40 #样本数\n",
    "BATCH_SIZE = 40  #批量大小\n",
    "NOISE_RATE = 0.2  #噪声率\n",
    "INPUT_DIM = 1  #输入维度\n",
    "HIDDEN_DIM = 100  #隐藏层维度\n",
    "OUTPUT_DIM = 1   #输出维度\n",
    "N_LAYERS = 6 #隐藏层数目\n",
    "ITERATION = 1500   #最大输入迭代\n",
    "LEARNING_RATE = 0.003  #学习率\n",
    "DROPOUT_RATE = 0.7\n",
    "WEIGHT_DECAY = 1e-4  #L2正则化惩罚数值\n",
    "MAX_COUNT = 20  #最早终止参数\n",
    "ACTIVATION = nn.LeakyReLU #激活函数\n",
    "\n",
    "#固定结果\n",
    "def fix_seed(seed=1):\n",
    "    # reproducible\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 小批量样本索引\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dcffa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立数据集x值，y值\n",
    "fix_seed(5)\n",
    "data_x = np.linspace(-1, 1, num=int(N_SAMPLES*2.5))[:, np.newaxis]\n",
    "data_y = np.cos(np.pi*data_x)\n",
    "p = np.random.permutation(len(data_x))\n",
    "#建立训练集，测试集，验证集\n",
    "train_x, train_y = data_x[p[0:N_SAMPLES]], data_y[p[0:N_SAMPLES]]\n",
    "test_x, test_y = data_x[p[N_SAMPLES:N_SAMPLES*2]], data_y[p[N_SAMPLES:N_SAMPLES*2]]\n",
    "validate_x, validate_y = data_x[p[N_SAMPLES*2:]], data_y[p[N_SAMPLES*2:]]\n",
    "#设置y值噪声\n",
    "noise = np.random.normal(0, NOISE_RATE, train_y.shape)\n",
    "train_y += noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c749650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义Cosine网络\n",
    "class CosineNet(nn.Cell):\n",
    "    def __init__(self, batchnorm, dropout):\n",
    "        super(CosineNet, self).__init__()\n",
    "        layers = []\n",
    "        if batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(INPUT_DIM))\n",
    "        \n",
    "        # 初始化隐含层\n",
    "        for l_n in range(N_LAYERS):\n",
    "            in_channels = HIDDEN_DIM if l_n > 0 else INPUT_DIM\n",
    "            # 这里使用1x1Conv代替全连接算子，可以与BatchNorm2d算子配合的更好\n",
    "            conv = nn.Conv2d(in_channels, HIDDEN_DIM, kernel_size=1, pad_mode='valid', has_bias=True, weight_init=Normal(0.01))\n",
    "            layers.append(conv)\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(HIDDEN_DIM))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(DROPOUT_RATE))\n",
    "            layers.append(ACTIVATION())\n",
    "        self.layers = nn.SequentialCell(layers)\n",
    "        \n",
    "        # 初始化输出层\n",
    "        self.flatten = nn.Flatten() # 将(N,C,H,W)4维数据转为(N,C*H*W)2维\n",
    "        self.fc = nn.Dense(HIDDEN_DIM, OUTPUT_DIM, weight_init=Normal(0.1), bias_init='zeros')\n",
    "        \n",
    "def construct(self, x):\n",
    "        # 构建隐含层\n",
    "        x = self.layers(x)\n",
    "        # 构建输出层\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a22d2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fn(batchnorm, dropout, l2):\n",
    "    # 实例化网络、Loss、optimizer\n",
    "    net = CosineNet(batchnorm=batchnorm, dropout=dropout)\n",
    "    loss = nn.loss.MSELoss()\n",
    "    opt = nn.optim.Adam(net.trainable_params(), learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY if l2 else 0.0)\n",
    "    # 构建计算loss和训练用的模块\n",
    "    with_loss = nn.WithLossCell(net, loss)\n",
    "    train_step = nn.TrainOneStepCell(with_loss, opt).set_train()\n",
    "    return train_step, with_loss, net\n",
    "\n",
    "# 针对5种不同设置，创建不同的训练任务\n",
    "fc_train, fc_loss, fc_predict = build_fn(batchnorm=False, dropout=False, l2=False) # 默认任务\n",
    "dropout_train, dropout_loss, dropout_predict = build_fn(batchnorm=False, dropout=True, l2=False) # 实验dropout功能\n",
    "bn_train, bn_loss, bn_predict = build_fn(batchnorm=True, dropout=False, l2=False) # 实验batchnorm功能\n",
    "l2_train, l2_loss, l2_predict = build_fn(batchnorm=False, dropout=False, l2=True) # 实验l2 regularization功能\n",
    "early_stop_train, early_stop_loss, early_stop_predict = build_fn(batchnorm=False, dropout=False, l2=False) # 实验Early Stop功能\n",
    "\n",
    "# 辅助函数，用于设置网络是否为train状态，用于batchnorm，dropout等算子判断是否处于train状态。\n",
    "nets_train = [fc_train, dropout_train, bn_train, l2_train, early_stop_train]\n",
    "nets_loss = [fc_loss, dropout_loss, bn_loss, l2_loss, early_stop_loss]\n",
    "nets_predict = [fc_predict, dropout_predict, bn_predict, l2_predict, early_stop_predict]\n",
    "\n",
    "def set_train(nets, mode=True):\n",
    "    for net in nets:\n",
    "        net.set_train(mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a534c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将喂给网络的数据由(N,C)2维转为将(N,C,H,W)4维\n",
    "data_xt, data_yt = ms.Tensor(data_x.reshape(data_x.shape + (1, 1)), ms.float32), ms.Tensor(data_y, ms.float32)\n",
    "test_xt, test_yt = ms.Tensor(test_x.reshape(test_x.shape + (1, 1)), ms.float32), ms.Tensor(test_y, ms.float32)\n",
    "validate_xt, validate_yt = ms.Tensor(validate_x.reshape(validate_x.shape + (1, 1)), ms.float32), ms.Tensor(validate_y, ms.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0a6742",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The 'sub' operation does not support the type [None, Tensor[Float32]].\nThe supported types of overload function `sub` is: [Tensor, List], [List, Tensor], [Tensor, Tensor], [Tuple, Tensor], [Number, Number], [Number, Tensor], [Tensor, Number], [Tensor, Tuple].\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore\\ccsrc\\frontend\\operator\\composite\\multitype_funcgraph.cc:160 GenerateFromTypes\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1424\\4198133038.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mset_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnets_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 将网络设置为train状态\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mfc_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdropout_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mbn_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\mindspore\\nn\\cell.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    576\u001b[0m                 logger.warning(f\"For 'Cell', it's not support hook function in graph mode. If you want to use hook \"\n\u001b[0;32m    577\u001b[0m                                f\"function, please use context.set_context to set pynative mode.\")\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_and_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\mindspore\\nn\\cell.py\u001b[0m in \u001b[0;36mcompile_and_run\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    963\u001b[0m         \"\"\"\n\u001b[0;32m    964\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_auto_parallel_compile_and_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[0mnew_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\mindspore\\nn\\cell.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    936\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_shape_inputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_shape_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m             _cell_graph_executor.compile(self, *inputs, phase=self.phase, auto_parallel_mode=self._auto_parallel_mode,\n\u001b[1;32m--> 938\u001b[1;33m                                          jit_config_dict=self._jit_config_dict)\n\u001b[0m\u001b[0;32m    939\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_compile_dynamic_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\mindspore\\common\\api.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, obj, phase, do_convert, auto_parallel_mode, jit_config_dict, *args)\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mjit_config_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_jit_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjit_config_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1137\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_vm_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1138\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The 'sub' operation does not support the type [None, Tensor[Float32]].\nThe supported types of overload function `sub` is: [Tensor, List], [List, Tensor], [Tensor, Tensor], [Tuple, Tensor], [Number, Number], [Number, Tensor], [Tensor, Number], [Tensor, Tuple].\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore\\ccsrc\\frontend\\operator\\composite\\multitype_funcgraph.cc:160 GenerateFromTypes\n"
     ]
    }
   ],
   "source": [
    "# 设置提前终止(Early Stop)用到的一些指标\n",
    "early_stop = False # 为True时，终止相关的训练\n",
    "min_val_loss = 1 # 足够大的初始值，训练过程中用于记录最小的验证loss\n",
    "count = 0 # 训练迭代过程中，验证loss连续多少次大于min_val_loss\n",
    "\n",
    "for it in range(ITERATION):\n",
    "    # 每个迭代随机从训练集中选择一个batch的样本，当batch_size==N_SAMPLES时，仅作了shuffle\n",
    "    mb_idx = sample_idx(N_SAMPLES, BATCH_SIZE)\n",
    "    x_batch, y_batch = train_x[mb_idx, :], train_y[mb_idx, :]\n",
    "    x_batch, y_batch = ms.Tensor(x_batch.reshape(x_batch.shape + (1, 1)), ms.float32), ms.Tensor(y_batch, ms.float32)\n",
    "    \n",
    "    set_train(nets_train, True) # 将网络设置为train状态\n",
    "    fc_train(x_batch, y_batch)\n",
    "    dropout_train(x_batch, y_batch)\n",
    "    bn_train(x_batch, y_batch)\n",
    "    l2_train(x_batch, y_batch)\n",
    "    # 为True时，终止相关的训练\n",
    "    if not early_stop:\n",
    "        early_stop_train(x_batch, y_batch)\n",
    "    \n",
    "    if it % 20 == 0:\n",
    "        set_train(nets_loss+nets_predict, False) # 将网络设置为非train状态\n",
    "        # 计算各模型在测试集上的loss\n",
    "        loss_fc = fc_loss(test_xt, test_yt)\n",
    "        loss_dropout = dropout_loss(test_xt, test_yt)\n",
    "        loss_bn = bn_loss(test_xt, test_yt)\n",
    "        loss_l2 = l2_loss(test_xt, test_yt)\n",
    "        loss_early_stop = early_stop_loss(test_xt, test_yt)\n",
    "        \n",
    "        # 计算各模型在全量样本上的预测值，用于评估模型的拟合效果\n",
    "        all_fc = fc_predict(data_xt)\n",
    "        all_dropout = dropout_predict(data_xt)\n",
    "        all_bn = bn_predict(data_xt)\n",
    "        all_l2 = l2_predict(data_xt)\n",
    "        all_early_stop = early_stop_predict(data_xt)\n",
    "        \n",
    "        # 对于Early Stop任务，当验证集loss连续MAX_COUNT次大于min_val_loss时，终止该任务的训练\n",
    "        if not early_stop:\n",
    "            val_loss = early_stop_loss(validate_xt, validate_yt)\n",
    "            if val_loss > min_val_loss:\n",
    "                count += 1\n",
    "            else:\n",
    "                min_val_loss = val_loss\n",
    "                count = 0\n",
    "            \n",
    "            if count == MAX_COUNT:\n",
    "                early_stop = True\n",
    "                print('='*10, 'early stopped', '='*10)\n",
    "        \n",
    "        # 画图\n",
    "        plt.figure(1, figsize=(15,10))\n",
    "        plt.cla()\n",
    "        plt.scatter(train_x, train_y, c='magenta', s=50, alpha=0.3, label='train samples')\n",
    "        plt.scatter(test_x, test_y, c='cyan', s=50, alpha=0.3, label='test samples')\n",
    "        plt.plot(data_x, all_fc.asnumpy(), 'r', label='overfitting')\n",
    "        plt.plot(data_x, all_l2.asnumpy(), 'y', label='L2 regularization')\n",
    "        plt.plot(data_x, all_early_stop.asnumpy(), 'k', label='early stopping')\n",
    "        plt.plot(data_x, all_dropout.asnumpy(), 'b', label='dropout({})'.format(DROPOUT_RATE))\n",
    "        plt.plot(data_x, all_bn.asnumpy(), 'g', label='batch normalization')\n",
    "        plt.text(-0.1, -1.2, 'overfitting loss=%.4f' % loss_fc.asnumpy(), fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.text(-0.1, -1.5, 'L2 regularization loss=%.4f' % loss_l2.asnumpy(), fontdict={'size': 20, 'color': 'y'})\n",
    "        plt.text(-0.1, -1.8, 'early stopping loss=%.4f' % loss_early_stop.asnumpy(), fontdict={'size': 20, 'color': 'black'})\n",
    "        plt.text(-0.1, -2.1, 'dropout loss=%.4f' % loss_dropout.asnumpy(), fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.text(-0.1, -2.4, 'batch normalization loss=%.4f' % loss_bn.asnumpy(), fontdict={'size': 20, 'color': 'green'})\n",
    "\n",
    "        plt.legend(loc='upper left');\n",
    "        plt.ylim((-2.5, 2.5));\n",
    "        clear_output(wait=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d80ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
