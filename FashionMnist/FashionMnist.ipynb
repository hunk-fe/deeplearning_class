{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import sys\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore import Tensor\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target='CPU') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = edict({\n",
    "    'train_size': 60000,  # 训练集大小\n",
    "    'test_size': 10000,  # 测试集大小\n",
    "    'channel': 1,  # 图片通道数\n",
    "    'image_height': 28,  # 图片高度\n",
    "    'image_width': 28,  # 图片宽度\n",
    "    'batch_size': 64,\n",
    "    'num_classes': 10,  # 分类类别\n",
    "    'lr': 0.001,  # 学习率\n",
    "    'epoch_size': 20,  # 训练次数\n",
    "    'data_dir_train': os.path.join('E:/DeepLearning/FashionMnist/fashion-mnist', 'train'),\n",
    "    'data_dir_test': os.path.join('E:/DeepLearning/FashionMnist/fashion-mnist', 'test'),\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_name):\n",
    "    '''\n",
    "    :param file_name: 文件路径\n",
    "    :return:  训练或者测试数据\n",
    "    如下是训练的图片的二进制格式\n",
    "    [offset] [type]          [value]          [description]\n",
    "    0000     32 bit integer  0x00000803(2051) magic number\n",
    "    0004     32 bit integer  60000            number of images\n",
    "    0008     32 bit integer  28               number of rows\n",
    "    0012     32 bit integer  28               number of columns\n",
    "    0016     unsigned byte   ??               pixel\n",
    "    0017     unsigned byte   ??               pixel\n",
    "    ........\n",
    "    xxxx     unsigned byte   ??               pixel\n",
    "    '''\n",
    "    file_handle = open(file_name, \"rb\")  # 以二进制打开文档\n",
    "    file_content = file_handle.read()  # 读取到缓冲区中\n",
    "    head = struct.unpack_from('>IIII', file_content, 0)  # 取前4个整数，返回一个元组\n",
    "    offset = struct.calcsize('>IIII')\n",
    "    imgNum = head[1]  # 图片数\n",
    "    width = head[2]  # 宽度\n",
    "    height = head[3]  # 高度\n",
    "    bits = imgNum * width * height  # data一共有60000*28*28个像素值\n",
    "    bitsString = '>' + str(bits) + 'B'  # fmt格式：'>47040000B'\n",
    "    imgs = struct.unpack_from(bitsString, file_content, offset)  # 取data数据，返回一个元组\n",
    "    imgs_array = np.array(imgs, np.float32).reshape((imgNum, width * height))  # 最后将读取的数据reshape成 【图片数，图片像素】二维数组\n",
    "    return imgs_array\n",
    "\n",
    "\n",
    "def read_label(file_name):\n",
    "    '''\n",
    "    :param file_name:\n",
    "    :return:\n",
    "    标签的格式如下：\n",
    "    [offset] [type]          [value]          [description]\n",
    "    0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n",
    "    0004     32 bit integer  60000            number of items\n",
    "    0008     unsigned byte   ??               label\n",
    "    0009     unsigned byte   ??               label\n",
    "    ........\n",
    "    xxxx     unsigned byte   ??               label\n",
    "    The labels values are 0 to 9.\n",
    "    '''\n",
    "    file_handle = open(file_name, \"rb\")  # 以二进制打开文档\n",
    "    file_content = file_handle.read()  # 读取到缓冲区中\n",
    "    head = struct.unpack_from('>II', file_content, 0)  # 取前2个整数，返回一个元组\n",
    "    offset = struct.calcsize('>II')\n",
    "    labelNum = head[1]  # label数\n",
    "    bitsString = '>' + str(labelNum) + 'B'  # fmt格式：'>47040000B'\n",
    "    label = struct.unpack_from(bitsString, file_content, offset)  # 取data数据，返回一个元组\n",
    "    return np.array(label, np.int32)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # 文件获取\n",
    "    train_image = os.path.join(cfg.data_dir_train, 'train-images-idx3-ubyte')\n",
    "    test_image = os.path.join(cfg.data_dir_test, \"t10k-images-idx3-ubyte\")\n",
    "    train_label = os.path.join(cfg.data_dir_train, \"train-labels-idx1-ubyte\")\n",
    "    test_label = os.path.join(cfg.data_dir_test, \"t10k-labels-idx1-ubyte\")\n",
    "    # 读取数据\n",
    "    train_x = read_image(train_image)\n",
    "    test_x = read_image(test_image)\n",
    "    train_y = read_label(train_label)\n",
    "    test_y = read_label(test_label)\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集样本数： 60000\n",
      "测试数据集样本数： 10000\n",
      "通道数/图像长/宽： (1, 28, 28)\n",
      "一张图像的标签样式： 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = get_data()\n",
    "train_x = train_x.reshape(-1, 1, cfg.image_height, cfg.image_width)\n",
    "test_x = test_x.reshape(-1, 1, cfg.image_height, cfg.image_width)\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "print('训练数据集样本数：', train_x.shape[0])\n",
    "print('测试数据集样本数：', test_y.shape[0])\n",
    "print('通道数/图像长/宽：', train_x.shape[1:])\n",
    "print('一张图像的标签样式：', train_y[0])  # 一共10类，用0-9的数字表达类别。\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_x[0,0,...])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换数据类型为Dataset\n",
    "def create_dataset():\n",
    "    XY_train = list(zip(train_x, train_y))\n",
    "    ds_train = ds.GeneratorDataset(XY_train, ['x', 'y'])\n",
    "    ds_train = ds_train.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    XY_test = list(zip(test_x, test_y))\n",
    "    ds_test = ds.GeneratorDataset(XY_test, ['x', 'y'])\n",
    "    ds_test = ds_test.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    return ds_train, ds_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，无正则化\n",
    "class ForwardFashion(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashion, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(128 * 11 * 11, 128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，有正则化\n",
    "class ForwardFashionRegularization(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashionRegularization, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(3200, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Net):\n",
    "    ds_train, ds_test = create_dataset()\n",
    "    # 构建网络\n",
    "    network = Net(cfg.num_classes)\n",
    "    # 定义模型的损失函数，优化器\n",
    "    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "    net_opt = nn.Adam(network.trainable_params(), cfg.lr)\n",
    "    # 训练模型\n",
    "    model = Model(network, loss_fn=net_loss, optimizer=net_opt, metrics={'acc': Accuracy()})\n",
    "    loss_cb = LossMonitor()\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(30, ds_train, callbacks=[loss_cb], dataset_sink_mode=True)\n",
    "    # 验证\n",
    "    metric = model.eval(ds_test)\n",
    "    print(metric)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(976:16928,MainProcess):2022-10-17-12:22:57.841.795 [mindspore\\dataset\\engine\\datasets_user_defined.py:656] Python multiprocessing is not supported on Windows platform.\n",
      "[WARNING] ME(976:16928,MainProcess):2022-10-17-12:22:57.849.774 [mindspore\\dataset\\engine\\datasets_user_defined.py:656] Python multiprocessing is not supported on Windows platform.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.302593469619751\n",
      "epoch: 1 step: 2, loss is 2.302518367767334\n",
      "epoch: 1 step: 3, loss is 2.3025736808776855\n",
      "epoch: 1 step: 4, loss is 2.301427125930786\n",
      "epoch: 1 step: 5, loss is 2.3003411293029785\n",
      "epoch: 1 step: 6, loss is 2.2927796840667725\n",
      "epoch: 1 step: 7, loss is 2.280794382095337\n",
      "epoch: 1 step: 8, loss is 2.2729716300964355\n",
      "epoch: 1 step: 9, loss is 2.2175588607788086\n",
      "epoch: 1 step: 10, loss is 2.212278366088867\n",
      "epoch: 1 step: 11, loss is 2.1554458141326904\n",
      "epoch: 1 step: 12, loss is 2.081629753112793\n",
      "epoch: 1 step: 13, loss is 1.9350179433822632\n",
      "epoch: 1 step: 14, loss is 1.789394736289978\n",
      "epoch: 1 step: 15, loss is 1.4757170677185059\n",
      "epoch: 1 step: 16, loss is 1.5391600131988525\n",
      "epoch: 1 step: 17, loss is 1.46890127658844\n",
      "epoch: 1 step: 18, loss is 1.185867428779602\n",
      "epoch: 1 step: 19, loss is 1.2983123064041138\n",
      "epoch: 1 step: 20, loss is 1.4086023569107056\n",
      "epoch: 1 step: 21, loss is 1.6835355758666992\n",
      "epoch: 1 step: 22, loss is 1.1056842803955078\n",
      "epoch: 1 step: 23, loss is 1.1036043167114258\n",
      "epoch: 1 step: 24, loss is 1.491979718208313\n",
      "epoch: 1 step: 25, loss is 1.009720802307129\n",
      "epoch: 1 step: 26, loss is 1.358446717262268\n",
      "epoch: 1 step: 27, loss is 0.9660155177116394\n",
      "epoch: 1 step: 28, loss is 1.0866552591323853\n",
      "epoch: 1 step: 29, loss is 1.2056277990341187\n",
      "epoch: 1 step: 30, loss is 0.9990710616111755\n",
      "epoch: 1 step: 31, loss is 1.077144980430603\n",
      "epoch: 1 step: 32, loss is 1.1287163496017456\n",
      "epoch: 1 step: 33, loss is 1.1123175621032715\n",
      "epoch: 1 step: 34, loss is 0.9747536778450012\n",
      "epoch: 1 step: 35, loss is 1.1208205223083496\n",
      "epoch: 1 step: 36, loss is 1.0196701288223267\n",
      "epoch: 1 step: 37, loss is 0.900962233543396\n",
      "epoch: 1 step: 38, loss is 0.9186717867851257\n",
      "epoch: 1 step: 39, loss is 0.9724048376083374\n",
      "epoch: 1 step: 40, loss is 0.897426426410675\n",
      "epoch: 1 step: 41, loss is 0.989543080329895\n",
      "epoch: 1 step: 42, loss is 0.9674268960952759\n",
      "epoch: 1 step: 43, loss is 1.088707685470581\n",
      "epoch: 1 step: 44, loss is 1.177060604095459\n",
      "epoch: 1 step: 45, loss is 0.8939704298973083\n",
      "epoch: 1 step: 46, loss is 0.9199742674827576\n",
      "epoch: 1 step: 47, loss is 0.7628818154335022\n",
      "epoch: 1 step: 48, loss is 0.6613119840621948\n",
      "epoch: 1 step: 49, loss is 0.8804590702056885\n",
      "epoch: 1 step: 50, loss is 1.1704672574996948\n",
      "epoch: 1 step: 51, loss is 1.0070370435714722\n",
      "epoch: 1 step: 52, loss is 0.8624449968338013\n",
      "epoch: 1 step: 53, loss is 1.0256792306900024\n",
      "epoch: 1 step: 54, loss is 0.869688868522644\n",
      "epoch: 1 step: 55, loss is 0.7996812462806702\n",
      "epoch: 1 step: 56, loss is 0.80607008934021\n",
      "epoch: 1 step: 57, loss is 0.6517629027366638\n",
      "epoch: 1 step: 58, loss is 0.910289466381073\n",
      "epoch: 1 step: 59, loss is 0.7669817209243774\n",
      "epoch: 1 step: 60, loss is 0.737129271030426\n",
      "epoch: 1 step: 61, loss is 1.0071918964385986\n",
      "epoch: 1 step: 62, loss is 0.9226750135421753\n",
      "epoch: 1 step: 63, loss is 0.8818764090538025\n",
      "epoch: 1 step: 64, loss is 0.8448479771614075\n",
      "epoch: 1 step: 65, loss is 0.7293673753738403\n",
      "epoch: 1 step: 66, loss is 0.780312716960907\n",
      "epoch: 1 step: 67, loss is 0.8160970211029053\n",
      "epoch: 1 step: 68, loss is 0.8894058465957642\n",
      "epoch: 1 step: 69, loss is 0.9163976311683655\n",
      "epoch: 1 step: 70, loss is 0.7953612804412842\n",
      "epoch: 1 step: 71, loss is 0.8267161846160889\n",
      "epoch: 1 step: 72, loss is 0.8362084627151489\n",
      "epoch: 1 step: 73, loss is 0.7395513653755188\n",
      "epoch: 1 step: 74, loss is 0.7594184279441833\n",
      "epoch: 1 step: 75, loss is 0.8431093692779541\n",
      "epoch: 1 step: 76, loss is 0.6927077174186707\n",
      "epoch: 1 step: 77, loss is 0.6847561001777649\n",
      "epoch: 1 step: 78, loss is 0.8052571415901184\n",
      "epoch: 1 step: 79, loss is 0.8963964581489563\n",
      "epoch: 1 step: 80, loss is 0.8152700662612915\n",
      "epoch: 1 step: 81, loss is 0.8098754286766052\n",
      "epoch: 1 step: 82, loss is 0.7202832102775574\n",
      "epoch: 1 step: 83, loss is 0.500969409942627\n",
      "epoch: 1 step: 84, loss is 0.5474391579627991\n",
      "epoch: 1 step: 85, loss is 0.6549013257026672\n",
      "epoch: 1 step: 86, loss is 0.6359511017799377\n",
      "epoch: 1 step: 87, loss is 0.45015132427215576\n",
      "epoch: 1 step: 88, loss is 0.9898004531860352\n",
      "epoch: 1 step: 89, loss is 0.4293099045753479\n",
      "epoch: 1 step: 90, loss is 0.8683027029037476\n",
      "epoch: 1 step: 91, loss is 0.6755622625350952\n",
      "epoch: 1 step: 92, loss is 0.7022094130516052\n",
      "epoch: 1 step: 93, loss is 0.6206843852996826\n",
      "epoch: 1 step: 94, loss is 0.6488356590270996\n",
      "epoch: 1 step: 95, loss is 0.845050573348999\n",
      "epoch: 1 step: 96, loss is 0.5490736961364746\n",
      "epoch: 1 step: 97, loss is 1.0711506605148315\n",
      "epoch: 1 step: 98, loss is 0.8670296669006348\n",
      "epoch: 1 step: 99, loss is 0.7444801926612854\n",
      "epoch: 1 step: 100, loss is 0.6482188701629639\n",
      "epoch: 1 step: 101, loss is 0.690355122089386\n",
      "epoch: 1 step: 102, loss is 0.7036664485931396\n",
      "epoch: 1 step: 103, loss is 0.7156740427017212\n",
      "epoch: 1 step: 104, loss is 0.512933611869812\n",
      "epoch: 1 step: 105, loss is 0.550476610660553\n",
      "epoch: 1 step: 106, loss is 0.654097318649292\n",
      "epoch: 1 step: 107, loss is 0.910102367401123\n",
      "epoch: 1 step: 108, loss is 0.5562306046485901\n",
      "epoch: 1 step: 109, loss is 0.8051646947860718\n",
      "epoch: 1 step: 110, loss is 0.7307427525520325\n",
      "epoch: 1 step: 111, loss is 0.6138331890106201\n",
      "epoch: 1 step: 112, loss is 0.5662091374397278\n",
      "epoch: 1 step: 113, loss is 0.5963853001594543\n",
      "epoch: 1 step: 114, loss is 0.5742868185043335\n",
      "epoch: 1 step: 115, loss is 0.6904288530349731\n",
      "epoch: 1 step: 116, loss is 0.555152952671051\n",
      "epoch: 1 step: 117, loss is 0.6291969418525696\n",
      "epoch: 1 step: 118, loss is 0.6027982234954834\n",
      "epoch: 1 step: 119, loss is 0.6192499995231628\n",
      "epoch: 1 step: 120, loss is 0.4977811276912689\n",
      "epoch: 1 step: 121, loss is 0.8757199048995972\n",
      "epoch: 1 step: 122, loss is 0.6451374888420105\n",
      "epoch: 1 step: 123, loss is 0.6481490135192871\n",
      "epoch: 1 step: 124, loss is 0.7171726822853088\n",
      "epoch: 1 step: 125, loss is 0.5376806855201721\n",
      "epoch: 1 step: 126, loss is 0.7533275485038757\n",
      "epoch: 1 step: 127, loss is 0.5398966073989868\n",
      "epoch: 1 step: 128, loss is 0.6872909665107727\n",
      "epoch: 1 step: 129, loss is 0.7538097500801086\n",
      "epoch: 1 step: 130, loss is 0.5264782905578613\n",
      "epoch: 1 step: 131, loss is 0.4374634921550751\n",
      "epoch: 1 step: 132, loss is 0.6167879104614258\n",
      "epoch: 1 step: 133, loss is 0.651531457901001\n",
      "epoch: 1 step: 134, loss is 0.611102283000946\n",
      "epoch: 1 step: 135, loss is 0.4972746670246124\n",
      "epoch: 1 step: 136, loss is 0.501581072807312\n",
      "epoch: 1 step: 137, loss is 0.5560013055801392\n",
      "epoch: 1 step: 138, loss is 0.7240096926689148\n",
      "epoch: 1 step: 139, loss is 0.6578879952430725\n",
      "epoch: 1 step: 140, loss is 0.44710591435432434\n",
      "epoch: 1 step: 141, loss is 0.4852677881717682\n",
      "epoch: 1 step: 142, loss is 0.488805890083313\n",
      "epoch: 1 step: 143, loss is 0.46751919388771057\n",
      "epoch: 1 step: 144, loss is 0.48732632398605347\n",
      "epoch: 1 step: 145, loss is 0.5941771268844604\n",
      "epoch: 1 step: 146, loss is 0.7309825420379639\n",
      "epoch: 1 step: 147, loss is 0.6497096419334412\n",
      "epoch: 1 step: 148, loss is 0.6499898433685303\n",
      "epoch: 1 step: 149, loss is 0.6623613834381104\n",
      "epoch: 1 step: 150, loss is 0.5117298364639282\n",
      "epoch: 1 step: 151, loss is 0.39562293887138367\n",
      "epoch: 1 step: 152, loss is 0.7151076793670654\n",
      "epoch: 1 step: 153, loss is 0.7027890682220459\n",
      "epoch: 1 step: 154, loss is 0.47134536504745483\n",
      "epoch: 1 step: 155, loss is 0.5881654620170593\n",
      "epoch: 1 step: 156, loss is 0.5807442665100098\n",
      "epoch: 1 step: 157, loss is 0.47547754645347595\n",
      "epoch: 1 step: 158, loss is 0.4711707830429077\n",
      "epoch: 1 step: 159, loss is 0.6165226101875305\n",
      "epoch: 1 step: 160, loss is 0.4299984276294708\n",
      "epoch: 1 step: 161, loss is 0.5188532471656799\n",
      "epoch: 1 step: 162, loss is 0.6468402147293091\n",
      "epoch: 1 step: 163, loss is 0.6511651277542114\n",
      "epoch: 1 step: 164, loss is 0.5460737347602844\n",
      "epoch: 1 step: 165, loss is 0.43818074464797974\n",
      "epoch: 1 step: 166, loss is 0.34143757820129395\n",
      "epoch: 1 step: 167, loss is 0.6630604267120361\n",
      "epoch: 1 step: 168, loss is 0.6338624358177185\n",
      "epoch: 1 step: 169, loss is 0.6535175442695618\n",
      "epoch: 1 step: 170, loss is 0.5216202139854431\n",
      "epoch: 1 step: 171, loss is 0.627079963684082\n",
      "epoch: 1 step: 172, loss is 0.6172951459884644\n",
      "epoch: 1 step: 173, loss is 0.5216444730758667\n",
      "epoch: 1 step: 174, loss is 0.41967588663101196\n",
      "epoch: 1 step: 175, loss is 0.6538966298103333\n",
      "epoch: 1 step: 176, loss is 0.7549840211868286\n",
      "epoch: 1 step: 177, loss is 0.646342396736145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 178, loss is 0.6504468321800232\n",
      "epoch: 1 step: 179, loss is 0.5678136348724365\n",
      "epoch: 1 step: 180, loss is 0.7147209644317627\n",
      "epoch: 1 step: 181, loss is 0.4228847622871399\n",
      "epoch: 1 step: 182, loss is 0.42461490631103516\n",
      "epoch: 1 step: 183, loss is 0.41137996315956116\n",
      "epoch: 1 step: 184, loss is 0.47829413414001465\n",
      "epoch: 1 step: 185, loss is 0.5943102240562439\n",
      "epoch: 1 step: 186, loss is 0.6106183528900146\n",
      "epoch: 1 step: 187, loss is 0.5374863147735596\n",
      "epoch: 1 step: 188, loss is 0.6089520454406738\n",
      "epoch: 1 step: 189, loss is 0.3814203441143036\n",
      "epoch: 1 step: 190, loss is 0.5321703553199768\n",
      "epoch: 1 step: 191, loss is 0.5317269563674927\n",
      "epoch: 1 step: 192, loss is 0.43546390533447266\n",
      "epoch: 1 step: 193, loss is 0.42203494906425476\n",
      "epoch: 1 step: 194, loss is 0.5228715538978577\n",
      "epoch: 1 step: 195, loss is 0.5024772882461548\n",
      "epoch: 1 step: 196, loss is 0.5614913105964661\n",
      "epoch: 1 step: 197, loss is 0.5252940654754639\n",
      "epoch: 1 step: 198, loss is 0.6673060655593872\n",
      "epoch: 1 step: 199, loss is 0.6327089071273804\n",
      "epoch: 1 step: 200, loss is 0.4486248195171356\n",
      "epoch: 1 step: 201, loss is 0.41485488414764404\n",
      "epoch: 1 step: 202, loss is 0.4253555238246918\n",
      "epoch: 1 step: 203, loss is 0.3641227185726166\n",
      "epoch: 1 step: 204, loss is 0.7662346959114075\n",
      "epoch: 1 step: 205, loss is 0.4825446605682373\n",
      "epoch: 1 step: 206, loss is 0.459827721118927\n",
      "epoch: 1 step: 207, loss is 0.5490040183067322\n",
      "epoch: 1 step: 208, loss is 0.5562967658042908\n",
      "epoch: 1 step: 209, loss is 0.5496628880500793\n",
      "epoch: 1 step: 210, loss is 0.687230110168457\n",
      "epoch: 1 step: 211, loss is 0.5467705130577087\n",
      "epoch: 1 step: 212, loss is 0.6193888187408447\n",
      "epoch: 1 step: 213, loss is 0.40557146072387695\n",
      "epoch: 1 step: 214, loss is 0.6109316945075989\n",
      "epoch: 1 step: 215, loss is 0.34922245144844055\n",
      "epoch: 1 step: 216, loss is 0.5444924235343933\n",
      "epoch: 1 step: 217, loss is 0.6391746997833252\n",
      "epoch: 1 step: 218, loss is 0.5991758704185486\n",
      "epoch: 1 step: 219, loss is 0.5114772319793701\n",
      "epoch: 1 step: 220, loss is 0.6245723962783813\n",
      "epoch: 1 step: 221, loss is 0.4556390643119812\n",
      "epoch: 1 step: 222, loss is 0.5443823337554932\n",
      "epoch: 1 step: 223, loss is 0.4227028489112854\n",
      "epoch: 1 step: 224, loss is 0.5234771370887756\n",
      "epoch: 1 step: 225, loss is 0.45174843072891235\n",
      "epoch: 1 step: 226, loss is 0.5342414379119873\n",
      "epoch: 1 step: 227, loss is 0.5545701384544373\n",
      "epoch: 1 step: 228, loss is 0.5130397081375122\n",
      "epoch: 1 step: 229, loss is 0.6152212023735046\n",
      "epoch: 1 step: 230, loss is 0.5204340219497681\n",
      "epoch: 1 step: 231, loss is 0.544276237487793\n",
      "epoch: 1 step: 232, loss is 0.4850115478038788\n",
      "epoch: 1 step: 233, loss is 0.703265905380249\n",
      "epoch: 1 step: 234, loss is 0.4325060546398163\n",
      "epoch: 1 step: 235, loss is 0.5235550999641418\n",
      "epoch: 1 step: 236, loss is 0.6296917796134949\n",
      "epoch: 1 step: 237, loss is 0.7168610095977783\n",
      "epoch: 1 step: 238, loss is 0.5552207827568054\n",
      "epoch: 1 step: 239, loss is 0.6376208662986755\n",
      "epoch: 1 step: 240, loss is 0.5385217070579529\n",
      "epoch: 1 step: 241, loss is 0.41069912910461426\n",
      "epoch: 1 step: 242, loss is 0.30639156699180603\n",
      "epoch: 1 step: 243, loss is 0.3352574110031128\n",
      "epoch: 1 step: 244, loss is 0.484880656003952\n",
      "epoch: 1 step: 245, loss is 0.5915942192077637\n",
      "epoch: 1 step: 246, loss is 0.5000230073928833\n",
      "epoch: 1 step: 247, loss is 0.473339706659317\n",
      "epoch: 1 step: 248, loss is 0.45784029364585876\n",
      "epoch: 1 step: 249, loss is 0.5329545736312866\n",
      "epoch: 1 step: 250, loss is 0.443811297416687\n",
      "epoch: 1 step: 251, loss is 0.4791909456253052\n",
      "epoch: 1 step: 252, loss is 0.6681576371192932\n",
      "epoch: 1 step: 253, loss is 0.6360353827476501\n",
      "epoch: 1 step: 254, loss is 0.48277992010116577\n",
      "epoch: 1 step: 255, loss is 0.43840959668159485\n",
      "epoch: 1 step: 256, loss is 0.37645816802978516\n",
      "epoch: 1 step: 257, loss is 0.5129874348640442\n",
      "epoch: 1 step: 258, loss is 0.42213180661201477\n",
      "epoch: 1 step: 259, loss is 0.4310302734375\n",
      "epoch: 1 step: 260, loss is 0.65362948179245\n",
      "epoch: 1 step: 261, loss is 0.5685787796974182\n",
      "epoch: 1 step: 262, loss is 0.7042068839073181\n",
      "epoch: 1 step: 263, loss is 0.5144881010055542\n",
      "epoch: 1 step: 264, loss is 0.41016116738319397\n",
      "epoch: 1 step: 265, loss is 0.4109499156475067\n",
      "epoch: 1 step: 266, loss is 0.7509739995002747\n",
      "epoch: 1 step: 267, loss is 0.6338248252868652\n",
      "epoch: 1 step: 268, loss is 0.4724012315273285\n",
      "epoch: 1 step: 269, loss is 0.38961508870124817\n",
      "epoch: 1 step: 270, loss is 0.41093331575393677\n",
      "epoch: 1 step: 271, loss is 0.5442952513694763\n",
      "epoch: 1 step: 272, loss is 0.3347533643245697\n",
      "epoch: 1 step: 273, loss is 0.5123484134674072\n",
      "epoch: 1 step: 274, loss is 0.3752877116203308\n",
      "epoch: 1 step: 275, loss is 0.5137307643890381\n",
      "epoch: 1 step: 276, loss is 0.4121600091457367\n",
      "epoch: 1 step: 277, loss is 0.6162849068641663\n",
      "epoch: 1 step: 278, loss is 0.46806493401527405\n",
      "epoch: 1 step: 279, loss is 0.5955051183700562\n",
      "epoch: 1 step: 280, loss is 0.3946053981781006\n",
      "epoch: 1 step: 281, loss is 0.41971564292907715\n",
      "epoch: 1 step: 282, loss is 0.5549644231796265\n",
      "epoch: 1 step: 283, loss is 0.5345851182937622\n",
      "epoch: 1 step: 284, loss is 0.3605279326438904\n",
      "epoch: 1 step: 285, loss is 0.4914422929286957\n",
      "epoch: 1 step: 286, loss is 0.6786047220230103\n",
      "epoch: 1 step: 287, loss is 0.3759143650531769\n",
      "epoch: 1 step: 288, loss is 0.4814183712005615\n",
      "epoch: 1 step: 289, loss is 0.52055823802948\n",
      "epoch: 1 step: 290, loss is 0.44765400886535645\n",
      "epoch: 1 step: 291, loss is 0.7816460132598877\n",
      "epoch: 1 step: 292, loss is 0.46820971369743347\n",
      "epoch: 1 step: 293, loss is 0.6401058435440063\n",
      "epoch: 1 step: 294, loss is 0.5908283591270447\n",
      "epoch: 1 step: 295, loss is 0.3762003183364868\n",
      "epoch: 1 step: 296, loss is 0.5259845852851868\n",
      "epoch: 1 step: 297, loss is 0.470806360244751\n",
      "epoch: 1 step: 298, loss is 0.5278742909431458\n",
      "epoch: 1 step: 299, loss is 0.5294404029846191\n",
      "epoch: 1 step: 300, loss is 0.4343779385089874\n",
      "epoch: 1 step: 301, loss is 0.543450653553009\n",
      "epoch: 1 step: 302, loss is 0.36598238348960876\n",
      "epoch: 1 step: 303, loss is 0.41834014654159546\n",
      "epoch: 1 step: 304, loss is 0.3348065912723541\n",
      "epoch: 1 step: 305, loss is 0.5796058773994446\n",
      "epoch: 1 step: 306, loss is 0.596505880355835\n",
      "epoch: 1 step: 307, loss is 0.40228715538978577\n",
      "epoch: 1 step: 308, loss is 0.29081016778945923\n",
      "epoch: 1 step: 309, loss is 0.5643078684806824\n",
      "epoch: 1 step: 310, loss is 0.40371766686439514\n",
      "epoch: 1 step: 311, loss is 0.3874325454235077\n",
      "epoch: 1 step: 312, loss is 0.3735927641391754\n",
      "epoch: 1 step: 313, loss is 0.45212090015411377\n",
      "epoch: 1 step: 314, loss is 0.47934481501579285\n",
      "epoch: 1 step: 315, loss is 0.3894156515598297\n",
      "epoch: 1 step: 316, loss is 0.6043891906738281\n",
      "epoch: 1 step: 317, loss is 0.652258038520813\n",
      "epoch: 1 step: 318, loss is 0.6930999159812927\n",
      "epoch: 1 step: 319, loss is 0.4158281981945038\n",
      "epoch: 1 step: 320, loss is 0.326964795589447\n",
      "epoch: 1 step: 321, loss is 0.49578946828842163\n",
      "epoch: 1 step: 322, loss is 0.4622030258178711\n",
      "epoch: 1 step: 323, loss is 0.49651187658309937\n",
      "epoch: 1 step: 324, loss is 0.532004714012146\n",
      "epoch: 1 step: 325, loss is 0.4091303050518036\n",
      "epoch: 1 step: 326, loss is 0.4686722159385681\n",
      "epoch: 1 step: 327, loss is 0.41807761788368225\n",
      "epoch: 1 step: 328, loss is 0.4809704124927521\n",
      "epoch: 1 step: 329, loss is 0.4994800090789795\n",
      "epoch: 1 step: 330, loss is 0.5405473113059998\n",
      "epoch: 1 step: 331, loss is 0.535908043384552\n",
      "epoch: 1 step: 332, loss is 0.6169275045394897\n",
      "epoch: 1 step: 333, loss is 0.49385637044906616\n",
      "epoch: 1 step: 334, loss is 0.4427056312561035\n",
      "epoch: 1 step: 335, loss is 0.400383323431015\n",
      "epoch: 1 step: 336, loss is 0.3738436698913574\n",
      "epoch: 1 step: 337, loss is 0.49753791093826294\n",
      "epoch: 1 step: 338, loss is 0.422117680311203\n",
      "epoch: 1 step: 339, loss is 0.5566604137420654\n",
      "epoch: 1 step: 340, loss is 0.21567150950431824\n",
      "epoch: 1 step: 341, loss is 0.6404657959938049\n",
      "epoch: 1 step: 342, loss is 0.452206015586853\n",
      "epoch: 1 step: 343, loss is 0.402483195066452\n",
      "epoch: 1 step: 344, loss is 0.3642381429672241\n",
      "epoch: 1 step: 345, loss is 0.6335363984107971\n",
      "epoch: 1 step: 346, loss is 0.657576858997345\n",
      "epoch: 1 step: 347, loss is 0.4136543273925781\n",
      "epoch: 1 step: 348, loss is 0.4007406234741211\n",
      "epoch: 1 step: 349, loss is 0.5585774183273315\n",
      "epoch: 1 step: 350, loss is 0.5264145135879517\n",
      "epoch: 1 step: 351, loss is 0.3823240399360657\n",
      "epoch: 1 step: 352, loss is 0.4203313887119293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 353, loss is 0.39268094301223755\n",
      "epoch: 1 step: 354, loss is 0.5106400847434998\n",
      "epoch: 1 step: 355, loss is 0.29143136739730835\n",
      "epoch: 1 step: 356, loss is 0.2821182906627655\n",
      "epoch: 1 step: 357, loss is 0.4611077308654785\n",
      "epoch: 1 step: 358, loss is 0.3985447883605957\n",
      "epoch: 1 step: 359, loss is 0.5609055161476135\n",
      "epoch: 1 step: 360, loss is 0.6536924839019775\n",
      "epoch: 1 step: 361, loss is 0.5161142349243164\n",
      "epoch: 1 step: 362, loss is 0.7639474868774414\n",
      "epoch: 1 step: 363, loss is 0.5071642994880676\n",
      "epoch: 1 step: 364, loss is 0.3514431118965149\n",
      "epoch: 1 step: 365, loss is 0.46515437960624695\n",
      "epoch: 1 step: 366, loss is 0.5030009746551514\n",
      "epoch: 1 step: 367, loss is 0.5969384908676147\n",
      "epoch: 1 step: 368, loss is 0.8089406490325928\n",
      "epoch: 1 step: 369, loss is 0.6737209558486938\n",
      "epoch: 1 step: 370, loss is 0.4700644314289093\n",
      "epoch: 1 step: 371, loss is 0.6757851243019104\n",
      "epoch: 1 step: 372, loss is 0.5688777565956116\n",
      "epoch: 1 step: 373, loss is 0.38097432255744934\n",
      "epoch: 1 step: 374, loss is 0.5836157202720642\n",
      "epoch: 1 step: 375, loss is 0.401803582906723\n",
      "epoch: 1 step: 376, loss is 0.40583670139312744\n",
      "epoch: 1 step: 377, loss is 0.6034539937973022\n",
      "epoch: 1 step: 378, loss is 0.33512505888938904\n",
      "epoch: 1 step: 379, loss is 0.556311309337616\n",
      "epoch: 1 step: 380, loss is 0.22424861788749695\n",
      "epoch: 1 step: 381, loss is 0.4387213885784149\n",
      "epoch: 1 step: 382, loss is 0.46105334162712097\n",
      "epoch: 1 step: 383, loss is 0.6828359961509705\n",
      "epoch: 1 step: 384, loss is 0.6185625791549683\n",
      "epoch: 1 step: 385, loss is 0.7342898845672607\n",
      "epoch: 1 step: 386, loss is 0.430738240480423\n",
      "epoch: 1 step: 387, loss is 0.3811984658241272\n",
      "epoch: 1 step: 388, loss is 0.35959747433662415\n",
      "epoch: 1 step: 389, loss is 0.5431072115898132\n",
      "epoch: 1 step: 390, loss is 0.46890079975128174\n",
      "epoch: 1 step: 391, loss is 0.38952893018722534\n",
      "epoch: 1 step: 392, loss is 0.6091011166572571\n",
      "epoch: 1 step: 393, loss is 0.42963963747024536\n",
      "epoch: 1 step: 394, loss is 0.6609238982200623\n",
      "epoch: 1 step: 395, loss is 0.4469374120235443\n",
      "epoch: 1 step: 396, loss is 0.2920043468475342\n",
      "epoch: 1 step: 397, loss is 0.2507561147212982\n",
      "epoch: 1 step: 398, loss is 0.4100603461265564\n",
      "epoch: 1 step: 399, loss is 0.38645753264427185\n",
      "epoch: 1 step: 400, loss is 0.5964602828025818\n",
      "epoch: 1 step: 401, loss is 0.5007029175758362\n",
      "epoch: 1 step: 402, loss is 0.35668832063674927\n",
      "epoch: 1 step: 403, loss is 0.42293936014175415\n",
      "epoch: 1 step: 404, loss is 0.3840106129646301\n",
      "epoch: 1 step: 405, loss is 0.38541746139526367\n",
      "epoch: 1 step: 406, loss is 0.5391363501548767\n",
      "epoch: 1 step: 407, loss is 0.4481646418571472\n",
      "epoch: 1 step: 408, loss is 0.562688410282135\n",
      "epoch: 1 step: 409, loss is 0.3594915270805359\n",
      "epoch: 1 step: 410, loss is 0.5150942206382751\n",
      "epoch: 1 step: 411, loss is 0.5901246070861816\n",
      "epoch: 1 step: 412, loss is 0.45733824372291565\n",
      "epoch: 1 step: 413, loss is 0.3998771011829376\n",
      "epoch: 1 step: 414, loss is 0.45719656348228455\n",
      "epoch: 1 step: 415, loss is 0.40208491683006287\n",
      "epoch: 1 step: 416, loss is 0.4208920896053314\n",
      "epoch: 1 step: 417, loss is 0.3857492208480835\n",
      "epoch: 1 step: 418, loss is 0.4889447093009949\n",
      "epoch: 1 step: 419, loss is 0.4600040912628174\n",
      "epoch: 1 step: 420, loss is 0.6079121232032776\n",
      "epoch: 1 step: 421, loss is 0.3621148467063904\n",
      "epoch: 1 step: 422, loss is 0.34963300824165344\n",
      "epoch: 1 step: 423, loss is 0.517808198928833\n",
      "epoch: 1 step: 424, loss is 0.47272247076034546\n",
      "epoch: 1 step: 425, loss is 0.3736906051635742\n",
      "epoch: 1 step: 426, loss is 0.3713492155075073\n",
      "epoch: 1 step: 427, loss is 0.5684149861335754\n",
      "epoch: 1 step: 428, loss is 0.2884002923965454\n",
      "epoch: 1 step: 429, loss is 0.4361129403114319\n",
      "epoch: 1 step: 430, loss is 0.29709503054618835\n",
      "epoch: 1 step: 431, loss is 0.505581796169281\n",
      "epoch: 1 step: 432, loss is 0.4179346263408661\n",
      "epoch: 1 step: 433, loss is 0.4582797884941101\n",
      "epoch: 1 step: 434, loss is 0.5950509309768677\n",
      "epoch: 1 step: 435, loss is 0.43158289790153503\n",
      "epoch: 1 step: 436, loss is 0.5845913290977478\n",
      "epoch: 1 step: 437, loss is 0.3751109540462494\n",
      "epoch: 1 step: 438, loss is 0.391439288854599\n",
      "epoch: 1 step: 439, loss is 0.3678399622440338\n",
      "epoch: 1 step: 440, loss is 0.34865468740463257\n",
      "epoch: 1 step: 441, loss is 0.49486804008483887\n",
      "epoch: 1 step: 442, loss is 0.4273049533367157\n",
      "epoch: 1 step: 443, loss is 0.3942548334598541\n",
      "epoch: 1 step: 444, loss is 0.5401838421821594\n",
      "epoch: 1 step: 445, loss is 0.6261799931526184\n",
      "epoch: 1 step: 446, loss is 0.3051189184188843\n",
      "epoch: 1 step: 447, loss is 0.4933726191520691\n",
      "epoch: 1 step: 448, loss is 0.48511749505996704\n",
      "epoch: 1 step: 449, loss is 0.5617091059684753\n",
      "epoch: 1 step: 450, loss is 0.597088098526001\n",
      "epoch: 1 step: 451, loss is 0.6201690435409546\n",
      "epoch: 1 step: 452, loss is 0.49430957436561584\n",
      "epoch: 1 step: 453, loss is 0.4209830164909363\n",
      "epoch: 1 step: 454, loss is 0.45343294739723206\n",
      "epoch: 1 step: 455, loss is 0.5071907639503479\n",
      "epoch: 1 step: 456, loss is 0.45837339758872986\n",
      "epoch: 1 step: 457, loss is 0.49045422673225403\n",
      "epoch: 1 step: 458, loss is 0.5060617327690125\n",
      "epoch: 1 step: 459, loss is 0.494417279958725\n",
      "epoch: 1 step: 460, loss is 0.5358914136886597\n",
      "epoch: 1 step: 461, loss is 0.5550507307052612\n",
      "epoch: 1 step: 462, loss is 0.6184954643249512\n",
      "epoch: 1 step: 463, loss is 0.30995020270347595\n",
      "epoch: 1 step: 464, loss is 0.35225704312324524\n",
      "epoch: 1 step: 465, loss is 0.42693886160850525\n",
      "epoch: 1 step: 466, loss is 0.2788945734500885\n",
      "epoch: 1 step: 467, loss is 0.40019339323043823\n",
      "epoch: 1 step: 468, loss is 0.5739580988883972\n",
      "epoch: 1 step: 469, loss is 0.4442026615142822\n",
      "epoch: 1 step: 470, loss is 0.6198369264602661\n",
      "epoch: 1 step: 471, loss is 0.6225613951683044\n",
      "epoch: 1 step: 472, loss is 0.3924926519393921\n",
      "epoch: 1 step: 473, loss is 0.37637093663215637\n",
      "epoch: 1 step: 474, loss is 0.28277114033699036\n",
      "epoch: 1 step: 475, loss is 0.30894672870635986\n",
      "epoch: 1 step: 476, loss is 0.502068817615509\n",
      "epoch: 1 step: 477, loss is 0.4393291771411896\n",
      "epoch: 1 step: 478, loss is 0.5413703322410583\n",
      "epoch: 1 step: 479, loss is 0.3118772804737091\n",
      "epoch: 1 step: 480, loss is 0.5058204531669617\n",
      "epoch: 1 step: 481, loss is 0.4414638876914978\n",
      "epoch: 1 step: 482, loss is 0.564030110836029\n",
      "epoch: 1 step: 483, loss is 0.42853790521621704\n",
      "epoch: 1 step: 484, loss is 0.3036803603172302\n",
      "epoch: 1 step: 485, loss is 0.42902445793151855\n",
      "epoch: 1 step: 486, loss is 0.4690275192260742\n",
      "epoch: 1 step: 487, loss is 0.309895396232605\n",
      "epoch: 1 step: 488, loss is 0.36268168687820435\n",
      "epoch: 1 step: 489, loss is 0.4541540741920471\n",
      "epoch: 1 step: 490, loss is 0.30668461322784424\n",
      "epoch: 1 step: 491, loss is 0.39988070726394653\n",
      "epoch: 1 step: 492, loss is 0.44350552558898926\n",
      "epoch: 1 step: 493, loss is 0.5101597309112549\n",
      "epoch: 1 step: 494, loss is 0.4840123951435089\n",
      "epoch: 1 step: 495, loss is 0.43264761567115784\n",
      "epoch: 1 step: 496, loss is 0.5950076580047607\n",
      "epoch: 1 step: 497, loss is 0.3049808740615845\n",
      "epoch: 1 step: 498, loss is 0.38404807448387146\n",
      "epoch: 1 step: 499, loss is 0.42930349707603455\n",
      "epoch: 1 step: 500, loss is 0.3742792010307312\n",
      "epoch: 1 step: 501, loss is 0.3535086214542389\n",
      "epoch: 1 step: 502, loss is 0.4944153428077698\n",
      "epoch: 1 step: 503, loss is 0.49663153290748596\n",
      "epoch: 1 step: 504, loss is 0.3564077615737915\n",
      "epoch: 1 step: 505, loss is 0.806264340877533\n",
      "epoch: 1 step: 506, loss is 0.48234981298446655\n",
      "epoch: 1 step: 507, loss is 0.4005911350250244\n",
      "epoch: 1 step: 508, loss is 0.3365824222564697\n",
      "epoch: 1 step: 509, loss is 0.36167627573013306\n",
      "epoch: 1 step: 510, loss is 0.6519960761070251\n",
      "epoch: 1 step: 511, loss is 0.5619770288467407\n",
      "epoch: 1 step: 512, loss is 0.3584532141685486\n",
      "epoch: 1 step: 513, loss is 0.3050234615802765\n",
      "epoch: 1 step: 514, loss is 0.430939644575119\n",
      "epoch: 1 step: 515, loss is 0.29834628105163574\n",
      "epoch: 1 step: 516, loss is 0.4771449565887451\n",
      "epoch: 1 step: 517, loss is 0.39915165305137634\n",
      "epoch: 1 step: 518, loss is 0.37422969937324524\n",
      "epoch: 1 step: 519, loss is 0.5262020826339722\n",
      "epoch: 1 step: 520, loss is 0.21474020183086395\n",
      "epoch: 1 step: 521, loss is 0.27784985303878784\n",
      "epoch: 1 step: 522, loss is 0.4786064326763153\n",
      "epoch: 1 step: 523, loss is 0.45011064410209656\n",
      "epoch: 1 step: 524, loss is 0.35267373919487\n",
      "epoch: 1 step: 525, loss is 0.29811978340148926\n",
      "epoch: 1 step: 526, loss is 0.257465660572052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 527, loss is 0.4355168342590332\n",
      "epoch: 1 step: 528, loss is 0.2725103199481964\n",
      "epoch: 1 step: 529, loss is 0.5031691193580627\n",
      "epoch: 1 step: 530, loss is 0.3621505796909332\n",
      "epoch: 1 step: 531, loss is 0.6285059452056885\n",
      "epoch: 1 step: 532, loss is 0.37161022424697876\n",
      "epoch: 1 step: 533, loss is 0.3599417209625244\n",
      "epoch: 1 step: 534, loss is 0.5603554844856262\n",
      "epoch: 1 step: 535, loss is 0.29883086681365967\n",
      "epoch: 1 step: 536, loss is 0.587838351726532\n",
      "epoch: 1 step: 537, loss is 0.4753744602203369\n",
      "epoch: 1 step: 538, loss is 0.2816147804260254\n",
      "epoch: 1 step: 539, loss is 0.3785777986049652\n",
      "epoch: 1 step: 540, loss is 0.5224313139915466\n",
      "epoch: 1 step: 541, loss is 0.3897934556007385\n",
      "epoch: 1 step: 542, loss is 0.32851988077163696\n",
      "epoch: 1 step: 543, loss is 0.4575405418872833\n",
      "epoch: 1 step: 544, loss is 0.30424022674560547\n",
      "epoch: 1 step: 545, loss is 0.4272080957889557\n",
      "epoch: 1 step: 546, loss is 0.4766620397567749\n",
      "epoch: 1 step: 547, loss is 0.3726661503314972\n",
      "epoch: 1 step: 548, loss is 0.34184470772743225\n",
      "epoch: 1 step: 549, loss is 0.32243362069129944\n",
      "epoch: 1 step: 550, loss is 0.4365543723106384\n",
      "epoch: 1 step: 551, loss is 0.3528967797756195\n",
      "epoch: 1 step: 552, loss is 0.28553053736686707\n",
      "epoch: 1 step: 553, loss is 0.28112122416496277\n",
      "epoch: 1 step: 554, loss is 0.2798691391944885\n",
      "epoch: 1 step: 555, loss is 0.549172043800354\n",
      "epoch: 1 step: 556, loss is 0.291965514421463\n",
      "epoch: 1 step: 557, loss is 0.48895904421806335\n",
      "epoch: 1 step: 558, loss is 0.28744348883628845\n",
      "epoch: 1 step: 559, loss is 0.4982761740684509\n",
      "epoch: 1 step: 560, loss is 0.3409551978111267\n",
      "epoch: 1 step: 561, loss is 0.24339473247528076\n",
      "epoch: 1 step: 562, loss is 0.4030520021915436\n",
      "epoch: 1 step: 563, loss is 0.35474514961242676\n",
      "epoch: 1 step: 564, loss is 0.5111051797866821\n",
      "epoch: 1 step: 565, loss is 0.23390185832977295\n",
      "epoch: 1 step: 566, loss is 0.37361833453178406\n",
      "epoch: 1 step: 567, loss is 0.3482827842235565\n",
      "epoch: 1 step: 568, loss is 0.5232349634170532\n",
      "epoch: 1 step: 569, loss is 0.27014264464378357\n",
      "epoch: 1 step: 570, loss is 0.6659705638885498\n",
      "epoch: 1 step: 571, loss is 0.33663210272789\n",
      "epoch: 1 step: 572, loss is 0.3082822263240814\n",
      "epoch: 1 step: 573, loss is 0.47202837467193604\n",
      "epoch: 1 step: 574, loss is 0.4451988637447357\n",
      "epoch: 1 step: 575, loss is 0.2913375198841095\n",
      "epoch: 1 step: 576, loss is 0.44561731815338135\n",
      "epoch: 1 step: 577, loss is 0.6268060207366943\n",
      "epoch: 1 step: 578, loss is 0.39925873279571533\n",
      "epoch: 1 step: 579, loss is 0.339203417301178\n",
      "epoch: 1 step: 580, loss is 0.5756075978279114\n",
      "epoch: 1 step: 581, loss is 0.4944181740283966\n",
      "epoch: 1 step: 582, loss is 0.32662177085876465\n",
      "epoch: 1 step: 583, loss is 0.3213842511177063\n",
      "epoch: 1 step: 584, loss is 0.363330602645874\n",
      "epoch: 1 step: 585, loss is 0.36964064836502075\n",
      "epoch: 1 step: 586, loss is 0.5352932810783386\n",
      "epoch: 1 step: 587, loss is 0.22924859821796417\n",
      "epoch: 1 step: 588, loss is 0.3145395517349243\n",
      "epoch: 1 step: 589, loss is 0.3562392592430115\n",
      "epoch: 1 step: 590, loss is 0.3968313932418823\n",
      "epoch: 1 step: 591, loss is 0.36322832107543945\n",
      "epoch: 1 step: 592, loss is 0.5337496995925903\n",
      "epoch: 1 step: 593, loss is 0.5255656838417053\n",
      "epoch: 1 step: 594, loss is 0.33104652166366577\n",
      "epoch: 1 step: 595, loss is 0.4953470230102539\n",
      "epoch: 1 step: 596, loss is 0.29216814041137695\n",
      "epoch: 1 step: 597, loss is 0.4034656882286072\n",
      "epoch: 1 step: 598, loss is 0.3313879370689392\n",
      "epoch: 1 step: 599, loss is 0.33589091897010803\n",
      "epoch: 1 step: 600, loss is 0.4752173125743866\n",
      "epoch: 1 step: 601, loss is 0.48692986369132996\n",
      "epoch: 1 step: 602, loss is 0.4164649546146393\n",
      "epoch: 1 step: 603, loss is 0.38081443309783936\n",
      "epoch: 1 step: 604, loss is 0.23762069642543793\n",
      "epoch: 1 step: 605, loss is 0.32941389083862305\n",
      "epoch: 1 step: 606, loss is 0.42899757623672485\n",
      "epoch: 1 step: 607, loss is 0.32495200634002686\n",
      "epoch: 1 step: 608, loss is 0.36290550231933594\n",
      "epoch: 1 step: 609, loss is 0.3540157377719879\n",
      "epoch: 1 step: 610, loss is 0.34227603673934937\n",
      "epoch: 1 step: 611, loss is 0.4043675363063812\n",
      "epoch: 1 step: 612, loss is 0.36288806796073914\n",
      "epoch: 1 step: 613, loss is 0.35782864689826965\n",
      "epoch: 1 step: 614, loss is 0.14430637657642365\n",
      "epoch: 1 step: 615, loss is 0.26770105957984924\n",
      "epoch: 1 step: 616, loss is 0.3950088322162628\n",
      "epoch: 1 step: 617, loss is 0.33668941259384155\n",
      "epoch: 1 step: 618, loss is 0.40906885266304016\n",
      "epoch: 1 step: 619, loss is 0.6810956597328186\n",
      "epoch: 1 step: 620, loss is 0.29591262340545654\n",
      "epoch: 1 step: 621, loss is 0.4785592257976532\n",
      "epoch: 1 step: 622, loss is 0.25597646832466125\n",
      "epoch: 1 step: 623, loss is 0.41534504294395447\n",
      "epoch: 1 step: 624, loss is 0.2053884118795395\n",
      "epoch: 1 step: 625, loss is 0.311860591173172\n",
      "epoch: 1 step: 626, loss is 0.33484435081481934\n",
      "epoch: 1 step: 627, loss is 0.2543019950389862\n",
      "epoch: 1 step: 628, loss is 0.3835132420063019\n",
      "epoch: 1 step: 629, loss is 0.4564073085784912\n",
      "epoch: 1 step: 630, loss is 0.40086016058921814\n",
      "epoch: 1 step: 631, loss is 0.41394948959350586\n",
      "epoch: 1 step: 632, loss is 0.33959951996803284\n",
      "epoch: 1 step: 633, loss is 0.149929478764534\n",
      "epoch: 1 step: 634, loss is 0.4834473431110382\n",
      "epoch: 1 step: 635, loss is 0.31925997138023376\n",
      "epoch: 1 step: 636, loss is 0.4140487015247345\n",
      "epoch: 1 step: 637, loss is 0.4153338372707367\n",
      "epoch: 1 step: 638, loss is 0.23343881964683533\n",
      "epoch: 1 step: 639, loss is 0.3630508780479431\n",
      "epoch: 1 step: 640, loss is 0.5197443962097168\n",
      "epoch: 1 step: 641, loss is 0.3907805383205414\n",
      "epoch: 1 step: 642, loss is 0.28335195779800415\n",
      "epoch: 1 step: 643, loss is 0.47781047224998474\n",
      "epoch: 1 step: 644, loss is 0.48898881673812866\n",
      "epoch: 1 step: 645, loss is 0.3515221178531647\n",
      "epoch: 1 step: 646, loss is 0.30105137825012207\n",
      "epoch: 1 step: 647, loss is 0.6148097515106201\n",
      "epoch: 1 step: 648, loss is 0.20688463747501373\n",
      "epoch: 1 step: 649, loss is 0.3516668379306793\n",
      "epoch: 1 step: 650, loss is 0.6051985621452332\n",
      "epoch: 1 step: 651, loss is 0.5049142241477966\n",
      "epoch: 1 step: 652, loss is 0.39044973254203796\n",
      "epoch: 1 step: 653, loss is 0.3068593740463257\n",
      "epoch: 1 step: 654, loss is 0.37084275484085083\n",
      "epoch: 1 step: 655, loss is 0.27499595284461975\n",
      "epoch: 1 step: 656, loss is 0.29977744817733765\n",
      "epoch: 1 step: 657, loss is 0.43502482771873474\n",
      "epoch: 1 step: 658, loss is 0.3283270001411438\n",
      "epoch: 1 step: 659, loss is 0.20598019659519196\n",
      "epoch: 1 step: 660, loss is 0.3476470112800598\n",
      "epoch: 1 step: 661, loss is 0.3027651906013489\n",
      "epoch: 1 step: 662, loss is 0.33080512285232544\n",
      "epoch: 1 step: 663, loss is 0.2930353581905365\n",
      "epoch: 1 step: 664, loss is 0.4315944015979767\n",
      "epoch: 1 step: 665, loss is 0.6053626537322998\n",
      "epoch: 1 step: 666, loss is 0.3297242522239685\n",
      "epoch: 1 step: 667, loss is 0.2860168516635895\n",
      "epoch: 1 step: 668, loss is 0.38309112191200256\n",
      "epoch: 1 step: 669, loss is 0.4058181047439575\n",
      "epoch: 1 step: 670, loss is 0.28223109245300293\n",
      "epoch: 1 step: 671, loss is 0.6415808200836182\n",
      "epoch: 1 step: 672, loss is 0.544378399848938\n",
      "epoch: 1 step: 673, loss is 0.3862752616405487\n",
      "epoch: 1 step: 674, loss is 0.30314919352531433\n",
      "epoch: 1 step: 675, loss is 0.3506750762462616\n",
      "epoch: 1 step: 676, loss is 0.374013751745224\n",
      "epoch: 1 step: 677, loss is 0.3551674485206604\n",
      "epoch: 1 step: 678, loss is 0.4065104126930237\n",
      "epoch: 1 step: 679, loss is 0.34716829657554626\n",
      "epoch: 1 step: 680, loss is 0.31510865688323975\n",
      "epoch: 1 step: 681, loss is 0.39622268080711365\n",
      "epoch: 1 step: 682, loss is 0.3566679060459137\n",
      "epoch: 1 step: 683, loss is 0.3322030007839203\n",
      "epoch: 1 step: 684, loss is 0.397333562374115\n",
      "epoch: 1 step: 685, loss is 0.33343997597694397\n",
      "epoch: 1 step: 686, loss is 0.2326711118221283\n",
      "epoch: 1 step: 687, loss is 0.48152026534080505\n",
      "epoch: 1 step: 688, loss is 0.1835062950849533\n",
      "epoch: 1 step: 689, loss is 0.5554918050765991\n",
      "epoch: 1 step: 690, loss is 0.3355712890625\n",
      "epoch: 1 step: 691, loss is 0.15475530922412872\n",
      "epoch: 1 step: 692, loss is 0.37232375144958496\n",
      "epoch: 1 step: 693, loss is 0.17242871224880219\n",
      "epoch: 1 step: 694, loss is 0.3221467137336731\n",
      "epoch: 1 step: 695, loss is 0.5391254425048828\n",
      "epoch: 1 step: 696, loss is 0.2944301664829254\n",
      "epoch: 1 step: 697, loss is 0.3109828233718872\n",
      "epoch: 1 step: 698, loss is 0.3032732605934143\n",
      "epoch: 1 step: 699, loss is 0.2045564502477646\n",
      "epoch: 1 step: 700, loss is 0.3350091874599457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 701, loss is 0.3393130302429199\n",
      "epoch: 1 step: 702, loss is 0.4059203565120697\n",
      "epoch: 1 step: 703, loss is 0.3382461667060852\n",
      "epoch: 1 step: 704, loss is 0.28876739740371704\n",
      "epoch: 1 step: 705, loss is 0.2353939265012741\n",
      "epoch: 1 step: 706, loss is 0.4095781445503235\n",
      "epoch: 1 step: 707, loss is 0.32695844769477844\n",
      "epoch: 1 step: 708, loss is 0.3624081015586853\n",
      "epoch: 1 step: 709, loss is 0.5487909317016602\n",
      "epoch: 1 step: 710, loss is 0.548498272895813\n",
      "epoch: 1 step: 711, loss is 0.39077192544937134\n",
      "epoch: 1 step: 712, loss is 0.611285924911499\n",
      "epoch: 1 step: 713, loss is 0.33556023240089417\n",
      "epoch: 1 step: 714, loss is 0.35745275020599365\n",
      "epoch: 1 step: 715, loss is 0.3464873731136322\n",
      "epoch: 1 step: 716, loss is 0.4587341248989105\n",
      "epoch: 1 step: 717, loss is 0.5731801390647888\n",
      "epoch: 1 step: 718, loss is 0.37046438455581665\n",
      "epoch: 1 step: 719, loss is 0.4685395061969757\n",
      "epoch: 1 step: 720, loss is 0.3852262794971466\n",
      "epoch: 1 step: 721, loss is 0.33229079842567444\n",
      "epoch: 1 step: 722, loss is 0.3550322353839874\n",
      "epoch: 1 step: 723, loss is 0.3144819140434265\n",
      "epoch: 1 step: 724, loss is 0.26736682653427124\n",
      "epoch: 1 step: 725, loss is 0.27593016624450684\n",
      "epoch: 1 step: 726, loss is 0.3358190953731537\n",
      "epoch: 1 step: 727, loss is 0.6531327962875366\n",
      "epoch: 1 step: 728, loss is 0.5075452327728271\n",
      "epoch: 1 step: 729, loss is 0.3773978352546692\n",
      "epoch: 1 step: 730, loss is 0.6037247180938721\n",
      "epoch: 1 step: 731, loss is 0.3043079078197479\n",
      "epoch: 1 step: 732, loss is 0.47181037068367004\n",
      "epoch: 1 step: 733, loss is 0.3112848103046417\n",
      "epoch: 1 step: 734, loss is 0.4402538537979126\n",
      "epoch: 1 step: 735, loss is 0.34795382618904114\n",
      "epoch: 1 step: 736, loss is 0.30487215518951416\n",
      "epoch: 1 step: 737, loss is 0.3237261176109314\n",
      "epoch: 1 step: 738, loss is 0.3899650275707245\n",
      "epoch: 1 step: 739, loss is 0.4014072120189667\n",
      "epoch: 1 step: 740, loss is 0.6935982704162598\n",
      "epoch: 1 step: 741, loss is 0.3274957239627838\n",
      "epoch: 1 step: 742, loss is 0.4365658164024353\n",
      "epoch: 1 step: 743, loss is 0.4223054349422455\n",
      "epoch: 1 step: 744, loss is 0.37033894658088684\n",
      "epoch: 1 step: 745, loss is 0.33816179633140564\n",
      "epoch: 1 step: 746, loss is 0.2513010799884796\n",
      "epoch: 1 step: 747, loss is 0.28246402740478516\n",
      "epoch: 1 step: 748, loss is 0.30482059717178345\n",
      "epoch: 1 step: 749, loss is 0.24480916559696198\n",
      "epoch: 1 step: 750, loss is 0.46399322152137756\n",
      "epoch: 1 step: 751, loss is 0.37944167852401733\n",
      "epoch: 1 step: 752, loss is 0.4968544840812683\n",
      "epoch: 1 step: 753, loss is 0.49583762884140015\n",
      "epoch: 1 step: 754, loss is 0.32470589876174927\n",
      "epoch: 1 step: 755, loss is 0.3168846368789673\n",
      "epoch: 1 step: 756, loss is 0.3324601352214813\n",
      "epoch: 1 step: 757, loss is 0.289873331785202\n",
      "epoch: 1 step: 758, loss is 0.4378663897514343\n",
      "epoch: 1 step: 759, loss is 0.42063623666763306\n",
      "epoch: 1 step: 760, loss is 0.2776806354522705\n",
      "epoch: 1 step: 761, loss is 0.2342841923236847\n",
      "epoch: 1 step: 762, loss is 0.4893642067909241\n",
      "epoch: 1 step: 763, loss is 0.4869668781757355\n",
      "epoch: 1 step: 764, loss is 0.32873231172561646\n",
      "epoch: 1 step: 765, loss is 0.3772929310798645\n",
      "epoch: 1 step: 766, loss is 0.27882710099220276\n",
      "epoch: 1 step: 767, loss is 0.7715255618095398\n",
      "epoch: 1 step: 768, loss is 0.3414740562438965\n",
      "epoch: 1 step: 769, loss is 0.2971392869949341\n",
      "epoch: 1 step: 770, loss is 0.5466346740722656\n",
      "epoch: 1 step: 771, loss is 0.18621079623699188\n",
      "epoch: 1 step: 772, loss is 0.3831814229488373\n",
      "epoch: 1 step: 773, loss is 0.45437338948249817\n",
      "epoch: 1 step: 774, loss is 0.43254849314689636\n",
      "epoch: 1 step: 775, loss is 0.38811659812927246\n",
      "epoch: 1 step: 776, loss is 0.3536108434200287\n",
      "epoch: 1 step: 777, loss is 0.48306137323379517\n",
      "epoch: 1 step: 778, loss is 0.4454170763492584\n",
      "epoch: 1 step: 779, loss is 0.42757001519203186\n",
      "epoch: 1 step: 780, loss is 0.22025859355926514\n",
      "epoch: 1 step: 781, loss is 0.4851739704608917\n",
      "epoch: 1 step: 782, loss is 0.20167241990566254\n",
      "epoch: 1 step: 783, loss is 0.35666462779045105\n",
      "epoch: 1 step: 784, loss is 0.386247843503952\n",
      "epoch: 1 step: 785, loss is 0.3173248767852783\n",
      "epoch: 1 step: 786, loss is 0.3309454321861267\n",
      "epoch: 1 step: 787, loss is 0.17086012661457062\n",
      "epoch: 1 step: 788, loss is 0.49181896448135376\n",
      "epoch: 1 step: 789, loss is 0.32574811577796936\n",
      "epoch: 1 step: 790, loss is 0.3947101831436157\n",
      "epoch: 1 step: 791, loss is 0.21530750393867493\n",
      "epoch: 1 step: 792, loss is 0.3359898626804352\n",
      "epoch: 1 step: 793, loss is 0.2755037844181061\n",
      "epoch: 1 step: 794, loss is 0.28073298931121826\n",
      "epoch: 1 step: 795, loss is 0.3387119174003601\n",
      "epoch: 1 step: 796, loss is 0.42344021797180176\n",
      "epoch: 1 step: 797, loss is 0.34208178520202637\n",
      "epoch: 1 step: 798, loss is 0.17463557422161102\n",
      "epoch: 1 step: 799, loss is 0.374116986989975\n",
      "epoch: 1 step: 800, loss is 0.3188447654247284\n",
      "epoch: 1 step: 801, loss is 0.3341553807258606\n",
      "epoch: 1 step: 802, loss is 0.2821439802646637\n",
      "epoch: 1 step: 803, loss is 0.4214995503425598\n",
      "epoch: 1 step: 804, loss is 0.4607386291027069\n",
      "epoch: 1 step: 805, loss is 0.30853143334388733\n",
      "epoch: 1 step: 806, loss is 0.45616620779037476\n",
      "epoch: 1 step: 807, loss is 0.33561182022094727\n",
      "epoch: 1 step: 808, loss is 0.5304300785064697\n",
      "epoch: 1 step: 809, loss is 0.2650126516819\n",
      "epoch: 1 step: 810, loss is 0.4432041049003601\n",
      "epoch: 1 step: 811, loss is 0.32153797149658203\n",
      "epoch: 1 step: 812, loss is 0.3283569812774658\n",
      "epoch: 1 step: 813, loss is 0.19134823977947235\n",
      "epoch: 1 step: 814, loss is 0.36705681681632996\n",
      "epoch: 1 step: 815, loss is 0.3750012218952179\n",
      "epoch: 1 step: 816, loss is 0.37021246552467346\n",
      "epoch: 1 step: 817, loss is 0.3420813977718353\n",
      "epoch: 1 step: 818, loss is 0.3625158667564392\n",
      "epoch: 1 step: 819, loss is 0.3744414448738098\n",
      "epoch: 1 step: 820, loss is 0.2669908106327057\n",
      "epoch: 1 step: 821, loss is 0.461760014295578\n",
      "epoch: 1 step: 822, loss is 0.6243572235107422\n",
      "epoch: 1 step: 823, loss is 0.3711979389190674\n",
      "epoch: 1 step: 824, loss is 0.2950552701950073\n",
      "epoch: 1 step: 825, loss is 0.3243420422077179\n",
      "epoch: 1 step: 826, loss is 0.48504480719566345\n",
      "epoch: 1 step: 827, loss is 0.39489468932151794\n",
      "epoch: 1 step: 828, loss is 0.3503667116165161\n",
      "epoch: 1 step: 829, loss is 0.4395740330219269\n",
      "epoch: 1 step: 830, loss is 0.2853105664253235\n",
      "epoch: 1 step: 831, loss is 0.268283486366272\n",
      "epoch: 1 step: 832, loss is 0.2504079341888428\n",
      "epoch: 1 step: 833, loss is 0.37792283296585083\n",
      "epoch: 1 step: 834, loss is 0.40352189540863037\n",
      "epoch: 1 step: 835, loss is 0.402128130197525\n",
      "epoch: 1 step: 836, loss is 0.46682053804397583\n",
      "epoch: 1 step: 837, loss is 0.33046263456344604\n",
      "epoch: 1 step: 838, loss is 0.2874889075756073\n",
      "epoch: 1 step: 839, loss is 0.24843020737171173\n",
      "epoch: 1 step: 840, loss is 0.4072381258010864\n",
      "epoch: 1 step: 841, loss is 0.504464328289032\n",
      "epoch: 1 step: 842, loss is 0.394544780254364\n",
      "epoch: 1 step: 843, loss is 0.43488940596580505\n",
      "epoch: 1 step: 844, loss is 0.1895478069782257\n",
      "epoch: 1 step: 845, loss is 0.3370434045791626\n",
      "epoch: 1 step: 846, loss is 0.37031057476997375\n",
      "epoch: 1 step: 847, loss is 0.3060753047466278\n",
      "epoch: 1 step: 848, loss is 0.4975716471672058\n",
      "epoch: 1 step: 849, loss is 0.22190219163894653\n",
      "epoch: 1 step: 850, loss is 0.32635360956192017\n",
      "epoch: 1 step: 851, loss is 0.4674648642539978\n",
      "epoch: 1 step: 852, loss is 0.4598180055618286\n",
      "epoch: 1 step: 853, loss is 0.3397620618343353\n",
      "epoch: 1 step: 854, loss is 0.30810150504112244\n",
      "epoch: 1 step: 855, loss is 0.3570745289325714\n",
      "epoch: 1 step: 856, loss is 0.323778361082077\n",
      "epoch: 1 step: 857, loss is 0.591336190700531\n",
      "epoch: 1 step: 858, loss is 0.5064789056777954\n",
      "epoch: 1 step: 859, loss is 0.23609115183353424\n",
      "epoch: 1 step: 860, loss is 0.33377352356910706\n",
      "epoch: 1 step: 861, loss is 0.33594992756843567\n",
      "epoch: 1 step: 862, loss is 0.322573721408844\n",
      "epoch: 1 step: 863, loss is 0.24382661283016205\n",
      "epoch: 1 step: 864, loss is 0.3537510633468628\n",
      "epoch: 1 step: 865, loss is 0.4326290488243103\n",
      "epoch: 1 step: 866, loss is 0.4983650743961334\n",
      "epoch: 1 step: 867, loss is 0.4811435341835022\n",
      "epoch: 1 step: 868, loss is 0.7124098539352417\n",
      "epoch: 1 step: 869, loss is 0.19124023616313934\n",
      "epoch: 1 step: 870, loss is 0.5365781188011169\n",
      "epoch: 1 step: 871, loss is 0.4379954934120178\n",
      "epoch: 1 step: 872, loss is 0.44909751415252686\n",
      "epoch: 1 step: 873, loss is 0.3767767548561096\n",
      "epoch: 1 step: 874, loss is 0.40605396032333374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 875, loss is 0.34173545241355896\n",
      "epoch: 1 step: 876, loss is 0.20142963528633118\n",
      "epoch: 1 step: 877, loss is 0.273895263671875\n",
      "epoch: 1 step: 878, loss is 0.378560334444046\n",
      "epoch: 1 step: 879, loss is 0.4219202697277069\n",
      "epoch: 1 step: 880, loss is 0.38451337814331055\n",
      "epoch: 1 step: 881, loss is 0.3315209150314331\n",
      "epoch: 1 step: 882, loss is 0.4248208999633789\n",
      "epoch: 1 step: 883, loss is 0.17413415014743805\n",
      "epoch: 1 step: 884, loss is 0.43697139620780945\n",
      "epoch: 1 step: 885, loss is 0.32828882336616516\n",
      "epoch: 1 step: 886, loss is 0.24611517786979675\n",
      "epoch: 1 step: 887, loss is 0.3903692364692688\n",
      "epoch: 1 step: 888, loss is 0.39022260904312134\n",
      "epoch: 1 step: 889, loss is 0.498858779668808\n",
      "epoch: 1 step: 890, loss is 0.3581293523311615\n",
      "epoch: 1 step: 891, loss is 0.285839706659317\n",
      "epoch: 1 step: 892, loss is 0.22906699776649475\n",
      "epoch: 1 step: 893, loss is 0.29056087136268616\n",
      "epoch: 1 step: 894, loss is 0.30603060126304626\n",
      "epoch: 1 step: 895, loss is 0.5100064277648926\n",
      "epoch: 1 step: 896, loss is 0.49594172835350037\n",
      "epoch: 1 step: 897, loss is 0.24606797099113464\n",
      "epoch: 1 step: 898, loss is 0.3512449562549591\n",
      "epoch: 1 step: 899, loss is 0.4249165952205658\n",
      "epoch: 1 step: 900, loss is 0.26994258165359497\n",
      "epoch: 1 step: 901, loss is 0.3972910940647125\n",
      "epoch: 1 step: 902, loss is 0.7142452597618103\n",
      "epoch: 1 step: 903, loss is 0.2612350881099701\n",
      "epoch: 1 step: 904, loss is 0.3797760307788849\n",
      "epoch: 1 step: 905, loss is 0.38627365231513977\n",
      "epoch: 1 step: 906, loss is 0.34755054116249084\n",
      "epoch: 1 step: 907, loss is 0.41408470273017883\n",
      "epoch: 1 step: 908, loss is 0.4756602346897125\n",
      "epoch: 1 step: 909, loss is 0.28079020977020264\n",
      "epoch: 1 step: 910, loss is 0.32031822204589844\n",
      "epoch: 1 step: 911, loss is 0.4979042112827301\n",
      "epoch: 1 step: 912, loss is 0.23656003177165985\n",
      "epoch: 1 step: 913, loss is 0.35004594922065735\n",
      "epoch: 1 step: 914, loss is 0.2539122402667999\n",
      "epoch: 1 step: 915, loss is 0.5543991327285767\n",
      "epoch: 1 step: 916, loss is 0.3414824306964874\n",
      "epoch: 1 step: 917, loss is 0.23020900785923004\n",
      "epoch: 1 step: 918, loss is 0.32188957929611206\n",
      "epoch: 1 step: 919, loss is 0.4763117730617523\n",
      "epoch: 1 step: 920, loss is 0.6202594637870789\n",
      "epoch: 1 step: 921, loss is 0.26107412576675415\n",
      "epoch: 1 step: 922, loss is 0.27665236592292786\n",
      "epoch: 1 step: 923, loss is 0.279369980096817\n",
      "epoch: 1 step: 924, loss is 0.46002987027168274\n",
      "epoch: 1 step: 925, loss is 0.2578189969062805\n",
      "epoch: 1 step: 926, loss is 0.32747259736061096\n",
      "epoch: 1 step: 927, loss is 0.3337443470954895\n",
      "epoch: 1 step: 928, loss is 0.4258771240711212\n",
      "epoch: 1 step: 929, loss is 0.3139187693595886\n",
      "epoch: 1 step: 930, loss is 0.3797714114189148\n",
      "epoch: 1 step: 931, loss is 0.20997066795825958\n",
      "epoch: 1 step: 932, loss is 0.2636254131793976\n",
      "epoch: 1 step: 933, loss is 0.49156731367111206\n",
      "epoch: 1 step: 934, loss is 0.3720647692680359\n",
      "epoch: 1 step: 935, loss is 0.2189568430185318\n",
      "epoch: 1 step: 936, loss is 0.3129936158657074\n",
      "epoch: 1 step: 937, loss is 0.2408342957496643\n",
      "epoch: 2 step: 1, loss is 0.2700534760951996\n",
      "epoch: 2 step: 2, loss is 0.3544071614742279\n",
      "epoch: 2 step: 3, loss is 0.26099836826324463\n",
      "epoch: 2 step: 4, loss is 0.28665998578071594\n",
      "epoch: 2 step: 5, loss is 0.23274561762809753\n",
      "epoch: 2 step: 6, loss is 0.18991731107234955\n",
      "epoch: 2 step: 7, loss is 0.44879013299942017\n",
      "epoch: 2 step: 8, loss is 0.2483123540878296\n",
      "epoch: 2 step: 9, loss is 0.29310351610183716\n",
      "epoch: 2 step: 10, loss is 0.28316473960876465\n",
      "epoch: 2 step: 11, loss is 0.4601096212863922\n",
      "epoch: 2 step: 12, loss is 0.3010370433330536\n",
      "epoch: 2 step: 13, loss is 0.3971463441848755\n",
      "epoch: 2 step: 14, loss is 0.2591315805912018\n",
      "epoch: 2 step: 15, loss is 0.3701265752315521\n",
      "epoch: 2 step: 16, loss is 0.24801409244537354\n",
      "epoch: 2 step: 17, loss is 0.5137177109718323\n",
      "epoch: 2 step: 18, loss is 0.38018545508384705\n",
      "epoch: 2 step: 19, loss is 0.3658893406391144\n",
      "epoch: 2 step: 20, loss is 0.26361632347106934\n",
      "epoch: 2 step: 21, loss is 0.47979044914245605\n",
      "epoch: 2 step: 22, loss is 0.21347911655902863\n",
      "epoch: 2 step: 23, loss is 0.2847905457019806\n",
      "epoch: 2 step: 24, loss is 0.3500225245952606\n",
      "epoch: 2 step: 25, loss is 0.39243370294570923\n",
      "epoch: 2 step: 26, loss is 0.34260284900665283\n",
      "epoch: 2 step: 27, loss is 0.3876420557498932\n",
      "epoch: 2 step: 28, loss is 0.2262040376663208\n",
      "epoch: 2 step: 29, loss is 0.3802075684070587\n",
      "epoch: 2 step: 30, loss is 0.31044089794158936\n",
      "epoch: 2 step: 31, loss is 0.35274115204811096\n",
      "epoch: 2 step: 32, loss is 0.2079622596502304\n",
      "epoch: 2 step: 33, loss is 0.39286908507347107\n",
      "epoch: 2 step: 34, loss is 0.21594548225402832\n",
      "epoch: 2 step: 35, loss is 0.4149136245250702\n",
      "epoch: 2 step: 36, loss is 0.34578195214271545\n",
      "epoch: 2 step: 37, loss is 0.42228880524635315\n",
      "epoch: 2 step: 38, loss is 0.2656138837337494\n",
      "epoch: 2 step: 39, loss is 0.2814491391181946\n",
      "epoch: 2 step: 40, loss is 0.35162821412086487\n",
      "epoch: 2 step: 41, loss is 0.3211035132408142\n",
      "epoch: 2 step: 42, loss is 0.3829100728034973\n",
      "epoch: 2 step: 43, loss is 0.2066384106874466\n",
      "epoch: 2 step: 44, loss is 0.3787686824798584\n",
      "epoch: 2 step: 45, loss is 0.28362521529197693\n",
      "epoch: 2 step: 46, loss is 0.16894757747650146\n",
      "epoch: 2 step: 47, loss is 0.43637222051620483\n",
      "epoch: 2 step: 48, loss is 0.3466429114341736\n",
      "epoch: 2 step: 49, loss is 0.18832865357398987\n",
      "epoch: 2 step: 50, loss is 0.2571633458137512\n",
      "epoch: 2 step: 51, loss is 0.082926444709301\n",
      "epoch: 2 step: 52, loss is 0.31751880049705505\n",
      "epoch: 2 step: 53, loss is 0.4601680040359497\n",
      "epoch: 2 step: 54, loss is 0.2920859456062317\n",
      "epoch: 2 step: 55, loss is 0.2517305314540863\n",
      "epoch: 2 step: 56, loss is 0.3370019793510437\n",
      "epoch: 2 step: 57, loss is 0.29942798614501953\n",
      "epoch: 2 step: 58, loss is 0.5069814920425415\n",
      "epoch: 2 step: 59, loss is 0.1788831502199173\n",
      "epoch: 2 step: 60, loss is 0.3863076865673065\n",
      "epoch: 2 step: 61, loss is 0.3224204182624817\n",
      "epoch: 2 step: 62, loss is 0.4472731053829193\n",
      "epoch: 2 step: 63, loss is 0.4557867646217346\n",
      "epoch: 2 step: 64, loss is 0.4960823357105255\n",
      "epoch: 2 step: 65, loss is 0.3855880796909332\n",
      "epoch: 2 step: 66, loss is 0.3053302466869354\n",
      "epoch: 2 step: 67, loss is 0.2825813293457031\n",
      "epoch: 2 step: 68, loss is 0.26694607734680176\n",
      "epoch: 2 step: 69, loss is 0.33373314142227173\n",
      "epoch: 2 step: 70, loss is 0.3003022372722626\n",
      "epoch: 2 step: 71, loss is 0.2699985206127167\n",
      "epoch: 2 step: 72, loss is 0.2953231632709503\n",
      "epoch: 2 step: 73, loss is 0.3474452495574951\n",
      "epoch: 2 step: 74, loss is 0.38292786478996277\n",
      "epoch: 2 step: 75, loss is 0.4728693962097168\n",
      "epoch: 2 step: 76, loss is 0.3248504400253296\n",
      "epoch: 2 step: 77, loss is 0.26625749468803406\n",
      "epoch: 2 step: 78, loss is 0.3910071849822998\n",
      "epoch: 2 step: 79, loss is 0.2562910318374634\n",
      "epoch: 2 step: 80, loss is 0.20346985757350922\n",
      "epoch: 2 step: 81, loss is 0.48681512475013733\n",
      "epoch: 2 step: 82, loss is 0.33599790930747986\n",
      "epoch: 2 step: 83, loss is 0.41780826449394226\n",
      "epoch: 2 step: 84, loss is 0.3546026647090912\n",
      "epoch: 2 step: 85, loss is 0.3665909767150879\n",
      "epoch: 2 step: 86, loss is 0.4596076011657715\n",
      "epoch: 2 step: 87, loss is 0.40841010212898254\n",
      "epoch: 2 step: 88, loss is 0.27034515142440796\n",
      "epoch: 2 step: 89, loss is 0.33532628417015076\n",
      "epoch: 2 step: 90, loss is 0.34522610902786255\n",
      "epoch: 2 step: 91, loss is 0.387842059135437\n",
      "epoch: 2 step: 92, loss is 0.44469109177589417\n",
      "epoch: 2 step: 93, loss is 0.3270355463027954\n",
      "epoch: 2 step: 94, loss is 0.23910488188266754\n",
      "epoch: 2 step: 95, loss is 0.35482096672058105\n",
      "epoch: 2 step: 96, loss is 0.378260999917984\n",
      "epoch: 2 step: 97, loss is 0.4448781907558441\n",
      "epoch: 2 step: 98, loss is 0.2148880511522293\n",
      "epoch: 2 step: 99, loss is 0.2611766755580902\n",
      "epoch: 2 step: 100, loss is 0.27832287549972534\n",
      "epoch: 2 step: 101, loss is 0.13249263167381287\n",
      "epoch: 2 step: 102, loss is 0.4266662001609802\n",
      "epoch: 2 step: 103, loss is 0.38614410161972046\n",
      "epoch: 2 step: 104, loss is 0.4336557686328888\n",
      "epoch: 2 step: 105, loss is 0.3176703155040741\n",
      "epoch: 2 step: 106, loss is 0.2566196918487549\n",
      "epoch: 2 step: 107, loss is 0.28067997097969055\n",
      "epoch: 2 step: 108, loss is 0.4774661362171173\n",
      "epoch: 2 step: 109, loss is 0.29644259810447693\n",
      "epoch: 2 step: 110, loss is 0.26790833473205566\n",
      "epoch: 2 step: 111, loss is 0.23017853498458862\n",
      "epoch: 2 step: 112, loss is 0.7118818759918213\n",
      "epoch: 2 step: 113, loss is 0.24520689249038696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 114, loss is 0.30786052346229553\n",
      "epoch: 2 step: 115, loss is 0.32069939374923706\n",
      "epoch: 2 step: 116, loss is 0.4484564960002899\n",
      "epoch: 2 step: 117, loss is 0.14284548163414001\n",
      "epoch: 2 step: 118, loss is 0.39294666051864624\n",
      "epoch: 2 step: 119, loss is 0.22342100739479065\n",
      "epoch: 2 step: 120, loss is 0.3345882296562195\n",
      "epoch: 2 step: 121, loss is 0.45440787076950073\n",
      "epoch: 2 step: 122, loss is 0.3105141520500183\n",
      "epoch: 2 step: 123, loss is 0.3192940652370453\n",
      "epoch: 2 step: 124, loss is 0.33973392844200134\n",
      "epoch: 2 step: 125, loss is 0.262653648853302\n",
      "epoch: 2 step: 126, loss is 0.2197512835264206\n",
      "epoch: 2 step: 127, loss is 0.2770904004573822\n",
      "epoch: 2 step: 128, loss is 0.28698691725730896\n",
      "epoch: 2 step: 129, loss is 0.36877021193504333\n",
      "epoch: 2 step: 130, loss is 0.1418691724538803\n",
      "epoch: 2 step: 131, loss is 0.48066097497940063\n",
      "epoch: 2 step: 132, loss is 0.252928227186203\n",
      "epoch: 2 step: 133, loss is 0.19067257642745972\n",
      "epoch: 2 step: 134, loss is 0.29342105984687805\n",
      "epoch: 2 step: 135, loss is 0.26418235898017883\n",
      "epoch: 2 step: 136, loss is 0.2823199927806854\n",
      "epoch: 2 step: 137, loss is 0.2900741994380951\n",
      "epoch: 2 step: 138, loss is 0.45010536909103394\n",
      "epoch: 2 step: 139, loss is 0.4542756676673889\n",
      "epoch: 2 step: 140, loss is 0.24401532113552094\n",
      "epoch: 2 step: 141, loss is 0.13671661913394928\n",
      "epoch: 2 step: 142, loss is 0.22305315732955933\n",
      "epoch: 2 step: 143, loss is 0.2003719061613083\n",
      "epoch: 2 step: 144, loss is 0.22758404910564423\n",
      "epoch: 2 step: 145, loss is 0.2502264082431793\n",
      "epoch: 2 step: 146, loss is 0.26416581869125366\n",
      "epoch: 2 step: 147, loss is 0.3093709945678711\n",
      "epoch: 2 step: 148, loss is 0.40602409839630127\n",
      "epoch: 2 step: 149, loss is 0.32992449402809143\n",
      "epoch: 2 step: 150, loss is 0.4486320912837982\n",
      "epoch: 2 step: 151, loss is 0.1862596571445465\n",
      "epoch: 2 step: 152, loss is 0.4218577742576599\n",
      "epoch: 2 step: 153, loss is 0.30563730001449585\n",
      "epoch: 2 step: 154, loss is 0.470855176448822\n",
      "epoch: 2 step: 155, loss is 0.3492830693721771\n",
      "epoch: 2 step: 156, loss is 0.3769606649875641\n",
      "epoch: 2 step: 157, loss is 0.21456770598888397\n",
      "epoch: 2 step: 158, loss is 0.17743340134620667\n",
      "epoch: 2 step: 159, loss is 0.4147416949272156\n",
      "epoch: 2 step: 160, loss is 0.46753063797950745\n",
      "epoch: 2 step: 161, loss is 0.16806887090206146\n",
      "epoch: 2 step: 162, loss is 0.29176339507102966\n",
      "epoch: 2 step: 163, loss is 0.2454603910446167\n",
      "epoch: 2 step: 164, loss is 0.4246748089790344\n",
      "epoch: 2 step: 165, loss is 0.3715766966342926\n",
      "epoch: 2 step: 166, loss is 0.22924131155014038\n",
      "epoch: 2 step: 167, loss is 0.3040350377559662\n",
      "epoch: 2 step: 168, loss is 0.35320529341697693\n",
      "epoch: 2 step: 169, loss is 0.249826118350029\n",
      "epoch: 2 step: 170, loss is 0.2970111072063446\n",
      "epoch: 2 step: 171, loss is 0.2237861007452011\n",
      "epoch: 2 step: 172, loss is 0.35982227325439453\n",
      "epoch: 2 step: 173, loss is 0.26713860034942627\n",
      "epoch: 2 step: 174, loss is 0.2753334939479828\n",
      "epoch: 2 step: 175, loss is 0.31468307971954346\n",
      "epoch: 2 step: 176, loss is 0.19697391986846924\n",
      "epoch: 2 step: 177, loss is 0.3696434497833252\n",
      "epoch: 2 step: 178, loss is 0.3090289831161499\n",
      "epoch: 2 step: 179, loss is 0.3900112509727478\n",
      "epoch: 2 step: 180, loss is 0.3378671705722809\n",
      "epoch: 2 step: 181, loss is 0.3302232623100281\n",
      "epoch: 2 step: 182, loss is 0.29208487272262573\n",
      "epoch: 2 step: 183, loss is 0.30712148547172546\n",
      "epoch: 2 step: 184, loss is 0.4116300046443939\n",
      "epoch: 2 step: 185, loss is 0.2687925100326538\n",
      "epoch: 2 step: 186, loss is 0.3052040636539459\n",
      "epoch: 2 step: 187, loss is 0.22562488913536072\n",
      "epoch: 2 step: 188, loss is 0.2622663974761963\n",
      "epoch: 2 step: 189, loss is 0.3648189902305603\n",
      "epoch: 2 step: 190, loss is 0.38361412286758423\n",
      "epoch: 2 step: 191, loss is 0.28242698311805725\n",
      "epoch: 2 step: 192, loss is 0.36029788851737976\n",
      "epoch: 2 step: 193, loss is 0.4458116590976715\n",
      "epoch: 2 step: 194, loss is 0.2942538857460022\n",
      "epoch: 2 step: 195, loss is 0.2289460301399231\n",
      "epoch: 2 step: 196, loss is 0.33667758107185364\n",
      "epoch: 2 step: 197, loss is 0.3983704447746277\n",
      "epoch: 2 step: 198, loss is 0.23905789852142334\n",
      "epoch: 2 step: 199, loss is 0.43731436133384705\n",
      "epoch: 2 step: 200, loss is 0.20627890527248383\n",
      "epoch: 2 step: 201, loss is 0.31168103218078613\n",
      "epoch: 2 step: 202, loss is 0.25397807359695435\n",
      "epoch: 2 step: 203, loss is 0.26269927620887756\n",
      "epoch: 2 step: 204, loss is 0.3602321743965149\n",
      "epoch: 2 step: 205, loss is 0.3581787347793579\n",
      "epoch: 2 step: 206, loss is 0.318962961435318\n",
      "epoch: 2 step: 207, loss is 0.2520115077495575\n",
      "epoch: 2 step: 208, loss is 0.23405925929546356\n",
      "epoch: 2 step: 209, loss is 0.4496369957923889\n",
      "epoch: 2 step: 210, loss is 0.3470389246940613\n",
      "epoch: 2 step: 211, loss is 0.6067459583282471\n",
      "epoch: 2 step: 212, loss is 0.29414814710617065\n",
      "epoch: 2 step: 213, loss is 0.4594970941543579\n",
      "epoch: 2 step: 214, loss is 0.4149028956890106\n",
      "epoch: 2 step: 215, loss is 0.29900428652763367\n",
      "epoch: 2 step: 216, loss is 0.4607267677783966\n",
      "epoch: 2 step: 217, loss is 0.29864975810050964\n",
      "epoch: 2 step: 218, loss is 0.2722589671611786\n",
      "epoch: 2 step: 219, loss is 0.3550693094730377\n",
      "epoch: 2 step: 220, loss is 0.45172977447509766\n",
      "epoch: 2 step: 221, loss is 0.1558113694190979\n",
      "epoch: 2 step: 222, loss is 0.3537498712539673\n",
      "epoch: 2 step: 223, loss is 0.27841392159461975\n",
      "epoch: 2 step: 224, loss is 0.23074617981910706\n",
      "epoch: 2 step: 225, loss is 0.3115304708480835\n",
      "epoch: 2 step: 226, loss is 0.3579515814781189\n",
      "epoch: 2 step: 227, loss is 0.41164490580558777\n",
      "epoch: 2 step: 228, loss is 0.3782847225666046\n",
      "epoch: 2 step: 229, loss is 0.28490740060806274\n",
      "epoch: 2 step: 230, loss is 0.13090987503528595\n",
      "epoch: 2 step: 231, loss is 0.25520622730255127\n",
      "epoch: 2 step: 232, loss is 0.20854796469211578\n",
      "epoch: 2 step: 233, loss is 0.34748926758766174\n",
      "epoch: 2 step: 234, loss is 0.19511502981185913\n",
      "epoch: 2 step: 235, loss is 0.264797180891037\n",
      "epoch: 2 step: 236, loss is 0.3821733891963959\n",
      "epoch: 2 step: 237, loss is 0.39077624678611755\n",
      "epoch: 2 step: 238, loss is 0.2808639407157898\n",
      "epoch: 2 step: 239, loss is 0.2816089689731598\n",
      "epoch: 2 step: 240, loss is 0.29058414697647095\n",
      "epoch: 2 step: 241, loss is 0.2327781766653061\n",
      "epoch: 2 step: 242, loss is 0.4025491774082184\n",
      "epoch: 2 step: 243, loss is 0.2931080162525177\n",
      "epoch: 2 step: 244, loss is 0.3237449824810028\n",
      "epoch: 2 step: 245, loss is 0.3434625267982483\n",
      "epoch: 2 step: 246, loss is 0.3170377314090729\n",
      "epoch: 2 step: 247, loss is 0.21835380792617798\n",
      "epoch: 2 step: 248, loss is 0.2712530195713043\n",
      "epoch: 2 step: 249, loss is 0.23316740989685059\n",
      "epoch: 2 step: 250, loss is 0.19219505786895752\n",
      "epoch: 2 step: 251, loss is 0.1820092797279358\n",
      "epoch: 2 step: 252, loss is 0.4927871823310852\n",
      "epoch: 2 step: 253, loss is 0.35145145654678345\n",
      "epoch: 2 step: 254, loss is 0.2631371021270752\n",
      "epoch: 2 step: 255, loss is 0.35767418146133423\n",
      "epoch: 2 step: 256, loss is 0.3529491722583771\n",
      "epoch: 2 step: 257, loss is 0.17200645804405212\n",
      "epoch: 2 step: 258, loss is 0.26309341192245483\n",
      "epoch: 2 step: 259, loss is 0.24937660992145538\n",
      "epoch: 2 step: 260, loss is 0.2888546288013458\n",
      "epoch: 2 step: 261, loss is 0.429787814617157\n",
      "epoch: 2 step: 262, loss is 0.3957328200340271\n",
      "epoch: 2 step: 263, loss is 0.364164799451828\n",
      "epoch: 2 step: 264, loss is 0.2808454632759094\n",
      "epoch: 2 step: 265, loss is 0.18841896951198578\n",
      "epoch: 2 step: 266, loss is 0.259275883436203\n",
      "epoch: 2 step: 267, loss is 0.25133657455444336\n",
      "epoch: 2 step: 268, loss is 0.20832672715187073\n",
      "epoch: 2 step: 269, loss is 0.2580089271068573\n",
      "epoch: 2 step: 270, loss is 0.18317095935344696\n",
      "epoch: 2 step: 271, loss is 0.34867241978645325\n",
      "epoch: 2 step: 272, loss is 0.2866029739379883\n",
      "epoch: 2 step: 273, loss is 0.23893950879573822\n",
      "epoch: 2 step: 274, loss is 0.4189909100532532\n",
      "epoch: 2 step: 275, loss is 0.16675588488578796\n",
      "epoch: 2 step: 276, loss is 0.3932522237300873\n",
      "epoch: 2 step: 277, loss is 0.20418278872966766\n",
      "epoch: 2 step: 278, loss is 0.5811319351196289\n",
      "epoch: 2 step: 279, loss is 0.3241639733314514\n",
      "epoch: 2 step: 280, loss is 0.3002159297466278\n",
      "epoch: 2 step: 281, loss is 0.2679911255836487\n",
      "epoch: 2 step: 282, loss is 0.22970035672187805\n",
      "epoch: 2 step: 283, loss is 0.2697775065898895\n",
      "epoch: 2 step: 284, loss is 0.2947295904159546\n",
      "epoch: 2 step: 285, loss is 0.1995546817779541\n",
      "epoch: 2 step: 286, loss is 0.27479368448257446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 287, loss is 0.28891825675964355\n",
      "epoch: 2 step: 288, loss is 0.2446306049823761\n",
      "epoch: 2 step: 289, loss is 0.31870144605636597\n",
      "epoch: 2 step: 290, loss is 0.38049542903900146\n",
      "epoch: 2 step: 291, loss is 0.28861355781555176\n",
      "epoch: 2 step: 292, loss is 0.37821468710899353\n",
      "epoch: 2 step: 293, loss is 0.3455701768398285\n",
      "epoch: 2 step: 294, loss is 0.31895285844802856\n",
      "epoch: 2 step: 295, loss is 0.24744708836078644\n",
      "epoch: 2 step: 296, loss is 0.3306858539581299\n",
      "epoch: 2 step: 297, loss is 0.22401604056358337\n",
      "epoch: 2 step: 298, loss is 0.2037782073020935\n",
      "epoch: 2 step: 299, loss is 0.3832346498966217\n",
      "epoch: 2 step: 300, loss is 0.2398628145456314\n",
      "epoch: 2 step: 301, loss is 0.17701570689678192\n",
      "epoch: 2 step: 302, loss is 0.27279070019721985\n",
      "epoch: 2 step: 303, loss is 0.2849946618080139\n",
      "epoch: 2 step: 304, loss is 0.3759743571281433\n",
      "epoch: 2 step: 305, loss is 0.15881016850471497\n",
      "epoch: 2 step: 306, loss is 0.2949570417404175\n",
      "epoch: 2 step: 307, loss is 0.2954341173171997\n",
      "epoch: 2 step: 308, loss is 0.3226814568042755\n",
      "epoch: 2 step: 309, loss is 0.21068646013736725\n",
      "epoch: 2 step: 310, loss is 0.1923624575138092\n",
      "epoch: 2 step: 311, loss is 0.2006353884935379\n",
      "epoch: 2 step: 312, loss is 0.2353319674730301\n",
      "epoch: 2 step: 313, loss is 0.2654304802417755\n",
      "epoch: 2 step: 314, loss is 0.23629829287528992\n",
      "epoch: 2 step: 315, loss is 0.2905523478984833\n",
      "epoch: 2 step: 316, loss is 0.24134476482868195\n",
      "epoch: 2 step: 317, loss is 0.4247162640094757\n",
      "epoch: 2 step: 318, loss is 0.23001445829868317\n",
      "epoch: 2 step: 319, loss is 0.34015727043151855\n",
      "epoch: 2 step: 320, loss is 0.16336673498153687\n",
      "epoch: 2 step: 321, loss is 0.4160638153553009\n",
      "epoch: 2 step: 322, loss is 0.15375684201717377\n",
      "epoch: 2 step: 323, loss is 0.23657645285129547\n",
      "epoch: 2 step: 324, loss is 0.31811609864234924\n",
      "epoch: 2 step: 325, loss is 0.37635692954063416\n",
      "epoch: 2 step: 326, loss is 0.4179578423500061\n",
      "epoch: 2 step: 327, loss is 0.26031649112701416\n",
      "epoch: 2 step: 328, loss is 0.16079361736774445\n",
      "epoch: 2 step: 329, loss is 0.428740918636322\n",
      "epoch: 2 step: 330, loss is 0.40484002232551575\n",
      "epoch: 2 step: 331, loss is 0.7917041182518005\n",
      "epoch: 2 step: 332, loss is 0.2687225937843323\n",
      "epoch: 2 step: 333, loss is 0.24769292771816254\n",
      "epoch: 2 step: 334, loss is 0.4174167513847351\n",
      "epoch: 2 step: 335, loss is 0.469188392162323\n",
      "epoch: 2 step: 336, loss is 0.17925797402858734\n",
      "epoch: 2 step: 337, loss is 0.3991541266441345\n",
      "epoch: 2 step: 338, loss is 0.27121615409851074\n",
      "epoch: 2 step: 339, loss is 0.3222506046295166\n",
      "epoch: 2 step: 340, loss is 0.35850492119789124\n",
      "epoch: 2 step: 341, loss is 0.29141494631767273\n",
      "epoch: 2 step: 342, loss is 0.20935839414596558\n",
      "epoch: 2 step: 343, loss is 0.364716112613678\n",
      "epoch: 2 step: 344, loss is 0.29654842615127563\n",
      "epoch: 2 step: 345, loss is 0.3695475459098816\n",
      "epoch: 2 step: 346, loss is 0.3042159974575043\n",
      "epoch: 2 step: 347, loss is 0.3864099681377411\n",
      "epoch: 2 step: 348, loss is 0.3571767210960388\n",
      "epoch: 2 step: 349, loss is 0.16871589422225952\n",
      "epoch: 2 step: 350, loss is 0.29075443744659424\n",
      "epoch: 2 step: 351, loss is 0.34244057536125183\n",
      "epoch: 2 step: 352, loss is 0.31485408544540405\n",
      "epoch: 2 step: 353, loss is 0.3218153417110443\n",
      "epoch: 2 step: 354, loss is 0.3486250638961792\n",
      "epoch: 2 step: 355, loss is 0.25929975509643555\n",
      "epoch: 2 step: 356, loss is 0.34457504749298096\n",
      "epoch: 2 step: 357, loss is 0.3170706629753113\n",
      "epoch: 2 step: 358, loss is 0.36682844161987305\n",
      "epoch: 2 step: 359, loss is 0.420451283454895\n",
      "epoch: 2 step: 360, loss is 0.2960851192474365\n",
      "epoch: 2 step: 361, loss is 0.311955988407135\n",
      "epoch: 2 step: 362, loss is 0.3166240453720093\n",
      "epoch: 2 step: 363, loss is 0.35939905047416687\n",
      "epoch: 2 step: 364, loss is 0.17851579189300537\n",
      "epoch: 2 step: 365, loss is 0.5151704549789429\n",
      "epoch: 2 step: 366, loss is 0.3146795630455017\n",
      "epoch: 2 step: 367, loss is 0.2810279428958893\n",
      "epoch: 2 step: 368, loss is 0.3302980363368988\n",
      "epoch: 2 step: 369, loss is 0.4144227206707001\n",
      "epoch: 2 step: 370, loss is 0.18992048501968384\n",
      "epoch: 2 step: 371, loss is 0.2697928845882416\n",
      "epoch: 2 step: 372, loss is 0.3387778401374817\n",
      "epoch: 2 step: 373, loss is 0.33963483572006226\n",
      "epoch: 2 step: 374, loss is 0.2702603042125702\n",
      "epoch: 2 step: 375, loss is 0.29895228147506714\n",
      "epoch: 2 step: 376, loss is 0.19278182089328766\n",
      "epoch: 2 step: 377, loss is 0.3869110345840454\n",
      "epoch: 2 step: 378, loss is 0.25776195526123047\n",
      "epoch: 2 step: 379, loss is 0.2674359381198883\n",
      "epoch: 2 step: 380, loss is 0.3256833851337433\n",
      "epoch: 2 step: 381, loss is 0.4958844780921936\n",
      "epoch: 2 step: 382, loss is 0.21581944823265076\n",
      "epoch: 2 step: 383, loss is 0.4271829128265381\n",
      "epoch: 2 step: 384, loss is 0.15724922716617584\n",
      "epoch: 2 step: 385, loss is 0.210731640458107\n",
      "epoch: 2 step: 386, loss is 0.4067818224430084\n",
      "epoch: 2 step: 387, loss is 0.3683553636074066\n",
      "epoch: 2 step: 388, loss is 0.2915008068084717\n",
      "epoch: 2 step: 389, loss is 0.35518789291381836\n",
      "epoch: 2 step: 390, loss is 0.3986545205116272\n",
      "epoch: 2 step: 391, loss is 0.4190898537635803\n",
      "epoch: 2 step: 392, loss is 0.34464168548583984\n",
      "epoch: 2 step: 393, loss is 0.4806559383869171\n",
      "epoch: 2 step: 394, loss is 0.2656852602958679\n",
      "epoch: 2 step: 395, loss is 0.33518192172050476\n",
      "epoch: 2 step: 396, loss is 0.4149489104747772\n",
      "epoch: 2 step: 397, loss is 0.3579067289829254\n",
      "epoch: 2 step: 398, loss is 0.1551843285560608\n",
      "epoch: 2 step: 399, loss is 0.21418127417564392\n",
      "epoch: 2 step: 400, loss is 0.30147412419319153\n",
      "epoch: 2 step: 401, loss is 0.26346883177757263\n",
      "epoch: 2 step: 402, loss is 0.5064439177513123\n",
      "epoch: 2 step: 403, loss is 0.32610464096069336\n",
      "epoch: 2 step: 404, loss is 0.43053776025772095\n",
      "epoch: 2 step: 405, loss is 0.2840947210788727\n",
      "epoch: 2 step: 406, loss is 0.29794490337371826\n",
      "epoch: 2 step: 407, loss is 0.33270612359046936\n",
      "epoch: 2 step: 408, loss is 0.22961488366127014\n",
      "epoch: 2 step: 409, loss is 0.3725200295448303\n",
      "epoch: 2 step: 410, loss is 0.27721142768859863\n",
      "epoch: 2 step: 411, loss is 0.31183773279190063\n",
      "epoch: 2 step: 412, loss is 0.1104307696223259\n",
      "epoch: 2 step: 413, loss is 0.3629484474658966\n",
      "epoch: 2 step: 414, loss is 0.1880529224872589\n",
      "epoch: 2 step: 415, loss is 0.4369320571422577\n",
      "epoch: 2 step: 416, loss is 0.12835735082626343\n",
      "epoch: 2 step: 417, loss is 0.3008168935775757\n",
      "epoch: 2 step: 418, loss is 0.3759874701499939\n",
      "epoch: 2 step: 419, loss is 0.24279822409152985\n",
      "epoch: 2 step: 420, loss is 0.35793089866638184\n",
      "epoch: 2 step: 421, loss is 0.3745077848434448\n",
      "epoch: 2 step: 422, loss is 0.19891168177127838\n",
      "epoch: 2 step: 423, loss is 0.2401835024356842\n",
      "epoch: 2 step: 424, loss is 0.35773351788520813\n",
      "epoch: 2 step: 425, loss is 0.35390302538871765\n",
      "epoch: 2 step: 426, loss is 0.16303636133670807\n",
      "epoch: 2 step: 427, loss is 0.2913159728050232\n",
      "epoch: 2 step: 428, loss is 0.22432182729244232\n",
      "epoch: 2 step: 429, loss is 0.35488447546958923\n",
      "epoch: 2 step: 430, loss is 0.16068816184997559\n",
      "epoch: 2 step: 431, loss is 0.4201416075229645\n",
      "epoch: 2 step: 432, loss is 0.19209159910678864\n",
      "epoch: 2 step: 433, loss is 0.36963212490081787\n",
      "epoch: 2 step: 434, loss is 0.339724600315094\n",
      "epoch: 2 step: 435, loss is 0.4851699769496918\n",
      "epoch: 2 step: 436, loss is 0.14557340741157532\n",
      "epoch: 2 step: 437, loss is 0.28792333602905273\n",
      "epoch: 2 step: 438, loss is 0.6918737888336182\n",
      "epoch: 2 step: 439, loss is 0.3849586546421051\n",
      "epoch: 2 step: 440, loss is 0.29613080620765686\n",
      "epoch: 2 step: 441, loss is 0.4377375543117523\n",
      "epoch: 2 step: 442, loss is 0.2911287844181061\n",
      "epoch: 2 step: 443, loss is 0.28962457180023193\n",
      "epoch: 2 step: 444, loss is 0.3972302973270416\n",
      "epoch: 2 step: 445, loss is 0.5169521570205688\n",
      "epoch: 2 step: 446, loss is 0.49769100546836853\n",
      "epoch: 2 step: 447, loss is 0.3293629586696625\n",
      "epoch: 2 step: 448, loss is 0.4569169580936432\n",
      "epoch: 2 step: 449, loss is 0.3076991140842438\n",
      "epoch: 2 step: 450, loss is 0.296371191740036\n",
      "epoch: 2 step: 451, loss is 0.19568850100040436\n",
      "epoch: 2 step: 452, loss is 0.3678869307041168\n",
      "epoch: 2 step: 453, loss is 0.4707353711128235\n",
      "epoch: 2 step: 454, loss is 0.29438599944114685\n",
      "epoch: 2 step: 455, loss is 0.43662163615226746\n",
      "epoch: 2 step: 456, loss is 0.47229906916618347\n",
      "epoch: 2 step: 457, loss is 0.3198874890804291\n",
      "epoch: 2 step: 458, loss is 0.2624504566192627\n",
      "epoch: 2 step: 459, loss is 0.18541768193244934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 460, loss is 0.32317498326301575\n",
      "epoch: 2 step: 461, loss is 0.2880496084690094\n",
      "epoch: 2 step: 462, loss is 0.19588825106620789\n",
      "epoch: 2 step: 463, loss is 0.2657625377178192\n",
      "epoch: 2 step: 464, loss is 0.26696908473968506\n",
      "epoch: 2 step: 465, loss is 0.25414204597473145\n",
      "epoch: 2 step: 466, loss is 0.26861876249313354\n",
      "epoch: 2 step: 467, loss is 0.3354898989200592\n",
      "epoch: 2 step: 468, loss is 0.21759769320487976\n",
      "epoch: 2 step: 469, loss is 0.48927199840545654\n",
      "epoch: 2 step: 470, loss is 0.3913348317146301\n",
      "epoch: 2 step: 471, loss is 0.2689790427684784\n",
      "epoch: 2 step: 472, loss is 0.3739909529685974\n",
      "epoch: 2 step: 473, loss is 0.3247342109680176\n",
      "epoch: 2 step: 474, loss is 0.2759391665458679\n",
      "epoch: 2 step: 475, loss is 0.2783336937427521\n",
      "epoch: 2 step: 476, loss is 0.23696167767047882\n",
      "epoch: 2 step: 477, loss is 0.36463746428489685\n",
      "epoch: 2 step: 478, loss is 0.3528202772140503\n",
      "epoch: 2 step: 479, loss is 0.33884933590888977\n",
      "epoch: 2 step: 480, loss is 0.2799036204814911\n",
      "epoch: 2 step: 481, loss is 0.2897520363330841\n",
      "epoch: 2 step: 482, loss is 0.2553418278694153\n",
      "epoch: 2 step: 483, loss is 0.3749607503414154\n",
      "epoch: 2 step: 484, loss is 0.3383060097694397\n",
      "epoch: 2 step: 485, loss is 0.2412215918302536\n",
      "epoch: 2 step: 486, loss is 0.2804540693759918\n",
      "epoch: 2 step: 487, loss is 0.3953280746936798\n",
      "epoch: 2 step: 488, loss is 0.48608094453811646\n",
      "epoch: 2 step: 489, loss is 0.26109224557876587\n",
      "epoch: 2 step: 490, loss is 0.2657213509082794\n",
      "epoch: 2 step: 491, loss is 0.22142919898033142\n",
      "epoch: 2 step: 492, loss is 0.28465113043785095\n",
      "epoch: 2 step: 493, loss is 0.16835945844650269\n",
      "epoch: 2 step: 494, loss is 0.3038746416568756\n",
      "epoch: 2 step: 495, loss is 0.27408209443092346\n",
      "epoch: 2 step: 496, loss is 0.2094527930021286\n",
      "epoch: 2 step: 497, loss is 0.4697326719760895\n",
      "epoch: 2 step: 498, loss is 0.12346211075782776\n",
      "epoch: 2 step: 499, loss is 0.32060688734054565\n",
      "epoch: 2 step: 500, loss is 0.22433635592460632\n",
      "epoch: 2 step: 501, loss is 0.4306665360927582\n",
      "epoch: 2 step: 502, loss is 0.2828981578350067\n",
      "epoch: 2 step: 503, loss is 0.2587588429450989\n",
      "epoch: 2 step: 504, loss is 0.2734091579914093\n",
      "epoch: 2 step: 505, loss is 0.36129093170166016\n",
      "epoch: 2 step: 506, loss is 0.30676665902137756\n",
      "epoch: 2 step: 507, loss is 0.2834121286869049\n",
      "epoch: 2 step: 508, loss is 0.3329748511314392\n",
      "epoch: 2 step: 509, loss is 0.5563237071037292\n",
      "epoch: 2 step: 510, loss is 0.21411757171154022\n",
      "epoch: 2 step: 511, loss is 0.22296474874019623\n",
      "epoch: 2 step: 512, loss is 0.30592823028564453\n",
      "epoch: 2 step: 513, loss is 0.2327193021774292\n",
      "epoch: 2 step: 514, loss is 0.33172374963760376\n",
      "epoch: 2 step: 515, loss is 0.455723375082016\n",
      "epoch: 2 step: 516, loss is 0.21967683732509613\n",
      "epoch: 2 step: 517, loss is 0.26192405819892883\n",
      "epoch: 2 step: 518, loss is 0.3100801110267639\n",
      "epoch: 2 step: 519, loss is 0.37214556336402893\n",
      "epoch: 2 step: 520, loss is 0.3383256196975708\n",
      "epoch: 2 step: 521, loss is 0.5845499634742737\n",
      "epoch: 2 step: 522, loss is 0.30074602365493774\n",
      "epoch: 2 step: 523, loss is 0.17137925326824188\n",
      "epoch: 2 step: 524, loss is 0.43308040499687195\n",
      "epoch: 2 step: 525, loss is 0.29901647567749023\n",
      "epoch: 2 step: 526, loss is 0.22301501035690308\n",
      "epoch: 2 step: 527, loss is 0.2923119366168976\n",
      "epoch: 2 step: 528, loss is 0.3838438093662262\n",
      "epoch: 2 step: 529, loss is 0.2326004058122635\n",
      "epoch: 2 step: 530, loss is 0.31802377104759216\n",
      "epoch: 2 step: 531, loss is 0.3443598449230194\n",
      "epoch: 2 step: 532, loss is 0.392900675535202\n",
      "epoch: 2 step: 533, loss is 0.24543821811676025\n",
      "epoch: 2 step: 534, loss is 0.3440913259983063\n",
      "epoch: 2 step: 535, loss is 0.22876566648483276\n",
      "epoch: 2 step: 536, loss is 0.27273330092430115\n",
      "epoch: 2 step: 537, loss is 0.16780629754066467\n",
      "epoch: 2 step: 538, loss is 0.30104881525039673\n",
      "epoch: 2 step: 539, loss is 0.44483044743537903\n",
      "epoch: 2 step: 540, loss is 0.4148901402950287\n",
      "epoch: 2 step: 541, loss is 0.2537212371826172\n",
      "epoch: 2 step: 542, loss is 0.23981525003910065\n",
      "epoch: 2 step: 543, loss is 0.3969637155532837\n",
      "epoch: 2 step: 544, loss is 0.28780990839004517\n",
      "epoch: 2 step: 545, loss is 0.2866489887237549\n",
      "epoch: 2 step: 546, loss is 0.30366456508636475\n",
      "epoch: 2 step: 547, loss is 0.3249298334121704\n",
      "epoch: 2 step: 548, loss is 0.42252838611602783\n",
      "epoch: 2 step: 549, loss is 0.24555523693561554\n",
      "epoch: 2 step: 550, loss is 0.43998175859451294\n",
      "epoch: 2 step: 551, loss is 0.29604923725128174\n",
      "epoch: 2 step: 552, loss is 0.30621281266212463\n",
      "epoch: 2 step: 553, loss is 0.31689542531967163\n",
      "epoch: 2 step: 554, loss is 0.3104736804962158\n",
      "epoch: 2 step: 555, loss is 0.4659077525138855\n",
      "epoch: 2 step: 556, loss is 0.28162434697151184\n",
      "epoch: 2 step: 557, loss is 0.2968451976776123\n",
      "epoch: 2 step: 558, loss is 0.22608886659145355\n",
      "epoch: 2 step: 559, loss is 0.32274287939071655\n",
      "epoch: 2 step: 560, loss is 0.23137842118740082\n",
      "epoch: 2 step: 561, loss is 0.33633318543434143\n",
      "epoch: 2 step: 562, loss is 0.20155948400497437\n",
      "epoch: 2 step: 563, loss is 0.21528667211532593\n",
      "epoch: 2 step: 564, loss is 0.23828113079071045\n",
      "epoch: 2 step: 565, loss is 0.1748676747083664\n",
      "epoch: 2 step: 566, loss is 0.39500588178634644\n",
      "epoch: 2 step: 567, loss is 0.3616030216217041\n",
      "epoch: 2 step: 568, loss is 0.2779729962348938\n",
      "epoch: 2 step: 569, loss is 0.42683935165405273\n",
      "epoch: 2 step: 570, loss is 0.1856643408536911\n",
      "epoch: 2 step: 571, loss is 0.42477428913116455\n",
      "epoch: 2 step: 572, loss is 0.5985457897186279\n",
      "epoch: 2 step: 573, loss is 0.5264171361923218\n",
      "epoch: 2 step: 574, loss is 0.3344730734825134\n",
      "epoch: 2 step: 575, loss is 0.33393430709838867\n",
      "epoch: 2 step: 576, loss is 0.31863826513290405\n",
      "epoch: 2 step: 577, loss is 0.40134069323539734\n",
      "epoch: 2 step: 578, loss is 0.37983083724975586\n",
      "epoch: 2 step: 579, loss is 0.46847090125083923\n",
      "epoch: 2 step: 580, loss is 0.31868410110473633\n",
      "epoch: 2 step: 581, loss is 0.360965371131897\n",
      "epoch: 2 step: 582, loss is 0.21512478590011597\n",
      "epoch: 2 step: 583, loss is 0.39591482281684875\n",
      "epoch: 2 step: 584, loss is 0.19163523614406586\n",
      "epoch: 2 step: 585, loss is 0.2764260768890381\n",
      "epoch: 2 step: 586, loss is 0.4294453561306\n",
      "epoch: 2 step: 587, loss is 0.6054482460021973\n",
      "epoch: 2 step: 588, loss is 0.3244052231311798\n",
      "epoch: 2 step: 589, loss is 0.3717888295650482\n",
      "epoch: 2 step: 590, loss is 0.3033986985683441\n",
      "epoch: 2 step: 591, loss is 0.24997654557228088\n",
      "epoch: 2 step: 592, loss is 0.42857640981674194\n",
      "epoch: 2 step: 593, loss is 0.20498143136501312\n",
      "epoch: 2 step: 594, loss is 0.2487751990556717\n",
      "epoch: 2 step: 595, loss is 0.3687663972377777\n",
      "epoch: 2 step: 596, loss is 0.4423334300518036\n",
      "epoch: 2 step: 597, loss is 0.2639615535736084\n",
      "epoch: 2 step: 598, loss is 0.3581264317035675\n",
      "epoch: 2 step: 599, loss is 0.25695735216140747\n",
      "epoch: 2 step: 600, loss is 0.21247965097427368\n",
      "epoch: 2 step: 601, loss is 0.2461184561252594\n",
      "epoch: 2 step: 602, loss is 0.34841543436050415\n",
      "epoch: 2 step: 603, loss is 0.26817774772644043\n",
      "epoch: 2 step: 604, loss is 0.29655033349990845\n",
      "epoch: 2 step: 605, loss is 0.42421334981918335\n",
      "epoch: 2 step: 606, loss is 0.2656811475753784\n",
      "epoch: 2 step: 607, loss is 0.18349356949329376\n",
      "epoch: 2 step: 608, loss is 0.262288361787796\n",
      "epoch: 2 step: 609, loss is 0.21855045855045319\n",
      "epoch: 2 step: 610, loss is 0.29621216654777527\n",
      "epoch: 2 step: 611, loss is 0.33733895421028137\n",
      "epoch: 2 step: 612, loss is 0.1737075299024582\n",
      "epoch: 2 step: 613, loss is 0.5959139466285706\n",
      "epoch: 2 step: 614, loss is 0.2870308756828308\n",
      "epoch: 2 step: 615, loss is 0.18184256553649902\n",
      "epoch: 2 step: 616, loss is 0.21290279924869537\n",
      "epoch: 2 step: 617, loss is 0.24922677874565125\n",
      "epoch: 2 step: 618, loss is 0.3091112971305847\n",
      "epoch: 2 step: 619, loss is 0.6039425134658813\n",
      "epoch: 2 step: 620, loss is 0.30235543847084045\n",
      "epoch: 2 step: 621, loss is 0.2997882068157196\n",
      "epoch: 2 step: 622, loss is 0.291790246963501\n",
      "epoch: 2 step: 623, loss is 0.47128912806510925\n",
      "epoch: 2 step: 624, loss is 0.19537979364395142\n",
      "epoch: 2 step: 625, loss is 0.305611789226532\n",
      "epoch: 2 step: 626, loss is 0.40436646342277527\n",
      "epoch: 2 step: 627, loss is 0.2487446814775467\n",
      "epoch: 2 step: 628, loss is 0.28057825565338135\n",
      "epoch: 2 step: 629, loss is 0.24814492464065552\n",
      "epoch: 2 step: 630, loss is 0.2651936709880829\n",
      "epoch: 2 step: 631, loss is 0.267695814371109\n",
      "epoch: 2 step: 632, loss is 0.35037219524383545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 633, loss is 0.20487730205059052\n",
      "epoch: 2 step: 634, loss is 0.34145206212997437\n",
      "epoch: 2 step: 635, loss is 0.4403434097766876\n",
      "epoch: 2 step: 636, loss is 0.33870670199394226\n",
      "epoch: 2 step: 637, loss is 0.19096562266349792\n",
      "epoch: 2 step: 638, loss is 0.35385116934776306\n",
      "epoch: 2 step: 639, loss is 0.24828697741031647\n",
      "epoch: 2 step: 640, loss is 0.3371331989765167\n",
      "epoch: 2 step: 641, loss is 0.1767507791519165\n",
      "epoch: 2 step: 642, loss is 0.2087949961423874\n",
      "epoch: 2 step: 643, loss is 0.1879092901945114\n",
      "epoch: 2 step: 644, loss is 0.30735576152801514\n",
      "epoch: 2 step: 645, loss is 0.14013329148292542\n",
      "epoch: 2 step: 646, loss is 0.2699846625328064\n",
      "epoch: 2 step: 647, loss is 0.36216413974761963\n",
      "epoch: 2 step: 648, loss is 0.2662035822868347\n",
      "epoch: 2 step: 649, loss is 0.1255786418914795\n",
      "epoch: 2 step: 650, loss is 0.3680592179298401\n",
      "epoch: 2 step: 651, loss is 0.09415265917778015\n",
      "epoch: 2 step: 652, loss is 0.1927170753479004\n",
      "epoch: 2 step: 653, loss is 0.2354709357023239\n",
      "epoch: 2 step: 654, loss is 0.39742642641067505\n",
      "epoch: 2 step: 655, loss is 0.4122604429721832\n",
      "epoch: 2 step: 656, loss is 0.2115151435136795\n",
      "epoch: 2 step: 657, loss is 0.3035569489002228\n",
      "epoch: 2 step: 658, loss is 0.21832989156246185\n",
      "epoch: 2 step: 659, loss is 0.23953291773796082\n",
      "epoch: 2 step: 660, loss is 0.2698690891265869\n",
      "epoch: 2 step: 661, loss is 0.3067510426044464\n",
      "epoch: 2 step: 662, loss is 0.5537648797035217\n",
      "epoch: 2 step: 663, loss is 0.239825040102005\n",
      "epoch: 2 step: 664, loss is 0.3092537224292755\n",
      "epoch: 2 step: 665, loss is 0.2513001263141632\n",
      "epoch: 2 step: 666, loss is 0.2038935124874115\n",
      "epoch: 2 step: 667, loss is 0.19418524205684662\n",
      "epoch: 2 step: 668, loss is 0.3661666810512543\n",
      "epoch: 2 step: 669, loss is 0.29647526144981384\n",
      "epoch: 2 step: 670, loss is 0.29642176628112793\n",
      "epoch: 2 step: 671, loss is 0.18947437405586243\n",
      "epoch: 2 step: 672, loss is 0.36580443382263184\n",
      "epoch: 2 step: 673, loss is 0.17934629321098328\n",
      "epoch: 2 step: 674, loss is 0.25350838899612427\n",
      "epoch: 2 step: 675, loss is 0.25693729519844055\n",
      "epoch: 2 step: 676, loss is 0.3000180423259735\n",
      "epoch: 2 step: 677, loss is 0.35914742946624756\n",
      "epoch: 2 step: 678, loss is 0.371602326631546\n",
      "epoch: 2 step: 679, loss is 0.2877075672149658\n",
      "epoch: 2 step: 680, loss is 0.3657136857509613\n",
      "epoch: 2 step: 681, loss is 0.31301480531692505\n",
      "epoch: 2 step: 682, loss is 0.4158722758293152\n",
      "epoch: 2 step: 683, loss is 0.15980443358421326\n",
      "epoch: 2 step: 684, loss is 0.274796724319458\n",
      "epoch: 2 step: 685, loss is 0.17939181625843048\n",
      "epoch: 2 step: 686, loss is 0.3584969937801361\n",
      "epoch: 2 step: 687, loss is 0.2500167191028595\n",
      "epoch: 2 step: 688, loss is 0.21927984058856964\n",
      "epoch: 2 step: 689, loss is 0.3230452835559845\n",
      "epoch: 2 step: 690, loss is 0.19886088371276855\n",
      "epoch: 2 step: 691, loss is 0.22313517332077026\n",
      "epoch: 2 step: 692, loss is 0.28962257504463196\n",
      "epoch: 2 step: 693, loss is 0.3505798280239105\n",
      "epoch: 2 step: 694, loss is 0.19712086021900177\n",
      "epoch: 2 step: 695, loss is 0.22841382026672363\n",
      "epoch: 2 step: 696, loss is 0.2503780722618103\n",
      "epoch: 2 step: 697, loss is 0.34332332015037537\n",
      "epoch: 2 step: 698, loss is 0.24769283831119537\n",
      "epoch: 2 step: 699, loss is 0.3308086395263672\n",
      "epoch: 2 step: 700, loss is 0.27553534507751465\n",
      "epoch: 2 step: 701, loss is 0.2204681932926178\n",
      "epoch: 2 step: 702, loss is 0.1649402529001236\n",
      "epoch: 2 step: 703, loss is 0.2509813606739044\n",
      "epoch: 2 step: 704, loss is 0.1705397069454193\n",
      "epoch: 2 step: 705, loss is 0.420309841632843\n",
      "epoch: 2 step: 706, loss is 0.16286753118038177\n",
      "epoch: 2 step: 707, loss is 0.4229201674461365\n",
      "epoch: 2 step: 708, loss is 0.4087095260620117\n",
      "epoch: 2 step: 709, loss is 0.343983918428421\n",
      "epoch: 2 step: 710, loss is 0.3805830776691437\n",
      "epoch: 2 step: 711, loss is 0.31375569105148315\n",
      "epoch: 2 step: 712, loss is 0.16486649215221405\n",
      "epoch: 2 step: 713, loss is 0.23624663054943085\n",
      "epoch: 2 step: 714, loss is 0.261290580034256\n",
      "epoch: 2 step: 715, loss is 0.5194793939590454\n",
      "epoch: 2 step: 716, loss is 0.3581123650074005\n",
      "epoch: 2 step: 717, loss is 0.2482917606830597\n",
      "epoch: 2 step: 718, loss is 0.41476964950561523\n",
      "epoch: 2 step: 719, loss is 0.38649240136146545\n",
      "epoch: 2 step: 720, loss is 0.2879711389541626\n",
      "epoch: 2 step: 721, loss is 0.19047927856445312\n",
      "epoch: 2 step: 722, loss is 0.24188271164894104\n",
      "epoch: 2 step: 723, loss is 0.2181984931230545\n",
      "epoch: 2 step: 724, loss is 0.24328576028347015\n",
      "epoch: 2 step: 725, loss is 0.23317675292491913\n",
      "epoch: 2 step: 726, loss is 0.34511658549308777\n",
      "epoch: 2 step: 727, loss is 0.13207677006721497\n",
      "epoch: 2 step: 728, loss is 0.25439491868019104\n",
      "epoch: 2 step: 729, loss is 0.21042843163013458\n",
      "epoch: 2 step: 730, loss is 0.24953800439834595\n",
      "epoch: 2 step: 731, loss is 0.2989426255226135\n",
      "epoch: 2 step: 732, loss is 0.4070277214050293\n",
      "epoch: 2 step: 733, loss is 0.235530823469162\n",
      "epoch: 2 step: 734, loss is 0.2817983329296112\n",
      "epoch: 2 step: 735, loss is 0.14485765993595123\n",
      "epoch: 2 step: 736, loss is 0.30119141936302185\n",
      "epoch: 2 step: 737, loss is 0.1803557127714157\n",
      "epoch: 2 step: 738, loss is 0.3286932408809662\n",
      "epoch: 2 step: 739, loss is 0.28524354100227356\n",
      "epoch: 2 step: 740, loss is 0.2048003077507019\n",
      "epoch: 2 step: 741, loss is 0.22955569624900818\n",
      "epoch: 2 step: 742, loss is 0.23631176352500916\n",
      "epoch: 2 step: 743, loss is 0.35165292024612427\n",
      "epoch: 2 step: 744, loss is 0.3220904469490051\n",
      "epoch: 2 step: 745, loss is 0.37061208486557007\n",
      "epoch: 2 step: 746, loss is 0.582636296749115\n",
      "epoch: 2 step: 747, loss is 0.2869614362716675\n",
      "epoch: 2 step: 748, loss is 0.27749791741371155\n",
      "epoch: 2 step: 749, loss is 0.3093307316303253\n",
      "epoch: 2 step: 750, loss is 0.46041545271873474\n",
      "epoch: 2 step: 751, loss is 0.19278228282928467\n",
      "epoch: 2 step: 752, loss is 0.3100406527519226\n",
      "epoch: 2 step: 753, loss is 0.29013168811798096\n",
      "epoch: 2 step: 754, loss is 0.25741297006607056\n",
      "epoch: 2 step: 755, loss is 0.35146364569664\n",
      "epoch: 2 step: 756, loss is 0.14341400563716888\n",
      "epoch: 2 step: 757, loss is 0.5142214298248291\n",
      "epoch: 2 step: 758, loss is 0.29893165826797485\n",
      "epoch: 2 step: 759, loss is 0.2361501306295395\n",
      "epoch: 2 step: 760, loss is 0.5251410007476807\n",
      "epoch: 2 step: 761, loss is 0.24548490345478058\n",
      "epoch: 2 step: 762, loss is 0.2249843329191208\n",
      "epoch: 2 step: 763, loss is 0.2919584810733795\n",
      "epoch: 2 step: 764, loss is 0.34718766808509827\n",
      "epoch: 2 step: 765, loss is 0.27738916873931885\n",
      "epoch: 2 step: 766, loss is 0.19123180210590363\n",
      "epoch: 2 step: 767, loss is 0.3476594388484955\n",
      "epoch: 2 step: 768, loss is 0.2704804837703705\n",
      "epoch: 2 step: 769, loss is 0.4398137032985687\n",
      "epoch: 2 step: 770, loss is 0.2476695328950882\n",
      "epoch: 2 step: 771, loss is 0.31817004084587097\n",
      "epoch: 2 step: 772, loss is 0.3053952753543854\n",
      "epoch: 2 step: 773, loss is 0.17643912136554718\n",
      "epoch: 2 step: 774, loss is 0.20533564686775208\n",
      "epoch: 2 step: 775, loss is 0.2062697559595108\n",
      "epoch: 2 step: 776, loss is 0.28583648800849915\n",
      "epoch: 2 step: 777, loss is 0.33907565474510193\n",
      "epoch: 2 step: 778, loss is 0.2667592167854309\n",
      "epoch: 2 step: 779, loss is 0.26968640089035034\n",
      "epoch: 2 step: 780, loss is 0.3174797296524048\n",
      "epoch: 2 step: 781, loss is 0.15576614439487457\n",
      "epoch: 2 step: 782, loss is 0.14871281385421753\n",
      "epoch: 2 step: 783, loss is 0.3710317611694336\n",
      "epoch: 2 step: 784, loss is 0.3676900267601013\n",
      "epoch: 2 step: 785, loss is 0.09406506270170212\n",
      "epoch: 2 step: 786, loss is 0.29324793815612793\n",
      "epoch: 2 step: 787, loss is 0.2554585337638855\n",
      "epoch: 2 step: 788, loss is 0.31622233986854553\n",
      "epoch: 2 step: 789, loss is 0.29383978247642517\n",
      "epoch: 2 step: 790, loss is 0.28853654861450195\n",
      "epoch: 2 step: 791, loss is 0.1547861546278\n",
      "epoch: 2 step: 792, loss is 0.24143610894680023\n",
      "epoch: 2 step: 793, loss is 0.3125702738761902\n",
      "epoch: 2 step: 794, loss is 0.1664395034313202\n",
      "epoch: 2 step: 795, loss is 0.16412082314491272\n",
      "epoch: 2 step: 796, loss is 0.20108447968959808\n",
      "epoch: 2 step: 797, loss is 0.41435059905052185\n",
      "epoch: 2 step: 798, loss is 0.439018189907074\n",
      "epoch: 2 step: 799, loss is 0.29735681414604187\n",
      "epoch: 2 step: 800, loss is 0.3569689989089966\n",
      "epoch: 2 step: 801, loss is 0.23167261481285095\n",
      "epoch: 2 step: 802, loss is 0.2065625786781311\n",
      "epoch: 2 step: 803, loss is 0.31878846883773804\n",
      "epoch: 2 step: 804, loss is 0.18970292806625366\n",
      "epoch: 2 step: 805, loss is 0.31833377480506897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 806, loss is 0.31995925307273865\n",
      "epoch: 2 step: 807, loss is 0.4774962067604065\n",
      "epoch: 2 step: 808, loss is 0.2499147653579712\n",
      "epoch: 2 step: 809, loss is 0.2535303831100464\n",
      "epoch: 2 step: 810, loss is 0.34984755516052246\n",
      "epoch: 2 step: 811, loss is 0.3882533311843872\n",
      "epoch: 2 step: 812, loss is 0.3281422555446625\n",
      "epoch: 2 step: 813, loss is 0.23283232748508453\n",
      "epoch: 2 step: 814, loss is 0.32471269369125366\n",
      "epoch: 2 step: 815, loss is 0.30322620272636414\n",
      "epoch: 2 step: 816, loss is 0.24459443986415863\n",
      "epoch: 2 step: 817, loss is 0.2655720114707947\n",
      "epoch: 2 step: 818, loss is 0.27211475372314453\n",
      "epoch: 2 step: 819, loss is 0.2817833423614502\n",
      "epoch: 2 step: 820, loss is 0.208547905087471\n",
      "epoch: 2 step: 821, loss is 0.2970927357673645\n",
      "epoch: 2 step: 822, loss is 0.1912347376346588\n",
      "epoch: 2 step: 823, loss is 0.19814272224903107\n",
      "epoch: 2 step: 824, loss is 0.20647330582141876\n",
      "epoch: 2 step: 825, loss is 0.095177061855793\n",
      "epoch: 2 step: 826, loss is 0.31496331095695496\n",
      "epoch: 2 step: 827, loss is 0.12943069636821747\n",
      "epoch: 2 step: 828, loss is 0.38844403624534607\n",
      "epoch: 2 step: 829, loss is 0.20130226016044617\n",
      "epoch: 2 step: 830, loss is 0.2366301268339157\n",
      "epoch: 2 step: 831, loss is 0.26887980103492737\n",
      "epoch: 2 step: 832, loss is 0.39132291078567505\n",
      "epoch: 2 step: 833, loss is 0.31691551208496094\n",
      "epoch: 2 step: 834, loss is 0.18498395383358002\n",
      "epoch: 2 step: 835, loss is 0.12080185115337372\n",
      "epoch: 2 step: 836, loss is 0.40802478790283203\n",
      "epoch: 2 step: 837, loss is 0.2883102297782898\n",
      "epoch: 2 step: 838, loss is 0.22078414261341095\n",
      "epoch: 2 step: 839, loss is 0.25589483976364136\n",
      "epoch: 2 step: 840, loss is 0.3718619644641876\n",
      "epoch: 2 step: 841, loss is 0.25423505902290344\n",
      "epoch: 2 step: 842, loss is 0.3665785491466522\n",
      "epoch: 2 step: 843, loss is 0.2701459228992462\n",
      "epoch: 2 step: 844, loss is 0.22441552579402924\n",
      "epoch: 2 step: 845, loss is 0.23451560735702515\n",
      "epoch: 2 step: 846, loss is 0.1822386384010315\n",
      "epoch: 2 step: 847, loss is 0.3391127288341522\n",
      "epoch: 2 step: 848, loss is 0.31999316811561584\n",
      "epoch: 2 step: 849, loss is 0.24083474278450012\n",
      "epoch: 2 step: 850, loss is 0.22234372794628143\n",
      "epoch: 2 step: 851, loss is 0.1893472969532013\n",
      "epoch: 2 step: 852, loss is 0.3281640410423279\n",
      "epoch: 2 step: 853, loss is 0.34588003158569336\n",
      "epoch: 2 step: 854, loss is 0.13632886111736298\n",
      "epoch: 2 step: 855, loss is 0.22978048026561737\n",
      "epoch: 2 step: 856, loss is 0.21857066452503204\n",
      "epoch: 2 step: 857, loss is 0.32915598154067993\n",
      "epoch: 2 step: 858, loss is 0.18621279299259186\n",
      "epoch: 2 step: 859, loss is 0.3524147570133209\n",
      "epoch: 2 step: 860, loss is 0.21060186624526978\n",
      "epoch: 2 step: 861, loss is 0.21839439868927002\n",
      "epoch: 2 step: 862, loss is 0.26721933484077454\n",
      "epoch: 2 step: 863, loss is 0.24086704850196838\n",
      "epoch: 2 step: 864, loss is 0.14549273252487183\n",
      "epoch: 2 step: 865, loss is 0.2409457117319107\n",
      "epoch: 2 step: 866, loss is 0.27928271889686584\n",
      "epoch: 2 step: 867, loss is 0.3798055648803711\n",
      "epoch: 2 step: 868, loss is 0.40546733140945435\n",
      "epoch: 2 step: 869, loss is 0.40351396799087524\n",
      "epoch: 2 step: 870, loss is 0.3367784023284912\n",
      "epoch: 2 step: 871, loss is 0.17273932695388794\n",
      "epoch: 2 step: 872, loss is 0.20933447778224945\n",
      "epoch: 2 step: 873, loss is 0.3540908098220825\n",
      "epoch: 2 step: 874, loss is 0.29245802760124207\n",
      "epoch: 2 step: 875, loss is 0.24205705523490906\n",
      "epoch: 2 step: 876, loss is 0.35907989740371704\n",
      "epoch: 2 step: 877, loss is 0.25745537877082825\n",
      "epoch: 2 step: 878, loss is 0.188541978597641\n",
      "epoch: 2 step: 879, loss is 0.32908520102500916\n",
      "epoch: 2 step: 880, loss is 0.3258931040763855\n",
      "epoch: 2 step: 881, loss is 0.35417234897613525\n",
      "epoch: 2 step: 882, loss is 0.2290925830602646\n",
      "epoch: 2 step: 883, loss is 0.4011458158493042\n",
      "epoch: 2 step: 884, loss is 0.32745781540870667\n",
      "epoch: 2 step: 885, loss is 0.37058424949645996\n",
      "epoch: 2 step: 886, loss is 0.1943432092666626\n",
      "epoch: 2 step: 887, loss is 0.2747901976108551\n",
      "epoch: 2 step: 888, loss is 0.48834994435310364\n",
      "epoch: 2 step: 889, loss is 0.2627638876438141\n",
      "epoch: 2 step: 890, loss is 0.23982718586921692\n",
      "epoch: 2 step: 891, loss is 0.405123770236969\n",
      "epoch: 2 step: 892, loss is 0.33503785729408264\n",
      "epoch: 2 step: 893, loss is 0.3046087920665741\n",
      "epoch: 2 step: 894, loss is 0.17988818883895874\n",
      "epoch: 2 step: 895, loss is 0.2936041057109833\n",
      "epoch: 2 step: 896, loss is 0.4093286097049713\n",
      "epoch: 2 step: 897, loss is 0.27972546219825745\n",
      "epoch: 2 step: 898, loss is 0.27392223477363586\n",
      "epoch: 2 step: 899, loss is 0.3689221143722534\n",
      "epoch: 2 step: 900, loss is 0.13853339850902557\n",
      "epoch: 2 step: 901, loss is 0.38322973251342773\n",
      "epoch: 2 step: 902, loss is 0.3275739848613739\n",
      "epoch: 2 step: 903, loss is 0.2411653697490692\n",
      "epoch: 2 step: 904, loss is 0.2966427803039551\n",
      "epoch: 2 step: 905, loss is 0.38350552320480347\n",
      "epoch: 2 step: 906, loss is 0.3469386100769043\n",
      "epoch: 2 step: 907, loss is 0.3683749735355377\n",
      "epoch: 2 step: 908, loss is 0.22277621924877167\n",
      "epoch: 2 step: 909, loss is 0.23793882131576538\n",
      "epoch: 2 step: 910, loss is 0.29521894454956055\n",
      "epoch: 2 step: 911, loss is 0.3720965087413788\n",
      "epoch: 2 step: 912, loss is 0.1001131683588028\n",
      "epoch: 2 step: 913, loss is 0.2582896649837494\n",
      "epoch: 2 step: 914, loss is 0.4125465750694275\n",
      "epoch: 2 step: 915, loss is 0.3073800206184387\n",
      "epoch: 2 step: 916, loss is 0.27184775471687317\n",
      "epoch: 2 step: 917, loss is 0.22274091839790344\n",
      "epoch: 2 step: 918, loss is 0.2594572901725769\n",
      "epoch: 2 step: 919, loss is 0.10014805197715759\n",
      "epoch: 2 step: 920, loss is 0.3388303518295288\n",
      "epoch: 2 step: 921, loss is 0.27727019786834717\n",
      "epoch: 2 step: 922, loss is 0.2638145089149475\n",
      "epoch: 2 step: 923, loss is 0.27794572710990906\n",
      "epoch: 2 step: 924, loss is 0.35484686493873596\n",
      "epoch: 2 step: 925, loss is 0.3874180018901825\n",
      "epoch: 2 step: 926, loss is 0.39302247762680054\n",
      "epoch: 2 step: 927, loss is 0.20849239826202393\n",
      "epoch: 2 step: 928, loss is 0.26280295848846436\n",
      "epoch: 2 step: 929, loss is 0.2608470320701599\n",
      "epoch: 2 step: 930, loss is 0.2391919195652008\n",
      "epoch: 2 step: 931, loss is 0.21331089735031128\n",
      "epoch: 2 step: 932, loss is 0.4766974449157715\n",
      "epoch: 2 step: 933, loss is 0.39945170283317566\n",
      "epoch: 2 step: 934, loss is 0.31225815415382385\n",
      "epoch: 2 step: 935, loss is 0.19633011519908905\n",
      "epoch: 2 step: 936, loss is 0.31800809502601624\n",
      "epoch: 2 step: 937, loss is 0.3359718918800354\n",
      "epoch: 3 step: 1, loss is 0.20844201743602753\n",
      "epoch: 3 step: 2, loss is 0.17391395568847656\n",
      "epoch: 3 step: 3, loss is 0.14593926072120667\n",
      "epoch: 3 step: 4, loss is 0.2351704239845276\n",
      "epoch: 3 step: 5, loss is 0.237910658121109\n",
      "epoch: 3 step: 6, loss is 0.18038120865821838\n",
      "epoch: 3 step: 7, loss is 0.25341877341270447\n",
      "epoch: 3 step: 8, loss is 0.23039855062961578\n",
      "epoch: 3 step: 9, loss is 0.19703805446624756\n",
      "epoch: 3 step: 10, loss is 0.3161269724369049\n",
      "epoch: 3 step: 11, loss is 0.20084607601165771\n",
      "epoch: 3 step: 12, loss is 0.24129411578178406\n",
      "epoch: 3 step: 13, loss is 0.3329677879810333\n",
      "epoch: 3 step: 14, loss is 0.21114251017570496\n",
      "epoch: 3 step: 15, loss is 0.16449424624443054\n",
      "epoch: 3 step: 16, loss is 0.2868157923221588\n",
      "epoch: 3 step: 17, loss is 0.2622393071651459\n",
      "epoch: 3 step: 18, loss is 0.18000973761081696\n",
      "epoch: 3 step: 19, loss is 0.30855149030685425\n",
      "epoch: 3 step: 20, loss is 0.3024917542934418\n",
      "epoch: 3 step: 21, loss is 0.15109451115131378\n",
      "epoch: 3 step: 22, loss is 0.3871985375881195\n",
      "epoch: 3 step: 23, loss is 0.23074224591255188\n",
      "epoch: 3 step: 24, loss is 0.2232179343700409\n",
      "epoch: 3 step: 25, loss is 0.28150323033332825\n",
      "epoch: 3 step: 26, loss is 0.1369916945695877\n",
      "epoch: 3 step: 27, loss is 0.15411671996116638\n",
      "epoch: 3 step: 28, loss is 0.40684401988983154\n",
      "epoch: 3 step: 29, loss is 0.146993488073349\n",
      "epoch: 3 step: 30, loss is 0.20007190108299255\n",
      "epoch: 3 step: 31, loss is 0.2556533217430115\n",
      "epoch: 3 step: 32, loss is 0.32278677821159363\n",
      "epoch: 3 step: 33, loss is 0.3721442222595215\n",
      "epoch: 3 step: 34, loss is 0.2634373605251312\n",
      "epoch: 3 step: 35, loss is 0.1613360047340393\n",
      "epoch: 3 step: 36, loss is 0.16480250656604767\n",
      "epoch: 3 step: 37, loss is 0.20658284425735474\n",
      "epoch: 3 step: 38, loss is 0.10774896293878555\n",
      "epoch: 3 step: 39, loss is 0.15829746425151825\n",
      "epoch: 3 step: 40, loss is 0.24999693036079407\n",
      "epoch: 3 step: 41, loss is 0.4406711459159851\n",
      "epoch: 3 step: 42, loss is 0.07292550802230835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 43, loss is 0.3281875550746918\n",
      "epoch: 3 step: 44, loss is 0.23036614060401917\n",
      "epoch: 3 step: 45, loss is 0.20308369398117065\n",
      "epoch: 3 step: 46, loss is 0.19022220373153687\n",
      "epoch: 3 step: 47, loss is 0.1516513228416443\n",
      "epoch: 3 step: 48, loss is 0.10262922197580338\n",
      "epoch: 3 step: 49, loss is 0.35099729895591736\n",
      "epoch: 3 step: 50, loss is 0.1465609222650528\n",
      "epoch: 3 step: 51, loss is 0.189748615026474\n",
      "epoch: 3 step: 52, loss is 0.16383710503578186\n",
      "epoch: 3 step: 53, loss is 0.28487592935562134\n",
      "epoch: 3 step: 54, loss is 0.29748639464378357\n",
      "epoch: 3 step: 55, loss is 0.20878736674785614\n",
      "epoch: 3 step: 56, loss is 0.06803664565086365\n",
      "epoch: 3 step: 57, loss is 0.21897554397583008\n",
      "epoch: 3 step: 58, loss is 0.32452619075775146\n",
      "epoch: 3 step: 59, loss is 0.19628222286701202\n",
      "epoch: 3 step: 60, loss is 0.295436292886734\n",
      "epoch: 3 step: 61, loss is 0.18935726583003998\n",
      "epoch: 3 step: 62, loss is 0.32087740302085876\n",
      "epoch: 3 step: 63, loss is 0.14353421330451965\n",
      "epoch: 3 step: 64, loss is 0.2855764925479889\n",
      "epoch: 3 step: 65, loss is 0.28352275490760803\n",
      "epoch: 3 step: 66, loss is 0.3792415261268616\n",
      "epoch: 3 step: 67, loss is 0.28216153383255005\n",
      "epoch: 3 step: 68, loss is 0.21359282732009888\n",
      "epoch: 3 step: 69, loss is 0.34159132838249207\n",
      "epoch: 3 step: 70, loss is 0.3073872923851013\n",
      "epoch: 3 step: 71, loss is 0.23577870428562164\n",
      "epoch: 3 step: 72, loss is 0.22816015779972076\n",
      "epoch: 3 step: 73, loss is 0.2946537435054779\n",
      "epoch: 3 step: 74, loss is 0.20634275674819946\n",
      "epoch: 3 step: 75, loss is 0.34777775406837463\n",
      "epoch: 3 step: 76, loss is 0.28846409916877747\n",
      "epoch: 3 step: 77, loss is 0.34645676612854004\n",
      "epoch: 3 step: 78, loss is 0.2992931604385376\n",
      "epoch: 3 step: 79, loss is 0.27864301204681396\n",
      "epoch: 3 step: 80, loss is 0.25084176659584045\n",
      "epoch: 3 step: 81, loss is 0.15493963658809662\n",
      "epoch: 3 step: 82, loss is 0.154266357421875\n",
      "epoch: 3 step: 83, loss is 0.22135894000530243\n",
      "epoch: 3 step: 84, loss is 0.21535921096801758\n",
      "epoch: 3 step: 85, loss is 0.28918924927711487\n",
      "epoch: 3 step: 86, loss is 0.28277963399887085\n",
      "epoch: 3 step: 87, loss is 0.14253830909729004\n",
      "epoch: 3 step: 88, loss is 0.2879118025302887\n",
      "epoch: 3 step: 89, loss is 0.1963675618171692\n",
      "epoch: 3 step: 90, loss is 0.37617358565330505\n",
      "epoch: 3 step: 91, loss is 0.3056928515434265\n",
      "epoch: 3 step: 92, loss is 0.10963981598615646\n",
      "epoch: 3 step: 93, loss is 0.22900573909282684\n",
      "epoch: 3 step: 94, loss is 0.1601928472518921\n",
      "epoch: 3 step: 95, loss is 0.21355190873146057\n",
      "epoch: 3 step: 96, loss is 0.21725709736347198\n",
      "epoch: 3 step: 97, loss is 0.18831206858158112\n",
      "epoch: 3 step: 98, loss is 0.20471040904521942\n",
      "epoch: 3 step: 99, loss is 0.23567527532577515\n",
      "epoch: 3 step: 100, loss is 0.1519322246313095\n",
      "epoch: 3 step: 101, loss is 0.1665804237127304\n",
      "epoch: 3 step: 102, loss is 0.40959927439689636\n",
      "epoch: 3 step: 103, loss is 0.22094430029392242\n",
      "epoch: 3 step: 104, loss is 0.23381857573986053\n",
      "epoch: 3 step: 105, loss is 0.21258774399757385\n",
      "epoch: 3 step: 106, loss is 0.18823418021202087\n",
      "epoch: 3 step: 107, loss is 0.2761339247226715\n",
      "epoch: 3 step: 108, loss is 0.37950754165649414\n",
      "epoch: 3 step: 109, loss is 0.3446885049343109\n",
      "epoch: 3 step: 110, loss is 0.3355872631072998\n",
      "epoch: 3 step: 111, loss is 0.20880664885044098\n",
      "epoch: 3 step: 112, loss is 0.16079553961753845\n",
      "epoch: 3 step: 113, loss is 0.24863852560520172\n",
      "epoch: 3 step: 114, loss is 0.13392764329910278\n",
      "epoch: 3 step: 115, loss is 0.2867231070995331\n",
      "epoch: 3 step: 116, loss is 0.22393906116485596\n",
      "epoch: 3 step: 117, loss is 0.2368040382862091\n",
      "epoch: 3 step: 118, loss is 0.23240266740322113\n",
      "epoch: 3 step: 119, loss is 0.2144363820552826\n",
      "epoch: 3 step: 120, loss is 0.23569415509700775\n",
      "epoch: 3 step: 121, loss is 0.1975899636745453\n",
      "epoch: 3 step: 122, loss is 0.26442816853523254\n",
      "epoch: 3 step: 123, loss is 0.20159602165222168\n",
      "epoch: 3 step: 124, loss is 0.18379926681518555\n",
      "epoch: 3 step: 125, loss is 0.3669566512107849\n",
      "epoch: 3 step: 126, loss is 0.26895540952682495\n",
      "epoch: 3 step: 127, loss is 0.1660558134317398\n",
      "epoch: 3 step: 128, loss is 0.2089248150587082\n",
      "epoch: 3 step: 129, loss is 0.3526597321033478\n",
      "epoch: 3 step: 130, loss is 0.20210182666778564\n",
      "epoch: 3 step: 131, loss is 0.08315443992614746\n",
      "epoch: 3 step: 132, loss is 0.3250402510166168\n",
      "epoch: 3 step: 133, loss is 0.23549602925777435\n",
      "epoch: 3 step: 134, loss is 0.17761509120464325\n",
      "epoch: 3 step: 135, loss is 0.258706271648407\n",
      "epoch: 3 step: 136, loss is 0.23218680918216705\n",
      "epoch: 3 step: 137, loss is 0.17181922495365143\n",
      "epoch: 3 step: 138, loss is 0.2553279399871826\n",
      "epoch: 3 step: 139, loss is 0.10818126052618027\n",
      "epoch: 3 step: 140, loss is 0.3913745582103729\n",
      "epoch: 3 step: 141, loss is 0.13946862518787384\n",
      "epoch: 3 step: 142, loss is 0.22101308405399323\n",
      "epoch: 3 step: 143, loss is 0.27839040756225586\n",
      "epoch: 3 step: 144, loss is 0.2207583487033844\n",
      "epoch: 3 step: 145, loss is 0.13747572898864746\n",
      "epoch: 3 step: 146, loss is 0.4436749517917633\n",
      "epoch: 3 step: 147, loss is 0.17131049931049347\n",
      "epoch: 3 step: 148, loss is 0.33214157819747925\n",
      "epoch: 3 step: 149, loss is 0.42716193199157715\n",
      "epoch: 3 step: 150, loss is 0.16873914003372192\n",
      "epoch: 3 step: 151, loss is 0.18420380353927612\n",
      "epoch: 3 step: 152, loss is 0.2598635256290436\n",
      "epoch: 3 step: 153, loss is 0.2537122368812561\n",
      "epoch: 3 step: 154, loss is 0.21611838042736053\n",
      "epoch: 3 step: 155, loss is 0.2954646348953247\n",
      "epoch: 3 step: 156, loss is 0.29287174344062805\n",
      "epoch: 3 step: 157, loss is 0.15928150713443756\n",
      "epoch: 3 step: 158, loss is 0.19169145822525024\n",
      "epoch: 3 step: 159, loss is 0.18291041254997253\n",
      "epoch: 3 step: 160, loss is 0.38090112805366516\n",
      "epoch: 3 step: 161, loss is 0.24768386781215668\n",
      "epoch: 3 step: 162, loss is 0.1817546784877777\n",
      "epoch: 3 step: 163, loss is 0.19752930104732513\n",
      "epoch: 3 step: 164, loss is 0.18958528339862823\n",
      "epoch: 3 step: 165, loss is 0.3256111145019531\n",
      "epoch: 3 step: 166, loss is 0.1591748148202896\n",
      "epoch: 3 step: 167, loss is 0.1821785271167755\n",
      "epoch: 3 step: 168, loss is 0.1935744732618332\n",
      "epoch: 3 step: 169, loss is 0.4281439185142517\n",
      "epoch: 3 step: 170, loss is 0.23114530742168427\n",
      "epoch: 3 step: 171, loss is 0.21861931681632996\n",
      "epoch: 3 step: 172, loss is 0.1469191163778305\n",
      "epoch: 3 step: 173, loss is 0.3486955165863037\n",
      "epoch: 3 step: 174, loss is 0.2120063751935959\n",
      "epoch: 3 step: 175, loss is 0.15896901488304138\n",
      "epoch: 3 step: 176, loss is 0.3326016068458557\n",
      "epoch: 3 step: 177, loss is 0.3764789402484894\n",
      "epoch: 3 step: 178, loss is 0.35540300607681274\n",
      "epoch: 3 step: 179, loss is 0.14627261459827423\n",
      "epoch: 3 step: 180, loss is 0.4245508313179016\n",
      "epoch: 3 step: 181, loss is 0.13218553364276886\n",
      "epoch: 3 step: 182, loss is 0.11907763034105301\n",
      "epoch: 3 step: 183, loss is 0.20182938873767853\n",
      "epoch: 3 step: 184, loss is 0.2723400890827179\n",
      "epoch: 3 step: 185, loss is 0.2573866844177246\n",
      "epoch: 3 step: 186, loss is 0.29048091173171997\n",
      "epoch: 3 step: 187, loss is 0.209939107298851\n",
      "epoch: 3 step: 188, loss is 0.23335984349250793\n",
      "epoch: 3 step: 189, loss is 0.5090494751930237\n",
      "epoch: 3 step: 190, loss is 0.18521587550640106\n",
      "epoch: 3 step: 191, loss is 0.2022062987089157\n",
      "epoch: 3 step: 192, loss is 0.20931489765644073\n",
      "epoch: 3 step: 193, loss is 0.28380367159843445\n",
      "epoch: 3 step: 194, loss is 0.29819998145103455\n",
      "epoch: 3 step: 195, loss is 0.35639575123786926\n",
      "epoch: 3 step: 196, loss is 0.18359851837158203\n",
      "epoch: 3 step: 197, loss is 0.11042090505361557\n",
      "epoch: 3 step: 198, loss is 0.22478170692920685\n",
      "epoch: 3 step: 199, loss is 0.3373231291770935\n",
      "epoch: 3 step: 200, loss is 0.3141602575778961\n",
      "epoch: 3 step: 201, loss is 0.368281751871109\n",
      "epoch: 3 step: 202, loss is 0.196061909198761\n",
      "epoch: 3 step: 203, loss is 0.29400306940078735\n",
      "epoch: 3 step: 204, loss is 0.22101464867591858\n",
      "epoch: 3 step: 205, loss is 0.29626205563545227\n",
      "epoch: 3 step: 206, loss is 0.14725558459758759\n",
      "epoch: 3 step: 207, loss is 0.2700466513633728\n",
      "epoch: 3 step: 208, loss is 0.20728719234466553\n",
      "epoch: 3 step: 209, loss is 0.10244517028331757\n",
      "epoch: 3 step: 210, loss is 0.21111732721328735\n",
      "epoch: 3 step: 211, loss is 0.3610038459300995\n",
      "epoch: 3 step: 212, loss is 0.30181047320365906\n",
      "epoch: 3 step: 213, loss is 0.3371347486972809\n",
      "epoch: 3 step: 214, loss is 0.15170958638191223\n",
      "epoch: 3 step: 215, loss is 0.20324531197547913\n",
      "epoch: 3 step: 216, loss is 0.12922607362270355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 217, loss is 0.2422773391008377\n",
      "epoch: 3 step: 218, loss is 0.3243641257286072\n",
      "epoch: 3 step: 219, loss is 0.16769197583198547\n",
      "epoch: 3 step: 220, loss is 0.3712370693683624\n",
      "epoch: 3 step: 221, loss is 0.3302832245826721\n",
      "epoch: 3 step: 222, loss is 0.2528115510940552\n",
      "epoch: 3 step: 223, loss is 0.39192962646484375\n",
      "epoch: 3 step: 224, loss is 0.32361990213394165\n",
      "epoch: 3 step: 225, loss is 0.24500501155853271\n",
      "epoch: 3 step: 226, loss is 0.13656029105186462\n",
      "epoch: 3 step: 227, loss is 0.1922370344400406\n",
      "epoch: 3 step: 228, loss is 0.26880258321762085\n",
      "epoch: 3 step: 229, loss is 0.26922371983528137\n",
      "epoch: 3 step: 230, loss is 0.3033320903778076\n",
      "epoch: 3 step: 231, loss is 0.2760904133319855\n",
      "epoch: 3 step: 232, loss is 0.1905682384967804\n",
      "epoch: 3 step: 233, loss is 0.18615776300430298\n",
      "epoch: 3 step: 234, loss is 0.25657474994659424\n",
      "epoch: 3 step: 235, loss is 0.2594369947910309\n",
      "epoch: 3 step: 236, loss is 0.48433125019073486\n",
      "epoch: 3 step: 237, loss is 0.1910281777381897\n",
      "epoch: 3 step: 238, loss is 0.12959107756614685\n",
      "epoch: 3 step: 239, loss is 0.3355025351047516\n",
      "epoch: 3 step: 240, loss is 0.15085923671722412\n",
      "epoch: 3 step: 241, loss is 0.21324482560157776\n",
      "epoch: 3 step: 242, loss is 0.2614724934101105\n",
      "epoch: 3 step: 243, loss is 0.299209862947464\n",
      "epoch: 3 step: 244, loss is 0.13162192702293396\n",
      "epoch: 3 step: 245, loss is 0.09264739602804184\n",
      "epoch: 3 step: 246, loss is 0.17607925832271576\n",
      "epoch: 3 step: 247, loss is 0.3029356896877289\n",
      "epoch: 3 step: 248, loss is 0.21542416512966156\n",
      "epoch: 3 step: 249, loss is 0.2790564000606537\n",
      "epoch: 3 step: 250, loss is 0.18208031356334686\n",
      "epoch: 3 step: 251, loss is 0.10270078480243683\n",
      "epoch: 3 step: 252, loss is 0.26743558049201965\n",
      "epoch: 3 step: 253, loss is 0.31954631209373474\n",
      "epoch: 3 step: 254, loss is 0.24664661288261414\n",
      "epoch: 3 step: 255, loss is 0.14360767602920532\n",
      "epoch: 3 step: 256, loss is 0.164213165640831\n",
      "epoch: 3 step: 257, loss is 0.4086668789386749\n",
      "epoch: 3 step: 258, loss is 0.06858712434768677\n",
      "epoch: 3 step: 259, loss is 0.23640389740467072\n",
      "epoch: 3 step: 260, loss is 0.32592350244522095\n",
      "epoch: 3 step: 261, loss is 0.31110504269599915\n",
      "epoch: 3 step: 262, loss is 0.2795649766921997\n",
      "epoch: 3 step: 263, loss is 0.13758507370948792\n",
      "epoch: 3 step: 264, loss is 0.10921476781368256\n",
      "epoch: 3 step: 265, loss is 0.3631077706813812\n",
      "epoch: 3 step: 266, loss is 0.3690226972103119\n",
      "epoch: 3 step: 267, loss is 0.2524343729019165\n",
      "epoch: 3 step: 268, loss is 0.3763369917869568\n",
      "epoch: 3 step: 269, loss is 0.24525167047977448\n",
      "epoch: 3 step: 270, loss is 0.30588552355766296\n",
      "epoch: 3 step: 271, loss is 0.11833729594945908\n",
      "epoch: 3 step: 272, loss is 0.22586433589458466\n",
      "epoch: 3 step: 273, loss is 0.1763826161623001\n",
      "epoch: 3 step: 274, loss is 0.4259372651576996\n",
      "epoch: 3 step: 275, loss is 0.19040754437446594\n",
      "epoch: 3 step: 276, loss is 0.3136553168296814\n",
      "epoch: 3 step: 277, loss is 0.29884544014930725\n",
      "epoch: 3 step: 278, loss is 0.23327326774597168\n",
      "epoch: 3 step: 279, loss is 0.2783873379230499\n",
      "epoch: 3 step: 280, loss is 0.26984742283821106\n",
      "epoch: 3 step: 281, loss is 0.27613896131515503\n",
      "epoch: 3 step: 282, loss is 0.35304924845695496\n",
      "epoch: 3 step: 283, loss is 0.310828298330307\n",
      "epoch: 3 step: 284, loss is 0.21478009223937988\n",
      "epoch: 3 step: 285, loss is 0.2702163755893707\n",
      "epoch: 3 step: 286, loss is 0.1804787963628769\n",
      "epoch: 3 step: 287, loss is 0.3911975026130676\n",
      "epoch: 3 step: 288, loss is 0.2610107660293579\n",
      "epoch: 3 step: 289, loss is 0.2425432801246643\n",
      "epoch: 3 step: 290, loss is 0.3761090338230133\n",
      "epoch: 3 step: 291, loss is 0.2672211825847626\n",
      "epoch: 3 step: 292, loss is 0.16650719940662384\n",
      "epoch: 3 step: 293, loss is 0.12667249143123627\n",
      "epoch: 3 step: 294, loss is 0.11075282841920853\n",
      "epoch: 3 step: 295, loss is 0.3239836096763611\n",
      "epoch: 3 step: 296, loss is 0.12098194658756256\n",
      "epoch: 3 step: 297, loss is 0.11879874765872955\n",
      "epoch: 3 step: 298, loss is 0.33985501527786255\n",
      "epoch: 3 step: 299, loss is 0.28207361698150635\n",
      "epoch: 3 step: 300, loss is 0.2908148467540741\n",
      "epoch: 3 step: 301, loss is 0.23238316178321838\n",
      "epoch: 3 step: 302, loss is 0.1400320678949356\n",
      "epoch: 3 step: 303, loss is 0.14275014400482178\n",
      "epoch: 3 step: 304, loss is 0.3411358892917633\n",
      "epoch: 3 step: 305, loss is 0.309874564409256\n",
      "epoch: 3 step: 306, loss is 0.4012936055660248\n",
      "epoch: 3 step: 307, loss is 0.28179535269737244\n",
      "epoch: 3 step: 308, loss is 0.3199273645877838\n",
      "epoch: 3 step: 309, loss is 0.2878187596797943\n",
      "epoch: 3 step: 310, loss is 0.3934263288974762\n",
      "epoch: 3 step: 311, loss is 0.31106483936309814\n",
      "epoch: 3 step: 312, loss is 0.25973403453826904\n",
      "epoch: 3 step: 313, loss is 0.42269352078437805\n",
      "epoch: 3 step: 314, loss is 0.2793816328048706\n",
      "epoch: 3 step: 315, loss is 0.3544245958328247\n",
      "epoch: 3 step: 316, loss is 0.2974785566329956\n",
      "epoch: 3 step: 317, loss is 0.17197445034980774\n",
      "epoch: 3 step: 318, loss is 0.3314560353755951\n",
      "epoch: 3 step: 319, loss is 0.42898625135421753\n",
      "epoch: 3 step: 320, loss is 0.2984924018383026\n",
      "epoch: 3 step: 321, loss is 0.20771414041519165\n",
      "epoch: 3 step: 322, loss is 0.37433505058288574\n",
      "epoch: 3 step: 323, loss is 0.23143690824508667\n",
      "epoch: 3 step: 324, loss is 0.2663954198360443\n",
      "epoch: 3 step: 325, loss is 0.3360847234725952\n",
      "epoch: 3 step: 326, loss is 0.31958141922950745\n",
      "epoch: 3 step: 327, loss is 0.373612642288208\n",
      "epoch: 3 step: 328, loss is 0.32173952460289\n",
      "epoch: 3 step: 329, loss is 0.20386317372322083\n",
      "epoch: 3 step: 330, loss is 0.21366772055625916\n",
      "epoch: 3 step: 331, loss is 0.21777497231960297\n",
      "epoch: 3 step: 332, loss is 0.247460275888443\n",
      "epoch: 3 step: 333, loss is 0.19708602130413055\n",
      "epoch: 3 step: 334, loss is 0.2613506317138672\n",
      "epoch: 3 step: 335, loss is 0.28022345900535583\n",
      "epoch: 3 step: 336, loss is 0.327999085187912\n",
      "epoch: 3 step: 337, loss is 0.2979399859905243\n",
      "epoch: 3 step: 338, loss is 0.47415030002593994\n",
      "epoch: 3 step: 339, loss is 0.1491675078868866\n",
      "epoch: 3 step: 340, loss is 0.2920076847076416\n",
      "epoch: 3 step: 341, loss is 0.2736493647098541\n",
      "epoch: 3 step: 342, loss is 0.14171722531318665\n",
      "epoch: 3 step: 343, loss is 0.21665170788764954\n",
      "epoch: 3 step: 344, loss is 0.09467273950576782\n",
      "epoch: 3 step: 345, loss is 0.1793375015258789\n",
      "epoch: 3 step: 346, loss is 0.25239020586013794\n",
      "epoch: 3 step: 347, loss is 0.2822459042072296\n",
      "epoch: 3 step: 348, loss is 0.2709570825099945\n",
      "epoch: 3 step: 349, loss is 0.3418596088886261\n",
      "epoch: 3 step: 350, loss is 0.23643143475055695\n",
      "epoch: 3 step: 351, loss is 0.15015029907226562\n",
      "epoch: 3 step: 352, loss is 0.2307453602552414\n",
      "epoch: 3 step: 353, loss is 0.15822096168994904\n",
      "epoch: 3 step: 354, loss is 0.2689024806022644\n",
      "epoch: 3 step: 355, loss is 0.1583479940891266\n",
      "epoch: 3 step: 356, loss is 0.34267258644104004\n",
      "epoch: 3 step: 357, loss is 0.28289204835891724\n",
      "epoch: 3 step: 358, loss is 0.16647253930568695\n",
      "epoch: 3 step: 359, loss is 0.3510530889034271\n",
      "epoch: 3 step: 360, loss is 0.33791688084602356\n",
      "epoch: 3 step: 361, loss is 0.21439006924629211\n",
      "epoch: 3 step: 362, loss is 0.228335902094841\n",
      "epoch: 3 step: 363, loss is 0.26695945858955383\n",
      "epoch: 3 step: 364, loss is 0.2102503627538681\n",
      "epoch: 3 step: 365, loss is 0.16393108665943146\n",
      "epoch: 3 step: 366, loss is 0.34925204515457153\n",
      "epoch: 3 step: 367, loss is 0.25862202048301697\n",
      "epoch: 3 step: 368, loss is 0.2067810744047165\n",
      "epoch: 3 step: 369, loss is 0.2913941442966461\n",
      "epoch: 3 step: 370, loss is 0.2665490508079529\n",
      "epoch: 3 step: 371, loss is 0.2973750829696655\n",
      "epoch: 3 step: 372, loss is 0.5022758841514587\n",
      "epoch: 3 step: 373, loss is 0.17977631092071533\n",
      "epoch: 3 step: 374, loss is 0.2843203842639923\n",
      "epoch: 3 step: 375, loss is 0.25934740900993347\n",
      "epoch: 3 step: 376, loss is 0.20518960058689117\n",
      "epoch: 3 step: 377, loss is 0.17367219924926758\n",
      "epoch: 3 step: 378, loss is 0.3903460204601288\n",
      "epoch: 3 step: 379, loss is 0.24554041028022766\n",
      "epoch: 3 step: 380, loss is 0.33502197265625\n",
      "epoch: 3 step: 381, loss is 0.15177053213119507\n",
      "epoch: 3 step: 382, loss is 0.4779864549636841\n",
      "epoch: 3 step: 383, loss is 0.32221758365631104\n",
      "epoch: 3 step: 384, loss is 0.43083563446998596\n",
      "epoch: 3 step: 385, loss is 0.2672956585884094\n",
      "epoch: 3 step: 386, loss is 0.19507834315299988\n",
      "epoch: 3 step: 387, loss is 0.13097696006298065\n",
      "epoch: 3 step: 388, loss is 0.17002560198307037\n",
      "epoch: 3 step: 389, loss is 0.25136953592300415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 390, loss is 0.18155694007873535\n",
      "epoch: 3 step: 391, loss is 0.41039595007896423\n",
      "epoch: 3 step: 392, loss is 0.20745772123336792\n",
      "epoch: 3 step: 393, loss is 0.1784820854663849\n",
      "epoch: 3 step: 394, loss is 0.2892773151397705\n",
      "epoch: 3 step: 395, loss is 0.1504744589328766\n",
      "epoch: 3 step: 396, loss is 0.3387872278690338\n",
      "epoch: 3 step: 397, loss is 0.2441471666097641\n",
      "epoch: 3 step: 398, loss is 0.20433473587036133\n",
      "epoch: 3 step: 399, loss is 0.40718764066696167\n",
      "epoch: 3 step: 400, loss is 0.2810392379760742\n",
      "epoch: 3 step: 401, loss is 0.27781471610069275\n",
      "epoch: 3 step: 402, loss is 0.1710219830274582\n",
      "epoch: 3 step: 403, loss is 0.31821104884147644\n",
      "epoch: 3 step: 404, loss is 0.2243339866399765\n",
      "epoch: 3 step: 405, loss is 0.3623232841491699\n",
      "epoch: 3 step: 406, loss is 0.15284623205661774\n",
      "epoch: 3 step: 407, loss is 0.37416133284568787\n",
      "epoch: 3 step: 408, loss is 0.22626037895679474\n",
      "epoch: 3 step: 409, loss is 0.2638627588748932\n",
      "epoch: 3 step: 410, loss is 0.10299115628004074\n",
      "epoch: 3 step: 411, loss is 0.13891401886940002\n",
      "epoch: 3 step: 412, loss is 0.4292960464954376\n",
      "epoch: 3 step: 413, loss is 0.39453208446502686\n",
      "epoch: 3 step: 414, loss is 0.2988024652004242\n",
      "epoch: 3 step: 415, loss is 0.21380719542503357\n",
      "epoch: 3 step: 416, loss is 0.16037720441818237\n",
      "epoch: 3 step: 417, loss is 0.36006149649620056\n",
      "epoch: 3 step: 418, loss is 0.21814486384391785\n",
      "epoch: 3 step: 419, loss is 0.39927777647972107\n",
      "epoch: 3 step: 420, loss is 0.2222072333097458\n",
      "epoch: 3 step: 421, loss is 0.2976645231246948\n",
      "epoch: 3 step: 422, loss is 0.18143169581890106\n",
      "epoch: 3 step: 423, loss is 0.16883070766925812\n",
      "epoch: 3 step: 424, loss is 0.24709603190422058\n",
      "epoch: 3 step: 425, loss is 0.17488721013069153\n",
      "epoch: 3 step: 426, loss is 0.21354125440120697\n",
      "epoch: 3 step: 427, loss is 0.18507124483585358\n",
      "epoch: 3 step: 428, loss is 0.47589224576950073\n",
      "epoch: 3 step: 429, loss is 0.19245637953281403\n",
      "epoch: 3 step: 430, loss is 0.25895413756370544\n",
      "epoch: 3 step: 431, loss is 0.31979671120643616\n",
      "epoch: 3 step: 432, loss is 0.18551012873649597\n",
      "epoch: 3 step: 433, loss is 0.35865122079849243\n",
      "epoch: 3 step: 434, loss is 0.16220255196094513\n",
      "epoch: 3 step: 435, loss is 0.2118096947669983\n",
      "epoch: 3 step: 436, loss is 0.28911563754081726\n",
      "epoch: 3 step: 437, loss is 0.22407954931259155\n",
      "epoch: 3 step: 438, loss is 0.17436335980892181\n",
      "epoch: 3 step: 439, loss is 0.22622475028038025\n",
      "epoch: 3 step: 440, loss is 0.10683470219373703\n",
      "epoch: 3 step: 441, loss is 0.22380957007408142\n",
      "epoch: 3 step: 442, loss is 0.16759030520915985\n",
      "epoch: 3 step: 443, loss is 0.29908493161201477\n",
      "epoch: 3 step: 444, loss is 0.2792956829071045\n",
      "epoch: 3 step: 445, loss is 0.2636741101741791\n",
      "epoch: 3 step: 446, loss is 0.11754781007766724\n",
      "epoch: 3 step: 447, loss is 0.20382054150104523\n",
      "epoch: 3 step: 448, loss is 0.3083232045173645\n",
      "epoch: 3 step: 449, loss is 0.3165253698825836\n",
      "epoch: 3 step: 450, loss is 0.32770732045173645\n",
      "epoch: 3 step: 451, loss is 0.10136277228593826\n",
      "epoch: 3 step: 452, loss is 0.20901310443878174\n",
      "epoch: 3 step: 453, loss is 0.25368812680244446\n",
      "epoch: 3 step: 454, loss is 0.27041900157928467\n",
      "epoch: 3 step: 455, loss is 0.26650306582450867\n",
      "epoch: 3 step: 456, loss is 0.25037142634391785\n",
      "epoch: 3 step: 457, loss is 0.336684912443161\n",
      "epoch: 3 step: 458, loss is 0.18962237238883972\n",
      "epoch: 3 step: 459, loss is 0.20969165861606598\n",
      "epoch: 3 step: 460, loss is 0.2171083390712738\n",
      "epoch: 3 step: 461, loss is 0.32883399724960327\n",
      "epoch: 3 step: 462, loss is 0.32134106755256653\n",
      "epoch: 3 step: 463, loss is 0.1637449562549591\n",
      "epoch: 3 step: 464, loss is 0.21966823935508728\n",
      "epoch: 3 step: 465, loss is 0.33665555715560913\n",
      "epoch: 3 step: 466, loss is 0.25811517238616943\n",
      "epoch: 3 step: 467, loss is 0.18221929669380188\n",
      "epoch: 3 step: 468, loss is 0.2158142626285553\n",
      "epoch: 3 step: 469, loss is 0.3287172317504883\n",
      "epoch: 3 step: 470, loss is 0.149905726313591\n",
      "epoch: 3 step: 471, loss is 0.26564475893974304\n",
      "epoch: 3 step: 472, loss is 0.23919838666915894\n",
      "epoch: 3 step: 473, loss is 0.10332022607326508\n",
      "epoch: 3 step: 474, loss is 0.19918428361415863\n",
      "epoch: 3 step: 475, loss is 0.23228192329406738\n",
      "epoch: 3 step: 476, loss is 0.18308131396770477\n",
      "epoch: 3 step: 477, loss is 0.24731124937534332\n",
      "epoch: 3 step: 478, loss is 0.1905324012041092\n",
      "epoch: 3 step: 479, loss is 0.26844295859336853\n",
      "epoch: 3 step: 480, loss is 0.17904570698738098\n",
      "epoch: 3 step: 481, loss is 0.14744429290294647\n",
      "epoch: 3 step: 482, loss is 0.49171051383018494\n",
      "epoch: 3 step: 483, loss is 0.20864823460578918\n",
      "epoch: 3 step: 484, loss is 0.11443214118480682\n",
      "epoch: 3 step: 485, loss is 0.09039340168237686\n",
      "epoch: 3 step: 486, loss is 0.31951749324798584\n",
      "epoch: 3 step: 487, loss is 0.1212686076760292\n",
      "epoch: 3 step: 488, loss is 0.16251537203788757\n",
      "epoch: 3 step: 489, loss is 0.4431880712509155\n",
      "epoch: 3 step: 490, loss is 0.3555830121040344\n",
      "epoch: 3 step: 491, loss is 0.2558487355709076\n",
      "epoch: 3 step: 492, loss is 0.11977420747280121\n",
      "epoch: 3 step: 493, loss is 0.17933981120586395\n",
      "epoch: 3 step: 494, loss is 0.3622698485851288\n",
      "epoch: 3 step: 495, loss is 0.1642703264951706\n",
      "epoch: 3 step: 496, loss is 0.276938796043396\n",
      "epoch: 3 step: 497, loss is 0.16923511028289795\n",
      "epoch: 3 step: 498, loss is 0.22820985317230225\n",
      "epoch: 3 step: 499, loss is 0.11030495911836624\n",
      "epoch: 3 step: 500, loss is 0.2769799828529358\n",
      "epoch: 3 step: 501, loss is 0.2639663815498352\n",
      "epoch: 3 step: 502, loss is 0.21552307903766632\n",
      "epoch: 3 step: 503, loss is 0.40558329224586487\n",
      "epoch: 3 step: 504, loss is 0.41471731662750244\n",
      "epoch: 3 step: 505, loss is 0.36918267607688904\n",
      "epoch: 3 step: 506, loss is 0.23023906350135803\n",
      "epoch: 3 step: 507, loss is 0.15761850774288177\n",
      "epoch: 3 step: 508, loss is 0.16551095247268677\n",
      "epoch: 3 step: 509, loss is 0.30144548416137695\n",
      "epoch: 3 step: 510, loss is 0.32576486468315125\n",
      "epoch: 3 step: 511, loss is 0.37019088864326477\n",
      "epoch: 3 step: 512, loss is 0.30806824564933777\n",
      "epoch: 3 step: 513, loss is 0.33755365014076233\n",
      "epoch: 3 step: 514, loss is 0.23746632039546967\n",
      "epoch: 3 step: 515, loss is 0.24047163128852844\n",
      "epoch: 3 step: 516, loss is 0.24011753499507904\n",
      "epoch: 3 step: 517, loss is 0.24680382013320923\n",
      "epoch: 3 step: 518, loss is 0.15334749221801758\n",
      "epoch: 3 step: 519, loss is 0.1751345843076706\n",
      "epoch: 3 step: 520, loss is 0.2685035169124603\n",
      "epoch: 3 step: 521, loss is 0.37421754002571106\n",
      "epoch: 3 step: 522, loss is 0.2120099812746048\n",
      "epoch: 3 step: 523, loss is 0.3192209303379059\n",
      "epoch: 3 step: 524, loss is 0.3600054681301117\n",
      "epoch: 3 step: 525, loss is 0.1725558191537857\n",
      "epoch: 3 step: 526, loss is 0.3311373293399811\n",
      "epoch: 3 step: 527, loss is 0.16481341421604156\n",
      "epoch: 3 step: 528, loss is 0.3135712444782257\n",
      "epoch: 3 step: 529, loss is 0.3073383867740631\n",
      "epoch: 3 step: 530, loss is 0.20783664286136627\n",
      "epoch: 3 step: 531, loss is 0.2336510568857193\n",
      "epoch: 3 step: 532, loss is 0.2029733657836914\n",
      "epoch: 3 step: 533, loss is 0.1806706041097641\n",
      "epoch: 3 step: 534, loss is 0.24787718057632446\n",
      "epoch: 3 step: 535, loss is 0.21383029222488403\n",
      "epoch: 3 step: 536, loss is 0.25093474984169006\n",
      "epoch: 3 step: 537, loss is 0.3381461501121521\n",
      "epoch: 3 step: 538, loss is 0.31102392077445984\n",
      "epoch: 3 step: 539, loss is 0.25963094830513\n",
      "epoch: 3 step: 540, loss is 0.2782140374183655\n",
      "epoch: 3 step: 541, loss is 0.25341081619262695\n",
      "epoch: 3 step: 542, loss is 0.3578149974346161\n",
      "epoch: 3 step: 543, loss is 0.2681030035018921\n",
      "epoch: 3 step: 544, loss is 0.4607854187488556\n",
      "epoch: 3 step: 545, loss is 0.3828723728656769\n",
      "epoch: 3 step: 546, loss is 0.31221824884414673\n",
      "epoch: 3 step: 547, loss is 0.20210929214954376\n",
      "epoch: 3 step: 548, loss is 0.1861501783132553\n",
      "epoch: 3 step: 549, loss is 0.22988513112068176\n",
      "epoch: 3 step: 550, loss is 0.20464499294757843\n",
      "epoch: 3 step: 551, loss is 0.2953713834285736\n",
      "epoch: 3 step: 552, loss is 0.17718227207660675\n",
      "epoch: 3 step: 553, loss is 0.28661099076271057\n",
      "epoch: 3 step: 554, loss is 0.3215266466140747\n",
      "epoch: 3 step: 555, loss is 0.24002505838871002\n",
      "epoch: 3 step: 556, loss is 0.07670418918132782\n",
      "epoch: 3 step: 557, loss is 0.17576076090335846\n",
      "epoch: 3 step: 558, loss is 0.21272936463356018\n",
      "epoch: 3 step: 559, loss is 0.19165414571762085\n",
      "epoch: 3 step: 560, loss is 0.13695886731147766\n",
      "epoch: 3 step: 561, loss is 0.23732343316078186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 562, loss is 0.15446837246418\n",
      "epoch: 3 step: 563, loss is 0.24790096282958984\n",
      "epoch: 3 step: 564, loss is 0.30108150839805603\n",
      "epoch: 3 step: 565, loss is 0.23998552560806274\n",
      "epoch: 3 step: 566, loss is 0.3535943627357483\n",
      "epoch: 3 step: 567, loss is 0.2677081525325775\n",
      "epoch: 3 step: 568, loss is 0.24372735619544983\n",
      "epoch: 3 step: 569, loss is 0.22429130971431732\n",
      "epoch: 3 step: 570, loss is 0.386518657207489\n",
      "epoch: 3 step: 571, loss is 0.2782982289791107\n",
      "epoch: 3 step: 572, loss is 0.20771412551403046\n",
      "epoch: 3 step: 573, loss is 0.22882790863513947\n",
      "epoch: 3 step: 574, loss is 0.24709667265415192\n",
      "epoch: 3 step: 575, loss is 0.3695167005062103\n",
      "epoch: 3 step: 576, loss is 0.31655919551849365\n",
      "epoch: 3 step: 577, loss is 0.3354872465133667\n",
      "epoch: 3 step: 578, loss is 0.14394694566726685\n",
      "epoch: 3 step: 579, loss is 0.35384586453437805\n",
      "epoch: 3 step: 580, loss is 0.21506747603416443\n",
      "epoch: 3 step: 581, loss is 0.23233947157859802\n",
      "epoch: 3 step: 582, loss is 0.19394949078559875\n",
      "epoch: 3 step: 583, loss is 0.36003607511520386\n",
      "epoch: 3 step: 584, loss is 0.3016670048236847\n",
      "epoch: 3 step: 585, loss is 0.31501540541648865\n",
      "epoch: 3 step: 586, loss is 0.26728355884552\n",
      "epoch: 3 step: 587, loss is 0.26836496591567993\n",
      "epoch: 3 step: 588, loss is 0.19189825654029846\n",
      "epoch: 3 step: 589, loss is 0.1801227182149887\n",
      "epoch: 3 step: 590, loss is 0.1339675933122635\n",
      "epoch: 3 step: 591, loss is 0.3335855007171631\n",
      "epoch: 3 step: 592, loss is 0.258827269077301\n",
      "epoch: 3 step: 593, loss is 0.23246079683303833\n",
      "epoch: 3 step: 594, loss is 0.13031308352947235\n",
      "epoch: 3 step: 595, loss is 0.16678071022033691\n",
      "epoch: 3 step: 596, loss is 0.26610326766967773\n",
      "epoch: 3 step: 597, loss is 0.09540216624736786\n",
      "epoch: 3 step: 598, loss is 0.3042922019958496\n",
      "epoch: 3 step: 599, loss is 0.2641119658946991\n",
      "epoch: 3 step: 600, loss is 0.25825029611587524\n",
      "epoch: 3 step: 601, loss is 0.25035038590431213\n",
      "epoch: 3 step: 602, loss is 0.1832571178674698\n",
      "epoch: 3 step: 603, loss is 0.21913465857505798\n",
      "epoch: 3 step: 604, loss is 0.08172722160816193\n",
      "epoch: 3 step: 605, loss is 0.3315783739089966\n",
      "epoch: 3 step: 606, loss is 0.17573077976703644\n",
      "epoch: 3 step: 607, loss is 0.15559759736061096\n",
      "epoch: 3 step: 608, loss is 0.17301294207572937\n",
      "epoch: 3 step: 609, loss is 0.1676536649465561\n",
      "epoch: 3 step: 610, loss is 0.24023906886577606\n",
      "epoch: 3 step: 611, loss is 0.2560068666934967\n",
      "epoch: 3 step: 612, loss is 0.2979462742805481\n",
      "epoch: 3 step: 613, loss is 0.2275044322013855\n",
      "epoch: 3 step: 614, loss is 0.2997009754180908\n",
      "epoch: 3 step: 615, loss is 0.22562220692634583\n",
      "epoch: 3 step: 616, loss is 0.16685034334659576\n",
      "epoch: 3 step: 617, loss is 0.22610753774642944\n",
      "epoch: 3 step: 618, loss is 0.2984529137611389\n",
      "epoch: 3 step: 619, loss is 0.07816708087921143\n",
      "epoch: 3 step: 620, loss is 0.2631695568561554\n",
      "epoch: 3 step: 621, loss is 0.3602852523326874\n",
      "epoch: 3 step: 622, loss is 0.2423853576183319\n",
      "epoch: 3 step: 623, loss is 0.22571447491645813\n",
      "epoch: 3 step: 624, loss is 0.10239876806735992\n",
      "epoch: 3 step: 625, loss is 0.2705955505371094\n",
      "epoch: 3 step: 626, loss is 0.14353464543819427\n",
      "epoch: 3 step: 627, loss is 0.24500344693660736\n",
      "epoch: 3 step: 628, loss is 0.2542998790740967\n",
      "epoch: 3 step: 629, loss is 0.12612442672252655\n",
      "epoch: 3 step: 630, loss is 0.20844878256320953\n",
      "epoch: 3 step: 631, loss is 0.21922001242637634\n",
      "epoch: 3 step: 632, loss is 0.14361980557441711\n",
      "epoch: 3 step: 633, loss is 0.16973654925823212\n",
      "epoch: 3 step: 634, loss is 0.2883698046207428\n",
      "epoch: 3 step: 635, loss is 0.2577478289604187\n",
      "epoch: 3 step: 636, loss is 0.34623780846595764\n",
      "epoch: 3 step: 637, loss is 0.19556951522827148\n",
      "epoch: 3 step: 638, loss is 0.24584941565990448\n",
      "epoch: 3 step: 639, loss is 0.3785393536090851\n",
      "epoch: 3 step: 640, loss is 0.2340518683195114\n",
      "epoch: 3 step: 641, loss is 0.1831083744764328\n",
      "epoch: 3 step: 642, loss is 0.19715604186058044\n",
      "epoch: 3 step: 643, loss is 0.44685548543930054\n",
      "epoch: 3 step: 644, loss is 0.3290059268474579\n",
      "epoch: 3 step: 645, loss is 0.1764828860759735\n",
      "epoch: 3 step: 646, loss is 0.3313279151916504\n",
      "epoch: 3 step: 647, loss is 0.2156064808368683\n",
      "epoch: 3 step: 648, loss is 0.1902114301919937\n",
      "epoch: 3 step: 649, loss is 0.40843433141708374\n",
      "epoch: 3 step: 650, loss is 0.32041290402412415\n",
      "epoch: 3 step: 651, loss is 0.25789955258369446\n",
      "epoch: 3 step: 652, loss is 0.10257408767938614\n",
      "epoch: 3 step: 653, loss is 0.30524227023124695\n",
      "epoch: 3 step: 654, loss is 0.43999648094177246\n",
      "epoch: 3 step: 655, loss is 0.38212141394615173\n",
      "epoch: 3 step: 656, loss is 0.2535102069377899\n",
      "epoch: 3 step: 657, loss is 0.1281527876853943\n",
      "epoch: 3 step: 658, loss is 0.2422715723514557\n",
      "epoch: 3 step: 659, loss is 0.23453450202941895\n",
      "epoch: 3 step: 660, loss is 0.11587927490472794\n",
      "epoch: 3 step: 661, loss is 0.22306525707244873\n",
      "epoch: 3 step: 662, loss is 0.2999325394630432\n",
      "epoch: 3 step: 663, loss is 0.22752511501312256\n",
      "epoch: 3 step: 664, loss is 0.14121249318122864\n",
      "epoch: 3 step: 665, loss is 0.38277655839920044\n",
      "epoch: 3 step: 666, loss is 0.15916693210601807\n",
      "epoch: 3 step: 667, loss is 0.23825785517692566\n",
      "epoch: 3 step: 668, loss is 0.13241849839687347\n",
      "epoch: 3 step: 669, loss is 0.2886843979358673\n",
      "epoch: 3 step: 670, loss is 0.2543582320213318\n",
      "epoch: 3 step: 671, loss is 0.17126013338565826\n",
      "epoch: 3 step: 672, loss is 0.44489675760269165\n",
      "epoch: 3 step: 673, loss is 0.23814624547958374\n",
      "epoch: 3 step: 674, loss is 0.2492549568414688\n",
      "epoch: 3 step: 675, loss is 0.326383501291275\n",
      "epoch: 3 step: 676, loss is 0.17489151656627655\n",
      "epoch: 3 step: 677, loss is 0.27906185388565063\n",
      "epoch: 3 step: 678, loss is 0.21307477355003357\n",
      "epoch: 3 step: 679, loss is 0.2776053249835968\n",
      "epoch: 3 step: 680, loss is 0.2771186828613281\n",
      "epoch: 3 step: 681, loss is 0.16897639632225037\n",
      "epoch: 3 step: 682, loss is 0.20065703988075256\n",
      "epoch: 3 step: 683, loss is 0.3084394037723541\n",
      "epoch: 3 step: 684, loss is 0.1845993846654892\n",
      "epoch: 3 step: 685, loss is 0.2268371433019638\n",
      "epoch: 3 step: 686, loss is 0.20233742892742157\n",
      "epoch: 3 step: 687, loss is 0.2913212776184082\n",
      "epoch: 3 step: 688, loss is 0.16238026320934296\n",
      "epoch: 3 step: 689, loss is 0.3139553368091583\n",
      "epoch: 3 step: 690, loss is 0.27506303787231445\n",
      "epoch: 3 step: 691, loss is 0.24196797609329224\n",
      "epoch: 3 step: 692, loss is 0.12762844562530518\n",
      "epoch: 3 step: 693, loss is 0.19608323276042938\n",
      "epoch: 3 step: 694, loss is 0.20266039669513702\n",
      "epoch: 3 step: 695, loss is 0.14771538972854614\n",
      "epoch: 3 step: 696, loss is 0.2247427999973297\n",
      "epoch: 3 step: 697, loss is 0.29004278779029846\n",
      "epoch: 3 step: 698, loss is 0.2373119592666626\n",
      "epoch: 3 step: 699, loss is 0.33682212233543396\n",
      "epoch: 3 step: 700, loss is 0.17731057107448578\n",
      "epoch: 3 step: 701, loss is 0.2908537685871124\n",
      "epoch: 3 step: 702, loss is 0.22615987062454224\n",
      "epoch: 3 step: 703, loss is 0.2788878083229065\n",
      "epoch: 3 step: 704, loss is 0.12893575429916382\n",
      "epoch: 3 step: 705, loss is 0.2736635208129883\n",
      "epoch: 3 step: 706, loss is 0.09672553092241287\n",
      "epoch: 3 step: 707, loss is 0.291826456785202\n",
      "epoch: 3 step: 708, loss is 0.18553465604782104\n",
      "epoch: 3 step: 709, loss is 0.1656319499015808\n",
      "epoch: 3 step: 710, loss is 0.2382991909980774\n",
      "epoch: 3 step: 711, loss is 0.23140496015548706\n",
      "epoch: 3 step: 712, loss is 0.15208874642848969\n",
      "epoch: 3 step: 713, loss is 0.07294867187738419\n",
      "epoch: 3 step: 714, loss is 0.3147800862789154\n",
      "epoch: 3 step: 715, loss is 0.2667519450187683\n",
      "epoch: 3 step: 716, loss is 0.3359498679637909\n",
      "epoch: 3 step: 717, loss is 0.3199407458305359\n",
      "epoch: 3 step: 718, loss is 0.30429941415786743\n",
      "epoch: 3 step: 719, loss is 0.1498241126537323\n",
      "epoch: 3 step: 720, loss is 0.17331066727638245\n",
      "epoch: 3 step: 721, loss is 0.24115929007530212\n",
      "epoch: 3 step: 722, loss is 0.3613605201244354\n",
      "epoch: 3 step: 723, loss is 0.28291139006614685\n",
      "epoch: 3 step: 724, loss is 0.20871998369693756\n",
      "epoch: 3 step: 725, loss is 0.2848645746707916\n",
      "epoch: 3 step: 726, loss is 0.25085973739624023\n",
      "epoch: 3 step: 727, loss is 0.27179956436157227\n",
      "epoch: 3 step: 728, loss is 0.18340659141540527\n",
      "epoch: 3 step: 729, loss is 0.27288368344306946\n",
      "epoch: 3 step: 730, loss is 0.21033383905887604\n",
      "epoch: 3 step: 731, loss is 0.18116287887096405\n",
      "epoch: 3 step: 732, loss is 0.3426160514354706\n",
      "epoch: 3 step: 733, loss is 0.24687205255031586\n",
      "epoch: 3 step: 734, loss is 0.2929729223251343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 735, loss is 0.11199584603309631\n",
      "epoch: 3 step: 736, loss is 0.1814562976360321\n",
      "epoch: 3 step: 737, loss is 0.16991008818149567\n",
      "epoch: 3 step: 738, loss is 0.42862531542778015\n",
      "epoch: 3 step: 739, loss is 0.1987995207309723\n",
      "epoch: 3 step: 740, loss is 0.22865734994411469\n",
      "epoch: 3 step: 741, loss is 0.22073090076446533\n",
      "epoch: 3 step: 742, loss is 0.4006754159927368\n",
      "epoch: 3 step: 743, loss is 0.2506173849105835\n",
      "epoch: 3 step: 744, loss is 0.2771695852279663\n",
      "epoch: 3 step: 745, loss is 0.20767027139663696\n",
      "epoch: 3 step: 746, loss is 0.22068750858306885\n",
      "epoch: 3 step: 747, loss is 0.1457710713148117\n",
      "epoch: 3 step: 748, loss is 0.30789557099342346\n",
      "epoch: 3 step: 749, loss is 0.26703310012817383\n",
      "epoch: 3 step: 750, loss is 0.12852157652378082\n",
      "epoch: 3 step: 751, loss is 0.4042782187461853\n",
      "epoch: 3 step: 752, loss is 0.21809601783752441\n",
      "epoch: 3 step: 753, loss is 0.3290923535823822\n",
      "epoch: 3 step: 754, loss is 0.12551717460155487\n",
      "epoch: 3 step: 755, loss is 0.23740123212337494\n",
      "epoch: 3 step: 756, loss is 0.27139294147491455\n",
      "epoch: 3 step: 757, loss is 0.33175721764564514\n",
      "epoch: 3 step: 758, loss is 0.2748070955276489\n",
      "epoch: 3 step: 759, loss is 0.3625534772872925\n",
      "epoch: 3 step: 760, loss is 0.300522118806839\n",
      "epoch: 3 step: 761, loss is 0.13367025554180145\n",
      "epoch: 3 step: 762, loss is 0.26472702622413635\n",
      "epoch: 3 step: 763, loss is 0.32861727476119995\n",
      "epoch: 3 step: 764, loss is 0.163407564163208\n",
      "epoch: 3 step: 765, loss is 0.42227405309677124\n",
      "epoch: 3 step: 766, loss is 0.34477153420448303\n",
      "epoch: 3 step: 767, loss is 0.16272737085819244\n",
      "epoch: 3 step: 768, loss is 0.22177934646606445\n",
      "epoch: 3 step: 769, loss is 0.17120003700256348\n",
      "epoch: 3 step: 770, loss is 0.20422214269638062\n",
      "epoch: 3 step: 771, loss is 0.2226117104291916\n",
      "epoch: 3 step: 772, loss is 0.29709187150001526\n",
      "epoch: 3 step: 773, loss is 0.31999409198760986\n",
      "epoch: 3 step: 774, loss is 0.2499152570962906\n",
      "epoch: 3 step: 775, loss is 0.09398435801267624\n",
      "epoch: 3 step: 776, loss is 0.3066936135292053\n",
      "epoch: 3 step: 777, loss is 0.22001391649246216\n",
      "epoch: 3 step: 778, loss is 0.22889652848243713\n",
      "epoch: 3 step: 779, loss is 0.24761949479579926\n",
      "epoch: 3 step: 780, loss is 0.31051093339920044\n",
      "epoch: 3 step: 781, loss is 0.11100757867097855\n",
      "epoch: 3 step: 782, loss is 0.2712036371231079\n",
      "epoch: 3 step: 783, loss is 0.2813761234283447\n",
      "epoch: 3 step: 784, loss is 0.2161853164434433\n",
      "epoch: 3 step: 785, loss is 0.3235805034637451\n",
      "epoch: 3 step: 786, loss is 0.24670416116714478\n",
      "epoch: 3 step: 787, loss is 0.14804592728614807\n",
      "epoch: 3 step: 788, loss is 0.43683764338493347\n",
      "epoch: 3 step: 789, loss is 0.13019512593746185\n",
      "epoch: 3 step: 790, loss is 0.19999919831752777\n",
      "epoch: 3 step: 791, loss is 0.43007567524909973\n",
      "epoch: 3 step: 792, loss is 0.32576867938041687\n",
      "epoch: 3 step: 793, loss is 0.14938341081142426\n",
      "epoch: 3 step: 794, loss is 0.3885078430175781\n",
      "epoch: 3 step: 795, loss is 0.23602606356143951\n",
      "epoch: 3 step: 796, loss is 0.22888752818107605\n",
      "epoch: 3 step: 797, loss is 0.12622138857841492\n",
      "epoch: 3 step: 798, loss is 0.37599852681159973\n",
      "epoch: 3 step: 799, loss is 0.2006407380104065\n",
      "epoch: 3 step: 800, loss is 0.16246534883975983\n",
      "epoch: 3 step: 801, loss is 0.3019046485424042\n",
      "epoch: 3 step: 802, loss is 0.139665424823761\n",
      "epoch: 3 step: 803, loss is 0.16350290179252625\n",
      "epoch: 3 step: 804, loss is 0.2930338680744171\n",
      "epoch: 3 step: 805, loss is 0.24357642233371735\n",
      "epoch: 3 step: 806, loss is 0.12877295911312103\n",
      "epoch: 3 step: 807, loss is 0.177949920296669\n",
      "epoch: 3 step: 808, loss is 0.19221769273281097\n",
      "epoch: 3 step: 809, loss is 0.26225194334983826\n",
      "epoch: 3 step: 810, loss is 0.22428055107593536\n",
      "epoch: 3 step: 811, loss is 0.4713306128978729\n",
      "epoch: 3 step: 812, loss is 0.27119845151901245\n",
      "epoch: 3 step: 813, loss is 0.27475589513778687\n",
      "epoch: 3 step: 814, loss is 0.23942464590072632\n",
      "epoch: 3 step: 815, loss is 0.25773799419403076\n",
      "epoch: 3 step: 816, loss is 0.2330954670906067\n",
      "epoch: 3 step: 817, loss is 0.23834921419620514\n",
      "epoch: 3 step: 818, loss is 0.16939575970172882\n",
      "epoch: 3 step: 819, loss is 0.1092413142323494\n",
      "epoch: 3 step: 820, loss is 0.2295454740524292\n",
      "epoch: 3 step: 821, loss is 0.2058742493391037\n",
      "epoch: 3 step: 822, loss is 0.22413840889930725\n",
      "epoch: 3 step: 823, loss is 0.1959843933582306\n",
      "epoch: 3 step: 824, loss is 0.41554996371269226\n",
      "epoch: 3 step: 825, loss is 0.26040276885032654\n",
      "epoch: 3 step: 826, loss is 0.13578638434410095\n",
      "epoch: 3 step: 827, loss is 0.33169975876808167\n",
      "epoch: 3 step: 828, loss is 0.24333573877811432\n",
      "epoch: 3 step: 829, loss is 0.1726863831281662\n",
      "epoch: 3 step: 830, loss is 0.3150690495967865\n",
      "epoch: 3 step: 831, loss is 0.12589256465435028\n",
      "epoch: 3 step: 832, loss is 0.15587961673736572\n",
      "epoch: 3 step: 833, loss is 0.36138778924942017\n",
      "epoch: 3 step: 834, loss is 0.30547085404396057\n",
      "epoch: 3 step: 835, loss is 0.184658482670784\n",
      "epoch: 3 step: 836, loss is 0.310552716255188\n",
      "epoch: 3 step: 837, loss is 0.14860795438289642\n",
      "epoch: 3 step: 838, loss is 0.2483440488576889\n",
      "epoch: 3 step: 839, loss is 0.3559170067310333\n",
      "epoch: 3 step: 840, loss is 0.2902722656726837\n",
      "epoch: 3 step: 841, loss is 0.1863735020160675\n",
      "epoch: 3 step: 842, loss is 0.18809014558792114\n",
      "epoch: 3 step: 843, loss is 0.3359881341457367\n",
      "epoch: 3 step: 844, loss is 0.2098667025566101\n",
      "epoch: 3 step: 845, loss is 0.26801756024360657\n",
      "epoch: 3 step: 846, loss is 0.2476392537355423\n",
      "epoch: 3 step: 847, loss is 0.21350356936454773\n",
      "epoch: 3 step: 848, loss is 0.4215116798877716\n",
      "epoch: 3 step: 849, loss is 0.28367117047309875\n",
      "epoch: 3 step: 850, loss is 0.1733546406030655\n",
      "epoch: 3 step: 851, loss is 0.3837668299674988\n",
      "epoch: 3 step: 852, loss is 0.20561352372169495\n",
      "epoch: 3 step: 853, loss is 0.24769297242164612\n",
      "epoch: 3 step: 854, loss is 0.303112268447876\n",
      "epoch: 3 step: 855, loss is 0.18423227965831757\n",
      "epoch: 3 step: 856, loss is 0.204268217086792\n",
      "epoch: 3 step: 857, loss is 0.19177018105983734\n",
      "epoch: 3 step: 858, loss is 0.2220272719860077\n",
      "epoch: 3 step: 859, loss is 0.22974202036857605\n",
      "epoch: 3 step: 860, loss is 0.23532430827617645\n",
      "epoch: 3 step: 861, loss is 0.2240917831659317\n",
      "epoch: 3 step: 862, loss is 0.22518888115882874\n",
      "epoch: 3 step: 863, loss is 0.20240911841392517\n",
      "epoch: 3 step: 864, loss is 0.15978796780109406\n",
      "epoch: 3 step: 865, loss is 0.28411242365837097\n",
      "epoch: 3 step: 866, loss is 0.2593024671077728\n",
      "epoch: 3 step: 867, loss is 0.11947271972894669\n",
      "epoch: 3 step: 868, loss is 0.3839978575706482\n",
      "epoch: 3 step: 869, loss is 0.20898018777370453\n",
      "epoch: 3 step: 870, loss is 0.16126880049705505\n",
      "epoch: 3 step: 871, loss is 0.2856869697570801\n",
      "epoch: 3 step: 872, loss is 0.377227246761322\n",
      "epoch: 3 step: 873, loss is 0.23250049352645874\n",
      "epoch: 3 step: 874, loss is 0.23050101101398468\n",
      "epoch: 3 step: 875, loss is 0.21049073338508606\n",
      "epoch: 3 step: 876, loss is 0.21928741037845612\n",
      "epoch: 3 step: 877, loss is 0.23210236430168152\n",
      "epoch: 3 step: 878, loss is 0.24279725551605225\n",
      "epoch: 3 step: 879, loss is 0.22755832970142365\n",
      "epoch: 3 step: 880, loss is 0.22992652654647827\n",
      "epoch: 3 step: 881, loss is 0.1954692155122757\n",
      "epoch: 3 step: 882, loss is 0.20858493447303772\n",
      "epoch: 3 step: 883, loss is 0.10288836061954498\n",
      "epoch: 3 step: 884, loss is 0.17559893429279327\n",
      "epoch: 3 step: 885, loss is 0.24356701970100403\n",
      "epoch: 3 step: 886, loss is 0.18626339733600616\n",
      "epoch: 3 step: 887, loss is 0.22727054357528687\n",
      "epoch: 3 step: 888, loss is 0.17410419881343842\n",
      "epoch: 3 step: 889, loss is 0.2105276882648468\n",
      "epoch: 3 step: 890, loss is 0.23050400614738464\n",
      "epoch: 3 step: 891, loss is 0.2640327215194702\n",
      "epoch: 3 step: 892, loss is 0.38350415229797363\n",
      "epoch: 3 step: 893, loss is 0.17928862571716309\n",
      "epoch: 3 step: 894, loss is 0.28615763783454895\n",
      "epoch: 3 step: 895, loss is 0.14016743004322052\n",
      "epoch: 3 step: 896, loss is 0.18395376205444336\n",
      "epoch: 3 step: 897, loss is 0.13335685431957245\n",
      "epoch: 3 step: 898, loss is 0.19201728701591492\n",
      "epoch: 3 step: 899, loss is 0.2657991945743561\n",
      "epoch: 3 step: 900, loss is 0.16384316980838776\n",
      "epoch: 3 step: 901, loss is 0.30144980549812317\n",
      "epoch: 3 step: 902, loss is 0.2074025422334671\n",
      "epoch: 3 step: 903, loss is 0.24868400394916534\n",
      "epoch: 3 step: 904, loss is 0.0753854438662529\n",
      "epoch: 3 step: 905, loss is 0.28651162981987\n",
      "epoch: 3 step: 906, loss is 0.2492218315601349\n",
      "epoch: 3 step: 907, loss is 0.3895312249660492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 908, loss is 0.19900521636009216\n",
      "epoch: 3 step: 909, loss is 0.26312577724456787\n",
      "epoch: 3 step: 910, loss is 0.2352500706911087\n",
      "epoch: 3 step: 911, loss is 0.2243928164243698\n",
      "epoch: 3 step: 912, loss is 0.24779993295669556\n",
      "epoch: 3 step: 913, loss is 0.25201618671417236\n",
      "epoch: 3 step: 914, loss is 0.233528271317482\n",
      "epoch: 3 step: 915, loss is 0.1750892996788025\n",
      "epoch: 3 step: 916, loss is 0.14304645359516144\n",
      "epoch: 3 step: 917, loss is 0.18399092555046082\n",
      "epoch: 3 step: 918, loss is 0.1680360585451126\n",
      "epoch: 3 step: 919, loss is 0.24548554420471191\n",
      "epoch: 3 step: 920, loss is 0.20112769305706024\n",
      "epoch: 3 step: 921, loss is 0.3028355836868286\n",
      "epoch: 3 step: 922, loss is 0.3023183047771454\n",
      "epoch: 3 step: 923, loss is 0.3125321865081787\n",
      "epoch: 3 step: 924, loss is 0.2854313552379608\n",
      "epoch: 3 step: 925, loss is 0.26348885893821716\n",
      "epoch: 3 step: 926, loss is 0.19524037837982178\n",
      "epoch: 3 step: 927, loss is 0.3533034026622772\n",
      "epoch: 3 step: 928, loss is 0.2470637410879135\n",
      "epoch: 3 step: 929, loss is 0.3005310297012329\n",
      "epoch: 3 step: 930, loss is 0.1897764950990677\n",
      "epoch: 3 step: 931, loss is 0.2893175482749939\n",
      "epoch: 3 step: 932, loss is 0.24794477224349976\n",
      "epoch: 3 step: 933, loss is 0.16564631462097168\n",
      "epoch: 3 step: 934, loss is 0.3224797248840332\n",
      "epoch: 3 step: 935, loss is 0.2214306890964508\n",
      "epoch: 3 step: 936, loss is 0.18935444951057434\n",
      "epoch: 3 step: 937, loss is 0.2208966314792633\n",
      "epoch: 4 step: 1, loss is 0.06010613590478897\n",
      "epoch: 4 step: 2, loss is 0.1406460404396057\n",
      "epoch: 4 step: 3, loss is 0.2733844220638275\n",
      "epoch: 4 step: 4, loss is 0.20176437497138977\n",
      "epoch: 4 step: 5, loss is 0.19272682070732117\n",
      "epoch: 4 step: 6, loss is 0.20047402381896973\n",
      "epoch: 4 step: 7, loss is 0.07835683971643448\n",
      "epoch: 4 step: 8, loss is 0.22377049922943115\n",
      "epoch: 4 step: 9, loss is 0.21353118121623993\n",
      "epoch: 4 step: 10, loss is 0.14953464269638062\n",
      "epoch: 4 step: 11, loss is 0.19639238715171814\n",
      "epoch: 4 step: 12, loss is 0.21405574679374695\n",
      "epoch: 4 step: 13, loss is 0.09317800402641296\n",
      "epoch: 4 step: 14, loss is 0.16209493577480316\n",
      "epoch: 4 step: 15, loss is 0.2783345580101013\n",
      "epoch: 4 step: 16, loss is 0.13690410554409027\n",
      "epoch: 4 step: 17, loss is 0.251149982213974\n",
      "epoch: 4 step: 18, loss is 0.24406252801418304\n",
      "epoch: 4 step: 19, loss is 0.23100732266902924\n",
      "epoch: 4 step: 20, loss is 0.14184337854385376\n",
      "epoch: 4 step: 21, loss is 0.13823115825653076\n",
      "epoch: 4 step: 22, loss is 0.0862138494849205\n",
      "epoch: 4 step: 23, loss is 0.24135898053646088\n",
      "epoch: 4 step: 24, loss is 0.08485276252031326\n",
      "epoch: 4 step: 25, loss is 0.2987993061542511\n",
      "epoch: 4 step: 26, loss is 0.13825717568397522\n",
      "epoch: 4 step: 27, loss is 0.26777398586273193\n",
      "epoch: 4 step: 28, loss is 0.1857946217060089\n",
      "epoch: 4 step: 29, loss is 0.2258991003036499\n",
      "epoch: 4 step: 30, loss is 0.2273338884115219\n",
      "epoch: 4 step: 31, loss is 0.16123837232589722\n",
      "epoch: 4 step: 32, loss is 0.2759401500225067\n",
      "epoch: 4 step: 33, loss is 0.2267346829175949\n",
      "epoch: 4 step: 34, loss is 0.12664349377155304\n",
      "epoch: 4 step: 35, loss is 0.20420676469802856\n",
      "epoch: 4 step: 36, loss is 0.26720768213272095\n",
      "epoch: 4 step: 37, loss is 0.18126438558101654\n",
      "epoch: 4 step: 38, loss is 0.27323123812675476\n",
      "epoch: 4 step: 39, loss is 0.11730580776929855\n",
      "epoch: 4 step: 40, loss is 0.2844417691230774\n",
      "epoch: 4 step: 41, loss is 0.128192737698555\n",
      "epoch: 4 step: 42, loss is 0.14053140580654144\n",
      "epoch: 4 step: 43, loss is 0.1405271738767624\n",
      "epoch: 4 step: 44, loss is 0.2622280418872833\n",
      "epoch: 4 step: 45, loss is 0.22009705007076263\n",
      "epoch: 4 step: 46, loss is 0.31036999821662903\n",
      "epoch: 4 step: 47, loss is 0.17463351786136627\n",
      "epoch: 4 step: 48, loss is 0.17075715959072113\n",
      "epoch: 4 step: 49, loss is 0.3329656720161438\n",
      "epoch: 4 step: 50, loss is 0.16935479640960693\n",
      "epoch: 4 step: 51, loss is 0.22933195531368256\n",
      "epoch: 4 step: 52, loss is 0.2621649205684662\n",
      "epoch: 4 step: 53, loss is 0.2857764661312103\n",
      "epoch: 4 step: 54, loss is 0.22704489529132843\n",
      "epoch: 4 step: 55, loss is 0.28181251883506775\n",
      "epoch: 4 step: 56, loss is 0.2748350501060486\n",
      "epoch: 4 step: 57, loss is 0.21548932790756226\n",
      "epoch: 4 step: 58, loss is 0.12690389156341553\n",
      "epoch: 4 step: 59, loss is 0.14876751601696014\n",
      "epoch: 4 step: 60, loss is 0.0579925961792469\n",
      "epoch: 4 step: 61, loss is 0.11342258751392365\n",
      "epoch: 4 step: 62, loss is 0.2244299352169037\n",
      "epoch: 4 step: 63, loss is 0.13875755667686462\n",
      "epoch: 4 step: 64, loss is 0.22773046791553497\n",
      "epoch: 4 step: 65, loss is 0.15650317072868347\n",
      "epoch: 4 step: 66, loss is 0.10961637645959854\n",
      "epoch: 4 step: 67, loss is 0.1716611385345459\n",
      "epoch: 4 step: 68, loss is 0.22370083630084991\n",
      "epoch: 4 step: 69, loss is 0.29167303442955017\n",
      "epoch: 4 step: 70, loss is 0.19002653658390045\n",
      "epoch: 4 step: 71, loss is 0.13815255463123322\n",
      "epoch: 4 step: 72, loss is 0.19415956735610962\n",
      "epoch: 4 step: 73, loss is 0.14233145117759705\n",
      "epoch: 4 step: 74, loss is 0.1936665028333664\n",
      "epoch: 4 step: 75, loss is 0.24537187814712524\n",
      "epoch: 4 step: 76, loss is 0.18845607340335846\n",
      "epoch: 4 step: 77, loss is 0.11751857399940491\n",
      "epoch: 4 step: 78, loss is 0.07501862198114395\n",
      "epoch: 4 step: 79, loss is 0.2512776553630829\n",
      "epoch: 4 step: 80, loss is 0.2765304744243622\n",
      "epoch: 4 step: 81, loss is 0.23870313167572021\n",
      "epoch: 4 step: 82, loss is 0.3817097544670105\n",
      "epoch: 4 step: 83, loss is 0.3810823857784271\n",
      "epoch: 4 step: 84, loss is 0.15333929657936096\n",
      "epoch: 4 step: 85, loss is 0.19717718660831451\n",
      "epoch: 4 step: 86, loss is 0.1199113056063652\n",
      "epoch: 4 step: 87, loss is 0.22902925312519073\n",
      "epoch: 4 step: 88, loss is 0.27793484926223755\n",
      "epoch: 4 step: 89, loss is 0.11225321888923645\n",
      "epoch: 4 step: 90, loss is 0.290569931268692\n",
      "epoch: 4 step: 91, loss is 0.23326824605464935\n",
      "epoch: 4 step: 92, loss is 0.13287796080112457\n",
      "epoch: 4 step: 93, loss is 0.16842474043369293\n",
      "epoch: 4 step: 94, loss is 0.14229290187358856\n",
      "epoch: 4 step: 95, loss is 0.3093254566192627\n",
      "epoch: 4 step: 96, loss is 0.13412904739379883\n",
      "epoch: 4 step: 97, loss is 0.1957007646560669\n",
      "epoch: 4 step: 98, loss is 0.21240921318531036\n",
      "epoch: 4 step: 99, loss is 0.208894744515419\n",
      "epoch: 4 step: 100, loss is 0.31015193462371826\n",
      "epoch: 4 step: 101, loss is 0.19394148886203766\n",
      "epoch: 4 step: 102, loss is 0.13604706525802612\n",
      "epoch: 4 step: 103, loss is 0.14546963572502136\n",
      "epoch: 4 step: 104, loss is 0.18784622848033905\n",
      "epoch: 4 step: 105, loss is 0.07620028406381607\n",
      "epoch: 4 step: 106, loss is 0.203113853931427\n",
      "epoch: 4 step: 107, loss is 0.26039791107177734\n",
      "epoch: 4 step: 108, loss is 0.2799556851387024\n",
      "epoch: 4 step: 109, loss is 0.1829676777124405\n",
      "epoch: 4 step: 110, loss is 0.08958063274621964\n",
      "epoch: 4 step: 111, loss is 0.25284460186958313\n",
      "epoch: 4 step: 112, loss is 0.16454294323921204\n",
      "epoch: 4 step: 113, loss is 0.22800016403198242\n",
      "epoch: 4 step: 114, loss is 0.21492663025856018\n",
      "epoch: 4 step: 115, loss is 0.12197673320770264\n",
      "epoch: 4 step: 116, loss is 0.09832318872213364\n",
      "epoch: 4 step: 117, loss is 0.18577465415000916\n",
      "epoch: 4 step: 118, loss is 0.2811255156993866\n",
      "epoch: 4 step: 119, loss is 0.19039402902126312\n",
      "epoch: 4 step: 120, loss is 0.16437320411205292\n",
      "epoch: 4 step: 121, loss is 0.1815890222787857\n",
      "epoch: 4 step: 122, loss is 0.30339157581329346\n",
      "epoch: 4 step: 123, loss is 0.2113865464925766\n",
      "epoch: 4 step: 124, loss is 0.21693678200244904\n",
      "epoch: 4 step: 125, loss is 0.23645250499248505\n",
      "epoch: 4 step: 126, loss is 0.16496524214744568\n",
      "epoch: 4 step: 127, loss is 0.24240504205226898\n",
      "epoch: 4 step: 128, loss is 0.15938211977481842\n",
      "epoch: 4 step: 129, loss is 0.2624080777168274\n",
      "epoch: 4 step: 130, loss is 0.2923690974712372\n",
      "epoch: 4 step: 131, loss is 0.28122928738594055\n",
      "epoch: 4 step: 132, loss is 0.21477025747299194\n",
      "epoch: 4 step: 133, loss is 0.1728275865316391\n",
      "epoch: 4 step: 134, loss is 0.3128340542316437\n",
      "epoch: 4 step: 135, loss is 0.13218317925930023\n",
      "epoch: 4 step: 136, loss is 0.31732621788978577\n",
      "epoch: 4 step: 137, loss is 0.2867320775985718\n",
      "epoch: 4 step: 138, loss is 0.24244453012943268\n",
      "epoch: 4 step: 139, loss is 0.27474063634872437\n",
      "epoch: 4 step: 140, loss is 0.36646655201911926\n",
      "epoch: 4 step: 141, loss is 0.21452361345291138\n",
      "epoch: 4 step: 142, loss is 0.18107004463672638\n",
      "epoch: 4 step: 143, loss is 0.14423589408397675\n",
      "epoch: 4 step: 144, loss is 0.17350541055202484\n",
      "epoch: 4 step: 145, loss is 0.24611882865428925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 146, loss is 0.285091370344162\n",
      "epoch: 4 step: 147, loss is 0.19317883253097534\n",
      "epoch: 4 step: 148, loss is 0.24223749339580536\n",
      "epoch: 4 step: 149, loss is 0.1205291897058487\n",
      "epoch: 4 step: 150, loss is 0.20957373082637787\n",
      "epoch: 4 step: 151, loss is 0.1998528093099594\n",
      "epoch: 4 step: 152, loss is 0.1755398064851761\n",
      "epoch: 4 step: 153, loss is 0.25579991936683655\n",
      "epoch: 4 step: 154, loss is 0.10351505130529404\n",
      "epoch: 4 step: 155, loss is 0.27954018115997314\n",
      "epoch: 4 step: 156, loss is 0.2014296054840088\n",
      "epoch: 4 step: 157, loss is 0.19088004529476166\n",
      "epoch: 4 step: 158, loss is 0.17715556919574738\n",
      "epoch: 4 step: 159, loss is 0.16985094547271729\n",
      "epoch: 4 step: 160, loss is 0.21540971100330353\n",
      "epoch: 4 step: 161, loss is 0.1735011786222458\n",
      "epoch: 4 step: 162, loss is 0.1982579082250595\n",
      "epoch: 4 step: 163, loss is 0.2123638093471527\n",
      "epoch: 4 step: 164, loss is 0.07154121994972229\n",
      "epoch: 4 step: 165, loss is 0.3503755033016205\n",
      "epoch: 4 step: 166, loss is 0.17125503718852997\n",
      "epoch: 4 step: 167, loss is 0.2144339233636856\n",
      "epoch: 4 step: 168, loss is 0.22882740199565887\n",
      "epoch: 4 step: 169, loss is 0.17471741139888763\n",
      "epoch: 4 step: 170, loss is 0.1640189290046692\n",
      "epoch: 4 step: 171, loss is 0.15931671857833862\n",
      "epoch: 4 step: 172, loss is 0.3102726638317108\n",
      "epoch: 4 step: 173, loss is 0.1992359310388565\n",
      "epoch: 4 step: 174, loss is 0.11339191347360611\n",
      "epoch: 4 step: 175, loss is 0.15913346409797668\n",
      "epoch: 4 step: 176, loss is 0.18186171352863312\n",
      "epoch: 4 step: 177, loss is 0.17163541913032532\n",
      "epoch: 4 step: 178, loss is 0.4244673550128937\n",
      "epoch: 4 step: 179, loss is 0.2564326524734497\n",
      "epoch: 4 step: 180, loss is 0.2638167142868042\n",
      "epoch: 4 step: 181, loss is 0.08848150819540024\n",
      "epoch: 4 step: 182, loss is 0.16678090393543243\n",
      "epoch: 4 step: 183, loss is 0.18534480035305023\n",
      "epoch: 4 step: 184, loss is 0.16897520422935486\n",
      "epoch: 4 step: 185, loss is 0.19764092564582825\n",
      "epoch: 4 step: 186, loss is 0.2375507354736328\n",
      "epoch: 4 step: 187, loss is 0.2689647078514099\n",
      "epoch: 4 step: 188, loss is 0.2905275821685791\n",
      "epoch: 4 step: 189, loss is 0.14558525383472443\n",
      "epoch: 4 step: 190, loss is 0.15160612761974335\n",
      "epoch: 4 step: 191, loss is 0.3176427483558655\n",
      "epoch: 4 step: 192, loss is 0.09678176045417786\n",
      "epoch: 4 step: 193, loss is 0.2608143985271454\n",
      "epoch: 4 step: 194, loss is 0.2156519889831543\n",
      "epoch: 4 step: 195, loss is 0.19268523156642914\n",
      "epoch: 4 step: 196, loss is 0.1897362619638443\n",
      "epoch: 4 step: 197, loss is 0.2696759104728699\n",
      "epoch: 4 step: 198, loss is 0.16999652981758118\n",
      "epoch: 4 step: 199, loss is 0.2986529767513275\n",
      "epoch: 4 step: 200, loss is 0.20067276060581207\n",
      "epoch: 4 step: 201, loss is 0.21383287012577057\n",
      "epoch: 4 step: 202, loss is 0.16580738127231598\n",
      "epoch: 4 step: 203, loss is 0.13101905584335327\n",
      "epoch: 4 step: 204, loss is 0.18399429321289062\n",
      "epoch: 4 step: 205, loss is 0.3980695605278015\n",
      "epoch: 4 step: 206, loss is 0.27974098920822144\n",
      "epoch: 4 step: 207, loss is 0.1477680802345276\n",
      "epoch: 4 step: 208, loss is 0.28911158442497253\n",
      "epoch: 4 step: 209, loss is 0.14430223405361176\n",
      "epoch: 4 step: 210, loss is 0.1637643575668335\n",
      "epoch: 4 step: 211, loss is 0.23956048488616943\n",
      "epoch: 4 step: 212, loss is 0.19149747490882874\n",
      "epoch: 4 step: 213, loss is 0.2185841202735901\n",
      "epoch: 4 step: 214, loss is 0.15522581338882446\n",
      "epoch: 4 step: 215, loss is 0.04209978133440018\n",
      "epoch: 4 step: 216, loss is 0.2898246943950653\n",
      "epoch: 4 step: 217, loss is 0.12284255772829056\n",
      "epoch: 4 step: 218, loss is 0.18942539393901825\n",
      "epoch: 4 step: 219, loss is 0.33714741468429565\n",
      "epoch: 4 step: 220, loss is 0.1418713629245758\n",
      "epoch: 4 step: 221, loss is 0.22568778693675995\n",
      "epoch: 4 step: 222, loss is 0.17109981179237366\n",
      "epoch: 4 step: 223, loss is 0.2360769808292389\n",
      "epoch: 4 step: 224, loss is 0.179386705160141\n",
      "epoch: 4 step: 225, loss is 0.24377867579460144\n",
      "epoch: 4 step: 226, loss is 0.24545907974243164\n",
      "epoch: 4 step: 227, loss is 0.35002321004867554\n",
      "epoch: 4 step: 228, loss is 0.15926048159599304\n",
      "epoch: 4 step: 229, loss is 0.10202395915985107\n",
      "epoch: 4 step: 230, loss is 0.14681528508663177\n",
      "epoch: 4 step: 231, loss is 0.275419682264328\n",
      "epoch: 4 step: 232, loss is 0.18667817115783691\n",
      "epoch: 4 step: 233, loss is 0.15518806874752045\n",
      "epoch: 4 step: 234, loss is 0.3716355264186859\n",
      "epoch: 4 step: 235, loss is 0.1530192643404007\n",
      "epoch: 4 step: 236, loss is 0.3024924099445343\n",
      "epoch: 4 step: 237, loss is 0.25511467456817627\n",
      "epoch: 4 step: 238, loss is 0.08610708266496658\n",
      "epoch: 4 step: 239, loss is 0.21142473816871643\n",
      "epoch: 4 step: 240, loss is 0.19221103191375732\n",
      "epoch: 4 step: 241, loss is 0.23334147036075592\n",
      "epoch: 4 step: 242, loss is 0.12955878674983978\n",
      "epoch: 4 step: 243, loss is 0.18362781405448914\n",
      "epoch: 4 step: 244, loss is 0.15797865390777588\n",
      "epoch: 4 step: 245, loss is 0.16291433572769165\n",
      "epoch: 4 step: 246, loss is 0.23078320920467377\n",
      "epoch: 4 step: 247, loss is 0.09844970703125\n",
      "epoch: 4 step: 248, loss is 0.09977146238088608\n",
      "epoch: 4 step: 249, loss is 0.18684059381484985\n",
      "epoch: 4 step: 250, loss is 0.36628273129463196\n",
      "epoch: 4 step: 251, loss is 0.06932496279478073\n",
      "epoch: 4 step: 252, loss is 0.23067747056484222\n",
      "epoch: 4 step: 253, loss is 0.20926089584827423\n",
      "epoch: 4 step: 254, loss is 0.2611849308013916\n",
      "epoch: 4 step: 255, loss is 0.19961681962013245\n",
      "epoch: 4 step: 256, loss is 0.11562792211771011\n",
      "epoch: 4 step: 257, loss is 0.43853336572647095\n",
      "epoch: 4 step: 258, loss is 0.07633228600025177\n",
      "epoch: 4 step: 259, loss is 0.1932455599308014\n",
      "epoch: 4 step: 260, loss is 0.3114332854747772\n",
      "epoch: 4 step: 261, loss is 0.20273981988430023\n",
      "epoch: 4 step: 262, loss is 0.2527388632297516\n",
      "epoch: 4 step: 263, loss is 0.17289625108242035\n",
      "epoch: 4 step: 264, loss is 0.19969964027404785\n",
      "epoch: 4 step: 265, loss is 0.21550090610980988\n",
      "epoch: 4 step: 266, loss is 0.29435741901397705\n",
      "epoch: 4 step: 267, loss is 0.21323977410793304\n",
      "epoch: 4 step: 268, loss is 0.14449475705623627\n",
      "epoch: 4 step: 269, loss is 0.11740158498287201\n",
      "epoch: 4 step: 270, loss is 0.24603962898254395\n",
      "epoch: 4 step: 271, loss is 0.14541319012641907\n",
      "epoch: 4 step: 272, loss is 0.24572846293449402\n",
      "epoch: 4 step: 273, loss is 0.08055724948644638\n",
      "epoch: 4 step: 274, loss is 0.15747758746147156\n",
      "epoch: 4 step: 275, loss is 0.2423182725906372\n",
      "epoch: 4 step: 276, loss is 0.2965313792228699\n",
      "epoch: 4 step: 277, loss is 0.2931547164916992\n",
      "epoch: 4 step: 278, loss is 0.16913555562496185\n",
      "epoch: 4 step: 279, loss is 0.20400452613830566\n",
      "epoch: 4 step: 280, loss is 0.1617266833782196\n",
      "epoch: 4 step: 281, loss is 0.2668263614177704\n",
      "epoch: 4 step: 282, loss is 0.237810879945755\n",
      "epoch: 4 step: 283, loss is 0.13528548181056976\n",
      "epoch: 4 step: 284, loss is 0.24234968423843384\n",
      "epoch: 4 step: 285, loss is 0.30444273352622986\n",
      "epoch: 4 step: 286, loss is 0.22188709676265717\n",
      "epoch: 4 step: 287, loss is 0.3146750032901764\n",
      "epoch: 4 step: 288, loss is 0.2152419090270996\n",
      "epoch: 4 step: 289, loss is 0.2814555764198303\n",
      "epoch: 4 step: 290, loss is 0.21608632802963257\n",
      "epoch: 4 step: 291, loss is 0.12910102307796478\n",
      "epoch: 4 step: 292, loss is 0.3450978696346283\n",
      "epoch: 4 step: 293, loss is 0.14075873792171478\n",
      "epoch: 4 step: 294, loss is 0.0838344544172287\n",
      "epoch: 4 step: 295, loss is 0.1697261929512024\n",
      "epoch: 4 step: 296, loss is 0.17600910365581512\n",
      "epoch: 4 step: 297, loss is 0.1073995977640152\n",
      "epoch: 4 step: 298, loss is 0.2098599076271057\n",
      "epoch: 4 step: 299, loss is 0.2252243012189865\n",
      "epoch: 4 step: 300, loss is 0.27646058797836304\n",
      "epoch: 4 step: 301, loss is 0.22222642600536346\n",
      "epoch: 4 step: 302, loss is 0.11613910645246506\n",
      "epoch: 4 step: 303, loss is 0.19984070956707\n",
      "epoch: 4 step: 304, loss is 0.23647214472293854\n",
      "epoch: 4 step: 305, loss is 0.19485263526439667\n",
      "epoch: 4 step: 306, loss is 0.16269749402999878\n",
      "epoch: 4 step: 307, loss is 0.060085229575634\n",
      "epoch: 4 step: 308, loss is 0.17103458940982819\n",
      "epoch: 4 step: 309, loss is 0.14468978345394135\n",
      "epoch: 4 step: 310, loss is 0.2664982080459595\n",
      "epoch: 4 step: 311, loss is 0.24404598772525787\n",
      "epoch: 4 step: 312, loss is 0.2645828425884247\n",
      "epoch: 4 step: 313, loss is 0.18785688281059265\n",
      "epoch: 4 step: 314, loss is 0.06515355408191681\n",
      "epoch: 4 step: 315, loss is 0.23468638956546783\n",
      "epoch: 4 step: 316, loss is 0.1551390141248703\n",
      "epoch: 4 step: 317, loss is 0.19723118841648102\n",
      "epoch: 4 step: 318, loss is 0.23708447813987732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 319, loss is 0.3156700134277344\n",
      "epoch: 4 step: 320, loss is 0.2344241440296173\n",
      "epoch: 4 step: 321, loss is 0.3218613862991333\n",
      "epoch: 4 step: 322, loss is 0.18648971617221832\n",
      "epoch: 4 step: 323, loss is 0.10495571792125702\n",
      "epoch: 4 step: 324, loss is 0.3898410499095917\n",
      "epoch: 4 step: 325, loss is 0.30378174781799316\n",
      "epoch: 4 step: 326, loss is 0.13433869183063507\n",
      "epoch: 4 step: 327, loss is 0.4566611349582672\n",
      "epoch: 4 step: 328, loss is 0.19467170536518097\n",
      "epoch: 4 step: 329, loss is 0.16284415125846863\n",
      "epoch: 4 step: 330, loss is 0.19389913976192474\n",
      "epoch: 4 step: 331, loss is 0.4073903560638428\n",
      "epoch: 4 step: 332, loss is 0.11050797998905182\n",
      "epoch: 4 step: 333, loss is 0.17426100373268127\n",
      "epoch: 4 step: 334, loss is 0.12917695939540863\n",
      "epoch: 4 step: 335, loss is 0.27943214774131775\n",
      "epoch: 4 step: 336, loss is 0.17512641847133636\n",
      "epoch: 4 step: 337, loss is 0.18912799656391144\n",
      "epoch: 4 step: 338, loss is 0.16099822521209717\n",
      "epoch: 4 step: 339, loss is 0.27269527316093445\n",
      "epoch: 4 step: 340, loss is 0.32954323291778564\n",
      "epoch: 4 step: 341, loss is 0.17242664098739624\n",
      "epoch: 4 step: 342, loss is 0.20643648505210876\n",
      "epoch: 4 step: 343, loss is 0.33514049649238586\n",
      "epoch: 4 step: 344, loss is 0.1472255438566208\n",
      "epoch: 4 step: 345, loss is 0.5129839181900024\n",
      "epoch: 4 step: 346, loss is 0.14229953289031982\n",
      "epoch: 4 step: 347, loss is 0.20945647358894348\n",
      "epoch: 4 step: 348, loss is 0.10431070625782013\n",
      "epoch: 4 step: 349, loss is 0.1800266057252884\n",
      "epoch: 4 step: 350, loss is 0.16980034112930298\n",
      "epoch: 4 step: 351, loss is 0.3152255415916443\n",
      "epoch: 4 step: 352, loss is 0.26985105872154236\n",
      "epoch: 4 step: 353, loss is 0.24128392338752747\n",
      "epoch: 4 step: 354, loss is 0.19809994101524353\n",
      "epoch: 4 step: 355, loss is 0.17296527326107025\n",
      "epoch: 4 step: 356, loss is 0.14153309166431427\n",
      "epoch: 4 step: 357, loss is 0.2859119474887848\n",
      "epoch: 4 step: 358, loss is 0.14980947971343994\n",
      "epoch: 4 step: 359, loss is 0.18631908297538757\n",
      "epoch: 4 step: 360, loss is 0.18845215439796448\n",
      "epoch: 4 step: 361, loss is 0.3462337851524353\n",
      "epoch: 4 step: 362, loss is 0.17380033433437347\n",
      "epoch: 4 step: 363, loss is 0.15693272650241852\n",
      "epoch: 4 step: 364, loss is 0.12421420216560364\n",
      "epoch: 4 step: 365, loss is 0.2704693078994751\n",
      "epoch: 4 step: 366, loss is 0.16846992075443268\n",
      "epoch: 4 step: 367, loss is 0.08903127908706665\n",
      "epoch: 4 step: 368, loss is 0.07948598265647888\n",
      "epoch: 4 step: 369, loss is 0.23187360167503357\n",
      "epoch: 4 step: 370, loss is 0.2631179094314575\n",
      "epoch: 4 step: 371, loss is 0.2556951642036438\n",
      "epoch: 4 step: 372, loss is 0.2727656364440918\n",
      "epoch: 4 step: 373, loss is 0.2269778996706009\n",
      "epoch: 4 step: 374, loss is 0.21774287521839142\n",
      "epoch: 4 step: 375, loss is 0.24563835561275482\n",
      "epoch: 4 step: 376, loss is 0.10941886156797409\n",
      "epoch: 4 step: 377, loss is 0.3475915491580963\n",
      "epoch: 4 step: 378, loss is 0.29457888007164\n",
      "epoch: 4 step: 379, loss is 0.096702940762043\n",
      "epoch: 4 step: 380, loss is 0.18777039647102356\n",
      "epoch: 4 step: 381, loss is 0.090288907289505\n",
      "epoch: 4 step: 382, loss is 0.35220590233802795\n",
      "epoch: 4 step: 383, loss is 0.13313761353492737\n",
      "epoch: 4 step: 384, loss is 0.2320713996887207\n",
      "epoch: 4 step: 385, loss is 0.10438740253448486\n",
      "epoch: 4 step: 386, loss is 0.24841876327991486\n",
      "epoch: 4 step: 387, loss is 0.21656052768230438\n",
      "epoch: 4 step: 388, loss is 0.27729493379592896\n",
      "epoch: 4 step: 389, loss is 0.20393995940685272\n",
      "epoch: 4 step: 390, loss is 0.16323548555374146\n",
      "epoch: 4 step: 391, loss is 0.1787756234407425\n",
      "epoch: 4 step: 392, loss is 0.17147362232208252\n",
      "epoch: 4 step: 393, loss is 0.0962892398238182\n",
      "epoch: 4 step: 394, loss is 0.15430676937103271\n",
      "epoch: 4 step: 395, loss is 0.06772524118423462\n",
      "epoch: 4 step: 396, loss is 0.13328231871128082\n",
      "epoch: 4 step: 397, loss is 0.15671133995056152\n",
      "epoch: 4 step: 398, loss is 0.16984640061855316\n",
      "epoch: 4 step: 399, loss is 0.19701960682868958\n",
      "epoch: 4 step: 400, loss is 0.334369033575058\n",
      "epoch: 4 step: 401, loss is 0.25785863399505615\n",
      "epoch: 4 step: 402, loss is 0.2232486456632614\n",
      "epoch: 4 step: 403, loss is 0.22857676446437836\n",
      "epoch: 4 step: 404, loss is 0.21715553104877472\n",
      "epoch: 4 step: 405, loss is 0.1862763911485672\n",
      "epoch: 4 step: 406, loss is 0.22869953513145447\n",
      "epoch: 4 step: 407, loss is 0.23373572528362274\n",
      "epoch: 4 step: 408, loss is 0.3848269283771515\n",
      "epoch: 4 step: 409, loss is 0.18512402474880219\n",
      "epoch: 4 step: 410, loss is 0.20681095123291016\n",
      "epoch: 4 step: 411, loss is 0.1778498739004135\n",
      "epoch: 4 step: 412, loss is 0.13380832970142365\n",
      "epoch: 4 step: 413, loss is 0.26133492588996887\n",
      "epoch: 4 step: 414, loss is 0.10412485152482986\n",
      "epoch: 4 step: 415, loss is 0.21086710691452026\n",
      "epoch: 4 step: 416, loss is 0.2956068515777588\n",
      "epoch: 4 step: 417, loss is 0.38450416922569275\n",
      "epoch: 4 step: 418, loss is 0.299992173910141\n",
      "epoch: 4 step: 419, loss is 0.24753642082214355\n",
      "epoch: 4 step: 420, loss is 0.13149650394916534\n",
      "epoch: 4 step: 421, loss is 0.18587684631347656\n",
      "epoch: 4 step: 422, loss is 0.18250104784965515\n",
      "epoch: 4 step: 423, loss is 0.2914804220199585\n",
      "epoch: 4 step: 424, loss is 0.1715143769979477\n",
      "epoch: 4 step: 425, loss is 0.18424876034259796\n",
      "epoch: 4 step: 426, loss is 0.19002848863601685\n",
      "epoch: 4 step: 427, loss is 0.2607271075248718\n",
      "epoch: 4 step: 428, loss is 0.1826106309890747\n",
      "epoch: 4 step: 429, loss is 0.20721325278282166\n",
      "epoch: 4 step: 430, loss is 0.1664959043264389\n",
      "epoch: 4 step: 431, loss is 0.20070868730545044\n",
      "epoch: 4 step: 432, loss is 0.4039892554283142\n",
      "epoch: 4 step: 433, loss is 0.2327936440706253\n",
      "epoch: 4 step: 434, loss is 0.10719796270132065\n",
      "epoch: 4 step: 435, loss is 0.2761934697628021\n",
      "epoch: 4 step: 436, loss is 0.16049790382385254\n",
      "epoch: 4 step: 437, loss is 0.21579082310199738\n",
      "epoch: 4 step: 438, loss is 0.17799602448940277\n",
      "epoch: 4 step: 439, loss is 0.14489033818244934\n",
      "epoch: 4 step: 440, loss is 0.16315244138240814\n",
      "epoch: 4 step: 441, loss is 0.2261725515127182\n",
      "epoch: 4 step: 442, loss is 0.2593563199043274\n",
      "epoch: 4 step: 443, loss is 0.1517857164144516\n",
      "epoch: 4 step: 444, loss is 0.3325009047985077\n",
      "epoch: 4 step: 445, loss is 0.2286306619644165\n",
      "epoch: 4 step: 446, loss is 0.12969142198562622\n",
      "epoch: 4 step: 447, loss is 0.10502801835536957\n",
      "epoch: 4 step: 448, loss is 0.12277474999427795\n",
      "epoch: 4 step: 449, loss is 0.10837863385677338\n",
      "epoch: 4 step: 450, loss is 0.15412738919258118\n",
      "epoch: 4 step: 451, loss is 0.13795435428619385\n",
      "epoch: 4 step: 452, loss is 0.2304805964231491\n",
      "epoch: 4 step: 453, loss is 0.18454806506633759\n",
      "epoch: 4 step: 454, loss is 0.18721017241477966\n",
      "epoch: 4 step: 455, loss is 0.31810012459754944\n",
      "epoch: 4 step: 456, loss is 0.2162742018699646\n",
      "epoch: 4 step: 457, loss is 0.18038377165794373\n",
      "epoch: 4 step: 458, loss is 0.2650984227657318\n",
      "epoch: 4 step: 459, loss is 0.1816350221633911\n",
      "epoch: 4 step: 460, loss is 0.3033680319786072\n",
      "epoch: 4 step: 461, loss is 0.19100408256053925\n",
      "epoch: 4 step: 462, loss is 0.18408985435962677\n",
      "epoch: 4 step: 463, loss is 0.19883112609386444\n",
      "epoch: 4 step: 464, loss is 0.12169962376356125\n",
      "epoch: 4 step: 465, loss is 0.3767460882663727\n",
      "epoch: 4 step: 466, loss is 0.16565287113189697\n",
      "epoch: 4 step: 467, loss is 0.10810821503400803\n",
      "epoch: 4 step: 468, loss is 0.17459776997566223\n",
      "epoch: 4 step: 469, loss is 0.24636681377887726\n",
      "epoch: 4 step: 470, loss is 0.11380770802497864\n",
      "epoch: 4 step: 471, loss is 0.11091557145118713\n",
      "epoch: 4 step: 472, loss is 0.17456862330436707\n",
      "epoch: 4 step: 473, loss is 0.1779124140739441\n",
      "epoch: 4 step: 474, loss is 0.39325764775276184\n",
      "epoch: 4 step: 475, loss is 0.23767536878585815\n",
      "epoch: 4 step: 476, loss is 0.17694221436977386\n",
      "epoch: 4 step: 477, loss is 0.19678938388824463\n",
      "epoch: 4 step: 478, loss is 0.2211681604385376\n",
      "epoch: 4 step: 479, loss is 0.19230139255523682\n",
      "epoch: 4 step: 480, loss is 0.12368203699588776\n",
      "epoch: 4 step: 481, loss is 0.34404256939888\n",
      "epoch: 4 step: 482, loss is 0.11062078922986984\n",
      "epoch: 4 step: 483, loss is 0.3173096776008606\n",
      "epoch: 4 step: 484, loss is 0.1453920602798462\n",
      "epoch: 4 step: 485, loss is 0.23896880447864532\n",
      "epoch: 4 step: 486, loss is 0.18803110718727112\n",
      "epoch: 4 step: 487, loss is 0.18787719309329987\n",
      "epoch: 4 step: 488, loss is 0.20897363126277924\n",
      "epoch: 4 step: 489, loss is 0.17512750625610352\n",
      "epoch: 4 step: 490, loss is 0.1522306501865387\n",
      "epoch: 4 step: 491, loss is 0.10360290110111237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 492, loss is 0.36978819966316223\n",
      "epoch: 4 step: 493, loss is 0.343303918838501\n",
      "epoch: 4 step: 494, loss is 0.10614243149757385\n",
      "epoch: 4 step: 495, loss is 0.16009612381458282\n",
      "epoch: 4 step: 496, loss is 0.24185900390148163\n",
      "epoch: 4 step: 497, loss is 0.1942409873008728\n",
      "epoch: 4 step: 498, loss is 0.2206696718931198\n",
      "epoch: 4 step: 499, loss is 0.150003582239151\n",
      "epoch: 4 step: 500, loss is 0.1625824123620987\n",
      "epoch: 4 step: 501, loss is 0.1205221638083458\n",
      "epoch: 4 step: 502, loss is 0.19399394094944\n",
      "epoch: 4 step: 503, loss is 0.15821655094623566\n",
      "epoch: 4 step: 504, loss is 0.2796790301799774\n",
      "epoch: 4 step: 505, loss is 0.2821316421031952\n",
      "epoch: 4 step: 506, loss is 0.35396498441696167\n",
      "epoch: 4 step: 507, loss is 0.1351230889558792\n",
      "epoch: 4 step: 508, loss is 0.21638129651546478\n",
      "epoch: 4 step: 509, loss is 0.21501895785331726\n",
      "epoch: 4 step: 510, loss is 0.4482504427433014\n",
      "epoch: 4 step: 511, loss is 0.07659091055393219\n",
      "epoch: 4 step: 512, loss is 0.25059542059898376\n",
      "epoch: 4 step: 513, loss is 0.1607351154088974\n",
      "epoch: 4 step: 514, loss is 0.2052379846572876\n",
      "epoch: 4 step: 515, loss is 0.27350667119026184\n",
      "epoch: 4 step: 516, loss is 0.21775513887405396\n",
      "epoch: 4 step: 517, loss is 0.30995628237724304\n",
      "epoch: 4 step: 518, loss is 0.22943760454654694\n",
      "epoch: 4 step: 519, loss is 0.20427054166793823\n",
      "epoch: 4 step: 520, loss is 0.11431808024644852\n",
      "epoch: 4 step: 521, loss is 0.19468893110752106\n",
      "epoch: 4 step: 522, loss is 0.22225521504878998\n",
      "epoch: 4 step: 523, loss is 0.37269625067710876\n",
      "epoch: 4 step: 524, loss is 0.11548265069723129\n",
      "epoch: 4 step: 525, loss is 0.193094864487648\n",
      "epoch: 4 step: 526, loss is 0.18149542808532715\n",
      "epoch: 4 step: 527, loss is 0.2598915994167328\n",
      "epoch: 4 step: 528, loss is 0.2959282100200653\n",
      "epoch: 4 step: 529, loss is 0.08424728363752365\n",
      "epoch: 4 step: 530, loss is 0.15847209095954895\n",
      "epoch: 4 step: 531, loss is 0.4471704959869385\n",
      "epoch: 4 step: 532, loss is 0.16415688395500183\n",
      "epoch: 4 step: 533, loss is 0.2973407804965973\n",
      "epoch: 4 step: 534, loss is 0.16265560686588287\n",
      "epoch: 4 step: 535, loss is 0.17094287276268005\n",
      "epoch: 4 step: 536, loss is 0.2847701609134674\n",
      "epoch: 4 step: 537, loss is 0.12026134878396988\n",
      "epoch: 4 step: 538, loss is 0.295469731092453\n",
      "epoch: 4 step: 539, loss is 0.1675308495759964\n",
      "epoch: 4 step: 540, loss is 0.13531066477298737\n",
      "epoch: 4 step: 541, loss is 0.2716658115386963\n",
      "epoch: 4 step: 542, loss is 0.14660851657390594\n",
      "epoch: 4 step: 543, loss is 0.3212096095085144\n",
      "epoch: 4 step: 544, loss is 0.24133460223674774\n",
      "epoch: 4 step: 545, loss is 0.4984792172908783\n",
      "epoch: 4 step: 546, loss is 0.20547594130039215\n",
      "epoch: 4 step: 547, loss is 0.3019455373287201\n",
      "epoch: 4 step: 548, loss is 0.2964912950992584\n",
      "epoch: 4 step: 549, loss is 0.15064936876296997\n",
      "epoch: 4 step: 550, loss is 0.1615995317697525\n",
      "epoch: 4 step: 551, loss is 0.12550927698612213\n",
      "epoch: 4 step: 552, loss is 0.3053358793258667\n",
      "epoch: 4 step: 553, loss is 0.18444454669952393\n",
      "epoch: 4 step: 554, loss is 0.1414988934993744\n",
      "epoch: 4 step: 555, loss is 0.10079964995384216\n",
      "epoch: 4 step: 556, loss is 0.33542999625205994\n",
      "epoch: 4 step: 557, loss is 0.16451965272426605\n",
      "epoch: 4 step: 558, loss is 0.20429286360740662\n",
      "epoch: 4 step: 559, loss is 0.40843135118484497\n",
      "epoch: 4 step: 560, loss is 0.13363605737686157\n",
      "epoch: 4 step: 561, loss is 0.10953929275274277\n",
      "epoch: 4 step: 562, loss is 0.1151815801858902\n",
      "epoch: 4 step: 563, loss is 0.0662783607840538\n",
      "epoch: 4 step: 564, loss is 0.1614781767129898\n",
      "epoch: 4 step: 565, loss is 0.13130886852741241\n",
      "epoch: 4 step: 566, loss is 0.16835518181324005\n",
      "epoch: 4 step: 567, loss is 0.2547667324542999\n",
      "epoch: 4 step: 568, loss is 0.21374428272247314\n",
      "epoch: 4 step: 569, loss is 0.2134988158941269\n",
      "epoch: 4 step: 570, loss is 0.13085031509399414\n",
      "epoch: 4 step: 571, loss is 0.34101954102516174\n",
      "epoch: 4 step: 572, loss is 0.14469696581363678\n",
      "epoch: 4 step: 573, loss is 0.17610155045986176\n",
      "epoch: 4 step: 574, loss is 0.09155848622322083\n",
      "epoch: 4 step: 575, loss is 0.19434615969657898\n",
      "epoch: 4 step: 576, loss is 0.2413369119167328\n",
      "epoch: 4 step: 577, loss is 0.23470158874988556\n",
      "epoch: 4 step: 578, loss is 0.1487841010093689\n",
      "epoch: 4 step: 579, loss is 0.17201389372348785\n",
      "epoch: 4 step: 580, loss is 0.1936524212360382\n",
      "epoch: 4 step: 581, loss is 0.154929518699646\n",
      "epoch: 4 step: 582, loss is 0.3234463632106781\n",
      "epoch: 4 step: 583, loss is 0.19787336885929108\n",
      "epoch: 4 step: 584, loss is 0.2557249665260315\n",
      "epoch: 4 step: 585, loss is 0.15714651346206665\n",
      "epoch: 4 step: 586, loss is 0.24890100955963135\n",
      "epoch: 4 step: 587, loss is 0.33444029092788696\n",
      "epoch: 4 step: 588, loss is 0.46574097871780396\n",
      "epoch: 4 step: 589, loss is 0.28270870447158813\n",
      "epoch: 4 step: 590, loss is 0.2305096685886383\n",
      "epoch: 4 step: 591, loss is 0.13192687928676605\n",
      "epoch: 4 step: 592, loss is 0.11193259060382843\n",
      "epoch: 4 step: 593, loss is 0.13564418256282806\n",
      "epoch: 4 step: 594, loss is 0.18718747794628143\n",
      "epoch: 4 step: 595, loss is 0.20672458410263062\n",
      "epoch: 4 step: 596, loss is 0.2767491936683655\n",
      "epoch: 4 step: 597, loss is 0.2274688482284546\n",
      "epoch: 4 step: 598, loss is 0.20114021003246307\n",
      "epoch: 4 step: 599, loss is 0.23425279557704926\n",
      "epoch: 4 step: 600, loss is 0.24420826137065887\n",
      "epoch: 4 step: 601, loss is 0.14307914674282074\n",
      "epoch: 4 step: 602, loss is 0.21295292675495148\n",
      "epoch: 4 step: 603, loss is 0.10313179343938828\n",
      "epoch: 4 step: 604, loss is 0.14731280505657196\n",
      "epoch: 4 step: 605, loss is 0.22755087912082672\n",
      "epoch: 4 step: 606, loss is 0.3195430636405945\n",
      "epoch: 4 step: 607, loss is 0.1820279210805893\n",
      "epoch: 4 step: 608, loss is 0.20121410489082336\n",
      "epoch: 4 step: 609, loss is 0.3376769423484802\n",
      "epoch: 4 step: 610, loss is 0.20764072239398956\n",
      "epoch: 4 step: 611, loss is 0.1955665796995163\n",
      "epoch: 4 step: 612, loss is 0.13236838579177856\n",
      "epoch: 4 step: 613, loss is 0.2908426523208618\n",
      "epoch: 4 step: 614, loss is 0.26903095841407776\n",
      "epoch: 4 step: 615, loss is 0.1751706302165985\n",
      "epoch: 4 step: 616, loss is 0.28894472122192383\n",
      "epoch: 4 step: 617, loss is 0.2418360412120819\n",
      "epoch: 4 step: 618, loss is 0.21739676594734192\n",
      "epoch: 4 step: 619, loss is 0.2590327560901642\n",
      "epoch: 4 step: 620, loss is 0.10643091797828674\n",
      "epoch: 4 step: 621, loss is 0.14808492362499237\n",
      "epoch: 4 step: 622, loss is 0.2814887464046478\n",
      "epoch: 4 step: 623, loss is 0.25036436319351196\n",
      "epoch: 4 step: 624, loss is 0.27371567487716675\n",
      "epoch: 4 step: 625, loss is 0.26051998138427734\n",
      "epoch: 4 step: 626, loss is 0.1025266945362091\n",
      "epoch: 4 step: 627, loss is 0.2783459424972534\n",
      "epoch: 4 step: 628, loss is 0.1854318380355835\n",
      "epoch: 4 step: 629, loss is 0.31641605496406555\n",
      "epoch: 4 step: 630, loss is 0.17738960683345795\n",
      "epoch: 4 step: 631, loss is 0.284502238035202\n",
      "epoch: 4 step: 632, loss is 0.26312071084976196\n",
      "epoch: 4 step: 633, loss is 0.2223348170518875\n",
      "epoch: 4 step: 634, loss is 0.2735099196434021\n",
      "epoch: 4 step: 635, loss is 0.4765566289424896\n",
      "epoch: 4 step: 636, loss is 0.2340812236070633\n",
      "epoch: 4 step: 637, loss is 0.21129439771175385\n",
      "epoch: 4 step: 638, loss is 0.2508299648761749\n",
      "epoch: 4 step: 639, loss is 0.2555793523788452\n",
      "epoch: 4 step: 640, loss is 0.1807396560907364\n",
      "epoch: 4 step: 641, loss is 0.2654529809951782\n",
      "epoch: 4 step: 642, loss is 0.11694514751434326\n",
      "epoch: 4 step: 643, loss is 0.2989807724952698\n",
      "epoch: 4 step: 644, loss is 0.1708318442106247\n",
      "epoch: 4 step: 645, loss is 0.18409234285354614\n",
      "epoch: 4 step: 646, loss is 0.20151084661483765\n",
      "epoch: 4 step: 647, loss is 0.16931676864624023\n",
      "epoch: 4 step: 648, loss is 0.2309642732143402\n",
      "epoch: 4 step: 649, loss is 0.23560631275177002\n",
      "epoch: 4 step: 650, loss is 0.18615105748176575\n",
      "epoch: 4 step: 651, loss is 0.19908984005451202\n",
      "epoch: 4 step: 652, loss is 0.19795262813568115\n",
      "epoch: 4 step: 653, loss is 0.21525734663009644\n",
      "epoch: 4 step: 654, loss is 0.24623508751392365\n",
      "epoch: 4 step: 655, loss is 0.3058401048183441\n",
      "epoch: 4 step: 656, loss is 0.2271307408809662\n",
      "epoch: 4 step: 657, loss is 0.1763279289007187\n",
      "epoch: 4 step: 658, loss is 0.3222934901714325\n",
      "epoch: 4 step: 659, loss is 0.04167957976460457\n",
      "epoch: 4 step: 660, loss is 0.3641476333141327\n",
      "epoch: 4 step: 661, loss is 0.2528762221336365\n",
      "epoch: 4 step: 662, loss is 0.2886481285095215\n",
      "epoch: 4 step: 663, loss is 0.17069238424301147\n",
      "epoch: 4 step: 664, loss is 0.24012123048305511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 665, loss is 0.1337946206331253\n",
      "epoch: 4 step: 666, loss is 0.1785929650068283\n",
      "epoch: 4 step: 667, loss is 0.2981879413127899\n",
      "epoch: 4 step: 668, loss is 0.27655932307243347\n",
      "epoch: 4 step: 669, loss is 0.18621201813220978\n",
      "epoch: 4 step: 670, loss is 0.1151672899723053\n",
      "epoch: 4 step: 671, loss is 0.3100114166736603\n",
      "epoch: 4 step: 672, loss is 0.13714386522769928\n",
      "epoch: 4 step: 673, loss is 0.2919139564037323\n",
      "epoch: 4 step: 674, loss is 0.16851933300495148\n",
      "epoch: 4 step: 675, loss is 0.20578768849372864\n",
      "epoch: 4 step: 676, loss is 0.10319651663303375\n",
      "epoch: 4 step: 677, loss is 0.22182884812355042\n",
      "epoch: 4 step: 678, loss is 0.34102895855903625\n",
      "epoch: 4 step: 679, loss is 0.29324519634246826\n",
      "epoch: 4 step: 680, loss is 0.171977236866951\n",
      "epoch: 4 step: 681, loss is 0.09083646535873413\n",
      "epoch: 4 step: 682, loss is 0.17147429287433624\n",
      "epoch: 4 step: 683, loss is 0.16922375559806824\n",
      "epoch: 4 step: 684, loss is 0.3095393478870392\n",
      "epoch: 4 step: 685, loss is 0.16256819665431976\n",
      "epoch: 4 step: 686, loss is 0.1474643051624298\n",
      "epoch: 4 step: 687, loss is 0.235178604722023\n",
      "epoch: 4 step: 688, loss is 0.2321380227804184\n",
      "epoch: 4 step: 689, loss is 0.2790093421936035\n",
      "epoch: 4 step: 690, loss is 0.2393030822277069\n",
      "epoch: 4 step: 691, loss is 0.08492070436477661\n",
      "epoch: 4 step: 692, loss is 0.2221638560295105\n",
      "epoch: 4 step: 693, loss is 0.14593976736068726\n",
      "epoch: 4 step: 694, loss is 0.1191069558262825\n",
      "epoch: 4 step: 695, loss is 0.14114424586296082\n",
      "epoch: 4 step: 696, loss is 0.3015836477279663\n",
      "epoch: 4 step: 697, loss is 0.2509401738643646\n",
      "epoch: 4 step: 698, loss is 0.1600666642189026\n",
      "epoch: 4 step: 699, loss is 0.09204103797674179\n",
      "epoch: 4 step: 700, loss is 0.1512071043252945\n",
      "epoch: 4 step: 701, loss is 0.15222018957138062\n",
      "epoch: 4 step: 702, loss is 0.30013248324394226\n",
      "epoch: 4 step: 703, loss is 0.25078076124191284\n",
      "epoch: 4 step: 704, loss is 0.19135069847106934\n",
      "epoch: 4 step: 705, loss is 0.33722907304763794\n",
      "epoch: 4 step: 706, loss is 0.12930928170681\n",
      "epoch: 4 step: 707, loss is 0.1813092827796936\n",
      "epoch: 4 step: 708, loss is 0.22943374514579773\n",
      "epoch: 4 step: 709, loss is 0.13100892305374146\n",
      "epoch: 4 step: 710, loss is 0.36767327785491943\n",
      "epoch: 4 step: 711, loss is 0.21502164006233215\n",
      "epoch: 4 step: 712, loss is 0.13793104887008667\n",
      "epoch: 4 step: 713, loss is 0.17627230286598206\n",
      "epoch: 4 step: 714, loss is 0.19773933291435242\n",
      "epoch: 4 step: 715, loss is 0.133175790309906\n",
      "epoch: 4 step: 716, loss is 0.2070561647415161\n",
      "epoch: 4 step: 717, loss is 0.31025874614715576\n",
      "epoch: 4 step: 718, loss is 0.1849672943353653\n",
      "epoch: 4 step: 719, loss is 0.320122629404068\n",
      "epoch: 4 step: 720, loss is 0.1196555495262146\n",
      "epoch: 4 step: 721, loss is 0.14564938843250275\n",
      "epoch: 4 step: 722, loss is 0.18862612545490265\n",
      "epoch: 4 step: 723, loss is 0.2243628352880478\n",
      "epoch: 4 step: 724, loss is 0.2463238686323166\n",
      "epoch: 4 step: 725, loss is 0.30802854895591736\n",
      "epoch: 4 step: 726, loss is 0.0830473005771637\n",
      "epoch: 4 step: 727, loss is 0.27215027809143066\n",
      "epoch: 4 step: 728, loss is 0.14529870450496674\n",
      "epoch: 4 step: 729, loss is 0.26371705532073975\n",
      "epoch: 4 step: 730, loss is 0.19173531234264374\n",
      "epoch: 4 step: 731, loss is 0.2241024672985077\n",
      "epoch: 4 step: 732, loss is 0.144939586520195\n",
      "epoch: 4 step: 733, loss is 0.10289658606052399\n",
      "epoch: 4 step: 734, loss is 0.22993610799312592\n",
      "epoch: 4 step: 735, loss is 0.25790712237358093\n",
      "epoch: 4 step: 736, loss is 0.3309595286846161\n",
      "epoch: 4 step: 737, loss is 0.13325512409210205\n",
      "epoch: 4 step: 738, loss is 0.11507877707481384\n",
      "epoch: 4 step: 739, loss is 0.24158801138401031\n",
      "epoch: 4 step: 740, loss is 0.2080737054347992\n",
      "epoch: 4 step: 741, loss is 0.1431989073753357\n",
      "epoch: 4 step: 742, loss is 0.26255133748054504\n",
      "epoch: 4 step: 743, loss is 0.21890495717525482\n",
      "epoch: 4 step: 744, loss is 0.20512333512306213\n",
      "epoch: 4 step: 745, loss is 0.1395273357629776\n",
      "epoch: 4 step: 746, loss is 0.1862974464893341\n",
      "epoch: 4 step: 747, loss is 0.19004373252391815\n",
      "epoch: 4 step: 748, loss is 0.3129608631134033\n",
      "epoch: 4 step: 749, loss is 0.3041943311691284\n",
      "epoch: 4 step: 750, loss is 0.32384809851646423\n",
      "epoch: 4 step: 751, loss is 0.15258429944515228\n",
      "epoch: 4 step: 752, loss is 0.267162561416626\n",
      "epoch: 4 step: 753, loss is 0.15541169047355652\n",
      "epoch: 4 step: 754, loss is 0.21543414890766144\n",
      "epoch: 4 step: 755, loss is 0.17575329542160034\n",
      "epoch: 4 step: 756, loss is 0.1903272271156311\n",
      "epoch: 4 step: 757, loss is 0.20348724722862244\n",
      "epoch: 4 step: 758, loss is 0.175051748752594\n",
      "epoch: 4 step: 759, loss is 0.17052727937698364\n",
      "epoch: 4 step: 760, loss is 0.17635375261306763\n",
      "epoch: 4 step: 761, loss is 0.1302337944507599\n",
      "epoch: 4 step: 762, loss is 0.24535295367240906\n",
      "epoch: 4 step: 763, loss is 0.42221328616142273\n",
      "epoch: 4 step: 764, loss is 0.14351938664913177\n",
      "epoch: 4 step: 765, loss is 0.22619745135307312\n",
      "epoch: 4 step: 766, loss is 0.2742609977722168\n",
      "epoch: 4 step: 767, loss is 0.1544571816921234\n",
      "epoch: 4 step: 768, loss is 0.12347272783517838\n",
      "epoch: 4 step: 769, loss is 0.19661682844161987\n",
      "epoch: 4 step: 770, loss is 0.287126749753952\n",
      "epoch: 4 step: 771, loss is 0.12007943540811539\n",
      "epoch: 4 step: 772, loss is 0.13698710501194\n",
      "epoch: 4 step: 773, loss is 0.09739033132791519\n",
      "epoch: 4 step: 774, loss is 0.3302120268344879\n",
      "epoch: 4 step: 775, loss is 0.21453523635864258\n",
      "epoch: 4 step: 776, loss is 0.09050320088863373\n",
      "epoch: 4 step: 777, loss is 0.1851503849029541\n",
      "epoch: 4 step: 778, loss is 0.24942947924137115\n",
      "epoch: 4 step: 779, loss is 0.18287858366966248\n",
      "epoch: 4 step: 780, loss is 0.21523544192314148\n",
      "epoch: 4 step: 781, loss is 0.2862914502620697\n",
      "epoch: 4 step: 782, loss is 0.16006053984165192\n",
      "epoch: 4 step: 783, loss is 0.1870909184217453\n",
      "epoch: 4 step: 784, loss is 0.20366233587265015\n",
      "epoch: 4 step: 785, loss is 0.15639790892601013\n",
      "epoch: 4 step: 786, loss is 0.2415333092212677\n",
      "epoch: 4 step: 787, loss is 0.05144181102514267\n",
      "epoch: 4 step: 788, loss is 0.19226323068141937\n",
      "epoch: 4 step: 789, loss is 0.32273295521736145\n",
      "epoch: 4 step: 790, loss is 0.20576968789100647\n",
      "epoch: 4 step: 791, loss is 0.1181384027004242\n",
      "epoch: 4 step: 792, loss is 0.16757884621620178\n",
      "epoch: 4 step: 793, loss is 0.3796732723712921\n",
      "epoch: 4 step: 794, loss is 0.20726826786994934\n",
      "epoch: 4 step: 795, loss is 0.32725149393081665\n",
      "epoch: 4 step: 796, loss is 0.26359760761260986\n",
      "epoch: 4 step: 797, loss is 0.3716665208339691\n",
      "epoch: 4 step: 798, loss is 0.18884706497192383\n",
      "epoch: 4 step: 799, loss is 0.3087525963783264\n",
      "epoch: 4 step: 800, loss is 0.10797221958637238\n",
      "epoch: 4 step: 801, loss is 0.1100631058216095\n",
      "epoch: 4 step: 802, loss is 0.15336641669273376\n",
      "epoch: 4 step: 803, loss is 0.17290858924388885\n",
      "epoch: 4 step: 804, loss is 0.36003968119621277\n",
      "epoch: 4 step: 805, loss is 0.14631813764572144\n",
      "epoch: 4 step: 806, loss is 0.18587008118629456\n",
      "epoch: 4 step: 807, loss is 0.19317488372325897\n",
      "epoch: 4 step: 808, loss is 0.19195979833602905\n",
      "epoch: 4 step: 809, loss is 0.19089791178703308\n",
      "epoch: 4 step: 810, loss is 0.27098652720451355\n",
      "epoch: 4 step: 811, loss is 0.22388623654842377\n",
      "epoch: 4 step: 812, loss is 0.12571223080158234\n",
      "epoch: 4 step: 813, loss is 0.2106768637895584\n",
      "epoch: 4 step: 814, loss is 0.20488020777702332\n",
      "epoch: 4 step: 815, loss is 0.10270272940397263\n",
      "epoch: 4 step: 816, loss is 0.20399120450019836\n",
      "epoch: 4 step: 817, loss is 0.1298864781856537\n",
      "epoch: 4 step: 818, loss is 0.2610563039779663\n",
      "epoch: 4 step: 819, loss is 0.1690482199192047\n",
      "epoch: 4 step: 820, loss is 0.22414763271808624\n",
      "epoch: 4 step: 821, loss is 0.15381969511508942\n",
      "epoch: 4 step: 822, loss is 0.16794468462467194\n",
      "epoch: 4 step: 823, loss is 0.29396894574165344\n",
      "epoch: 4 step: 824, loss is 0.09756966680288315\n",
      "epoch: 4 step: 825, loss is 0.20467542111873627\n",
      "epoch: 4 step: 826, loss is 0.15369611978530884\n",
      "epoch: 4 step: 827, loss is 0.18997888267040253\n",
      "epoch: 4 step: 828, loss is 0.21760915219783783\n",
      "epoch: 4 step: 829, loss is 0.17563790082931519\n",
      "epoch: 4 step: 830, loss is 0.1360747367143631\n",
      "epoch: 4 step: 831, loss is 0.3247186541557312\n",
      "epoch: 4 step: 832, loss is 0.13008801639080048\n",
      "epoch: 4 step: 833, loss is 0.12521712481975555\n",
      "epoch: 4 step: 834, loss is 0.3172394335269928\n",
      "epoch: 4 step: 835, loss is 0.43692436814308167\n",
      "epoch: 4 step: 836, loss is 0.17670215666294098\n",
      "epoch: 4 step: 837, loss is 0.2501800060272217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 838, loss is 0.29267948865890503\n",
      "epoch: 4 step: 839, loss is 0.148546501994133\n",
      "epoch: 4 step: 840, loss is 0.23220908641815186\n",
      "epoch: 4 step: 841, loss is 0.11387693881988525\n",
      "epoch: 4 step: 842, loss is 0.3659076988697052\n",
      "epoch: 4 step: 843, loss is 0.4066146910190582\n",
      "epoch: 4 step: 844, loss is 0.24083885550498962\n",
      "epoch: 4 step: 845, loss is 0.2600109279155731\n",
      "epoch: 4 step: 846, loss is 0.19488120079040527\n",
      "epoch: 4 step: 847, loss is 0.11561766266822815\n",
      "epoch: 4 step: 848, loss is 0.38060954213142395\n",
      "epoch: 4 step: 849, loss is 0.13501796126365662\n",
      "epoch: 4 step: 850, loss is 0.2719326615333557\n",
      "epoch: 4 step: 851, loss is 0.16959857940673828\n",
      "epoch: 4 step: 852, loss is 0.10816137492656708\n",
      "epoch: 4 step: 853, loss is 0.23077769577503204\n",
      "epoch: 4 step: 854, loss is 0.17059262096881866\n",
      "epoch: 4 step: 855, loss is 0.486311674118042\n",
      "epoch: 4 step: 856, loss is 0.13062332570552826\n",
      "epoch: 4 step: 857, loss is 0.22276951372623444\n",
      "epoch: 4 step: 858, loss is 0.16090501844882965\n",
      "epoch: 4 step: 859, loss is 0.2548133432865143\n",
      "epoch: 4 step: 860, loss is 0.1331770420074463\n",
      "epoch: 4 step: 861, loss is 0.1930767446756363\n",
      "epoch: 4 step: 862, loss is 0.16727492213249207\n",
      "epoch: 4 step: 863, loss is 0.25145038962364197\n",
      "epoch: 4 step: 864, loss is 0.34962332248687744\n",
      "epoch: 4 step: 865, loss is 0.17159801721572876\n",
      "epoch: 4 step: 866, loss is 0.17306062579154968\n",
      "epoch: 4 step: 867, loss is 0.24167697131633759\n",
      "epoch: 4 step: 868, loss is 0.10953042656183243\n",
      "epoch: 4 step: 869, loss is 0.18837621808052063\n",
      "epoch: 4 step: 870, loss is 0.17244121432304382\n",
      "epoch: 4 step: 871, loss is 0.26312246918678284\n",
      "epoch: 4 step: 872, loss is 0.1992296278476715\n",
      "epoch: 4 step: 873, loss is 0.19718997180461884\n",
      "epoch: 4 step: 874, loss is 0.18133722245693207\n",
      "epoch: 4 step: 875, loss is 0.14871495962142944\n",
      "epoch: 4 step: 876, loss is 0.23977170884609222\n",
      "epoch: 4 step: 877, loss is 0.15299849212169647\n",
      "epoch: 4 step: 878, loss is 0.14955556392669678\n",
      "epoch: 4 step: 879, loss is 0.057704389095306396\n",
      "epoch: 4 step: 880, loss is 0.15567100048065186\n",
      "epoch: 4 step: 881, loss is 0.14774499833583832\n",
      "epoch: 4 step: 882, loss is 0.16975490748882294\n",
      "epoch: 4 step: 883, loss is 0.15829509496688843\n",
      "epoch: 4 step: 884, loss is 0.15130515396595\n",
      "epoch: 4 step: 885, loss is 0.3616291582584381\n",
      "epoch: 4 step: 886, loss is 0.3053927421569824\n",
      "epoch: 4 step: 887, loss is 0.2512366771697998\n",
      "epoch: 4 step: 888, loss is 0.3082464933395386\n",
      "epoch: 4 step: 889, loss is 0.29954925179481506\n",
      "epoch: 4 step: 890, loss is 0.27268609404563904\n",
      "epoch: 4 step: 891, loss is 0.1785622239112854\n",
      "epoch: 4 step: 892, loss is 0.24324335157871246\n",
      "epoch: 4 step: 893, loss is 0.1379145085811615\n",
      "epoch: 4 step: 894, loss is 0.24074800312519073\n",
      "epoch: 4 step: 895, loss is 0.14202700555324554\n",
      "epoch: 4 step: 896, loss is 0.2818067669868469\n",
      "epoch: 4 step: 897, loss is 0.22638960182666779\n",
      "epoch: 4 step: 898, loss is 0.17148612439632416\n",
      "epoch: 4 step: 899, loss is 0.1876130849123001\n",
      "epoch: 4 step: 900, loss is 0.324369877576828\n",
      "epoch: 4 step: 901, loss is 0.17700059711933136\n",
      "epoch: 4 step: 902, loss is 0.26613932847976685\n",
      "epoch: 4 step: 903, loss is 0.15695185959339142\n",
      "epoch: 4 step: 904, loss is 0.26903024315834045\n",
      "epoch: 4 step: 905, loss is 0.12974528968334198\n",
      "epoch: 4 step: 906, loss is 0.1828191876411438\n",
      "epoch: 4 step: 907, loss is 0.19680339097976685\n",
      "epoch: 4 step: 908, loss is 0.20969533920288086\n",
      "epoch: 4 step: 909, loss is 0.16840995848178864\n",
      "epoch: 4 step: 910, loss is 0.2191927284002304\n",
      "epoch: 4 step: 911, loss is 0.2026747316122055\n",
      "epoch: 4 step: 912, loss is 0.31951984763145447\n",
      "epoch: 4 step: 913, loss is 0.3777565658092499\n",
      "epoch: 4 step: 914, loss is 0.31361329555511475\n",
      "epoch: 4 step: 915, loss is 0.11268751323223114\n",
      "epoch: 4 step: 916, loss is 0.11095037311315536\n",
      "epoch: 4 step: 917, loss is 0.2868591845035553\n",
      "epoch: 4 step: 918, loss is 0.13585183024406433\n",
      "epoch: 4 step: 919, loss is 0.34020695090293884\n",
      "epoch: 4 step: 920, loss is 0.09401203691959381\n",
      "epoch: 4 step: 921, loss is 0.2863490581512451\n",
      "epoch: 4 step: 922, loss is 0.12922446429729462\n",
      "epoch: 4 step: 923, loss is 0.2705804407596588\n",
      "epoch: 4 step: 924, loss is 0.15918676555156708\n",
      "epoch: 4 step: 925, loss is 0.33452221751213074\n",
      "epoch: 4 step: 926, loss is 0.2600579857826233\n",
      "epoch: 4 step: 927, loss is 0.20598745346069336\n",
      "epoch: 4 step: 928, loss is 0.2581993639469147\n",
      "epoch: 4 step: 929, loss is 0.2370409518480301\n",
      "epoch: 4 step: 930, loss is 0.19331766664981842\n",
      "epoch: 4 step: 931, loss is 0.1744287759065628\n",
      "epoch: 4 step: 932, loss is 0.1672448217868805\n",
      "epoch: 4 step: 933, loss is 0.2636610269546509\n",
      "epoch: 4 step: 934, loss is 0.1318528950214386\n",
      "epoch: 4 step: 935, loss is 0.13826657831668854\n",
      "epoch: 4 step: 936, loss is 0.28847837448120117\n",
      "epoch: 4 step: 937, loss is 0.10904967039823532\n",
      "epoch: 5 step: 1, loss is 0.12928318977355957\n",
      "epoch: 5 step: 2, loss is 0.146439790725708\n",
      "epoch: 5 step: 3, loss is 0.29226386547088623\n",
      "epoch: 5 step: 4, loss is 0.18669547140598297\n",
      "epoch: 5 step: 5, loss is 0.09927501529455185\n",
      "epoch: 5 step: 6, loss is 0.25688666105270386\n",
      "epoch: 5 step: 7, loss is 0.3090616762638092\n",
      "epoch: 5 step: 8, loss is 0.20316927134990692\n",
      "epoch: 5 step: 9, loss is 0.17660962045192719\n",
      "epoch: 5 step: 10, loss is 0.2854149341583252\n",
      "epoch: 5 step: 11, loss is 0.2733067274093628\n",
      "epoch: 5 step: 12, loss is 0.0941094383597374\n",
      "epoch: 5 step: 13, loss is 0.0930078998208046\n",
      "epoch: 5 step: 14, loss is 0.14562827348709106\n",
      "epoch: 5 step: 15, loss is 0.14278559386730194\n",
      "epoch: 5 step: 16, loss is 0.21187511086463928\n",
      "epoch: 5 step: 17, loss is 0.14437107741832733\n",
      "epoch: 5 step: 18, loss is 0.249253049492836\n",
      "epoch: 5 step: 19, loss is 0.13473421335220337\n",
      "epoch: 5 step: 20, loss is 0.21772369742393494\n",
      "epoch: 5 step: 21, loss is 0.2525178790092468\n",
      "epoch: 5 step: 22, loss is 0.15856443345546722\n",
      "epoch: 5 step: 23, loss is 0.05410560965538025\n",
      "epoch: 5 step: 24, loss is 0.19438262283802032\n",
      "epoch: 5 step: 25, loss is 0.10586661100387573\n",
      "epoch: 5 step: 26, loss is 0.19872890412807465\n",
      "epoch: 5 step: 27, loss is 0.0424959734082222\n",
      "epoch: 5 step: 28, loss is 0.34372034668922424\n",
      "epoch: 5 step: 29, loss is 0.2606836259365082\n",
      "epoch: 5 step: 30, loss is 0.13206377625465393\n",
      "epoch: 5 step: 31, loss is 0.11280383169651031\n",
      "epoch: 5 step: 32, loss is 0.16811993718147278\n",
      "epoch: 5 step: 33, loss is 0.2728756368160248\n",
      "epoch: 5 step: 34, loss is 0.23828978836536407\n",
      "epoch: 5 step: 35, loss is 0.14612029492855072\n",
      "epoch: 5 step: 36, loss is 0.13413268327713013\n",
      "epoch: 5 step: 37, loss is 0.15741275250911713\n",
      "epoch: 5 step: 38, loss is 0.10807903110980988\n",
      "epoch: 5 step: 39, loss is 0.12166931480169296\n",
      "epoch: 5 step: 40, loss is 0.20229408144950867\n",
      "epoch: 5 step: 41, loss is 0.16555185616016388\n",
      "epoch: 5 step: 42, loss is 0.16751477122306824\n",
      "epoch: 5 step: 43, loss is 0.12306347489356995\n",
      "epoch: 5 step: 44, loss is 0.23654474318027496\n",
      "epoch: 5 step: 45, loss is 0.15173716843128204\n",
      "epoch: 5 step: 46, loss is 0.09671378135681152\n",
      "epoch: 5 step: 47, loss is 0.15294547379016876\n",
      "epoch: 5 step: 48, loss is 0.1403866708278656\n",
      "epoch: 5 step: 49, loss is 0.3546825647354126\n",
      "epoch: 5 step: 50, loss is 0.18608352541923523\n",
      "epoch: 5 step: 51, loss is 0.06345097720623016\n",
      "epoch: 5 step: 52, loss is 0.2044582962989807\n",
      "epoch: 5 step: 53, loss is 0.11416656523942947\n",
      "epoch: 5 step: 54, loss is 0.13062028586864471\n",
      "epoch: 5 step: 55, loss is 0.1898190677165985\n",
      "epoch: 5 step: 56, loss is 0.1509770005941391\n",
      "epoch: 5 step: 57, loss is 0.19274134933948517\n",
      "epoch: 5 step: 58, loss is 0.09366492927074432\n",
      "epoch: 5 step: 59, loss is 0.17285136878490448\n",
      "epoch: 5 step: 60, loss is 0.20616492629051208\n",
      "epoch: 5 step: 61, loss is 0.2358166128396988\n",
      "epoch: 5 step: 62, loss is 0.16939552128314972\n",
      "epoch: 5 step: 63, loss is 0.2521303594112396\n",
      "epoch: 5 step: 64, loss is 0.17407846450805664\n",
      "epoch: 5 step: 65, loss is 0.14667028188705444\n",
      "epoch: 5 step: 66, loss is 0.163785919547081\n",
      "epoch: 5 step: 67, loss is 0.17047984898090363\n",
      "epoch: 5 step: 68, loss is 0.19198203086853027\n",
      "epoch: 5 step: 69, loss is 0.10629740357398987\n",
      "epoch: 5 step: 70, loss is 0.18435396254062653\n",
      "epoch: 5 step: 71, loss is 0.2402840107679367\n",
      "epoch: 5 step: 72, loss is 0.2186177670955658\n",
      "epoch: 5 step: 73, loss is 0.30287307500839233\n",
      "epoch: 5 step: 74, loss is 0.20592498779296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 75, loss is 0.16671030223369598\n",
      "epoch: 5 step: 76, loss is 0.26328060030937195\n",
      "epoch: 5 step: 77, loss is 0.1623077094554901\n",
      "epoch: 5 step: 78, loss is 0.2094089239835739\n",
      "epoch: 5 step: 79, loss is 0.2073643058538437\n",
      "epoch: 5 step: 80, loss is 0.061323992908000946\n",
      "epoch: 5 step: 81, loss is 0.12990011274814606\n",
      "epoch: 5 step: 82, loss is 0.13162265717983246\n",
      "epoch: 5 step: 83, loss is 0.1508641093969345\n",
      "epoch: 5 step: 84, loss is 0.2581482231616974\n",
      "epoch: 5 step: 85, loss is 0.1660604327917099\n",
      "epoch: 5 step: 86, loss is 0.23210503160953522\n",
      "epoch: 5 step: 87, loss is 0.10314619541168213\n",
      "epoch: 5 step: 88, loss is 0.11269554495811462\n",
      "epoch: 5 step: 89, loss is 0.07384044677019119\n",
      "epoch: 5 step: 90, loss is 0.1698334962129593\n",
      "epoch: 5 step: 91, loss is 0.24175024032592773\n",
      "epoch: 5 step: 92, loss is 0.11129669845104218\n",
      "epoch: 5 step: 93, loss is 0.19567300379276276\n",
      "epoch: 5 step: 94, loss is 0.16602523624897003\n",
      "epoch: 5 step: 95, loss is 0.14707228541374207\n",
      "epoch: 5 step: 96, loss is 0.16972050070762634\n",
      "epoch: 5 step: 97, loss is 0.14840185642242432\n",
      "epoch: 5 step: 98, loss is 0.3419402539730072\n",
      "epoch: 5 step: 99, loss is 0.13085736334323883\n",
      "epoch: 5 step: 100, loss is 0.24163420498371124\n",
      "epoch: 5 step: 101, loss is 0.21566428244113922\n",
      "epoch: 5 step: 102, loss is 0.09515323489904404\n",
      "epoch: 5 step: 103, loss is 0.2141188085079193\n",
      "epoch: 5 step: 104, loss is 0.1407259702682495\n",
      "epoch: 5 step: 105, loss is 0.28888586163520813\n",
      "epoch: 5 step: 106, loss is 0.16794395446777344\n",
      "epoch: 5 step: 107, loss is 0.20518381893634796\n",
      "epoch: 5 step: 108, loss is 0.11129727959632874\n",
      "epoch: 5 step: 109, loss is 0.1593673676252365\n",
      "epoch: 5 step: 110, loss is 0.4258325397968292\n",
      "epoch: 5 step: 111, loss is 0.18740393221378326\n",
      "epoch: 5 step: 112, loss is 0.17178748548030853\n",
      "epoch: 5 step: 113, loss is 0.07984305918216705\n",
      "epoch: 5 step: 114, loss is 0.2677679657936096\n",
      "epoch: 5 step: 115, loss is 0.18933755159378052\n",
      "epoch: 5 step: 116, loss is 0.22306828200817108\n",
      "epoch: 5 step: 117, loss is 0.2644725739955902\n",
      "epoch: 5 step: 118, loss is 0.1812639832496643\n",
      "epoch: 5 step: 119, loss is 0.15938782691955566\n",
      "epoch: 5 step: 120, loss is 0.10582996904850006\n",
      "epoch: 5 step: 121, loss is 0.10895676910877228\n",
      "epoch: 5 step: 122, loss is 0.09540246427059174\n",
      "epoch: 5 step: 123, loss is 0.2483750730752945\n",
      "epoch: 5 step: 124, loss is 0.25065183639526367\n",
      "epoch: 5 step: 125, loss is 0.14568369090557098\n",
      "epoch: 5 step: 126, loss is 0.10700849443674088\n",
      "epoch: 5 step: 127, loss is 0.1914217323064804\n",
      "epoch: 5 step: 128, loss is 0.22265274822711945\n",
      "epoch: 5 step: 129, loss is 0.19111290574073792\n",
      "epoch: 5 step: 130, loss is 0.15594740211963654\n",
      "epoch: 5 step: 131, loss is 0.27443891763687134\n",
      "epoch: 5 step: 132, loss is 0.06527086347341537\n",
      "epoch: 5 step: 133, loss is 0.24732623994350433\n",
      "epoch: 5 step: 134, loss is 0.2764509618282318\n",
      "epoch: 5 step: 135, loss is 0.07283052057027817\n",
      "epoch: 5 step: 136, loss is 0.29795461893081665\n",
      "epoch: 5 step: 137, loss is 0.25318169593811035\n",
      "epoch: 5 step: 138, loss is 0.25491684675216675\n",
      "epoch: 5 step: 139, loss is 0.20854003727436066\n",
      "epoch: 5 step: 140, loss is 0.15877294540405273\n",
      "epoch: 5 step: 141, loss is 0.12408515065908432\n",
      "epoch: 5 step: 142, loss is 0.21964526176452637\n",
      "epoch: 5 step: 143, loss is 0.1562991887331009\n",
      "epoch: 5 step: 144, loss is 0.08575315773487091\n",
      "epoch: 5 step: 145, loss is 0.09457915276288986\n",
      "epoch: 5 step: 146, loss is 0.20272091031074524\n",
      "epoch: 5 step: 147, loss is 0.2389170080423355\n",
      "epoch: 5 step: 148, loss is 0.16891998052597046\n",
      "epoch: 5 step: 149, loss is 0.2441091537475586\n",
      "epoch: 5 step: 150, loss is 0.08236982673406601\n",
      "epoch: 5 step: 151, loss is 0.2392825335264206\n",
      "epoch: 5 step: 152, loss is 0.15047812461853027\n",
      "epoch: 5 step: 153, loss is 0.15542134642601013\n",
      "epoch: 5 step: 154, loss is 0.09999800473451614\n",
      "epoch: 5 step: 155, loss is 0.23961141705513\n",
      "epoch: 5 step: 156, loss is 0.36165738105773926\n",
      "epoch: 5 step: 157, loss is 0.19488829374313354\n",
      "epoch: 5 step: 158, loss is 0.09500905871391296\n",
      "epoch: 5 step: 159, loss is 0.2619463801383972\n",
      "epoch: 5 step: 160, loss is 0.14128270745277405\n",
      "epoch: 5 step: 161, loss is 0.12147258967161179\n",
      "epoch: 5 step: 162, loss is 0.06643480062484741\n",
      "epoch: 5 step: 163, loss is 0.20069313049316406\n",
      "epoch: 5 step: 164, loss is 0.20443856716156006\n",
      "epoch: 5 step: 165, loss is 0.19668813049793243\n",
      "epoch: 5 step: 166, loss is 0.11112869530916214\n",
      "epoch: 5 step: 167, loss is 0.08474744856357574\n",
      "epoch: 5 step: 168, loss is 0.21369460225105286\n",
      "epoch: 5 step: 169, loss is 0.1835315227508545\n",
      "epoch: 5 step: 170, loss is 0.3965184986591339\n",
      "epoch: 5 step: 171, loss is 0.17434100806713104\n",
      "epoch: 5 step: 172, loss is 0.09948471188545227\n",
      "epoch: 5 step: 173, loss is 0.18045178055763245\n",
      "epoch: 5 step: 174, loss is 0.11760618537664413\n",
      "epoch: 5 step: 175, loss is 0.08403023332357407\n",
      "epoch: 5 step: 176, loss is 0.22355324029922485\n",
      "epoch: 5 step: 177, loss is 0.08178982883691788\n",
      "epoch: 5 step: 178, loss is 0.13352665305137634\n",
      "epoch: 5 step: 179, loss is 0.16408200562000275\n",
      "epoch: 5 step: 180, loss is 0.18354660272598267\n",
      "epoch: 5 step: 181, loss is 0.14274795353412628\n",
      "epoch: 5 step: 182, loss is 0.1516340672969818\n",
      "epoch: 5 step: 183, loss is 0.18983659148216248\n",
      "epoch: 5 step: 184, loss is 0.1345674842596054\n",
      "epoch: 5 step: 185, loss is 0.17827053368091583\n",
      "epoch: 5 step: 186, loss is 0.1500123143196106\n",
      "epoch: 5 step: 187, loss is 0.1338985562324524\n",
      "epoch: 5 step: 188, loss is 0.15511773526668549\n",
      "epoch: 5 step: 189, loss is 0.13914738595485687\n",
      "epoch: 5 step: 190, loss is 0.08963527530431747\n",
      "epoch: 5 step: 191, loss is 0.09752288460731506\n",
      "epoch: 5 step: 192, loss is 0.07859865576028824\n",
      "epoch: 5 step: 193, loss is 0.12595213949680328\n",
      "epoch: 5 step: 194, loss is 0.08653955906629562\n",
      "epoch: 5 step: 195, loss is 0.35417455434799194\n",
      "epoch: 5 step: 196, loss is 0.18418216705322266\n",
      "epoch: 5 step: 197, loss is 0.20585903525352478\n",
      "epoch: 5 step: 198, loss is 0.16997794806957245\n",
      "epoch: 5 step: 199, loss is 0.1943204253911972\n",
      "epoch: 5 step: 200, loss is 0.18291370570659637\n",
      "epoch: 5 step: 201, loss is 0.2396475225687027\n",
      "epoch: 5 step: 202, loss is 0.19002491235733032\n",
      "epoch: 5 step: 203, loss is 0.18046824634075165\n",
      "epoch: 5 step: 204, loss is 0.17958176136016846\n",
      "epoch: 5 step: 205, loss is 0.14061513543128967\n",
      "epoch: 5 step: 206, loss is 0.11338768899440765\n",
      "epoch: 5 step: 207, loss is 0.2568216621875763\n",
      "epoch: 5 step: 208, loss is 0.18634390830993652\n",
      "epoch: 5 step: 209, loss is 0.14542479813098907\n",
      "epoch: 5 step: 210, loss is 0.1305864155292511\n",
      "epoch: 5 step: 211, loss is 0.11273881047964096\n",
      "epoch: 5 step: 212, loss is 0.25755003094673157\n",
      "epoch: 5 step: 213, loss is 0.2516150176525116\n",
      "epoch: 5 step: 214, loss is 0.1182534471154213\n",
      "epoch: 5 step: 215, loss is 0.20612458884716034\n",
      "epoch: 5 step: 216, loss is 0.14238891005516052\n",
      "epoch: 5 step: 217, loss is 0.29751187562942505\n",
      "epoch: 5 step: 218, loss is 0.2725679576396942\n",
      "epoch: 5 step: 219, loss is 0.17045560479164124\n",
      "epoch: 5 step: 220, loss is 0.12237891554832458\n",
      "epoch: 5 step: 221, loss is 0.24993960559368134\n",
      "epoch: 5 step: 222, loss is 0.1351957470178604\n",
      "epoch: 5 step: 223, loss is 0.23651516437530518\n",
      "epoch: 5 step: 224, loss is 0.14955754578113556\n",
      "epoch: 5 step: 225, loss is 0.2751714885234833\n",
      "epoch: 5 step: 226, loss is 0.12153508514165878\n",
      "epoch: 5 step: 227, loss is 0.27473559975624084\n",
      "epoch: 5 step: 228, loss is 0.1526901125907898\n",
      "epoch: 5 step: 229, loss is 0.10457287728786469\n",
      "epoch: 5 step: 230, loss is 0.17659622430801392\n",
      "epoch: 5 step: 231, loss is 0.19598807394504547\n",
      "epoch: 5 step: 232, loss is 0.14402522146701813\n",
      "epoch: 5 step: 233, loss is 0.17546778917312622\n",
      "epoch: 5 step: 234, loss is 0.08443675190210342\n",
      "epoch: 5 step: 235, loss is 0.20430518686771393\n",
      "epoch: 5 step: 236, loss is 0.2522590458393097\n",
      "epoch: 5 step: 237, loss is 0.11508064717054367\n",
      "epoch: 5 step: 238, loss is 0.1226608008146286\n",
      "epoch: 5 step: 239, loss is 0.1665295958518982\n",
      "epoch: 5 step: 240, loss is 0.16127756237983704\n",
      "epoch: 5 step: 241, loss is 0.1718687266111374\n",
      "epoch: 5 step: 242, loss is 0.1751139760017395\n",
      "epoch: 5 step: 243, loss is 0.20160850882530212\n",
      "epoch: 5 step: 244, loss is 0.20436376333236694\n",
      "epoch: 5 step: 245, loss is 0.1355823576450348\n",
      "epoch: 5 step: 246, loss is 0.21952082216739655\n",
      "epoch: 5 step: 247, loss is 0.2858183681964874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 248, loss is 0.21776002645492554\n",
      "epoch: 5 step: 249, loss is 0.2429591715335846\n",
      "epoch: 5 step: 250, loss is 0.2830294668674469\n",
      "epoch: 5 step: 251, loss is 0.10168930888175964\n",
      "epoch: 5 step: 252, loss is 0.2631848454475403\n",
      "epoch: 5 step: 253, loss is 0.26106634736061096\n",
      "epoch: 5 step: 254, loss is 0.0923694521188736\n",
      "epoch: 5 step: 255, loss is 0.23190078139305115\n",
      "epoch: 5 step: 256, loss is 0.050587110221385956\n",
      "epoch: 5 step: 257, loss is 0.11270515620708466\n",
      "epoch: 5 step: 258, loss is 0.18236105144023895\n",
      "epoch: 5 step: 259, loss is 0.17294810712337494\n",
      "epoch: 5 step: 260, loss is 0.27824243903160095\n",
      "epoch: 5 step: 261, loss is 0.24750228226184845\n",
      "epoch: 5 step: 262, loss is 0.22156347334384918\n",
      "epoch: 5 step: 263, loss is 0.07462389767169952\n",
      "epoch: 5 step: 264, loss is 0.19630219042301178\n",
      "epoch: 5 step: 265, loss is 0.10576044023036957\n",
      "epoch: 5 step: 266, loss is 0.12057413160800934\n",
      "epoch: 5 step: 267, loss is 0.17771032452583313\n",
      "epoch: 5 step: 268, loss is 0.12982802093029022\n",
      "epoch: 5 step: 269, loss is 0.22469714283943176\n",
      "epoch: 5 step: 270, loss is 0.21074169874191284\n",
      "epoch: 5 step: 271, loss is 0.27663153409957886\n",
      "epoch: 5 step: 272, loss is 0.2438558042049408\n",
      "epoch: 5 step: 273, loss is 0.13921818137168884\n",
      "epoch: 5 step: 274, loss is 0.13672715425491333\n",
      "epoch: 5 step: 275, loss is 0.07923019677400589\n",
      "epoch: 5 step: 276, loss is 0.398307740688324\n",
      "epoch: 5 step: 277, loss is 0.2901960611343384\n",
      "epoch: 5 step: 278, loss is 0.1812203973531723\n",
      "epoch: 5 step: 279, loss is 0.08987706899642944\n",
      "epoch: 5 step: 280, loss is 0.21908636391162872\n",
      "epoch: 5 step: 281, loss is 0.2096477895975113\n",
      "epoch: 5 step: 282, loss is 0.15290036797523499\n",
      "epoch: 5 step: 283, loss is 0.1873156875371933\n",
      "epoch: 5 step: 284, loss is 0.1727684885263443\n",
      "epoch: 5 step: 285, loss is 0.1757843941450119\n",
      "epoch: 5 step: 286, loss is 0.22387105226516724\n",
      "epoch: 5 step: 287, loss is 0.20473799109458923\n",
      "epoch: 5 step: 288, loss is 0.17665652930736542\n",
      "epoch: 5 step: 289, loss is 0.08841569721698761\n",
      "epoch: 5 step: 290, loss is 0.10793361067771912\n",
      "epoch: 5 step: 291, loss is 0.17321354150772095\n",
      "epoch: 5 step: 292, loss is 0.20080693066120148\n",
      "epoch: 5 step: 293, loss is 0.06834862381219864\n",
      "epoch: 5 step: 294, loss is 0.1885654181241989\n",
      "epoch: 5 step: 295, loss is 0.21315500140190125\n",
      "epoch: 5 step: 296, loss is 0.16413471102714539\n",
      "epoch: 5 step: 297, loss is 0.21946018934249878\n",
      "epoch: 5 step: 298, loss is 0.10497523844242096\n",
      "epoch: 5 step: 299, loss is 0.1944766491651535\n",
      "epoch: 5 step: 300, loss is 0.11410947144031525\n",
      "epoch: 5 step: 301, loss is 0.265886127948761\n",
      "epoch: 5 step: 302, loss is 0.10807598382234573\n",
      "epoch: 5 step: 303, loss is 0.11948536336421967\n",
      "epoch: 5 step: 304, loss is 0.07627605646848679\n",
      "epoch: 5 step: 305, loss is 0.12773412466049194\n",
      "epoch: 5 step: 306, loss is 0.1693364530801773\n",
      "epoch: 5 step: 307, loss is 0.13279500603675842\n",
      "epoch: 5 step: 308, loss is 0.06160709634423256\n",
      "epoch: 5 step: 309, loss is 0.15997140109539032\n",
      "epoch: 5 step: 310, loss is 0.185854971408844\n",
      "epoch: 5 step: 311, loss is 0.36380594968795776\n",
      "epoch: 5 step: 312, loss is 0.12010572850704193\n",
      "epoch: 5 step: 313, loss is 0.0659104660153389\n",
      "epoch: 5 step: 314, loss is 0.20641276240348816\n",
      "epoch: 5 step: 315, loss is 0.35594743490219116\n",
      "epoch: 5 step: 316, loss is 0.15003728866577148\n",
      "epoch: 5 step: 317, loss is 0.30344128608703613\n",
      "epoch: 5 step: 318, loss is 0.14921586215496063\n",
      "epoch: 5 step: 319, loss is 0.2582503855228424\n",
      "epoch: 5 step: 320, loss is 0.24419760704040527\n",
      "epoch: 5 step: 321, loss is 0.14643892645835876\n",
      "epoch: 5 step: 322, loss is 0.18910333514213562\n",
      "epoch: 5 step: 323, loss is 0.2096419781446457\n",
      "epoch: 5 step: 324, loss is 0.23044675588607788\n",
      "epoch: 5 step: 325, loss is 0.216496080160141\n",
      "epoch: 5 step: 326, loss is 0.2354232519865036\n",
      "epoch: 5 step: 327, loss is 0.18994282186031342\n",
      "epoch: 5 step: 328, loss is 0.13194610178470612\n",
      "epoch: 5 step: 329, loss is 0.09398844093084335\n",
      "epoch: 5 step: 330, loss is 0.200608491897583\n",
      "epoch: 5 step: 331, loss is 0.17870821058750153\n",
      "epoch: 5 step: 332, loss is 0.16662511229515076\n",
      "epoch: 5 step: 333, loss is 0.25823158025741577\n",
      "epoch: 5 step: 334, loss is 0.2542881369590759\n",
      "epoch: 5 step: 335, loss is 0.2223406732082367\n",
      "epoch: 5 step: 336, loss is 0.13805367052555084\n",
      "epoch: 5 step: 337, loss is 0.2115059196949005\n",
      "epoch: 5 step: 338, loss is 0.1333080530166626\n",
      "epoch: 5 step: 339, loss is 0.218195840716362\n",
      "epoch: 5 step: 340, loss is 0.3432058095932007\n",
      "epoch: 5 step: 341, loss is 0.2669239342212677\n",
      "epoch: 5 step: 342, loss is 0.12056688964366913\n",
      "epoch: 5 step: 343, loss is 0.08097442239522934\n",
      "epoch: 5 step: 344, loss is 0.16761377453804016\n",
      "epoch: 5 step: 345, loss is 0.1976398527622223\n",
      "epoch: 5 step: 346, loss is 0.08641795814037323\n",
      "epoch: 5 step: 347, loss is 0.41814595460891724\n",
      "epoch: 5 step: 348, loss is 0.11264783143997192\n",
      "epoch: 5 step: 349, loss is 0.16506706178188324\n",
      "epoch: 5 step: 350, loss is 0.1805451512336731\n",
      "epoch: 5 step: 351, loss is 0.17941638827323914\n",
      "epoch: 5 step: 352, loss is 0.1863659918308258\n",
      "epoch: 5 step: 353, loss is 0.23026560246944427\n",
      "epoch: 5 step: 354, loss is 0.07514099776744843\n",
      "epoch: 5 step: 355, loss is 0.42975667119026184\n",
      "epoch: 5 step: 356, loss is 0.11713731288909912\n",
      "epoch: 5 step: 357, loss is 0.1696985960006714\n",
      "epoch: 5 step: 358, loss is 0.15729005634784698\n",
      "epoch: 5 step: 359, loss is 0.24616728723049164\n",
      "epoch: 5 step: 360, loss is 0.20510579645633698\n",
      "epoch: 5 step: 361, loss is 0.1026439368724823\n",
      "epoch: 5 step: 362, loss is 0.14701396226882935\n",
      "epoch: 5 step: 363, loss is 0.095964714884758\n",
      "epoch: 5 step: 364, loss is 0.08888118714094162\n",
      "epoch: 5 step: 365, loss is 0.2803179919719696\n",
      "epoch: 5 step: 366, loss is 0.15997976064682007\n",
      "epoch: 5 step: 367, loss is 0.07487425208091736\n",
      "epoch: 5 step: 368, loss is 0.14472587406635284\n",
      "epoch: 5 step: 369, loss is 0.24539658427238464\n",
      "epoch: 5 step: 370, loss is 0.07716678082942963\n",
      "epoch: 5 step: 371, loss is 0.11887359619140625\n",
      "epoch: 5 step: 372, loss is 0.23388271033763885\n",
      "epoch: 5 step: 373, loss is 0.19208188354969025\n",
      "epoch: 5 step: 374, loss is 0.17673683166503906\n",
      "epoch: 5 step: 375, loss is 0.09369749575853348\n",
      "epoch: 5 step: 376, loss is 0.2742919921875\n",
      "epoch: 5 step: 377, loss is 0.1615973711013794\n",
      "epoch: 5 step: 378, loss is 0.1347566843032837\n",
      "epoch: 5 step: 379, loss is 0.12094645202159882\n",
      "epoch: 5 step: 380, loss is 0.16761770844459534\n",
      "epoch: 5 step: 381, loss is 0.03232022374868393\n",
      "epoch: 5 step: 382, loss is 0.18574275076389313\n",
      "epoch: 5 step: 383, loss is 0.1423298418521881\n",
      "epoch: 5 step: 384, loss is 0.10611971467733383\n",
      "epoch: 5 step: 385, loss is 0.10379941016435623\n",
      "epoch: 5 step: 386, loss is 0.2139488011598587\n",
      "epoch: 5 step: 387, loss is 0.2393922358751297\n",
      "epoch: 5 step: 388, loss is 0.25618094205856323\n",
      "epoch: 5 step: 389, loss is 0.1302458941936493\n",
      "epoch: 5 step: 390, loss is 0.10608623176813126\n",
      "epoch: 5 step: 391, loss is 0.08605560660362244\n",
      "epoch: 5 step: 392, loss is 0.22796358168125153\n",
      "epoch: 5 step: 393, loss is 0.08323199301958084\n",
      "epoch: 5 step: 394, loss is 0.18617939949035645\n",
      "epoch: 5 step: 395, loss is 0.1753648966550827\n",
      "epoch: 5 step: 396, loss is 0.2182750105857849\n",
      "epoch: 5 step: 397, loss is 0.11083716154098511\n",
      "epoch: 5 step: 398, loss is 0.3301209807395935\n",
      "epoch: 5 step: 399, loss is 0.060067374259233475\n",
      "epoch: 5 step: 400, loss is 0.26583683490753174\n",
      "epoch: 5 step: 401, loss is 0.06028025969862938\n",
      "epoch: 5 step: 402, loss is 0.17589208483695984\n",
      "epoch: 5 step: 403, loss is 0.11993936449289322\n",
      "epoch: 5 step: 404, loss is 0.09397716075181961\n",
      "epoch: 5 step: 405, loss is 0.1969556361436844\n",
      "epoch: 5 step: 406, loss is 0.22722230851650238\n",
      "epoch: 5 step: 407, loss is 0.14543014764785767\n",
      "epoch: 5 step: 408, loss is 0.13898210227489471\n",
      "epoch: 5 step: 409, loss is 0.1300884485244751\n",
      "epoch: 5 step: 410, loss is 0.18329088389873505\n",
      "epoch: 5 step: 411, loss is 0.08939021080732346\n",
      "epoch: 5 step: 412, loss is 0.11371811479330063\n",
      "epoch: 5 step: 413, loss is 0.15914148092269897\n",
      "epoch: 5 step: 414, loss is 0.15014329552650452\n",
      "epoch: 5 step: 415, loss is 0.11111355572938919\n",
      "epoch: 5 step: 416, loss is 0.13648153841495514\n",
      "epoch: 5 step: 417, loss is 0.13498787581920624\n",
      "epoch: 5 step: 418, loss is 0.16545720398426056\n",
      "epoch: 5 step: 419, loss is 0.2294090986251831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 420, loss is 0.21836736798286438\n",
      "epoch: 5 step: 421, loss is 0.22291681170463562\n",
      "epoch: 5 step: 422, loss is 0.10755565762519836\n",
      "epoch: 5 step: 423, loss is 0.20992352068424225\n",
      "epoch: 5 step: 424, loss is 0.1813463568687439\n",
      "epoch: 5 step: 425, loss is 0.1479446440935135\n",
      "epoch: 5 step: 426, loss is 0.0608736053109169\n",
      "epoch: 5 step: 427, loss is 0.17361775040626526\n",
      "epoch: 5 step: 428, loss is 0.08969000726938248\n",
      "epoch: 5 step: 429, loss is 0.050539735704660416\n",
      "epoch: 5 step: 430, loss is 0.13551253080368042\n",
      "epoch: 5 step: 431, loss is 0.2019822597503662\n",
      "epoch: 5 step: 432, loss is 0.0476280078291893\n",
      "epoch: 5 step: 433, loss is 0.32920029759407043\n",
      "epoch: 5 step: 434, loss is 0.10216342657804489\n",
      "epoch: 5 step: 435, loss is 0.2946728467941284\n",
      "epoch: 5 step: 436, loss is 0.18508003652095795\n",
      "epoch: 5 step: 437, loss is 0.11956498771905899\n",
      "epoch: 5 step: 438, loss is 0.07618293911218643\n",
      "epoch: 5 step: 439, loss is 0.18556150794029236\n",
      "epoch: 5 step: 440, loss is 0.15733347833156586\n",
      "epoch: 5 step: 441, loss is 0.1707673817873001\n",
      "epoch: 5 step: 442, loss is 0.2544509470462799\n",
      "epoch: 5 step: 443, loss is 0.17921677231788635\n",
      "epoch: 5 step: 444, loss is 0.20785050094127655\n",
      "epoch: 5 step: 445, loss is 0.09368846565485\n",
      "epoch: 5 step: 446, loss is 0.0792316347360611\n",
      "epoch: 5 step: 447, loss is 0.11165358126163483\n",
      "epoch: 5 step: 448, loss is 0.10400965064764023\n",
      "epoch: 5 step: 449, loss is 0.23685742914676666\n",
      "epoch: 5 step: 450, loss is 0.15743181109428406\n",
      "epoch: 5 step: 451, loss is 0.14491578936576843\n",
      "epoch: 5 step: 452, loss is 0.13232892751693726\n",
      "epoch: 5 step: 453, loss is 0.11634401977062225\n",
      "epoch: 5 step: 454, loss is 0.12316273152828217\n",
      "epoch: 5 step: 455, loss is 0.26723581552505493\n",
      "epoch: 5 step: 456, loss is 0.12829598784446716\n",
      "epoch: 5 step: 457, loss is 0.10048999637365341\n",
      "epoch: 5 step: 458, loss is 0.1867688000202179\n",
      "epoch: 5 step: 459, loss is 0.4255409240722656\n",
      "epoch: 5 step: 460, loss is 0.2034686803817749\n",
      "epoch: 5 step: 461, loss is 0.12362118065357208\n",
      "epoch: 5 step: 462, loss is 0.14858250319957733\n",
      "epoch: 5 step: 463, loss is 0.2773464024066925\n",
      "epoch: 5 step: 464, loss is 0.1933480054140091\n",
      "epoch: 5 step: 465, loss is 0.14135658740997314\n",
      "epoch: 5 step: 466, loss is 0.3896922171115875\n",
      "epoch: 5 step: 467, loss is 0.3590431809425354\n",
      "epoch: 5 step: 468, loss is 0.12688608467578888\n",
      "epoch: 5 step: 469, loss is 0.1693946272134781\n",
      "epoch: 5 step: 470, loss is 0.05259758606553078\n",
      "epoch: 5 step: 471, loss is 0.2504243850708008\n",
      "epoch: 5 step: 472, loss is 0.2177906036376953\n",
      "epoch: 5 step: 473, loss is 0.23535773158073425\n",
      "epoch: 5 step: 474, loss is 0.236203134059906\n",
      "epoch: 5 step: 475, loss is 0.1916438341140747\n",
      "epoch: 5 step: 476, loss is 0.14640840888023376\n",
      "epoch: 5 step: 477, loss is 0.16315819323062897\n",
      "epoch: 5 step: 478, loss is 0.35368749499320984\n",
      "epoch: 5 step: 479, loss is 0.14043466746807098\n",
      "epoch: 5 step: 480, loss is 0.18798020482063293\n",
      "epoch: 5 step: 481, loss is 0.21555469930171967\n",
      "epoch: 5 step: 482, loss is 0.08782293647527695\n",
      "epoch: 5 step: 483, loss is 0.08406387269496918\n",
      "epoch: 5 step: 484, loss is 0.15754203498363495\n",
      "epoch: 5 step: 485, loss is 0.23057232797145844\n",
      "epoch: 5 step: 486, loss is 0.09380385279655457\n",
      "epoch: 5 step: 487, loss is 0.1265791803598404\n",
      "epoch: 5 step: 488, loss is 0.1435009390115738\n",
      "epoch: 5 step: 489, loss is 0.1961275339126587\n",
      "epoch: 5 step: 490, loss is 0.1971645951271057\n",
      "epoch: 5 step: 491, loss is 0.08410604298114777\n",
      "epoch: 5 step: 492, loss is 0.1576302945613861\n",
      "epoch: 5 step: 493, loss is 0.1866447627544403\n",
      "epoch: 5 step: 494, loss is 0.20693697035312653\n",
      "epoch: 5 step: 495, loss is 0.19153860211372375\n",
      "epoch: 5 step: 496, loss is 0.18564952909946442\n",
      "epoch: 5 step: 497, loss is 0.06852006912231445\n",
      "epoch: 5 step: 498, loss is 0.1910550594329834\n",
      "epoch: 5 step: 499, loss is 0.11276176571846008\n",
      "epoch: 5 step: 500, loss is 0.16365014016628265\n",
      "epoch: 5 step: 501, loss is 0.300692081451416\n",
      "epoch: 5 step: 502, loss is 0.17023110389709473\n",
      "epoch: 5 step: 503, loss is 0.17239691317081451\n",
      "epoch: 5 step: 504, loss is 0.20317648351192474\n",
      "epoch: 5 step: 505, loss is 0.10213115066289902\n",
      "epoch: 5 step: 506, loss is 0.11332007497549057\n",
      "epoch: 5 step: 507, loss is 0.19414176046848297\n",
      "epoch: 5 step: 508, loss is 0.1630573719739914\n",
      "epoch: 5 step: 509, loss is 0.3015129864215851\n",
      "epoch: 5 step: 510, loss is 0.19528627395629883\n",
      "epoch: 5 step: 511, loss is 0.22023731470108032\n",
      "epoch: 5 step: 512, loss is 0.16851282119750977\n",
      "epoch: 5 step: 513, loss is 0.2147207409143448\n",
      "epoch: 5 step: 514, loss is 0.28277990221977234\n",
      "epoch: 5 step: 515, loss is 0.11506392061710358\n",
      "epoch: 5 step: 516, loss is 0.2029724270105362\n",
      "epoch: 5 step: 517, loss is 0.09987203776836395\n",
      "epoch: 5 step: 518, loss is 0.2891262173652649\n",
      "epoch: 5 step: 519, loss is 0.23960062861442566\n",
      "epoch: 5 step: 520, loss is 0.1633998453617096\n",
      "epoch: 5 step: 521, loss is 0.05441559478640556\n",
      "epoch: 5 step: 522, loss is 0.17228814959526062\n",
      "epoch: 5 step: 523, loss is 0.26413097977638245\n",
      "epoch: 5 step: 524, loss is 0.13332171738147736\n",
      "epoch: 5 step: 525, loss is 0.1697649210691452\n",
      "epoch: 5 step: 526, loss is 0.24919497966766357\n",
      "epoch: 5 step: 527, loss is 0.14247582852840424\n",
      "epoch: 5 step: 528, loss is 0.13582755625247955\n",
      "epoch: 5 step: 529, loss is 0.1588916927576065\n",
      "epoch: 5 step: 530, loss is 0.10187198221683502\n",
      "epoch: 5 step: 531, loss is 0.24732255935668945\n",
      "epoch: 5 step: 532, loss is 0.21677632629871368\n",
      "epoch: 5 step: 533, loss is 0.21849143505096436\n",
      "epoch: 5 step: 534, loss is 0.08641860634088516\n",
      "epoch: 5 step: 535, loss is 0.20783579349517822\n",
      "epoch: 5 step: 536, loss is 0.1600712388753891\n",
      "epoch: 5 step: 537, loss is 0.27191802859306335\n",
      "epoch: 5 step: 538, loss is 0.10149817913770676\n",
      "epoch: 5 step: 539, loss is 0.11569459736347198\n",
      "epoch: 5 step: 540, loss is 0.13868379592895508\n",
      "epoch: 5 step: 541, loss is 0.18639621138572693\n",
      "epoch: 5 step: 542, loss is 0.19366170465946198\n",
      "epoch: 5 step: 543, loss is 0.24888212978839874\n",
      "epoch: 5 step: 544, loss is 0.09750732779502869\n",
      "epoch: 5 step: 545, loss is 0.11060976982116699\n",
      "epoch: 5 step: 546, loss is 0.12390422821044922\n",
      "epoch: 5 step: 547, loss is 0.29292184114456177\n",
      "epoch: 5 step: 548, loss is 0.23333729803562164\n",
      "epoch: 5 step: 549, loss is 0.13210450112819672\n",
      "epoch: 5 step: 550, loss is 0.1470804661512375\n",
      "epoch: 5 step: 551, loss is 0.2339785099029541\n",
      "epoch: 5 step: 552, loss is 0.17042838037014008\n",
      "epoch: 5 step: 553, loss is 0.2811540365219116\n",
      "epoch: 5 step: 554, loss is 0.4122794568538666\n",
      "epoch: 5 step: 555, loss is 0.1536681205034256\n",
      "epoch: 5 step: 556, loss is 0.2588250935077667\n",
      "epoch: 5 step: 557, loss is 0.09388912469148636\n",
      "epoch: 5 step: 558, loss is 0.23017792403697968\n",
      "epoch: 5 step: 559, loss is 0.11124776303768158\n",
      "epoch: 5 step: 560, loss is 0.2045190930366516\n",
      "epoch: 5 step: 561, loss is 0.05337189510464668\n",
      "epoch: 5 step: 562, loss is 0.274199903011322\n",
      "epoch: 5 step: 563, loss is 0.12290560454130173\n",
      "epoch: 5 step: 564, loss is 0.18732625246047974\n",
      "epoch: 5 step: 565, loss is 0.09073789417743683\n",
      "epoch: 5 step: 566, loss is 0.14256329834461212\n",
      "epoch: 5 step: 567, loss is 0.24073751270771027\n",
      "epoch: 5 step: 568, loss is 0.17264780402183533\n",
      "epoch: 5 step: 569, loss is 0.1209070011973381\n",
      "epoch: 5 step: 570, loss is 0.15885043144226074\n",
      "epoch: 5 step: 571, loss is 0.11509447544813156\n",
      "epoch: 5 step: 572, loss is 0.22642406821250916\n",
      "epoch: 5 step: 573, loss is 0.237564355134964\n",
      "epoch: 5 step: 574, loss is 0.08069446682929993\n",
      "epoch: 5 step: 575, loss is 0.12187014520168304\n",
      "epoch: 5 step: 576, loss is 0.18817301094532013\n",
      "epoch: 5 step: 577, loss is 0.20196744799613953\n",
      "epoch: 5 step: 578, loss is 0.33233943581581116\n",
      "epoch: 5 step: 579, loss is 0.19957955181598663\n",
      "epoch: 5 step: 580, loss is 0.11498333513736725\n",
      "epoch: 5 step: 581, loss is 0.1074879914522171\n",
      "epoch: 5 step: 582, loss is 0.21860651671886444\n",
      "epoch: 5 step: 583, loss is 0.14006607234477997\n",
      "epoch: 5 step: 584, loss is 0.2689931094646454\n",
      "epoch: 5 step: 585, loss is 0.15908290445804596\n",
      "epoch: 5 step: 586, loss is 0.18736274540424347\n",
      "epoch: 5 step: 587, loss is 0.30192291736602783\n",
      "epoch: 5 step: 588, loss is 0.1334497332572937\n",
      "epoch: 5 step: 589, loss is 0.23998382687568665\n",
      "epoch: 5 step: 590, loss is 0.14933061599731445\n",
      "epoch: 5 step: 591, loss is 0.10478229820728302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 592, loss is 0.15468457341194153\n",
      "epoch: 5 step: 593, loss is 0.11606714874505997\n",
      "epoch: 5 step: 594, loss is 0.1429564207792282\n",
      "epoch: 5 step: 595, loss is 0.15040519833564758\n",
      "epoch: 5 step: 596, loss is 0.07692863047122955\n",
      "epoch: 5 step: 597, loss is 0.14034685492515564\n",
      "epoch: 5 step: 598, loss is 0.2091480791568756\n",
      "epoch: 5 step: 599, loss is 0.1047421395778656\n",
      "epoch: 5 step: 600, loss is 0.17573173344135284\n",
      "epoch: 5 step: 601, loss is 0.09604417532682419\n",
      "epoch: 5 step: 602, loss is 0.24472671747207642\n",
      "epoch: 5 step: 603, loss is 0.28568917512893677\n",
      "epoch: 5 step: 604, loss is 0.35965755581855774\n",
      "epoch: 5 step: 605, loss is 0.16747933626174927\n",
      "epoch: 5 step: 606, loss is 0.15435707569122314\n",
      "epoch: 5 step: 607, loss is 0.07403671741485596\n",
      "epoch: 5 step: 608, loss is 0.056173596531152725\n",
      "epoch: 5 step: 609, loss is 0.17531850934028625\n",
      "epoch: 5 step: 610, loss is 0.14754585921764374\n",
      "epoch: 5 step: 611, loss is 0.2896045744419098\n",
      "epoch: 5 step: 612, loss is 0.2572232186794281\n",
      "epoch: 5 step: 613, loss is 0.18018393218517303\n",
      "epoch: 5 step: 614, loss is 0.20416179299354553\n",
      "epoch: 5 step: 615, loss is 0.09854763001203537\n",
      "epoch: 5 step: 616, loss is 0.18468694388866425\n",
      "epoch: 5 step: 617, loss is 0.19690696895122528\n",
      "epoch: 5 step: 618, loss is 0.1865016669034958\n",
      "epoch: 5 step: 619, loss is 0.15367096662521362\n",
      "epoch: 5 step: 620, loss is 0.18353833258152008\n",
      "epoch: 5 step: 621, loss is 0.0857059583067894\n",
      "epoch: 5 step: 622, loss is 0.17306722700595856\n",
      "epoch: 5 step: 623, loss is 0.16583885252475739\n",
      "epoch: 5 step: 624, loss is 0.16894744336605072\n",
      "epoch: 5 step: 625, loss is 0.13363230228424072\n",
      "epoch: 5 step: 626, loss is 0.055435631424188614\n",
      "epoch: 5 step: 627, loss is 0.176607146859169\n",
      "epoch: 5 step: 628, loss is 0.36517050862312317\n",
      "epoch: 5 step: 629, loss is 0.15852373838424683\n",
      "epoch: 5 step: 630, loss is 0.16179944574832916\n",
      "epoch: 5 step: 631, loss is 0.25116580724716187\n",
      "epoch: 5 step: 632, loss is 0.12712472677230835\n",
      "epoch: 5 step: 633, loss is 0.2517732083797455\n",
      "epoch: 5 step: 634, loss is 0.053417518734931946\n",
      "epoch: 5 step: 635, loss is 0.11335482448339462\n",
      "epoch: 5 step: 636, loss is 0.24573926627635956\n",
      "epoch: 5 step: 637, loss is 0.2156953662633896\n",
      "epoch: 5 step: 638, loss is 0.2238835245370865\n",
      "epoch: 5 step: 639, loss is 0.2009064108133316\n",
      "epoch: 5 step: 640, loss is 0.18025770783424377\n",
      "epoch: 5 step: 641, loss is 0.09245746582746506\n",
      "epoch: 5 step: 642, loss is 0.15615202486515045\n",
      "epoch: 5 step: 643, loss is 0.26426953077316284\n",
      "epoch: 5 step: 644, loss is 0.2902379035949707\n",
      "epoch: 5 step: 645, loss is 0.1413496732711792\n",
      "epoch: 5 step: 646, loss is 0.16634614765644073\n",
      "epoch: 5 step: 647, loss is 0.2763812839984894\n",
      "epoch: 5 step: 648, loss is 0.14884749054908752\n",
      "epoch: 5 step: 649, loss is 0.1149396225810051\n",
      "epoch: 5 step: 650, loss is 0.19275392591953278\n",
      "epoch: 5 step: 651, loss is 0.1324135959148407\n",
      "epoch: 5 step: 652, loss is 0.21736429631710052\n",
      "epoch: 5 step: 653, loss is 0.20861367881298065\n",
      "epoch: 5 step: 654, loss is 0.17395462095737457\n",
      "epoch: 5 step: 655, loss is 0.2185412347316742\n",
      "epoch: 5 step: 656, loss is 0.17697268724441528\n",
      "epoch: 5 step: 657, loss is 0.3621199429035187\n",
      "epoch: 5 step: 658, loss is 0.14931446313858032\n",
      "epoch: 5 step: 659, loss is 0.15228068828582764\n",
      "epoch: 5 step: 660, loss is 0.1535259485244751\n",
      "epoch: 5 step: 661, loss is 0.21276035904884338\n",
      "epoch: 5 step: 662, loss is 0.2023942917585373\n",
      "epoch: 5 step: 663, loss is 0.22031603753566742\n",
      "epoch: 5 step: 664, loss is 0.31851911544799805\n",
      "epoch: 5 step: 665, loss is 0.12003869563341141\n",
      "epoch: 5 step: 666, loss is 0.159742072224617\n",
      "epoch: 5 step: 667, loss is 0.0873028114438057\n",
      "epoch: 5 step: 668, loss is 0.13440866768360138\n",
      "epoch: 5 step: 669, loss is 0.12702952325344086\n",
      "epoch: 5 step: 670, loss is 0.14689220488071442\n",
      "epoch: 5 step: 671, loss is 0.08994525671005249\n",
      "epoch: 5 step: 672, loss is 0.17942216992378235\n",
      "epoch: 5 step: 673, loss is 0.19359466433525085\n",
      "epoch: 5 step: 674, loss is 0.25190699100494385\n",
      "epoch: 5 step: 675, loss is 0.32406702637672424\n",
      "epoch: 5 step: 676, loss is 0.32156649231910706\n",
      "epoch: 5 step: 677, loss is 0.1730240285396576\n",
      "epoch: 5 step: 678, loss is 0.0777764543890953\n",
      "epoch: 5 step: 679, loss is 0.1892453283071518\n",
      "epoch: 5 step: 680, loss is 0.17567358911037445\n",
      "epoch: 5 step: 681, loss is 0.0535476990044117\n",
      "epoch: 5 step: 682, loss is 0.3345791697502136\n",
      "epoch: 5 step: 683, loss is 0.09049864113330841\n",
      "epoch: 5 step: 684, loss is 0.17305271327495575\n",
      "epoch: 5 step: 685, loss is 0.19133268296718597\n",
      "epoch: 5 step: 686, loss is 0.07004359364509583\n",
      "epoch: 5 step: 687, loss is 0.1954765021800995\n",
      "epoch: 5 step: 688, loss is 0.28021490573883057\n",
      "epoch: 5 step: 689, loss is 0.10902101546525955\n",
      "epoch: 5 step: 690, loss is 0.16927379369735718\n",
      "epoch: 5 step: 691, loss is 0.15082837641239166\n",
      "epoch: 5 step: 692, loss is 0.11157460510730743\n",
      "epoch: 5 step: 693, loss is 0.2132149040699005\n",
      "epoch: 5 step: 694, loss is 0.18295836448669434\n",
      "epoch: 5 step: 695, loss is 0.12365136295557022\n",
      "epoch: 5 step: 696, loss is 0.24699270725250244\n",
      "epoch: 5 step: 697, loss is 0.15851017832756042\n",
      "epoch: 5 step: 698, loss is 0.2755719721317291\n",
      "epoch: 5 step: 699, loss is 0.2826113700866699\n",
      "epoch: 5 step: 700, loss is 0.11184312403202057\n",
      "epoch: 5 step: 701, loss is 0.15608477592468262\n",
      "epoch: 5 step: 702, loss is 0.18012961745262146\n",
      "epoch: 5 step: 703, loss is 0.2995699942111969\n",
      "epoch: 5 step: 704, loss is 0.16865752637386322\n",
      "epoch: 5 step: 705, loss is 0.2117541879415512\n",
      "epoch: 5 step: 706, loss is 0.17211925983428955\n",
      "epoch: 5 step: 707, loss is 0.10595164448022842\n",
      "epoch: 5 step: 708, loss is 0.17789757251739502\n",
      "epoch: 5 step: 709, loss is 0.09492140263319016\n",
      "epoch: 5 step: 710, loss is 0.21741127967834473\n",
      "epoch: 5 step: 711, loss is 0.18584060668945312\n",
      "epoch: 5 step: 712, loss is 0.09003494679927826\n",
      "epoch: 5 step: 713, loss is 0.2682490348815918\n",
      "epoch: 5 step: 714, loss is 0.19971013069152832\n",
      "epoch: 5 step: 715, loss is 0.1444370299577713\n",
      "epoch: 5 step: 716, loss is 0.2449566274881363\n",
      "epoch: 5 step: 717, loss is 0.07630601525306702\n",
      "epoch: 5 step: 718, loss is 0.06434301286935806\n",
      "epoch: 5 step: 719, loss is 0.06323488056659698\n",
      "epoch: 5 step: 720, loss is 0.10337033122777939\n",
      "epoch: 5 step: 721, loss is 0.26641133427619934\n",
      "epoch: 5 step: 722, loss is 0.10292346030473709\n",
      "epoch: 5 step: 723, loss is 0.12333977967500687\n",
      "epoch: 5 step: 724, loss is 0.13718433678150177\n",
      "epoch: 5 step: 725, loss is 0.23568038642406464\n",
      "epoch: 5 step: 726, loss is 0.2662309408187866\n",
      "epoch: 5 step: 727, loss is 0.397889107465744\n",
      "epoch: 5 step: 728, loss is 0.13093149662017822\n",
      "epoch: 5 step: 729, loss is 0.19727005064487457\n",
      "epoch: 5 step: 730, loss is 0.2062995582818985\n",
      "epoch: 5 step: 731, loss is 0.1498050093650818\n",
      "epoch: 5 step: 732, loss is 0.14448019862174988\n",
      "epoch: 5 step: 733, loss is 0.09738222509622574\n",
      "epoch: 5 step: 734, loss is 0.08004099130630493\n",
      "epoch: 5 step: 735, loss is 0.11849910020828247\n",
      "epoch: 5 step: 736, loss is 0.11866383999586105\n",
      "epoch: 5 step: 737, loss is 0.24803821742534637\n",
      "epoch: 5 step: 738, loss is 0.19624875485897064\n",
      "epoch: 5 step: 739, loss is 0.3047164976596832\n",
      "epoch: 5 step: 740, loss is 0.194627046585083\n",
      "epoch: 5 step: 741, loss is 0.16854649782180786\n",
      "epoch: 5 step: 742, loss is 0.304599791765213\n",
      "epoch: 5 step: 743, loss is 0.08139421045780182\n",
      "epoch: 5 step: 744, loss is 0.20330390334129333\n",
      "epoch: 5 step: 745, loss is 0.1946910172700882\n",
      "epoch: 5 step: 746, loss is 0.20838873088359833\n",
      "epoch: 5 step: 747, loss is 0.15114633738994598\n",
      "epoch: 5 step: 748, loss is 0.058217886835336685\n",
      "epoch: 5 step: 749, loss is 0.18423675000667572\n",
      "epoch: 5 step: 750, loss is 0.15917551517486572\n",
      "epoch: 5 step: 751, loss is 0.22328901290893555\n",
      "epoch: 5 step: 752, loss is 0.06685173511505127\n",
      "epoch: 5 step: 753, loss is 0.13869646191596985\n",
      "epoch: 5 step: 754, loss is 0.21769411861896515\n",
      "epoch: 5 step: 755, loss is 0.1594812273979187\n",
      "epoch: 5 step: 756, loss is 0.254041850566864\n",
      "epoch: 5 step: 757, loss is 0.1978178173303604\n",
      "epoch: 5 step: 758, loss is 0.1725562959909439\n",
      "epoch: 5 step: 759, loss is 0.31716832518577576\n",
      "epoch: 5 step: 760, loss is 0.10049138963222504\n",
      "epoch: 5 step: 761, loss is 0.0951666533946991\n",
      "epoch: 5 step: 762, loss is 0.18274690210819244\n",
      "epoch: 5 step: 763, loss is 0.06923865526914597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 764, loss is 0.09479071199893951\n",
      "epoch: 5 step: 765, loss is 0.08584948629140854\n",
      "epoch: 5 step: 766, loss is 0.08110705018043518\n",
      "epoch: 5 step: 767, loss is 0.250953733921051\n",
      "epoch: 5 step: 768, loss is 0.13073572516441345\n",
      "epoch: 5 step: 769, loss is 0.2841523587703705\n",
      "epoch: 5 step: 770, loss is 0.11198540776968002\n",
      "epoch: 5 step: 771, loss is 0.1487327665090561\n",
      "epoch: 5 step: 772, loss is 0.26889678835868835\n",
      "epoch: 5 step: 773, loss is 0.1611608862876892\n",
      "epoch: 5 step: 774, loss is 0.17898786067962646\n",
      "epoch: 5 step: 775, loss is 0.18528926372528076\n",
      "epoch: 5 step: 776, loss is 0.1858418583869934\n",
      "epoch: 5 step: 777, loss is 0.21667499840259552\n",
      "epoch: 5 step: 778, loss is 0.3015953302383423\n",
      "epoch: 5 step: 779, loss is 0.2968185544013977\n",
      "epoch: 5 step: 780, loss is 0.10868144035339355\n",
      "epoch: 5 step: 781, loss is 0.11992944031953812\n",
      "epoch: 5 step: 782, loss is 0.26364031434059143\n",
      "epoch: 5 step: 783, loss is 0.16949716210365295\n",
      "epoch: 5 step: 784, loss is 0.06745510548353195\n",
      "epoch: 5 step: 785, loss is 0.10640264302492142\n",
      "epoch: 5 step: 786, loss is 0.10141263157129288\n",
      "epoch: 5 step: 787, loss is 0.23079894483089447\n",
      "epoch: 5 step: 788, loss is 0.2820631265640259\n",
      "epoch: 5 step: 789, loss is 0.24927017092704773\n",
      "epoch: 5 step: 790, loss is 0.31060293316841125\n",
      "epoch: 5 step: 791, loss is 0.12104978412389755\n",
      "epoch: 5 step: 792, loss is 0.16200123727321625\n",
      "epoch: 5 step: 793, loss is 0.18611523509025574\n",
      "epoch: 5 step: 794, loss is 0.1908634752035141\n",
      "epoch: 5 step: 795, loss is 0.3674648404121399\n",
      "epoch: 5 step: 796, loss is 0.1570151001214981\n",
      "epoch: 5 step: 797, loss is 0.21871492266654968\n",
      "epoch: 5 step: 798, loss is 0.14295737445354462\n",
      "epoch: 5 step: 799, loss is 0.11773242801427841\n",
      "epoch: 5 step: 800, loss is 0.19068482518196106\n",
      "epoch: 5 step: 801, loss is 0.23258139193058014\n",
      "epoch: 5 step: 802, loss is 0.1694321632385254\n",
      "epoch: 5 step: 803, loss is 0.34675583243370056\n",
      "epoch: 5 step: 804, loss is 0.12115412205457687\n",
      "epoch: 5 step: 805, loss is 0.27717968821525574\n",
      "epoch: 5 step: 806, loss is 0.18469679355621338\n",
      "epoch: 5 step: 807, loss is 0.09768124669790268\n",
      "epoch: 5 step: 808, loss is 0.21559973061084747\n",
      "epoch: 5 step: 809, loss is 0.17013399302959442\n",
      "epoch: 5 step: 810, loss is 0.13537384569644928\n",
      "epoch: 5 step: 811, loss is 0.1378280073404312\n",
      "epoch: 5 step: 812, loss is 0.08259405940771103\n",
      "epoch: 5 step: 813, loss is 0.20638060569763184\n",
      "epoch: 5 step: 814, loss is 0.09898266941308975\n",
      "epoch: 5 step: 815, loss is 0.08115178346633911\n",
      "epoch: 5 step: 816, loss is 0.1304885596036911\n",
      "epoch: 5 step: 817, loss is 0.24179445207118988\n",
      "epoch: 5 step: 818, loss is 0.14463098347187042\n",
      "epoch: 5 step: 819, loss is 0.19493216276168823\n",
      "epoch: 5 step: 820, loss is 0.16756626963615417\n",
      "epoch: 5 step: 821, loss is 0.09594829380512238\n",
      "epoch: 5 step: 822, loss is 0.2694108188152313\n",
      "epoch: 5 step: 823, loss is 0.13893495500087738\n",
      "epoch: 5 step: 824, loss is 0.06286657601594925\n",
      "epoch: 5 step: 825, loss is 0.2823544442653656\n",
      "epoch: 5 step: 826, loss is 0.05145995691418648\n",
      "epoch: 5 step: 827, loss is 0.2880110442638397\n",
      "epoch: 5 step: 828, loss is 0.052532635629177094\n",
      "epoch: 5 step: 829, loss is 0.1933261603116989\n",
      "epoch: 5 step: 830, loss is 0.2831565737724304\n",
      "epoch: 5 step: 831, loss is 0.20346610248088837\n",
      "epoch: 5 step: 832, loss is 0.32185372710227966\n",
      "epoch: 5 step: 833, loss is 0.2242235243320465\n",
      "epoch: 5 step: 834, loss is 0.1597062200307846\n",
      "epoch: 5 step: 835, loss is 0.13675996661186218\n",
      "epoch: 5 step: 836, loss is 0.1174720823764801\n",
      "epoch: 5 step: 837, loss is 0.17234061658382416\n",
      "epoch: 5 step: 838, loss is 0.1722223460674286\n",
      "epoch: 5 step: 839, loss is 0.1800730973482132\n",
      "epoch: 5 step: 840, loss is 0.2577102780342102\n",
      "epoch: 5 step: 841, loss is 0.19513165950775146\n",
      "epoch: 5 step: 842, loss is 0.21531634032726288\n",
      "epoch: 5 step: 843, loss is 0.09070560336112976\n",
      "epoch: 5 step: 844, loss is 0.22093495726585388\n",
      "epoch: 5 step: 845, loss is 0.23454783856868744\n",
      "epoch: 5 step: 846, loss is 0.14956456422805786\n",
      "epoch: 5 step: 847, loss is 0.24543724954128265\n",
      "epoch: 5 step: 848, loss is 0.15034247934818268\n",
      "epoch: 5 step: 849, loss is 0.16208241879940033\n",
      "epoch: 5 step: 850, loss is 0.13429483771324158\n",
      "epoch: 5 step: 851, loss is 0.1973731964826584\n",
      "epoch: 5 step: 852, loss is 0.17023873329162598\n",
      "epoch: 5 step: 853, loss is 0.2856241762638092\n",
      "epoch: 5 step: 854, loss is 0.21496568620204926\n",
      "epoch: 5 step: 855, loss is 0.27871114015579224\n",
      "epoch: 5 step: 856, loss is 0.19892087578773499\n",
      "epoch: 5 step: 857, loss is 0.33191242814064026\n",
      "epoch: 5 step: 858, loss is 0.16399340331554413\n",
      "epoch: 5 step: 859, loss is 0.22136802971363068\n",
      "epoch: 5 step: 860, loss is 0.0823439210653305\n",
      "epoch: 5 step: 861, loss is 0.10735820978879929\n",
      "epoch: 5 step: 862, loss is 0.3212798237800598\n",
      "epoch: 5 step: 863, loss is 0.1494910717010498\n",
      "epoch: 5 step: 864, loss is 0.11808968335390091\n",
      "epoch: 5 step: 865, loss is 0.11064720898866653\n",
      "epoch: 5 step: 866, loss is 0.11009430140256882\n",
      "epoch: 5 step: 867, loss is 0.12050992250442505\n",
      "epoch: 5 step: 868, loss is 0.06669279932975769\n",
      "epoch: 5 step: 869, loss is 0.1426057517528534\n",
      "epoch: 5 step: 870, loss is 0.17043963074684143\n",
      "epoch: 5 step: 871, loss is 0.0716886818408966\n",
      "epoch: 5 step: 872, loss is 0.16339385509490967\n",
      "epoch: 5 step: 873, loss is 0.18568195402622223\n",
      "epoch: 5 step: 874, loss is 0.14347703754901886\n",
      "epoch: 5 step: 875, loss is 0.13842107355594635\n",
      "epoch: 5 step: 876, loss is 0.13248953223228455\n",
      "epoch: 5 step: 877, loss is 0.15456680953502655\n",
      "epoch: 5 step: 878, loss is 0.11690358072519302\n",
      "epoch: 5 step: 879, loss is 0.1834527999162674\n",
      "epoch: 5 step: 880, loss is 0.1403515487909317\n",
      "epoch: 5 step: 881, loss is 0.20697428286075592\n",
      "epoch: 5 step: 882, loss is 0.207462877035141\n",
      "epoch: 5 step: 883, loss is 0.041094422340393066\n",
      "epoch: 5 step: 884, loss is 0.12816016376018524\n",
      "epoch: 5 step: 885, loss is 0.24838276207447052\n",
      "epoch: 5 step: 886, loss is 0.1073514074087143\n",
      "epoch: 5 step: 887, loss is 0.10629408061504364\n",
      "epoch: 5 step: 888, loss is 0.21550306677818298\n",
      "epoch: 5 step: 889, loss is 0.07672478258609772\n",
      "epoch: 5 step: 890, loss is 0.08318260312080383\n",
      "epoch: 5 step: 891, loss is 0.11507593840360641\n",
      "epoch: 5 step: 892, loss is 0.12116985768079758\n",
      "epoch: 5 step: 893, loss is 0.28137490153312683\n",
      "epoch: 5 step: 894, loss is 0.12013562768697739\n",
      "epoch: 5 step: 895, loss is 0.17606160044670105\n",
      "epoch: 5 step: 896, loss is 0.20212069153785706\n",
      "epoch: 5 step: 897, loss is 0.15723837912082672\n",
      "epoch: 5 step: 898, loss is 0.07955099642276764\n",
      "epoch: 5 step: 899, loss is 0.17042304575443268\n",
      "epoch: 5 step: 900, loss is 0.13421304523944855\n",
      "epoch: 5 step: 901, loss is 0.13321027159690857\n",
      "epoch: 5 step: 902, loss is 0.2113369256258011\n",
      "epoch: 5 step: 903, loss is 0.1538209617137909\n",
      "epoch: 5 step: 904, loss is 0.17787174880504608\n",
      "epoch: 5 step: 905, loss is 0.2064521461725235\n",
      "epoch: 5 step: 906, loss is 0.2845788896083832\n",
      "epoch: 5 step: 907, loss is 0.12481259554624557\n",
      "epoch: 5 step: 908, loss is 0.05956714227795601\n",
      "epoch: 5 step: 909, loss is 0.2353174239397049\n",
      "epoch: 5 step: 910, loss is 0.1807667464017868\n",
      "epoch: 5 step: 911, loss is 0.31660744547843933\n",
      "epoch: 5 step: 912, loss is 0.12800484895706177\n",
      "epoch: 5 step: 913, loss is 0.17537637054920197\n",
      "epoch: 5 step: 914, loss is 0.1581648290157318\n",
      "epoch: 5 step: 915, loss is 0.21531948447227478\n",
      "epoch: 5 step: 916, loss is 0.33707597851753235\n",
      "epoch: 5 step: 917, loss is 0.3091168999671936\n",
      "epoch: 5 step: 918, loss is 0.13426457345485687\n",
      "epoch: 5 step: 919, loss is 0.20306596159934998\n",
      "epoch: 5 step: 920, loss is 0.19876253604888916\n",
      "epoch: 5 step: 921, loss is 0.1149517372250557\n",
      "epoch: 5 step: 922, loss is 0.08041924238204956\n",
      "epoch: 5 step: 923, loss is 0.2317531406879425\n",
      "epoch: 5 step: 924, loss is 0.08884976804256439\n",
      "epoch: 5 step: 925, loss is 0.1698327660560608\n",
      "epoch: 5 step: 926, loss is 0.2722196877002716\n",
      "epoch: 5 step: 927, loss is 0.10605543851852417\n",
      "epoch: 5 step: 928, loss is 0.09086877852678299\n",
      "epoch: 5 step: 929, loss is 0.12813304364681244\n",
      "epoch: 5 step: 930, loss is 0.1442078799009323\n",
      "epoch: 5 step: 931, loss is 0.12749382853507996\n",
      "epoch: 5 step: 932, loss is 0.13739091157913208\n",
      "epoch: 5 step: 933, loss is 0.1582900881767273\n",
      "epoch: 5 step: 934, loss is 0.15071514248847961\n",
      "epoch: 5 step: 935, loss is 0.20434963703155518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 936, loss is 0.16123467683792114\n",
      "epoch: 5 step: 937, loss is 0.12423129379749298\n",
      "epoch: 6 step: 1, loss is 0.15626408159732819\n",
      "epoch: 6 step: 2, loss is 0.09854679554700851\n",
      "epoch: 6 step: 3, loss is 0.04887395352125168\n",
      "epoch: 6 step: 4, loss is 0.0990520492196083\n",
      "epoch: 6 step: 5, loss is 0.0580134280025959\n",
      "epoch: 6 step: 6, loss is 0.2677857577800751\n",
      "epoch: 6 step: 7, loss is 0.15675479173660278\n",
      "epoch: 6 step: 8, loss is 0.062142182141542435\n",
      "epoch: 6 step: 9, loss is 0.19345417618751526\n",
      "epoch: 6 step: 10, loss is 0.15748944878578186\n",
      "epoch: 6 step: 11, loss is 0.0832495465874672\n",
      "epoch: 6 step: 12, loss is 0.10053452104330063\n",
      "epoch: 6 step: 13, loss is 0.11064879596233368\n",
      "epoch: 6 step: 14, loss is 0.11091635376214981\n",
      "epoch: 6 step: 15, loss is 0.11400363594293594\n",
      "epoch: 6 step: 16, loss is 0.050770703703165054\n",
      "epoch: 6 step: 17, loss is 0.18473108112812042\n",
      "epoch: 6 step: 18, loss is 0.11171962320804596\n",
      "epoch: 6 step: 19, loss is 0.2714492380619049\n",
      "epoch: 6 step: 20, loss is 0.28820228576660156\n",
      "epoch: 6 step: 21, loss is 0.32715633511543274\n",
      "epoch: 6 step: 22, loss is 0.128374844789505\n",
      "epoch: 6 step: 23, loss is 0.1603471338748932\n",
      "epoch: 6 step: 24, loss is 0.2452576458454132\n",
      "epoch: 6 step: 25, loss is 0.18775391578674316\n",
      "epoch: 6 step: 26, loss is 0.15860094130039215\n",
      "epoch: 6 step: 27, loss is 0.19570733606815338\n",
      "epoch: 6 step: 28, loss is 0.0619606114923954\n",
      "epoch: 6 step: 29, loss is 0.10228735953569412\n",
      "epoch: 6 step: 30, loss is 0.1255253702402115\n",
      "epoch: 6 step: 31, loss is 0.16414278745651245\n",
      "epoch: 6 step: 32, loss is 0.16146694123744965\n",
      "epoch: 6 step: 33, loss is 0.23586773872375488\n",
      "epoch: 6 step: 34, loss is 0.1362161934375763\n",
      "epoch: 6 step: 35, loss is 0.051384225487709045\n",
      "epoch: 6 step: 36, loss is 0.31745362281799316\n",
      "epoch: 6 step: 37, loss is 0.1130741760134697\n",
      "epoch: 6 step: 38, loss is 0.1582813262939453\n",
      "epoch: 6 step: 39, loss is 0.07967013120651245\n",
      "epoch: 6 step: 40, loss is 0.10981552302837372\n",
      "epoch: 6 step: 41, loss is 0.18911340832710266\n",
      "epoch: 6 step: 42, loss is 0.05434823036193848\n",
      "epoch: 6 step: 43, loss is 0.25629398226737976\n",
      "epoch: 6 step: 44, loss is 0.10569675266742706\n",
      "epoch: 6 step: 45, loss is 0.0738023892045021\n",
      "epoch: 6 step: 46, loss is 0.16227751970291138\n",
      "epoch: 6 step: 47, loss is 0.15029895305633545\n",
      "epoch: 6 step: 48, loss is 0.14982885122299194\n",
      "epoch: 6 step: 49, loss is 0.08073282241821289\n",
      "epoch: 6 step: 50, loss is 0.12547488510608673\n",
      "epoch: 6 step: 51, loss is 0.09785334765911102\n",
      "epoch: 6 step: 52, loss is 0.07410845160484314\n",
      "epoch: 6 step: 53, loss is 0.11941000074148178\n",
      "epoch: 6 step: 54, loss is 0.05158122256398201\n",
      "epoch: 6 step: 55, loss is 0.07555215060710907\n",
      "epoch: 6 step: 56, loss is 0.11526136100292206\n",
      "epoch: 6 step: 57, loss is 0.12068723142147064\n",
      "epoch: 6 step: 58, loss is 0.15721283853054047\n",
      "epoch: 6 step: 59, loss is 0.1452914923429489\n",
      "epoch: 6 step: 60, loss is 0.17948462069034576\n",
      "epoch: 6 step: 61, loss is 0.13864989578723907\n",
      "epoch: 6 step: 62, loss is 0.20998592674732208\n",
      "epoch: 6 step: 63, loss is 0.07452432811260223\n",
      "epoch: 6 step: 64, loss is 0.24411849677562714\n",
      "epoch: 6 step: 65, loss is 0.12965194880962372\n",
      "epoch: 6 step: 66, loss is 0.16892044246196747\n",
      "epoch: 6 step: 67, loss is 0.17377084493637085\n",
      "epoch: 6 step: 68, loss is 0.09288973361253738\n",
      "epoch: 6 step: 69, loss is 0.11965592950582504\n",
      "epoch: 6 step: 70, loss is 0.08174428343772888\n",
      "epoch: 6 step: 71, loss is 0.07194659858942032\n",
      "epoch: 6 step: 72, loss is 0.28338712453842163\n",
      "epoch: 6 step: 73, loss is 0.11845622956752777\n",
      "epoch: 6 step: 74, loss is 0.10714542865753174\n",
      "epoch: 6 step: 75, loss is 0.07270502299070358\n",
      "epoch: 6 step: 76, loss is 0.3136458992958069\n",
      "epoch: 6 step: 77, loss is 0.18424922227859497\n",
      "epoch: 6 step: 78, loss is 0.08187364786863327\n",
      "epoch: 6 step: 79, loss is 0.12332358956336975\n",
      "epoch: 6 step: 80, loss is 0.23373793065547943\n",
      "epoch: 6 step: 81, loss is 0.19232670962810516\n",
      "epoch: 6 step: 82, loss is 0.07658667862415314\n",
      "epoch: 6 step: 83, loss is 0.22045932710170746\n",
      "epoch: 6 step: 84, loss is 0.14593087136745453\n",
      "epoch: 6 step: 85, loss is 0.15801776945590973\n",
      "epoch: 6 step: 86, loss is 0.2292073667049408\n",
      "epoch: 6 step: 87, loss is 0.1837281882762909\n",
      "epoch: 6 step: 88, loss is 0.2528846561908722\n",
      "epoch: 6 step: 89, loss is 0.18973666429519653\n",
      "epoch: 6 step: 90, loss is 0.12125448137521744\n",
      "epoch: 6 step: 91, loss is 0.08449872583150864\n",
      "epoch: 6 step: 92, loss is 0.13260239362716675\n",
      "epoch: 6 step: 93, loss is 0.1761152297258377\n",
      "epoch: 6 step: 94, loss is 0.23449651896953583\n",
      "epoch: 6 step: 95, loss is 0.07154028862714767\n",
      "epoch: 6 step: 96, loss is 0.1249275729060173\n",
      "epoch: 6 step: 97, loss is 0.07737035304307938\n",
      "epoch: 6 step: 98, loss is 0.11250143498182297\n",
      "epoch: 6 step: 99, loss is 0.15227177739143372\n",
      "epoch: 6 step: 100, loss is 0.14188899099826813\n",
      "epoch: 6 step: 101, loss is 0.19276659190654755\n",
      "epoch: 6 step: 102, loss is 0.07726041227579117\n",
      "epoch: 6 step: 103, loss is 0.14435651898384094\n",
      "epoch: 6 step: 104, loss is 0.2333744317293167\n",
      "epoch: 6 step: 105, loss is 0.1582707166671753\n",
      "epoch: 6 step: 106, loss is 0.16095197200775146\n",
      "epoch: 6 step: 107, loss is 0.2215978503227234\n",
      "epoch: 6 step: 108, loss is 0.19978740811347961\n",
      "epoch: 6 step: 109, loss is 0.33539310097694397\n",
      "epoch: 6 step: 110, loss is 0.1435646414756775\n",
      "epoch: 6 step: 111, loss is 0.1737389713525772\n",
      "epoch: 6 step: 112, loss is 0.20660662651062012\n",
      "epoch: 6 step: 113, loss is 0.16815222799777985\n",
      "epoch: 6 step: 114, loss is 0.1770588755607605\n",
      "epoch: 6 step: 115, loss is 0.06750145554542542\n",
      "epoch: 6 step: 116, loss is 0.12792937457561493\n",
      "epoch: 6 step: 117, loss is 0.052194226533174515\n",
      "epoch: 6 step: 118, loss is 0.05923190340399742\n",
      "epoch: 6 step: 119, loss is 0.16949839890003204\n",
      "epoch: 6 step: 120, loss is 0.2007695883512497\n",
      "epoch: 6 step: 121, loss is 0.11392862349748611\n",
      "epoch: 6 step: 122, loss is 0.1986069530248642\n",
      "epoch: 6 step: 123, loss is 0.1767735630273819\n",
      "epoch: 6 step: 124, loss is 0.23127037286758423\n",
      "epoch: 6 step: 125, loss is 0.06778264790773392\n",
      "epoch: 6 step: 126, loss is 0.08092163503170013\n",
      "epoch: 6 step: 127, loss is 0.09091019630432129\n",
      "epoch: 6 step: 128, loss is 0.11243434250354767\n",
      "epoch: 6 step: 129, loss is 0.12180916219949722\n",
      "epoch: 6 step: 130, loss is 0.09522102028131485\n",
      "epoch: 6 step: 131, loss is 0.09627704322338104\n",
      "epoch: 6 step: 132, loss is 0.19010932743549347\n",
      "epoch: 6 step: 133, loss is 0.054838985204696655\n",
      "epoch: 6 step: 134, loss is 0.3547079563140869\n",
      "epoch: 6 step: 135, loss is 0.3956705927848816\n",
      "epoch: 6 step: 136, loss is 0.24654489755630493\n",
      "epoch: 6 step: 137, loss is 0.08572255074977875\n",
      "epoch: 6 step: 138, loss is 0.1392684280872345\n",
      "epoch: 6 step: 139, loss is 0.09144561737775803\n",
      "epoch: 6 step: 140, loss is 0.11831801384687424\n",
      "epoch: 6 step: 141, loss is 0.08477876335382462\n",
      "epoch: 6 step: 142, loss is 0.12743663787841797\n",
      "epoch: 6 step: 143, loss is 0.08954014629125595\n",
      "epoch: 6 step: 144, loss is 0.16885358095169067\n",
      "epoch: 6 step: 145, loss is 0.2620420455932617\n",
      "epoch: 6 step: 146, loss is 0.22602178156375885\n",
      "epoch: 6 step: 147, loss is 0.19259263575077057\n",
      "epoch: 6 step: 148, loss is 0.1835172176361084\n",
      "epoch: 6 step: 149, loss is 0.274385929107666\n",
      "epoch: 6 step: 150, loss is 0.07964334636926651\n",
      "epoch: 6 step: 151, loss is 0.060812413692474365\n",
      "epoch: 6 step: 152, loss is 0.18066026270389557\n",
      "epoch: 6 step: 153, loss is 0.20023642480373383\n",
      "epoch: 6 step: 154, loss is 0.1303442269563675\n",
      "epoch: 6 step: 155, loss is 0.127387136220932\n",
      "epoch: 6 step: 156, loss is 0.16020137071609497\n",
      "epoch: 6 step: 157, loss is 0.09650084376335144\n",
      "epoch: 6 step: 158, loss is 0.2206316888332367\n",
      "epoch: 6 step: 159, loss is 0.10023169964551926\n",
      "epoch: 6 step: 160, loss is 0.11162934452295303\n",
      "epoch: 6 step: 161, loss is 0.11014871299266815\n",
      "epoch: 6 step: 162, loss is 0.14749467372894287\n",
      "epoch: 6 step: 163, loss is 0.13641491532325745\n",
      "epoch: 6 step: 164, loss is 0.20190374553203583\n",
      "epoch: 6 step: 165, loss is 0.09216601401567459\n",
      "epoch: 6 step: 166, loss is 0.23172242939472198\n",
      "epoch: 6 step: 167, loss is 0.23532183468341827\n",
      "epoch: 6 step: 168, loss is 0.09495891630649567\n",
      "epoch: 6 step: 169, loss is 0.11827731132507324\n",
      "epoch: 6 step: 170, loss is 0.06859074532985687\n",
      "epoch: 6 step: 171, loss is 0.16554507613182068\n",
      "epoch: 6 step: 172, loss is 0.09656797349452972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 173, loss is 0.1282312124967575\n",
      "epoch: 6 step: 174, loss is 0.13902190327644348\n",
      "epoch: 6 step: 175, loss is 0.08849787712097168\n",
      "epoch: 6 step: 176, loss is 0.10186968743801117\n",
      "epoch: 6 step: 177, loss is 0.17536266148090363\n",
      "epoch: 6 step: 178, loss is 0.12434233725070953\n",
      "epoch: 6 step: 179, loss is 0.08008728176355362\n",
      "epoch: 6 step: 180, loss is 0.10796231776475906\n",
      "epoch: 6 step: 181, loss is 0.10930681228637695\n",
      "epoch: 6 step: 182, loss is 0.11735394597053528\n",
      "epoch: 6 step: 183, loss is 0.15880939364433289\n",
      "epoch: 6 step: 184, loss is 0.1476885825395584\n",
      "epoch: 6 step: 185, loss is 0.09023497998714447\n",
      "epoch: 6 step: 186, loss is 0.10298151522874832\n",
      "epoch: 6 step: 187, loss is 0.18081337213516235\n",
      "epoch: 6 step: 188, loss is 0.14218594133853912\n",
      "epoch: 6 step: 189, loss is 0.11573317646980286\n",
      "epoch: 6 step: 190, loss is 0.1211671531200409\n",
      "epoch: 6 step: 191, loss is 0.07066471874713898\n",
      "epoch: 6 step: 192, loss is 0.11390206217765808\n",
      "epoch: 6 step: 193, loss is 0.2020474225282669\n",
      "epoch: 6 step: 194, loss is 0.17508888244628906\n",
      "epoch: 6 step: 195, loss is 0.12681816518306732\n",
      "epoch: 6 step: 196, loss is 0.1668032854795456\n",
      "epoch: 6 step: 197, loss is 0.07154271751642227\n",
      "epoch: 6 step: 198, loss is 0.2111412137746811\n",
      "epoch: 6 step: 199, loss is 0.06695837527513504\n",
      "epoch: 6 step: 200, loss is 0.05778229609131813\n",
      "epoch: 6 step: 201, loss is 0.07851048558950424\n",
      "epoch: 6 step: 202, loss is 0.3396778702735901\n",
      "epoch: 6 step: 203, loss is 0.08252915740013123\n",
      "epoch: 6 step: 204, loss is 0.11701656877994537\n",
      "epoch: 6 step: 205, loss is 0.27395227551460266\n",
      "epoch: 6 step: 206, loss is 0.22284317016601562\n",
      "epoch: 6 step: 207, loss is 0.24886943399906158\n",
      "epoch: 6 step: 208, loss is 0.329781174659729\n",
      "epoch: 6 step: 209, loss is 0.11422763764858246\n",
      "epoch: 6 step: 210, loss is 0.09461288154125214\n",
      "epoch: 6 step: 211, loss is 0.1509885936975479\n",
      "epoch: 6 step: 212, loss is 0.08835349231958389\n",
      "epoch: 6 step: 213, loss is 0.1156323105096817\n",
      "epoch: 6 step: 214, loss is 0.17810304462909698\n",
      "epoch: 6 step: 215, loss is 0.08334320783615112\n",
      "epoch: 6 step: 216, loss is 0.17629146575927734\n",
      "epoch: 6 step: 217, loss is 0.2899055480957031\n",
      "epoch: 6 step: 218, loss is 0.20793592929840088\n",
      "epoch: 6 step: 219, loss is 0.1248646229505539\n",
      "epoch: 6 step: 220, loss is 0.24973267316818237\n",
      "epoch: 6 step: 221, loss is 0.13523556292057037\n",
      "epoch: 6 step: 222, loss is 0.11772768944501877\n",
      "epoch: 6 step: 223, loss is 0.23979894816875458\n",
      "epoch: 6 step: 224, loss is 0.14014220237731934\n",
      "epoch: 6 step: 225, loss is 0.28295600414276123\n",
      "epoch: 6 step: 226, loss is 0.3686807155609131\n",
      "epoch: 6 step: 227, loss is 0.07350578904151917\n",
      "epoch: 6 step: 228, loss is 0.18188884854316711\n",
      "epoch: 6 step: 229, loss is 0.21088936924934387\n",
      "epoch: 6 step: 230, loss is 0.06915570050477982\n",
      "epoch: 6 step: 231, loss is 0.18163278698921204\n",
      "epoch: 6 step: 232, loss is 0.2055221050977707\n",
      "epoch: 6 step: 233, loss is 0.16974051296710968\n",
      "epoch: 6 step: 234, loss is 0.19935734570026398\n",
      "epoch: 6 step: 235, loss is 0.2316383570432663\n",
      "epoch: 6 step: 236, loss is 0.1474105417728424\n",
      "epoch: 6 step: 237, loss is 0.13843466341495514\n",
      "epoch: 6 step: 238, loss is 0.2309875786304474\n",
      "epoch: 6 step: 239, loss is 0.23236960172653198\n",
      "epoch: 6 step: 240, loss is 0.09325490891933441\n",
      "epoch: 6 step: 241, loss is 0.0682792142033577\n",
      "epoch: 6 step: 242, loss is 0.08692574501037598\n",
      "epoch: 6 step: 243, loss is 0.10795081406831741\n",
      "epoch: 6 step: 244, loss is 0.10827048122882843\n",
      "epoch: 6 step: 245, loss is 0.13383212685585022\n",
      "epoch: 6 step: 246, loss is 0.08674914389848709\n",
      "epoch: 6 step: 247, loss is 0.14521615207195282\n",
      "epoch: 6 step: 248, loss is 0.14685304462909698\n",
      "epoch: 6 step: 249, loss is 0.22891128063201904\n",
      "epoch: 6 step: 250, loss is 0.24622109532356262\n",
      "epoch: 6 step: 251, loss is 0.18728148937225342\n",
      "epoch: 6 step: 252, loss is 0.09290789067745209\n",
      "epoch: 6 step: 253, loss is 0.1010705828666687\n",
      "epoch: 6 step: 254, loss is 0.1705433577299118\n",
      "epoch: 6 step: 255, loss is 0.039790380746126175\n",
      "epoch: 6 step: 256, loss is 0.29934975504875183\n",
      "epoch: 6 step: 257, loss is 0.10210254788398743\n",
      "epoch: 6 step: 258, loss is 0.24164360761642456\n",
      "epoch: 6 step: 259, loss is 0.17164966464042664\n",
      "epoch: 6 step: 260, loss is 0.2094295620918274\n",
      "epoch: 6 step: 261, loss is 0.1111363023519516\n",
      "epoch: 6 step: 262, loss is 0.20525194704532623\n",
      "epoch: 6 step: 263, loss is 0.09611660242080688\n",
      "epoch: 6 step: 264, loss is 0.20395688712596893\n",
      "epoch: 6 step: 265, loss is 0.1617083102464676\n",
      "epoch: 6 step: 266, loss is 0.17681436240673065\n",
      "epoch: 6 step: 267, loss is 0.2511547803878784\n",
      "epoch: 6 step: 268, loss is 0.16412706673145294\n",
      "epoch: 6 step: 269, loss is 0.046619758009910583\n",
      "epoch: 6 step: 270, loss is 0.14536547660827637\n",
      "epoch: 6 step: 271, loss is 0.04388682544231415\n",
      "epoch: 6 step: 272, loss is 0.1906772255897522\n",
      "epoch: 6 step: 273, loss is 0.13527649641036987\n",
      "epoch: 6 step: 274, loss is 0.0353882722556591\n",
      "epoch: 6 step: 275, loss is 0.19894538819789886\n",
      "epoch: 6 step: 276, loss is 0.07938835769891739\n",
      "epoch: 6 step: 277, loss is 0.17297305166721344\n",
      "epoch: 6 step: 278, loss is 0.08200690895318985\n",
      "epoch: 6 step: 279, loss is 0.16855394840240479\n",
      "epoch: 6 step: 280, loss is 0.1948511004447937\n",
      "epoch: 6 step: 281, loss is 0.180742084980011\n",
      "epoch: 6 step: 282, loss is 0.14342691004276276\n",
      "epoch: 6 step: 283, loss is 0.2943803369998932\n",
      "epoch: 6 step: 284, loss is 0.09810879826545715\n",
      "epoch: 6 step: 285, loss is 0.036803845316171646\n",
      "epoch: 6 step: 286, loss is 0.08514583855867386\n",
      "epoch: 6 step: 287, loss is 0.02524734102189541\n",
      "epoch: 6 step: 288, loss is 0.11709693819284439\n",
      "epoch: 6 step: 289, loss is 0.1297134906053543\n",
      "epoch: 6 step: 290, loss is 0.14455018937587738\n",
      "epoch: 6 step: 291, loss is 0.15267939865589142\n",
      "epoch: 6 step: 292, loss is 0.23995687067508698\n",
      "epoch: 6 step: 293, loss is 0.15933623909950256\n",
      "epoch: 6 step: 294, loss is 0.21026644110679626\n",
      "epoch: 6 step: 295, loss is 0.11909645050764084\n",
      "epoch: 6 step: 296, loss is 0.1797790378332138\n",
      "epoch: 6 step: 297, loss is 0.11185690760612488\n",
      "epoch: 6 step: 298, loss is 0.09957616776227951\n",
      "epoch: 6 step: 299, loss is 0.18821263313293457\n",
      "epoch: 6 step: 300, loss is 0.14578042924404144\n",
      "epoch: 6 step: 301, loss is 0.12719924747943878\n",
      "epoch: 6 step: 302, loss is 0.10829675197601318\n",
      "epoch: 6 step: 303, loss is 0.14293450117111206\n",
      "epoch: 6 step: 304, loss is 0.15133535861968994\n",
      "epoch: 6 step: 305, loss is 0.14069576561450958\n",
      "epoch: 6 step: 306, loss is 0.18647807836532593\n",
      "epoch: 6 step: 307, loss is 0.13133235275745392\n",
      "epoch: 6 step: 308, loss is 0.2471361607313156\n",
      "epoch: 6 step: 309, loss is 0.10975421965122223\n",
      "epoch: 6 step: 310, loss is 0.13168540596961975\n",
      "epoch: 6 step: 311, loss is 0.10930342227220535\n",
      "epoch: 6 step: 312, loss is 0.08002523332834244\n",
      "epoch: 6 step: 313, loss is 0.20391787588596344\n",
      "epoch: 6 step: 314, loss is 0.09610624611377716\n",
      "epoch: 6 step: 315, loss is 0.11374663561582565\n",
      "epoch: 6 step: 316, loss is 0.11925123631954193\n",
      "epoch: 6 step: 317, loss is 0.27299678325653076\n",
      "epoch: 6 step: 318, loss is 0.16263937950134277\n",
      "epoch: 6 step: 319, loss is 0.04912804439663887\n",
      "epoch: 6 step: 320, loss is 0.2110590636730194\n",
      "epoch: 6 step: 321, loss is 0.08263756334781647\n",
      "epoch: 6 step: 322, loss is 0.18295970559120178\n",
      "epoch: 6 step: 323, loss is 0.1414804905653\n",
      "epoch: 6 step: 324, loss is 0.1327940672636032\n",
      "epoch: 6 step: 325, loss is 0.12359833717346191\n",
      "epoch: 6 step: 326, loss is 0.18823891878128052\n",
      "epoch: 6 step: 327, loss is 0.13159134984016418\n",
      "epoch: 6 step: 328, loss is 0.07205136120319366\n",
      "epoch: 6 step: 329, loss is 0.1976228952407837\n",
      "epoch: 6 step: 330, loss is 0.12499621510505676\n",
      "epoch: 6 step: 331, loss is 0.13461348414421082\n",
      "epoch: 6 step: 332, loss is 0.07014814764261246\n",
      "epoch: 6 step: 333, loss is 0.1136690080165863\n",
      "epoch: 6 step: 334, loss is 0.16851015388965607\n",
      "epoch: 6 step: 335, loss is 0.12268351018428802\n",
      "epoch: 6 step: 336, loss is 0.11961907893419266\n",
      "epoch: 6 step: 337, loss is 0.10749851167201996\n",
      "epoch: 6 step: 338, loss is 0.18876075744628906\n",
      "epoch: 6 step: 339, loss is 0.09674961864948273\n",
      "epoch: 6 step: 340, loss is 0.20763187110424042\n",
      "epoch: 6 step: 341, loss is 0.30941876769065857\n",
      "epoch: 6 step: 342, loss is 0.07570375502109528\n",
      "epoch: 6 step: 343, loss is 0.3252356946468353\n",
      "epoch: 6 step: 344, loss is 0.08589920401573181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 345, loss is 0.10724955797195435\n",
      "epoch: 6 step: 346, loss is 0.07877245545387268\n",
      "epoch: 6 step: 347, loss is 0.19859685003757477\n",
      "epoch: 6 step: 348, loss is 0.16090154647827148\n",
      "epoch: 6 step: 349, loss is 0.14244991540908813\n",
      "epoch: 6 step: 350, loss is 0.23859281837940216\n",
      "epoch: 6 step: 351, loss is 0.04407937824726105\n",
      "epoch: 6 step: 352, loss is 0.13582667708396912\n",
      "epoch: 6 step: 353, loss is 0.167965367436409\n",
      "epoch: 6 step: 354, loss is 0.14500002562999725\n",
      "epoch: 6 step: 355, loss is 0.10288581252098083\n",
      "epoch: 6 step: 356, loss is 0.0948159396648407\n",
      "epoch: 6 step: 357, loss is 0.18457822501659393\n",
      "epoch: 6 step: 358, loss is 0.10607576370239258\n",
      "epoch: 6 step: 359, loss is 0.15809114277362823\n",
      "epoch: 6 step: 360, loss is 0.05619647353887558\n",
      "epoch: 6 step: 361, loss is 0.2162548154592514\n",
      "epoch: 6 step: 362, loss is 0.08465186506509781\n",
      "epoch: 6 step: 363, loss is 0.19136883318424225\n",
      "epoch: 6 step: 364, loss is 0.15574267506599426\n",
      "epoch: 6 step: 365, loss is 0.11564648151397705\n",
      "epoch: 6 step: 366, loss is 0.1451835334300995\n",
      "epoch: 6 step: 367, loss is 0.15118002891540527\n",
      "epoch: 6 step: 368, loss is 0.14889653027057648\n",
      "epoch: 6 step: 369, loss is 0.1265837699174881\n",
      "epoch: 6 step: 370, loss is 0.19333678483963013\n",
      "epoch: 6 step: 371, loss is 0.22154392302036285\n",
      "epoch: 6 step: 372, loss is 0.23956578969955444\n",
      "epoch: 6 step: 373, loss is 0.1980801373720169\n",
      "epoch: 6 step: 374, loss is 0.11956298351287842\n",
      "epoch: 6 step: 375, loss is 0.17469418048858643\n",
      "epoch: 6 step: 376, loss is 0.09489292651414871\n",
      "epoch: 6 step: 377, loss is 0.0658600702881813\n",
      "epoch: 6 step: 378, loss is 0.18463505804538727\n",
      "epoch: 6 step: 379, loss is 0.24830546975135803\n",
      "epoch: 6 step: 380, loss is 0.1069655492901802\n",
      "epoch: 6 step: 381, loss is 0.14849373698234558\n",
      "epoch: 6 step: 382, loss is 0.20687547326087952\n",
      "epoch: 6 step: 383, loss is 0.23450526595115662\n",
      "epoch: 6 step: 384, loss is 0.0821940079331398\n",
      "epoch: 6 step: 385, loss is 0.19652801752090454\n",
      "epoch: 6 step: 386, loss is 0.18931476771831512\n",
      "epoch: 6 step: 387, loss is 0.18107186257839203\n",
      "epoch: 6 step: 388, loss is 0.199675053358078\n",
      "epoch: 6 step: 389, loss is 0.08771108090877533\n",
      "epoch: 6 step: 390, loss is 0.12036541104316711\n",
      "epoch: 6 step: 391, loss is 0.1802588701248169\n",
      "epoch: 6 step: 392, loss is 0.13546079397201538\n",
      "epoch: 6 step: 393, loss is 0.20713579654693604\n",
      "epoch: 6 step: 394, loss is 0.2535027861595154\n",
      "epoch: 6 step: 395, loss is 0.11510677635669708\n",
      "epoch: 6 step: 396, loss is 0.09465254098176956\n",
      "epoch: 6 step: 397, loss is 0.2564123868942261\n",
      "epoch: 6 step: 398, loss is 0.14167742431163788\n",
      "epoch: 6 step: 399, loss is 0.12785692512989044\n",
      "epoch: 6 step: 400, loss is 0.12494508177042007\n",
      "epoch: 6 step: 401, loss is 0.18688426911830902\n",
      "epoch: 6 step: 402, loss is 0.15104207396507263\n",
      "epoch: 6 step: 403, loss is 0.1528196781873703\n",
      "epoch: 6 step: 404, loss is 0.17229746282100677\n",
      "epoch: 6 step: 405, loss is 0.09315510839223862\n",
      "epoch: 6 step: 406, loss is 0.16685812175273895\n",
      "epoch: 6 step: 407, loss is 0.1236550360918045\n",
      "epoch: 6 step: 408, loss is 0.1353791058063507\n",
      "epoch: 6 step: 409, loss is 0.15641221404075623\n",
      "epoch: 6 step: 410, loss is 0.050843290984630585\n",
      "epoch: 6 step: 411, loss is 0.11441227793693542\n",
      "epoch: 6 step: 412, loss is 0.10629835724830627\n",
      "epoch: 6 step: 413, loss is 0.1252700686454773\n",
      "epoch: 6 step: 414, loss is 0.217728853225708\n",
      "epoch: 6 step: 415, loss is 0.06680670380592346\n",
      "epoch: 6 step: 416, loss is 0.28712883591651917\n",
      "epoch: 6 step: 417, loss is 0.10908497869968414\n",
      "epoch: 6 step: 418, loss is 0.1564168781042099\n",
      "epoch: 6 step: 419, loss is 0.10949587821960449\n",
      "epoch: 6 step: 420, loss is 0.17005296051502228\n",
      "epoch: 6 step: 421, loss is 0.32808011770248413\n",
      "epoch: 6 step: 422, loss is 0.19879187643527985\n",
      "epoch: 6 step: 423, loss is 0.19836348295211792\n",
      "epoch: 6 step: 424, loss is 0.07845129817724228\n",
      "epoch: 6 step: 425, loss is 0.16887415945529938\n",
      "epoch: 6 step: 426, loss is 0.089457668364048\n",
      "epoch: 6 step: 427, loss is 0.05220722034573555\n",
      "epoch: 6 step: 428, loss is 0.2002669870853424\n",
      "epoch: 6 step: 429, loss is 0.08504191040992737\n",
      "epoch: 6 step: 430, loss is 0.11971243470907211\n",
      "epoch: 6 step: 431, loss is 0.11545486003160477\n",
      "epoch: 6 step: 432, loss is 0.22634769976139069\n",
      "epoch: 6 step: 433, loss is 0.06840376555919647\n",
      "epoch: 6 step: 434, loss is 0.05468113347887993\n",
      "epoch: 6 step: 435, loss is 0.1618019938468933\n",
      "epoch: 6 step: 436, loss is 0.14836785197257996\n",
      "epoch: 6 step: 437, loss is 0.0775516927242279\n",
      "epoch: 6 step: 438, loss is 0.2158556431531906\n",
      "epoch: 6 step: 439, loss is 0.08266913145780563\n",
      "epoch: 6 step: 440, loss is 0.14839787781238556\n",
      "epoch: 6 step: 441, loss is 0.15887019038200378\n",
      "epoch: 6 step: 442, loss is 0.07773802429437637\n",
      "epoch: 6 step: 443, loss is 0.20190472900867462\n",
      "epoch: 6 step: 444, loss is 0.08635891228914261\n",
      "epoch: 6 step: 445, loss is 0.22628982365131378\n",
      "epoch: 6 step: 446, loss is 0.24018174409866333\n",
      "epoch: 6 step: 447, loss is 0.15108905732631683\n",
      "epoch: 6 step: 448, loss is 0.11649863421916962\n",
      "epoch: 6 step: 449, loss is 0.14421570301055908\n",
      "epoch: 6 step: 450, loss is 0.14228074252605438\n",
      "epoch: 6 step: 451, loss is 0.08418576419353485\n",
      "epoch: 6 step: 452, loss is 0.15808485448360443\n",
      "epoch: 6 step: 453, loss is 0.11824820935726166\n",
      "epoch: 6 step: 454, loss is 0.0912981927394867\n",
      "epoch: 6 step: 455, loss is 0.1802142858505249\n",
      "epoch: 6 step: 456, loss is 0.09420852363109589\n",
      "epoch: 6 step: 457, loss is 0.21628575026988983\n",
      "epoch: 6 step: 458, loss is 0.18915002048015594\n",
      "epoch: 6 step: 459, loss is 0.155683696269989\n",
      "epoch: 6 step: 460, loss is 0.15138305723667145\n",
      "epoch: 6 step: 461, loss is 0.3700016438961029\n",
      "epoch: 6 step: 462, loss is 0.03614421561360359\n",
      "epoch: 6 step: 463, loss is 0.09694628417491913\n",
      "epoch: 6 step: 464, loss is 0.17128805816173553\n",
      "epoch: 6 step: 465, loss is 0.13518230617046356\n",
      "epoch: 6 step: 466, loss is 0.08780822902917862\n",
      "epoch: 6 step: 467, loss is 0.21331870555877686\n",
      "epoch: 6 step: 468, loss is 0.10412202030420303\n",
      "epoch: 6 step: 469, loss is 0.14502212405204773\n",
      "epoch: 6 step: 470, loss is 0.20994827151298523\n",
      "epoch: 6 step: 471, loss is 0.17028342187404633\n",
      "epoch: 6 step: 472, loss is 0.2042253017425537\n",
      "epoch: 6 step: 473, loss is 0.10779447108507156\n",
      "epoch: 6 step: 474, loss is 0.06624265760183334\n",
      "epoch: 6 step: 475, loss is 0.16913247108459473\n",
      "epoch: 6 step: 476, loss is 0.08540932089090347\n",
      "epoch: 6 step: 477, loss is 0.0576954111456871\n",
      "epoch: 6 step: 478, loss is 0.14796379208564758\n",
      "epoch: 6 step: 479, loss is 0.10650923103094101\n",
      "epoch: 6 step: 480, loss is 0.13452452421188354\n",
      "epoch: 6 step: 481, loss is 0.23206369578838348\n",
      "epoch: 6 step: 482, loss is 0.18051855266094208\n",
      "epoch: 6 step: 483, loss is 0.14949685335159302\n",
      "epoch: 6 step: 484, loss is 0.22429753839969635\n",
      "epoch: 6 step: 485, loss is 0.09245987236499786\n",
      "epoch: 6 step: 486, loss is 0.11670045554637909\n",
      "epoch: 6 step: 487, loss is 0.09369406849145889\n",
      "epoch: 6 step: 488, loss is 0.18856030702590942\n",
      "epoch: 6 step: 489, loss is 0.22717851400375366\n",
      "epoch: 6 step: 490, loss is 0.040920764207839966\n",
      "epoch: 6 step: 491, loss is 0.15181556344032288\n",
      "epoch: 6 step: 492, loss is 0.21422232687473297\n",
      "epoch: 6 step: 493, loss is 0.16663435101509094\n",
      "epoch: 6 step: 494, loss is 0.21928924322128296\n",
      "epoch: 6 step: 495, loss is 0.16366387903690338\n",
      "epoch: 6 step: 496, loss is 0.15148963034152985\n",
      "epoch: 6 step: 497, loss is 0.11465409398078918\n",
      "epoch: 6 step: 498, loss is 0.10614779591560364\n",
      "epoch: 6 step: 499, loss is 0.1582375019788742\n",
      "epoch: 6 step: 500, loss is 0.09961395710706711\n",
      "epoch: 6 step: 501, loss is 0.09178747981786728\n",
      "epoch: 6 step: 502, loss is 0.06351480633020401\n",
      "epoch: 6 step: 503, loss is 0.13616128265857697\n",
      "epoch: 6 step: 504, loss is 0.3362481892108917\n",
      "epoch: 6 step: 505, loss is 0.20119373500347137\n",
      "epoch: 6 step: 506, loss is 0.19434992969036102\n",
      "epoch: 6 step: 507, loss is 0.12002789229154587\n",
      "epoch: 6 step: 508, loss is 0.10952691733837128\n",
      "epoch: 6 step: 509, loss is 0.09304496645927429\n",
      "epoch: 6 step: 510, loss is 0.10762440413236618\n",
      "epoch: 6 step: 511, loss is 0.16655637323856354\n",
      "epoch: 6 step: 512, loss is 0.09068415313959122\n",
      "epoch: 6 step: 513, loss is 0.04451918601989746\n",
      "epoch: 6 step: 514, loss is 0.04200947657227516\n",
      "epoch: 6 step: 515, loss is 0.1448500007390976\n",
      "epoch: 6 step: 516, loss is 0.2225532978773117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 517, loss is 0.06307021528482437\n",
      "epoch: 6 step: 518, loss is 0.1401074230670929\n",
      "epoch: 6 step: 519, loss is 0.12330612540245056\n",
      "epoch: 6 step: 520, loss is 0.13974687457084656\n",
      "epoch: 6 step: 521, loss is 0.2217320054769516\n",
      "epoch: 6 step: 522, loss is 0.14189843833446503\n",
      "epoch: 6 step: 523, loss is 0.13940633833408356\n",
      "epoch: 6 step: 524, loss is 0.08160652220249176\n",
      "epoch: 6 step: 525, loss is 0.11588028818368912\n",
      "epoch: 6 step: 526, loss is 0.10474066436290741\n",
      "epoch: 6 step: 527, loss is 0.2033483237028122\n",
      "epoch: 6 step: 528, loss is 0.08669006824493408\n",
      "epoch: 6 step: 529, loss is 0.18729613721370697\n",
      "epoch: 6 step: 530, loss is 0.19016332924365997\n",
      "epoch: 6 step: 531, loss is 0.22561733424663544\n",
      "epoch: 6 step: 532, loss is 0.19491535425186157\n",
      "epoch: 6 step: 533, loss is 0.14507822692394257\n",
      "epoch: 6 step: 534, loss is 0.09597854316234589\n",
      "epoch: 6 step: 535, loss is 0.0954190343618393\n",
      "epoch: 6 step: 536, loss is 0.12925641238689423\n",
      "epoch: 6 step: 537, loss is 0.23719561100006104\n",
      "epoch: 6 step: 538, loss is 0.11594214290380478\n",
      "epoch: 6 step: 539, loss is 0.1454722285270691\n",
      "epoch: 6 step: 540, loss is 0.10603412240743637\n",
      "epoch: 6 step: 541, loss is 0.13465747237205505\n",
      "epoch: 6 step: 542, loss is 0.1076807752251625\n",
      "epoch: 6 step: 543, loss is 0.2022605836391449\n",
      "epoch: 6 step: 544, loss is 0.09534705430269241\n",
      "epoch: 6 step: 545, loss is 0.29318350553512573\n",
      "epoch: 6 step: 546, loss is 0.15637555718421936\n",
      "epoch: 6 step: 547, loss is 0.03682003542780876\n",
      "epoch: 6 step: 548, loss is 0.2855485677719116\n",
      "epoch: 6 step: 549, loss is 0.1200939491391182\n",
      "epoch: 6 step: 550, loss is 0.14985252916812897\n",
      "epoch: 6 step: 551, loss is 0.14076822996139526\n",
      "epoch: 6 step: 552, loss is 0.16673551499843597\n",
      "epoch: 6 step: 553, loss is 0.08736786991357803\n",
      "epoch: 6 step: 554, loss is 0.1947752833366394\n",
      "epoch: 6 step: 555, loss is 0.1331275850534439\n",
      "epoch: 6 step: 556, loss is 0.11681568622589111\n",
      "epoch: 6 step: 557, loss is 0.1584753841161728\n",
      "epoch: 6 step: 558, loss is 0.055117543786764145\n",
      "epoch: 6 step: 559, loss is 0.09072653949260712\n",
      "epoch: 6 step: 560, loss is 0.14412400126457214\n",
      "epoch: 6 step: 561, loss is 0.11732953041791916\n",
      "epoch: 6 step: 562, loss is 0.1576012820005417\n",
      "epoch: 6 step: 563, loss is 0.1906212717294693\n",
      "epoch: 6 step: 564, loss is 0.2905367612838745\n",
      "epoch: 6 step: 565, loss is 0.10507038235664368\n",
      "epoch: 6 step: 566, loss is 0.10231390595436096\n",
      "epoch: 6 step: 567, loss is 0.18081942200660706\n",
      "epoch: 6 step: 568, loss is 0.22297291457653046\n",
      "epoch: 6 step: 569, loss is 0.1438441127538681\n",
      "epoch: 6 step: 570, loss is 0.17381690442562103\n",
      "epoch: 6 step: 571, loss is 0.07616683095693588\n",
      "epoch: 6 step: 572, loss is 0.08133862167596817\n",
      "epoch: 6 step: 573, loss is 0.10893885791301727\n",
      "epoch: 6 step: 574, loss is 0.15254266560077667\n",
      "epoch: 6 step: 575, loss is 0.09960620105266571\n",
      "epoch: 6 step: 576, loss is 0.246614009141922\n",
      "epoch: 6 step: 577, loss is 0.1967136412858963\n",
      "epoch: 6 step: 578, loss is 0.22893042862415314\n",
      "epoch: 6 step: 579, loss is 0.2875845730304718\n",
      "epoch: 6 step: 580, loss is 0.0884360745549202\n",
      "epoch: 6 step: 581, loss is 0.13223572075366974\n",
      "epoch: 6 step: 582, loss is 0.08178896456956863\n",
      "epoch: 6 step: 583, loss is 0.10872174799442291\n",
      "epoch: 6 step: 584, loss is 0.13498371839523315\n",
      "epoch: 6 step: 585, loss is 0.13625626266002655\n",
      "epoch: 6 step: 586, loss is 0.11459866166114807\n",
      "epoch: 6 step: 587, loss is 0.24426937103271484\n",
      "epoch: 6 step: 588, loss is 0.07238666713237762\n",
      "epoch: 6 step: 589, loss is 0.15565159916877747\n",
      "epoch: 6 step: 590, loss is 0.19340035319328308\n",
      "epoch: 6 step: 591, loss is 0.22933432459831238\n",
      "epoch: 6 step: 592, loss is 0.17833058536052704\n",
      "epoch: 6 step: 593, loss is 0.05674678832292557\n",
      "epoch: 6 step: 594, loss is 0.031240735203027725\n",
      "epoch: 6 step: 595, loss is 0.2818189859390259\n",
      "epoch: 6 step: 596, loss is 0.054480936378240585\n",
      "epoch: 6 step: 597, loss is 0.1636628359556198\n",
      "epoch: 6 step: 598, loss is 0.08929844200611115\n",
      "epoch: 6 step: 599, loss is 0.2969679832458496\n",
      "epoch: 6 step: 600, loss is 0.07429841160774231\n",
      "epoch: 6 step: 601, loss is 0.1309824287891388\n",
      "epoch: 6 step: 602, loss is 0.03784279525279999\n",
      "epoch: 6 step: 603, loss is 0.1673956662416458\n",
      "epoch: 6 step: 604, loss is 0.12677797675132751\n",
      "epoch: 6 step: 605, loss is 0.2516539394855499\n",
      "epoch: 6 step: 606, loss is 0.15034392476081848\n",
      "epoch: 6 step: 607, loss is 0.09864319115877151\n",
      "epoch: 6 step: 608, loss is 0.17555782198905945\n",
      "epoch: 6 step: 609, loss is 0.059504322707653046\n",
      "epoch: 6 step: 610, loss is 0.21174398064613342\n",
      "epoch: 6 step: 611, loss is 0.13340161740779877\n",
      "epoch: 6 step: 612, loss is 0.19875691831111908\n",
      "epoch: 6 step: 613, loss is 0.06847438216209412\n",
      "epoch: 6 step: 614, loss is 0.0992354303598404\n",
      "epoch: 6 step: 615, loss is 0.2112502157688141\n",
      "epoch: 6 step: 616, loss is 0.2263374924659729\n",
      "epoch: 6 step: 617, loss is 0.13548853993415833\n",
      "epoch: 6 step: 618, loss is 0.06652278453111649\n",
      "epoch: 6 step: 619, loss is 0.11231114715337753\n",
      "epoch: 6 step: 620, loss is 0.13248030841350555\n",
      "epoch: 6 step: 621, loss is 0.08221384137868881\n",
      "epoch: 6 step: 622, loss is 0.16059303283691406\n",
      "epoch: 6 step: 623, loss is 0.09781324863433838\n",
      "epoch: 6 step: 624, loss is 0.1212829127907753\n",
      "epoch: 6 step: 625, loss is 0.15769876539707184\n",
      "epoch: 6 step: 626, loss is 0.10193049162626266\n",
      "epoch: 6 step: 627, loss is 0.25639447569847107\n",
      "epoch: 6 step: 628, loss is 0.12813888490200043\n",
      "epoch: 6 step: 629, loss is 0.26251479983329773\n",
      "epoch: 6 step: 630, loss is 0.1902482658624649\n",
      "epoch: 6 step: 631, loss is 0.21771858632564545\n",
      "epoch: 6 step: 632, loss is 0.2530408501625061\n",
      "epoch: 6 step: 633, loss is 0.14414294064044952\n",
      "epoch: 6 step: 634, loss is 0.04686146229505539\n",
      "epoch: 6 step: 635, loss is 0.2502341866493225\n",
      "epoch: 6 step: 636, loss is 0.030985433608293533\n",
      "epoch: 6 step: 637, loss is 0.15500767529010773\n",
      "epoch: 6 step: 638, loss is 0.0800551027059555\n",
      "epoch: 6 step: 639, loss is 0.09121505171060562\n",
      "epoch: 6 step: 640, loss is 0.1366203874349594\n",
      "epoch: 6 step: 641, loss is 0.35059624910354614\n",
      "epoch: 6 step: 642, loss is 0.15108038485050201\n",
      "epoch: 6 step: 643, loss is 0.06919208914041519\n",
      "epoch: 6 step: 644, loss is 0.2647605538368225\n",
      "epoch: 6 step: 645, loss is 0.20106391608715057\n",
      "epoch: 6 step: 646, loss is 0.04032197594642639\n",
      "epoch: 6 step: 647, loss is 0.07029347866773605\n",
      "epoch: 6 step: 648, loss is 0.17645908892154694\n",
      "epoch: 6 step: 649, loss is 0.1798745095729828\n",
      "epoch: 6 step: 650, loss is 0.06476213783025742\n",
      "epoch: 6 step: 651, loss is 0.05999358743429184\n",
      "epoch: 6 step: 652, loss is 0.1911042481660843\n",
      "epoch: 6 step: 653, loss is 0.051004037261009216\n",
      "epoch: 6 step: 654, loss is 0.06928574293851852\n",
      "epoch: 6 step: 655, loss is 0.08263814449310303\n",
      "epoch: 6 step: 656, loss is 0.1581420600414276\n",
      "epoch: 6 step: 657, loss is 0.18961134552955627\n",
      "epoch: 6 step: 658, loss is 0.11024806648492813\n",
      "epoch: 6 step: 659, loss is 0.11063212901353836\n",
      "epoch: 6 step: 660, loss is 0.09006413072347641\n",
      "epoch: 6 step: 661, loss is 0.10663164407014847\n",
      "epoch: 6 step: 662, loss is 0.09540902078151703\n",
      "epoch: 6 step: 663, loss is 0.17346462607383728\n",
      "epoch: 6 step: 664, loss is 0.09298999607563019\n",
      "epoch: 6 step: 665, loss is 0.10944812744855881\n",
      "epoch: 6 step: 666, loss is 0.1372419148683548\n",
      "epoch: 6 step: 667, loss is 0.1123468354344368\n",
      "epoch: 6 step: 668, loss is 0.21460038423538208\n",
      "epoch: 6 step: 669, loss is 0.16309677064418793\n",
      "epoch: 6 step: 670, loss is 0.11347593367099762\n",
      "epoch: 6 step: 671, loss is 0.26191452145576477\n",
      "epoch: 6 step: 672, loss is 0.2008211612701416\n",
      "epoch: 6 step: 673, loss is 0.0764276534318924\n",
      "epoch: 6 step: 674, loss is 0.11151716858148575\n",
      "epoch: 6 step: 675, loss is 0.24584707617759705\n",
      "epoch: 6 step: 676, loss is 0.3410842716693878\n",
      "epoch: 6 step: 677, loss is 0.08794044703245163\n",
      "epoch: 6 step: 678, loss is 0.12007477134466171\n",
      "epoch: 6 step: 679, loss is 0.1374906450510025\n",
      "epoch: 6 step: 680, loss is 0.31045687198638916\n",
      "epoch: 6 step: 681, loss is 0.19465366005897522\n",
      "epoch: 6 step: 682, loss is 0.17547482252120972\n",
      "epoch: 6 step: 683, loss is 0.14807499945163727\n",
      "epoch: 6 step: 684, loss is 0.16146691143512726\n",
      "epoch: 6 step: 685, loss is 0.2236543893814087\n",
      "epoch: 6 step: 686, loss is 0.15178635716438293\n",
      "epoch: 6 step: 687, loss is 0.1896665245294571\n",
      "epoch: 6 step: 688, loss is 0.2693352699279785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 689, loss is 0.11927655339241028\n",
      "epoch: 6 step: 690, loss is 0.22245097160339355\n",
      "epoch: 6 step: 691, loss is 0.15727351605892181\n",
      "epoch: 6 step: 692, loss is 0.21305066347122192\n",
      "epoch: 6 step: 693, loss is 0.16445580124855042\n",
      "epoch: 6 step: 694, loss is 0.12976066768169403\n",
      "epoch: 6 step: 695, loss is 0.23871973156929016\n",
      "epoch: 6 step: 696, loss is 0.10238863527774811\n",
      "epoch: 6 step: 697, loss is 0.08705723285675049\n",
      "epoch: 6 step: 698, loss is 0.10875850170850754\n",
      "epoch: 6 step: 699, loss is 0.1326483190059662\n",
      "epoch: 6 step: 700, loss is 0.1364518404006958\n",
      "epoch: 6 step: 701, loss is 0.1689913123846054\n",
      "epoch: 6 step: 702, loss is 0.12251973152160645\n",
      "epoch: 6 step: 703, loss is 0.29877665638923645\n",
      "epoch: 6 step: 704, loss is 0.13396230340003967\n",
      "epoch: 6 step: 705, loss is 0.121793732047081\n",
      "epoch: 6 step: 706, loss is 0.041439563035964966\n",
      "epoch: 6 step: 707, loss is 0.12061332911252975\n",
      "epoch: 6 step: 708, loss is 0.1450735181570053\n",
      "epoch: 6 step: 709, loss is 0.27161723375320435\n",
      "epoch: 6 step: 710, loss is 0.13695091009140015\n",
      "epoch: 6 step: 711, loss is 0.40807655453681946\n",
      "epoch: 6 step: 712, loss is 0.2020430713891983\n",
      "epoch: 6 step: 713, loss is 0.15073539316654205\n",
      "epoch: 6 step: 714, loss is 0.03787752985954285\n",
      "epoch: 6 step: 715, loss is 0.1218876987695694\n",
      "epoch: 6 step: 716, loss is 0.09019966423511505\n",
      "epoch: 6 step: 717, loss is 0.12620186805725098\n",
      "epoch: 6 step: 718, loss is 0.10335630923509598\n",
      "epoch: 6 step: 719, loss is 0.191704660654068\n",
      "epoch: 6 step: 720, loss is 0.10531914979219437\n",
      "epoch: 6 step: 721, loss is 0.15009738504886627\n",
      "epoch: 6 step: 722, loss is 0.43019840121269226\n",
      "epoch: 6 step: 723, loss is 0.1039876714348793\n",
      "epoch: 6 step: 724, loss is 0.15040543675422668\n",
      "epoch: 6 step: 725, loss is 0.04846767336130142\n",
      "epoch: 6 step: 726, loss is 0.3295935392379761\n",
      "epoch: 6 step: 727, loss is 0.14233793318271637\n",
      "epoch: 6 step: 728, loss is 0.1788998544216156\n",
      "epoch: 6 step: 729, loss is 0.20808537304401398\n",
      "epoch: 6 step: 730, loss is 0.12214095145463943\n",
      "epoch: 6 step: 731, loss is 0.061407510191202164\n",
      "epoch: 6 step: 732, loss is 0.1435764878988266\n",
      "epoch: 6 step: 733, loss is 0.22487112879753113\n",
      "epoch: 6 step: 734, loss is 0.12845198810100555\n",
      "epoch: 6 step: 735, loss is 0.26962903141975403\n",
      "epoch: 6 step: 736, loss is 0.10626368969678879\n",
      "epoch: 6 step: 737, loss is 0.14423047006130219\n",
      "epoch: 6 step: 738, loss is 0.1940889060497284\n",
      "epoch: 6 step: 739, loss is 0.11940938234329224\n",
      "epoch: 6 step: 740, loss is 0.30809667706489563\n",
      "epoch: 6 step: 741, loss is 0.11940591782331467\n",
      "epoch: 6 step: 742, loss is 0.062024470418691635\n",
      "epoch: 6 step: 743, loss is 0.08599517494440079\n",
      "epoch: 6 step: 744, loss is 0.17881007492542267\n",
      "epoch: 6 step: 745, loss is 0.07670143991708755\n",
      "epoch: 6 step: 746, loss is 0.22986488044261932\n",
      "epoch: 6 step: 747, loss is 0.15570028126239777\n",
      "epoch: 6 step: 748, loss is 0.18839415907859802\n",
      "epoch: 6 step: 749, loss is 0.10024023801088333\n",
      "epoch: 6 step: 750, loss is 0.25540417432785034\n",
      "epoch: 6 step: 751, loss is 0.08805873990058899\n",
      "epoch: 6 step: 752, loss is 0.26607751846313477\n",
      "epoch: 6 step: 753, loss is 0.12046384066343307\n",
      "epoch: 6 step: 754, loss is 0.21047289669513702\n",
      "epoch: 6 step: 755, loss is 0.19915443658828735\n",
      "epoch: 6 step: 756, loss is 0.1822761595249176\n",
      "epoch: 6 step: 757, loss is 0.22368136048316956\n",
      "epoch: 6 step: 758, loss is 0.10450027137994766\n",
      "epoch: 6 step: 759, loss is 0.07005589455366135\n",
      "epoch: 6 step: 760, loss is 0.17970788478851318\n",
      "epoch: 6 step: 761, loss is 0.1514493227005005\n",
      "epoch: 6 step: 762, loss is 0.1721545308828354\n",
      "epoch: 6 step: 763, loss is 0.06068800017237663\n",
      "epoch: 6 step: 764, loss is 0.24702982604503632\n",
      "epoch: 6 step: 765, loss is 0.21216797828674316\n",
      "epoch: 6 step: 766, loss is 0.15475116670131683\n",
      "epoch: 6 step: 767, loss is 0.11077713966369629\n",
      "epoch: 6 step: 768, loss is 0.08695995062589645\n",
      "epoch: 6 step: 769, loss is 0.16681863367557526\n",
      "epoch: 6 step: 770, loss is 0.07894590497016907\n",
      "epoch: 6 step: 771, loss is 0.09304098784923553\n",
      "epoch: 6 step: 772, loss is 0.1429872214794159\n",
      "epoch: 6 step: 773, loss is 0.1354881227016449\n",
      "epoch: 6 step: 774, loss is 0.1480153650045395\n",
      "epoch: 6 step: 775, loss is 0.18380193412303925\n",
      "epoch: 6 step: 776, loss is 0.0900672897696495\n",
      "epoch: 6 step: 777, loss is 0.16349609196186066\n",
      "epoch: 6 step: 778, loss is 0.17840896546840668\n",
      "epoch: 6 step: 779, loss is 0.07948049902915955\n",
      "epoch: 6 step: 780, loss is 0.12513847649097443\n",
      "epoch: 6 step: 781, loss is 0.08378206193447113\n",
      "epoch: 6 step: 782, loss is 0.07773436605930328\n",
      "epoch: 6 step: 783, loss is 0.19888539612293243\n",
      "epoch: 6 step: 784, loss is 0.19390824437141418\n",
      "epoch: 6 step: 785, loss is 0.10368642210960388\n",
      "epoch: 6 step: 786, loss is 0.11094093322753906\n",
      "epoch: 6 step: 787, loss is 0.08519167453050613\n",
      "epoch: 6 step: 788, loss is 0.13870088756084442\n",
      "epoch: 6 step: 789, loss is 0.10305609554052353\n",
      "epoch: 6 step: 790, loss is 0.062188245356082916\n",
      "epoch: 6 step: 791, loss is 0.12797728180885315\n",
      "epoch: 6 step: 792, loss is 0.2331608086824417\n",
      "epoch: 6 step: 793, loss is 0.16249269247055054\n",
      "epoch: 6 step: 794, loss is 0.18933849036693573\n",
      "epoch: 6 step: 795, loss is 0.13218970596790314\n",
      "epoch: 6 step: 796, loss is 0.10315099358558655\n",
      "epoch: 6 step: 797, loss is 0.06468703597784042\n",
      "epoch: 6 step: 798, loss is 0.32083311676979065\n",
      "epoch: 6 step: 799, loss is 0.22627684473991394\n",
      "epoch: 6 step: 800, loss is 0.18183386325836182\n",
      "epoch: 6 step: 801, loss is 0.12072907388210297\n",
      "epoch: 6 step: 802, loss is 0.19920559227466583\n",
      "epoch: 6 step: 803, loss is 0.19306433200836182\n",
      "epoch: 6 step: 804, loss is 0.29309192299842834\n",
      "epoch: 6 step: 805, loss is 0.2349342405796051\n",
      "epoch: 6 step: 806, loss is 0.11434561014175415\n",
      "epoch: 6 step: 807, loss is 0.2055555135011673\n",
      "epoch: 6 step: 808, loss is 0.08644010871648788\n",
      "epoch: 6 step: 809, loss is 0.1449185609817505\n",
      "epoch: 6 step: 810, loss is 0.16034159064292908\n",
      "epoch: 6 step: 811, loss is 0.1950816810131073\n",
      "epoch: 6 step: 812, loss is 0.10700304806232452\n",
      "epoch: 6 step: 813, loss is 0.1917465329170227\n",
      "epoch: 6 step: 814, loss is 0.10810229182243347\n",
      "epoch: 6 step: 815, loss is 0.21630676090717316\n",
      "epoch: 6 step: 816, loss is 0.17317847907543182\n",
      "epoch: 6 step: 817, loss is 0.11534944176673889\n",
      "epoch: 6 step: 818, loss is 0.15898773074150085\n",
      "epoch: 6 step: 819, loss is 0.09770318120718002\n",
      "epoch: 6 step: 820, loss is 0.16131822764873505\n",
      "epoch: 6 step: 821, loss is 0.1179620698094368\n",
      "epoch: 6 step: 822, loss is 0.13979652523994446\n",
      "epoch: 6 step: 823, loss is 0.1166219562292099\n",
      "epoch: 6 step: 824, loss is 0.13997724652290344\n",
      "epoch: 6 step: 825, loss is 0.04444357007741928\n",
      "epoch: 6 step: 826, loss is 0.19063811004161835\n",
      "epoch: 6 step: 827, loss is 0.2850027084350586\n",
      "epoch: 6 step: 828, loss is 0.06397208571434021\n",
      "epoch: 6 step: 829, loss is 0.2830365300178528\n",
      "epoch: 6 step: 830, loss is 0.22248592972755432\n",
      "epoch: 6 step: 831, loss is 0.08861604332923889\n",
      "epoch: 6 step: 832, loss is 0.05667963624000549\n",
      "epoch: 6 step: 833, loss is 0.13307669758796692\n",
      "epoch: 6 step: 834, loss is 0.1491059809923172\n",
      "epoch: 6 step: 835, loss is 0.14069072902202606\n",
      "epoch: 6 step: 836, loss is 0.17871208488941193\n",
      "epoch: 6 step: 837, loss is 0.07503048330545425\n",
      "epoch: 6 step: 838, loss is 0.06633145362138748\n",
      "epoch: 6 step: 839, loss is 0.17825298011302948\n",
      "epoch: 6 step: 840, loss is 0.24868954718112946\n",
      "epoch: 6 step: 841, loss is 0.09153994172811508\n",
      "epoch: 6 step: 842, loss is 0.07861100882291794\n",
      "epoch: 6 step: 843, loss is 0.1590985208749771\n",
      "epoch: 6 step: 844, loss is 0.188637837767601\n",
      "epoch: 6 step: 845, loss is 0.10927779972553253\n",
      "epoch: 6 step: 846, loss is 0.17463961243629456\n",
      "epoch: 6 step: 847, loss is 0.14186377823352814\n",
      "epoch: 6 step: 848, loss is 0.16214531660079956\n",
      "epoch: 6 step: 849, loss is 0.19441187381744385\n",
      "epoch: 6 step: 850, loss is 0.08872609585523605\n",
      "epoch: 6 step: 851, loss is 0.20433710515499115\n",
      "epoch: 6 step: 852, loss is 0.11229259520769119\n",
      "epoch: 6 step: 853, loss is 0.13492520153522491\n",
      "epoch: 6 step: 854, loss is 0.0868455097079277\n",
      "epoch: 6 step: 855, loss is 0.2782520055770874\n",
      "epoch: 6 step: 856, loss is 0.1336120069026947\n",
      "epoch: 6 step: 857, loss is 0.12744052708148956\n",
      "epoch: 6 step: 858, loss is 0.15247656404972076\n",
      "epoch: 6 step: 859, loss is 0.10991155356168747\n",
      "epoch: 6 step: 860, loss is 0.12674222886562347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 861, loss is 0.23242735862731934\n",
      "epoch: 6 step: 862, loss is 0.07363121211528778\n",
      "epoch: 6 step: 863, loss is 0.11233935505151749\n",
      "epoch: 6 step: 864, loss is 0.22710126638412476\n",
      "epoch: 6 step: 865, loss is 0.17628660798072815\n",
      "epoch: 6 step: 866, loss is 0.10527888685464859\n",
      "epoch: 6 step: 867, loss is 0.12725351750850677\n",
      "epoch: 6 step: 868, loss is 0.15075518190860748\n",
      "epoch: 6 step: 869, loss is 0.052033681422472\n",
      "epoch: 6 step: 870, loss is 0.12966671586036682\n",
      "epoch: 6 step: 871, loss is 0.132864311337471\n",
      "epoch: 6 step: 872, loss is 0.14947043359279633\n",
      "epoch: 6 step: 873, loss is 0.05912432819604874\n",
      "epoch: 6 step: 874, loss is 0.09895174205303192\n",
      "epoch: 6 step: 875, loss is 0.07836567610502243\n",
      "epoch: 6 step: 876, loss is 0.04641272500157356\n",
      "epoch: 6 step: 877, loss is 0.24866047501564026\n",
      "epoch: 6 step: 878, loss is 0.3190629184246063\n",
      "epoch: 6 step: 879, loss is 0.04265404865145683\n",
      "epoch: 6 step: 880, loss is 0.14169329404830933\n",
      "epoch: 6 step: 881, loss is 0.10868769139051437\n",
      "epoch: 6 step: 882, loss is 0.0445486456155777\n",
      "epoch: 6 step: 883, loss is 0.23369412124156952\n",
      "epoch: 6 step: 884, loss is 0.10801060497760773\n",
      "epoch: 6 step: 885, loss is 0.14428655803203583\n",
      "epoch: 6 step: 886, loss is 0.046656638383865356\n",
      "epoch: 6 step: 887, loss is 0.13283893465995789\n",
      "epoch: 6 step: 888, loss is 0.2713300585746765\n",
      "epoch: 6 step: 889, loss is 0.19699923694133759\n",
      "epoch: 6 step: 890, loss is 0.14683137834072113\n",
      "epoch: 6 step: 891, loss is 0.23516039550304413\n",
      "epoch: 6 step: 892, loss is 0.24443595111370087\n",
      "epoch: 6 step: 893, loss is 0.0767684355378151\n",
      "epoch: 6 step: 894, loss is 0.14445261657238007\n",
      "epoch: 6 step: 895, loss is 0.1417699009180069\n",
      "epoch: 6 step: 896, loss is 0.19703246653079987\n",
      "epoch: 6 step: 897, loss is 0.2042468935251236\n",
      "epoch: 6 step: 898, loss is 0.07704231888055801\n",
      "epoch: 6 step: 899, loss is 0.09457431733608246\n",
      "epoch: 6 step: 900, loss is 0.11178615689277649\n",
      "epoch: 6 step: 901, loss is 0.18571296334266663\n",
      "epoch: 6 step: 902, loss is 0.0681854784488678\n",
      "epoch: 6 step: 903, loss is 0.09454463422298431\n",
      "epoch: 6 step: 904, loss is 0.1221032589673996\n",
      "epoch: 6 step: 905, loss is 0.06526963412761688\n",
      "epoch: 6 step: 906, loss is 0.10380508750677109\n",
      "epoch: 6 step: 907, loss is 0.1385819911956787\n",
      "epoch: 6 step: 908, loss is 0.07004407048225403\n",
      "epoch: 6 step: 909, loss is 0.16769875586032867\n",
      "epoch: 6 step: 910, loss is 0.20913919806480408\n",
      "epoch: 6 step: 911, loss is 0.14363661408424377\n",
      "epoch: 6 step: 912, loss is 0.1966305673122406\n",
      "epoch: 6 step: 913, loss is 0.16046515107154846\n",
      "epoch: 6 step: 914, loss is 0.24787604808807373\n",
      "epoch: 6 step: 915, loss is 0.19575080275535583\n",
      "epoch: 6 step: 916, loss is 0.10617784410715103\n",
      "epoch: 6 step: 917, loss is 0.14938423037528992\n",
      "epoch: 6 step: 918, loss is 0.2640792727470398\n",
      "epoch: 6 step: 919, loss is 0.07802718877792358\n",
      "epoch: 6 step: 920, loss is 0.08379790931940079\n",
      "epoch: 6 step: 921, loss is 0.19466541707515717\n",
      "epoch: 6 step: 922, loss is 0.20018433034420013\n",
      "epoch: 6 step: 923, loss is 0.17490224540233612\n",
      "epoch: 6 step: 924, loss is 0.1289198398590088\n",
      "epoch: 6 step: 925, loss is 0.20225593447685242\n",
      "epoch: 6 step: 926, loss is 0.24570070207118988\n",
      "epoch: 6 step: 927, loss is 0.17940938472747803\n",
      "epoch: 6 step: 928, loss is 0.1717473864555359\n",
      "epoch: 6 step: 929, loss is 0.0911363884806633\n",
      "epoch: 6 step: 930, loss is 0.14098840951919556\n",
      "epoch: 6 step: 931, loss is 0.2646770179271698\n",
      "epoch: 6 step: 932, loss is 0.22457820177078247\n",
      "epoch: 6 step: 933, loss is 0.0862724781036377\n",
      "epoch: 6 step: 934, loss is 0.2354481816291809\n",
      "epoch: 6 step: 935, loss is 0.13340944051742554\n",
      "epoch: 6 step: 936, loss is 0.09883149713277817\n",
      "epoch: 6 step: 937, loss is 0.3305443525314331\n",
      "epoch: 7 step: 1, loss is 0.09455053508281708\n",
      "epoch: 7 step: 2, loss is 0.08424577116966248\n",
      "epoch: 7 step: 3, loss is 0.11865226924419403\n",
      "epoch: 7 step: 4, loss is 0.05655868351459503\n",
      "epoch: 7 step: 5, loss is 0.14928466081619263\n",
      "epoch: 7 step: 6, loss is 0.1392989158630371\n",
      "epoch: 7 step: 7, loss is 0.11588328331708908\n",
      "epoch: 7 step: 8, loss is 0.06086127832531929\n",
      "epoch: 7 step: 9, loss is 0.14838296175003052\n",
      "epoch: 7 step: 10, loss is 0.04227157309651375\n",
      "epoch: 7 step: 11, loss is 0.11372943222522736\n",
      "epoch: 7 step: 12, loss is 0.09558375179767609\n",
      "epoch: 7 step: 13, loss is 0.1506207287311554\n",
      "epoch: 7 step: 14, loss is 0.08292409777641296\n",
      "epoch: 7 step: 15, loss is 0.1045929342508316\n",
      "epoch: 7 step: 16, loss is 0.09125818312168121\n",
      "epoch: 7 step: 17, loss is 0.12612384557724\n",
      "epoch: 7 step: 18, loss is 0.12330009788274765\n",
      "epoch: 7 step: 19, loss is 0.11837653070688248\n",
      "epoch: 7 step: 20, loss is 0.0958007276058197\n",
      "epoch: 7 step: 21, loss is 0.057238418608903885\n",
      "epoch: 7 step: 22, loss is 0.03360498324036598\n",
      "epoch: 7 step: 23, loss is 0.18336261808872223\n",
      "epoch: 7 step: 24, loss is 0.047928400337696075\n",
      "epoch: 7 step: 25, loss is 0.09330684691667557\n",
      "epoch: 7 step: 26, loss is 0.12761518359184265\n",
      "epoch: 7 step: 27, loss is 0.16823428869247437\n",
      "epoch: 7 step: 28, loss is 0.16921593248844147\n",
      "epoch: 7 step: 29, loss is 0.13762347400188446\n",
      "epoch: 7 step: 30, loss is 0.06733333319425583\n",
      "epoch: 7 step: 31, loss is 0.08549711108207703\n",
      "epoch: 7 step: 32, loss is 0.1115354597568512\n",
      "epoch: 7 step: 33, loss is 0.12698347866535187\n",
      "epoch: 7 step: 34, loss is 0.04250694066286087\n",
      "epoch: 7 step: 35, loss is 0.1604309231042862\n",
      "epoch: 7 step: 36, loss is 0.0359649695456028\n",
      "epoch: 7 step: 37, loss is 0.029617181047797203\n",
      "epoch: 7 step: 38, loss is 0.12412913143634796\n",
      "epoch: 7 step: 39, loss is 0.12070298939943314\n",
      "epoch: 7 step: 40, loss is 0.04764015972614288\n",
      "epoch: 7 step: 41, loss is 0.08810371905565262\n",
      "epoch: 7 step: 42, loss is 0.04841655120253563\n",
      "epoch: 7 step: 43, loss is 0.1191612035036087\n",
      "epoch: 7 step: 44, loss is 0.045340489596128464\n",
      "epoch: 7 step: 45, loss is 0.1194886863231659\n",
      "epoch: 7 step: 46, loss is 0.10293373465538025\n",
      "epoch: 7 step: 47, loss is 0.07068267464637756\n",
      "epoch: 7 step: 48, loss is 0.23118646442890167\n",
      "epoch: 7 step: 49, loss is 0.08444515615701675\n",
      "epoch: 7 step: 50, loss is 0.16544680297374725\n",
      "epoch: 7 step: 51, loss is 0.1750413477420807\n",
      "epoch: 7 step: 52, loss is 0.18475905060768127\n",
      "epoch: 7 step: 53, loss is 0.07517983764410019\n",
      "epoch: 7 step: 54, loss is 0.08535204082727432\n",
      "epoch: 7 step: 55, loss is 0.18240079283714294\n",
      "epoch: 7 step: 56, loss is 0.07343586534261703\n",
      "epoch: 7 step: 57, loss is 0.08020594716072083\n",
      "epoch: 7 step: 58, loss is 0.15377101302146912\n",
      "epoch: 7 step: 59, loss is 0.07690263539552689\n",
      "epoch: 7 step: 60, loss is 0.06765575706958771\n",
      "epoch: 7 step: 61, loss is 0.11764156073331833\n",
      "epoch: 7 step: 62, loss is 0.261820912361145\n",
      "epoch: 7 step: 63, loss is 0.0955730676651001\n",
      "epoch: 7 step: 64, loss is 0.04686669260263443\n",
      "epoch: 7 step: 65, loss is 0.22753669321537018\n",
      "epoch: 7 step: 66, loss is 0.05105654150247574\n",
      "epoch: 7 step: 67, loss is 0.09933801740407944\n",
      "epoch: 7 step: 68, loss is 0.08525105565786362\n",
      "epoch: 7 step: 69, loss is 0.10994534194469452\n",
      "epoch: 7 step: 70, loss is 0.08416862040758133\n",
      "epoch: 7 step: 71, loss is 0.0265386700630188\n",
      "epoch: 7 step: 72, loss is 0.1045076996088028\n",
      "epoch: 7 step: 73, loss is 0.19669558107852936\n",
      "epoch: 7 step: 74, loss is 0.09350236505270004\n",
      "epoch: 7 step: 75, loss is 0.1300612986087799\n",
      "epoch: 7 step: 76, loss is 0.0432983860373497\n",
      "epoch: 7 step: 77, loss is 0.06545260548591614\n",
      "epoch: 7 step: 78, loss is 0.052475620061159134\n",
      "epoch: 7 step: 79, loss is 0.12391959875822067\n",
      "epoch: 7 step: 80, loss is 0.08319627493619919\n",
      "epoch: 7 step: 81, loss is 0.09107662737369537\n",
      "epoch: 7 step: 82, loss is 0.10586237162351608\n",
      "epoch: 7 step: 83, loss is 0.07817759364843369\n",
      "epoch: 7 step: 84, loss is 0.11110280454158783\n",
      "epoch: 7 step: 85, loss is 0.1724298745393753\n",
      "epoch: 7 step: 86, loss is 0.10164767503738403\n",
      "epoch: 7 step: 87, loss is 0.09393338859081268\n",
      "epoch: 7 step: 88, loss is 0.04910765215754509\n",
      "epoch: 7 step: 89, loss is 0.0745764747262001\n",
      "epoch: 7 step: 90, loss is 0.11088167876005173\n",
      "epoch: 7 step: 91, loss is 0.028485124930739403\n",
      "epoch: 7 step: 92, loss is 0.03269822895526886\n",
      "epoch: 7 step: 93, loss is 0.07486017048358917\n",
      "epoch: 7 step: 94, loss is 0.07752969115972519\n",
      "epoch: 7 step: 95, loss is 0.09049125015735626\n",
      "epoch: 7 step: 96, loss is 0.07160858809947968\n",
      "epoch: 7 step: 97, loss is 0.13267944753170013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 98, loss is 0.17465509474277496\n",
      "epoch: 7 step: 99, loss is 0.053536657243967056\n",
      "epoch: 7 step: 100, loss is 0.1436724215745926\n",
      "epoch: 7 step: 101, loss is 0.12180154025554657\n",
      "epoch: 7 step: 102, loss is 0.05777250975370407\n",
      "epoch: 7 step: 103, loss is 0.06808682531118393\n",
      "epoch: 7 step: 104, loss is 0.08700625598430634\n",
      "epoch: 7 step: 105, loss is 0.05693580582737923\n",
      "epoch: 7 step: 106, loss is 0.12391119450330734\n",
      "epoch: 7 step: 107, loss is 0.08651594817638397\n",
      "epoch: 7 step: 108, loss is 0.11275651305913925\n",
      "epoch: 7 step: 109, loss is 0.0529850609600544\n",
      "epoch: 7 step: 110, loss is 0.060792941600084305\n",
      "epoch: 7 step: 111, loss is 0.08681880682706833\n",
      "epoch: 7 step: 112, loss is 0.03665085881948471\n",
      "epoch: 7 step: 113, loss is 0.05740620568394661\n",
      "epoch: 7 step: 114, loss is 0.09751038998365402\n",
      "epoch: 7 step: 115, loss is 0.07013333588838577\n",
      "epoch: 7 step: 116, loss is 0.19659729301929474\n",
      "epoch: 7 step: 117, loss is 0.19260306656360626\n",
      "epoch: 7 step: 118, loss is 0.14891627430915833\n",
      "epoch: 7 step: 119, loss is 0.2029368132352829\n",
      "epoch: 7 step: 120, loss is 0.24507948756217957\n",
      "epoch: 7 step: 121, loss is 0.062394678592681885\n",
      "epoch: 7 step: 122, loss is 0.027064692229032516\n",
      "epoch: 7 step: 123, loss is 0.018544025719165802\n",
      "epoch: 7 step: 124, loss is 0.09443500638008118\n",
      "epoch: 7 step: 125, loss is 0.11180056631565094\n",
      "epoch: 7 step: 126, loss is 0.19840633869171143\n",
      "epoch: 7 step: 127, loss is 0.1711646467447281\n",
      "epoch: 7 step: 128, loss is 0.11200208961963654\n",
      "epoch: 7 step: 129, loss is 0.16283339262008667\n",
      "epoch: 7 step: 130, loss is 0.16906008124351501\n",
      "epoch: 7 step: 131, loss is 0.10134781152009964\n",
      "epoch: 7 step: 132, loss is 0.08585827052593231\n",
      "epoch: 7 step: 133, loss is 0.12155947834253311\n",
      "epoch: 7 step: 134, loss is 0.09040044248104095\n",
      "epoch: 7 step: 135, loss is 0.06891750544309616\n",
      "epoch: 7 step: 136, loss is 0.16423626244068146\n",
      "epoch: 7 step: 137, loss is 0.06719457358121872\n",
      "epoch: 7 step: 138, loss is 0.16441188752651215\n",
      "epoch: 7 step: 139, loss is 0.1224626824259758\n",
      "epoch: 7 step: 140, loss is 0.10851670801639557\n",
      "epoch: 7 step: 141, loss is 0.09867940843105316\n",
      "epoch: 7 step: 142, loss is 0.026215746998786926\n",
      "epoch: 7 step: 143, loss is 0.07723142206668854\n",
      "epoch: 7 step: 144, loss is 0.15125232934951782\n",
      "epoch: 7 step: 145, loss is 0.11868102103471756\n",
      "epoch: 7 step: 146, loss is 0.19149149954319\n",
      "epoch: 7 step: 147, loss is 0.02535814791917801\n",
      "epoch: 7 step: 148, loss is 0.21862562000751495\n",
      "epoch: 7 step: 149, loss is 0.0942029133439064\n",
      "epoch: 7 step: 150, loss is 0.09352178871631622\n",
      "epoch: 7 step: 151, loss is 0.13099922239780426\n",
      "epoch: 7 step: 152, loss is 0.0810103490948677\n",
      "epoch: 7 step: 153, loss is 0.13397912681102753\n",
      "epoch: 7 step: 154, loss is 0.0672936961054802\n",
      "epoch: 7 step: 155, loss is 0.08356726169586182\n",
      "epoch: 7 step: 156, loss is 0.11112722754478455\n",
      "epoch: 7 step: 157, loss is 0.07765880227088928\n",
      "epoch: 7 step: 158, loss is 0.1152212843298912\n",
      "epoch: 7 step: 159, loss is 0.10598646849393845\n",
      "epoch: 7 step: 160, loss is 0.13762329518795013\n",
      "epoch: 7 step: 161, loss is 0.10398967564105988\n",
      "epoch: 7 step: 162, loss is 0.12417967617511749\n",
      "epoch: 7 step: 163, loss is 0.05970504507422447\n",
      "epoch: 7 step: 164, loss is 0.11353358626365662\n",
      "epoch: 7 step: 165, loss is 0.0594458281993866\n",
      "epoch: 7 step: 166, loss is 0.13770616054534912\n",
      "epoch: 7 step: 167, loss is 0.09922393411397934\n",
      "epoch: 7 step: 168, loss is 0.21068544685840607\n",
      "epoch: 7 step: 169, loss is 0.09272646903991699\n",
      "epoch: 7 step: 170, loss is 0.09727822989225388\n",
      "epoch: 7 step: 171, loss is 0.052251871675252914\n",
      "epoch: 7 step: 172, loss is 0.08663854748010635\n",
      "epoch: 7 step: 173, loss is 0.3155071437358856\n",
      "epoch: 7 step: 174, loss is 0.12181948870420456\n",
      "epoch: 7 step: 175, loss is 0.02270614355802536\n",
      "epoch: 7 step: 176, loss is 0.08553073555231094\n",
      "epoch: 7 step: 177, loss is 0.12864848971366882\n",
      "epoch: 7 step: 178, loss is 0.1253204643726349\n",
      "epoch: 7 step: 179, loss is 0.1919761747121811\n",
      "epoch: 7 step: 180, loss is 0.11069603264331818\n",
      "epoch: 7 step: 181, loss is 0.05292120203375816\n",
      "epoch: 7 step: 182, loss is 0.07546614110469818\n",
      "epoch: 7 step: 183, loss is 0.1362704485654831\n",
      "epoch: 7 step: 184, loss is 0.2207687795162201\n",
      "epoch: 7 step: 185, loss is 0.06151232495903969\n",
      "epoch: 7 step: 186, loss is 0.1258394718170166\n",
      "epoch: 7 step: 187, loss is 0.02209067903459072\n",
      "epoch: 7 step: 188, loss is 0.1492680162191391\n",
      "epoch: 7 step: 189, loss is 0.1711312234401703\n",
      "epoch: 7 step: 190, loss is 0.13563208281993866\n",
      "epoch: 7 step: 191, loss is 0.10329616814851761\n",
      "epoch: 7 step: 192, loss is 0.1801251322031021\n",
      "epoch: 7 step: 193, loss is 0.1589275598526001\n",
      "epoch: 7 step: 194, loss is 0.08330363035202026\n",
      "epoch: 7 step: 195, loss is 0.10298812389373779\n",
      "epoch: 7 step: 196, loss is 0.10273395478725433\n",
      "epoch: 7 step: 197, loss is 0.23028521239757538\n",
      "epoch: 7 step: 198, loss is 0.07650478184223175\n",
      "epoch: 7 step: 199, loss is 0.17809079587459564\n",
      "epoch: 7 step: 200, loss is 0.060510069131851196\n",
      "epoch: 7 step: 201, loss is 0.02073529362678528\n",
      "epoch: 7 step: 202, loss is 0.11161953210830688\n",
      "epoch: 7 step: 203, loss is 0.24102871119976044\n",
      "epoch: 7 step: 204, loss is 0.060744430869817734\n",
      "epoch: 7 step: 205, loss is 0.15654903650283813\n",
      "epoch: 7 step: 206, loss is 0.08631408959627151\n",
      "epoch: 7 step: 207, loss is 0.07480603456497192\n",
      "epoch: 7 step: 208, loss is 0.153890460729599\n",
      "epoch: 7 step: 209, loss is 0.10407701879739761\n",
      "epoch: 7 step: 210, loss is 0.11106128990650177\n",
      "epoch: 7 step: 211, loss is 0.15237440168857574\n",
      "epoch: 7 step: 212, loss is 0.07481106370687485\n",
      "epoch: 7 step: 213, loss is 0.10355189442634583\n",
      "epoch: 7 step: 214, loss is 0.14513275027275085\n",
      "epoch: 7 step: 215, loss is 0.05530672147870064\n",
      "epoch: 7 step: 216, loss is 0.07182884216308594\n",
      "epoch: 7 step: 217, loss is 0.08156195282936096\n",
      "epoch: 7 step: 218, loss is 0.07797328382730484\n",
      "epoch: 7 step: 219, loss is 0.053598444908857346\n",
      "epoch: 7 step: 220, loss is 0.08755726367235184\n",
      "epoch: 7 step: 221, loss is 0.12812447547912598\n",
      "epoch: 7 step: 222, loss is 0.10884685069322586\n",
      "epoch: 7 step: 223, loss is 0.2691081166267395\n",
      "epoch: 7 step: 224, loss is 0.12298122048377991\n",
      "epoch: 7 step: 225, loss is 0.174141064286232\n",
      "epoch: 7 step: 226, loss is 0.13387435674667358\n",
      "epoch: 7 step: 227, loss is 0.0802990049123764\n",
      "epoch: 7 step: 228, loss is 0.08449757844209671\n",
      "epoch: 7 step: 229, loss is 0.07207663357257843\n",
      "epoch: 7 step: 230, loss is 0.06590482592582703\n",
      "epoch: 7 step: 231, loss is 0.1101711094379425\n",
      "epoch: 7 step: 232, loss is 0.2325231283903122\n",
      "epoch: 7 step: 233, loss is 0.18924783170223236\n",
      "epoch: 7 step: 234, loss is 0.09307802468538284\n",
      "epoch: 7 step: 235, loss is 0.10300498455762863\n",
      "epoch: 7 step: 236, loss is 0.15935586392879486\n",
      "epoch: 7 step: 237, loss is 0.09095040708780289\n",
      "epoch: 7 step: 238, loss is 0.19119572639465332\n",
      "epoch: 7 step: 239, loss is 0.07609324157238007\n",
      "epoch: 7 step: 240, loss is 0.13343915343284607\n",
      "epoch: 7 step: 241, loss is 0.11794356256723404\n",
      "epoch: 7 step: 242, loss is 0.10671476274728775\n",
      "epoch: 7 step: 243, loss is 0.11405663937330246\n",
      "epoch: 7 step: 244, loss is 0.21314285695552826\n",
      "epoch: 7 step: 245, loss is 0.0949343740940094\n",
      "epoch: 7 step: 246, loss is 0.033996548503637314\n",
      "epoch: 7 step: 247, loss is 0.054843779653310776\n",
      "epoch: 7 step: 248, loss is 0.0988243967294693\n",
      "epoch: 7 step: 249, loss is 0.0850265845656395\n",
      "epoch: 7 step: 250, loss is 0.09896093606948853\n",
      "epoch: 7 step: 251, loss is 0.1759016364812851\n",
      "epoch: 7 step: 252, loss is 0.20217785239219666\n",
      "epoch: 7 step: 253, loss is 0.11854098737239838\n",
      "epoch: 7 step: 254, loss is 0.10208868235349655\n",
      "epoch: 7 step: 255, loss is 0.10871728509664536\n",
      "epoch: 7 step: 256, loss is 0.13511741161346436\n",
      "epoch: 7 step: 257, loss is 0.1486656665802002\n",
      "epoch: 7 step: 258, loss is 0.13206103444099426\n",
      "epoch: 7 step: 259, loss is 0.09727413952350616\n",
      "epoch: 7 step: 260, loss is 0.0677458792924881\n",
      "epoch: 7 step: 261, loss is 0.05524792894721031\n",
      "epoch: 7 step: 262, loss is 0.08875904232263565\n",
      "epoch: 7 step: 263, loss is 0.1152992844581604\n",
      "epoch: 7 step: 264, loss is 0.16439060866832733\n",
      "epoch: 7 step: 265, loss is 0.1643446832895279\n",
      "epoch: 7 step: 266, loss is 0.04977050796151161\n",
      "epoch: 7 step: 267, loss is 0.1308404952287674\n",
      "epoch: 7 step: 268, loss is 0.16130317747592926\n",
      "epoch: 7 step: 269, loss is 0.2599487602710724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 270, loss is 0.07605617493391037\n",
      "epoch: 7 step: 271, loss is 0.07470551878213882\n",
      "epoch: 7 step: 272, loss is 0.18648438155651093\n",
      "epoch: 7 step: 273, loss is 0.16607990860939026\n",
      "epoch: 7 step: 274, loss is 0.20816011726856232\n",
      "epoch: 7 step: 275, loss is 0.1529950052499771\n",
      "epoch: 7 step: 276, loss is 0.07600592076778412\n",
      "epoch: 7 step: 277, loss is 0.12808233499526978\n",
      "epoch: 7 step: 278, loss is 0.14923131465911865\n",
      "epoch: 7 step: 279, loss is 0.1722293198108673\n",
      "epoch: 7 step: 280, loss is 0.09351908415555954\n",
      "epoch: 7 step: 281, loss is 0.2520933449268341\n",
      "epoch: 7 step: 282, loss is 0.1315959244966507\n",
      "epoch: 7 step: 283, loss is 0.08443086594343185\n",
      "epoch: 7 step: 284, loss is 0.1921636164188385\n",
      "epoch: 7 step: 285, loss is 0.04805423319339752\n",
      "epoch: 7 step: 286, loss is 0.2737678587436676\n",
      "epoch: 7 step: 287, loss is 0.08961699157953262\n",
      "epoch: 7 step: 288, loss is 0.06815642863512039\n",
      "epoch: 7 step: 289, loss is 0.1639920324087143\n",
      "epoch: 7 step: 290, loss is 0.1545722633600235\n",
      "epoch: 7 step: 291, loss is 0.0925721824169159\n",
      "epoch: 7 step: 292, loss is 0.08382111042737961\n",
      "epoch: 7 step: 293, loss is 0.2619273066520691\n",
      "epoch: 7 step: 294, loss is 0.07527830451726913\n",
      "epoch: 7 step: 295, loss is 0.09965944290161133\n",
      "epoch: 7 step: 296, loss is 0.1379498690366745\n",
      "epoch: 7 step: 297, loss is 0.08659551292657852\n",
      "epoch: 7 step: 298, loss is 0.09877579659223557\n",
      "epoch: 7 step: 299, loss is 0.04071442410349846\n",
      "epoch: 7 step: 300, loss is 0.09548279643058777\n",
      "epoch: 7 step: 301, loss is 0.16259127855300903\n",
      "epoch: 7 step: 302, loss is 0.0871550440788269\n",
      "epoch: 7 step: 303, loss is 0.03092833235859871\n",
      "epoch: 7 step: 304, loss is 0.14203102886676788\n",
      "epoch: 7 step: 305, loss is 0.06619682908058167\n",
      "epoch: 7 step: 306, loss is 0.1718473583459854\n",
      "epoch: 7 step: 307, loss is 0.15671159327030182\n",
      "epoch: 7 step: 308, loss is 0.11263269186019897\n",
      "epoch: 7 step: 309, loss is 0.03895188495516777\n",
      "epoch: 7 step: 310, loss is 0.14468157291412354\n",
      "epoch: 7 step: 311, loss is 0.11421943455934525\n",
      "epoch: 7 step: 312, loss is 0.17103300988674164\n",
      "epoch: 7 step: 313, loss is 0.10423554480075836\n",
      "epoch: 7 step: 314, loss is 0.16895771026611328\n",
      "epoch: 7 step: 315, loss is 0.06227429211139679\n",
      "epoch: 7 step: 316, loss is 0.05413665622472763\n",
      "epoch: 7 step: 317, loss is 0.15434439480304718\n",
      "epoch: 7 step: 318, loss is 0.1009543165564537\n",
      "epoch: 7 step: 319, loss is 0.1222701445221901\n",
      "epoch: 7 step: 320, loss is 0.0836978554725647\n",
      "epoch: 7 step: 321, loss is 0.15167082846164703\n",
      "epoch: 7 step: 322, loss is 0.14301718771457672\n",
      "epoch: 7 step: 323, loss is 0.1443549394607544\n",
      "epoch: 7 step: 324, loss is 0.02075362205505371\n",
      "epoch: 7 step: 325, loss is 0.15625867247581482\n",
      "epoch: 7 step: 326, loss is 0.11007201671600342\n",
      "epoch: 7 step: 327, loss is 0.2455485463142395\n",
      "epoch: 7 step: 328, loss is 0.17380037903785706\n",
      "epoch: 7 step: 329, loss is 0.09142007678747177\n",
      "epoch: 7 step: 330, loss is 0.20933017134666443\n",
      "epoch: 7 step: 331, loss is 0.12004668265581131\n",
      "epoch: 7 step: 332, loss is 0.06911253929138184\n",
      "epoch: 7 step: 333, loss is 0.03702118992805481\n",
      "epoch: 7 step: 334, loss is 0.1450636088848114\n",
      "epoch: 7 step: 335, loss is 0.25749871134757996\n",
      "epoch: 7 step: 336, loss is 0.1260039508342743\n",
      "epoch: 7 step: 337, loss is 0.1892712414264679\n",
      "epoch: 7 step: 338, loss is 0.26056432723999023\n",
      "epoch: 7 step: 339, loss is 0.03072522021830082\n",
      "epoch: 7 step: 340, loss is 0.13206471502780914\n",
      "epoch: 7 step: 341, loss is 0.08409217745065689\n",
      "epoch: 7 step: 342, loss is 0.14829792082309723\n",
      "epoch: 7 step: 343, loss is 0.08701401948928833\n",
      "epoch: 7 step: 344, loss is 0.2928200662136078\n",
      "epoch: 7 step: 345, loss is 0.13527078926563263\n",
      "epoch: 7 step: 346, loss is 0.18107320368289948\n",
      "epoch: 7 step: 347, loss is 0.1292920857667923\n",
      "epoch: 7 step: 348, loss is 0.1405930370092392\n",
      "epoch: 7 step: 349, loss is 0.1448182910680771\n",
      "epoch: 7 step: 350, loss is 0.14358440041542053\n",
      "epoch: 7 step: 351, loss is 0.17612352967262268\n",
      "epoch: 7 step: 352, loss is 0.11096128076314926\n",
      "epoch: 7 step: 353, loss is 0.06976956129074097\n",
      "epoch: 7 step: 354, loss is 0.23827090859413147\n",
      "epoch: 7 step: 355, loss is 0.07030761986970901\n",
      "epoch: 7 step: 356, loss is 0.13551172614097595\n",
      "epoch: 7 step: 357, loss is 0.1680546998977661\n",
      "epoch: 7 step: 358, loss is 0.15393932163715363\n",
      "epoch: 7 step: 359, loss is 0.3075445890426636\n",
      "epoch: 7 step: 360, loss is 0.1275031715631485\n",
      "epoch: 7 step: 361, loss is 0.059922099113464355\n",
      "epoch: 7 step: 362, loss is 0.10091941058635712\n",
      "epoch: 7 step: 363, loss is 0.10492942482233047\n",
      "epoch: 7 step: 364, loss is 0.09736958891153336\n",
      "epoch: 7 step: 365, loss is 0.19106769561767578\n",
      "epoch: 7 step: 366, loss is 0.19709336757659912\n",
      "epoch: 7 step: 367, loss is 0.15040411055088043\n",
      "epoch: 7 step: 368, loss is 0.21893835067749023\n",
      "epoch: 7 step: 369, loss is 0.13537098467350006\n",
      "epoch: 7 step: 370, loss is 0.11154726147651672\n",
      "epoch: 7 step: 371, loss is 0.1414344608783722\n",
      "epoch: 7 step: 372, loss is 0.13251812756061554\n",
      "epoch: 7 step: 373, loss is 0.18226435780525208\n",
      "epoch: 7 step: 374, loss is 0.05232742801308632\n",
      "epoch: 7 step: 375, loss is 0.07353385537862778\n",
      "epoch: 7 step: 376, loss is 0.0827321857213974\n",
      "epoch: 7 step: 377, loss is 0.15958531200885773\n",
      "epoch: 7 step: 378, loss is 0.21935154497623444\n",
      "epoch: 7 step: 379, loss is 0.10820426791906357\n",
      "epoch: 7 step: 380, loss is 0.10375461727380753\n",
      "epoch: 7 step: 381, loss is 0.17421852052211761\n",
      "epoch: 7 step: 382, loss is 0.11930111795663834\n",
      "epoch: 7 step: 383, loss is 0.04915660247206688\n",
      "epoch: 7 step: 384, loss is 0.11509335786104202\n",
      "epoch: 7 step: 385, loss is 0.2083970606327057\n",
      "epoch: 7 step: 386, loss is 0.06356997042894363\n",
      "epoch: 7 step: 387, loss is 0.17472727596759796\n",
      "epoch: 7 step: 388, loss is 0.08040101081132889\n",
      "epoch: 7 step: 389, loss is 0.07387055456638336\n",
      "epoch: 7 step: 390, loss is 0.13216231763362885\n",
      "epoch: 7 step: 391, loss is 0.23766954243183136\n",
      "epoch: 7 step: 392, loss is 0.027040893211960793\n",
      "epoch: 7 step: 393, loss is 0.09078441560268402\n",
      "epoch: 7 step: 394, loss is 0.07482870668172836\n",
      "epoch: 7 step: 395, loss is 0.021899288520216942\n",
      "epoch: 7 step: 396, loss is 0.11077176034450531\n",
      "epoch: 7 step: 397, loss is 0.08552886545658112\n",
      "epoch: 7 step: 398, loss is 0.13032206892967224\n",
      "epoch: 7 step: 399, loss is 0.04561278223991394\n",
      "epoch: 7 step: 400, loss is 0.14427998661994934\n",
      "epoch: 7 step: 401, loss is 0.10684084892272949\n",
      "epoch: 7 step: 402, loss is 0.09478471428155899\n",
      "epoch: 7 step: 403, loss is 0.18256250023841858\n",
      "epoch: 7 step: 404, loss is 0.15015627443790436\n",
      "epoch: 7 step: 405, loss is 0.12365826964378357\n",
      "epoch: 7 step: 406, loss is 0.05408036708831787\n",
      "epoch: 7 step: 407, loss is 0.20522305369377136\n",
      "epoch: 7 step: 408, loss is 0.14400045573711395\n",
      "epoch: 7 step: 409, loss is 0.03269635885953903\n",
      "epoch: 7 step: 410, loss is 0.036677781492471695\n",
      "epoch: 7 step: 411, loss is 0.130575031042099\n",
      "epoch: 7 step: 412, loss is 0.12351765483617783\n",
      "epoch: 7 step: 413, loss is 0.08941039443016052\n",
      "epoch: 7 step: 414, loss is 0.037603605538606644\n",
      "epoch: 7 step: 415, loss is 0.027293715626001358\n",
      "epoch: 7 step: 416, loss is 0.14356368780136108\n",
      "epoch: 7 step: 417, loss is 0.08601811528205872\n",
      "epoch: 7 step: 418, loss is 0.043273527175188065\n",
      "epoch: 7 step: 419, loss is 0.28985336422920227\n",
      "epoch: 7 step: 420, loss is 0.10313170403242111\n",
      "epoch: 7 step: 421, loss is 0.03738504275679588\n",
      "epoch: 7 step: 422, loss is 0.2870727479457855\n",
      "epoch: 7 step: 423, loss is 0.09152863174676895\n",
      "epoch: 7 step: 424, loss is 0.13561499118804932\n",
      "epoch: 7 step: 425, loss is 0.2047424614429474\n",
      "epoch: 7 step: 426, loss is 0.07429143041372299\n",
      "epoch: 7 step: 427, loss is 0.10759276151657104\n",
      "epoch: 7 step: 428, loss is 0.08947944641113281\n",
      "epoch: 7 step: 429, loss is 0.17133916914463043\n",
      "epoch: 7 step: 430, loss is 0.2813814878463745\n",
      "epoch: 7 step: 431, loss is 0.15125153958797455\n",
      "epoch: 7 step: 432, loss is 0.05553247407078743\n",
      "epoch: 7 step: 433, loss is 0.16959135234355927\n",
      "epoch: 7 step: 434, loss is 0.19832856953144073\n",
      "epoch: 7 step: 435, loss is 0.09411422163248062\n",
      "epoch: 7 step: 436, loss is 0.07006887346506119\n",
      "epoch: 7 step: 437, loss is 0.12640881538391113\n",
      "epoch: 7 step: 438, loss is 0.09516461193561554\n",
      "epoch: 7 step: 439, loss is 0.2768077850341797\n",
      "epoch: 7 step: 440, loss is 0.10741891711950302\n",
      "epoch: 7 step: 441, loss is 0.0494280606508255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 442, loss is 0.014317439869046211\n",
      "epoch: 7 step: 443, loss is 0.05903153866529465\n",
      "epoch: 7 step: 444, loss is 0.07182784378528595\n",
      "epoch: 7 step: 445, loss is 0.18202058970928192\n",
      "epoch: 7 step: 446, loss is 0.15071925520896912\n",
      "epoch: 7 step: 447, loss is 0.09150048345327377\n",
      "epoch: 7 step: 448, loss is 0.10256751626729965\n",
      "epoch: 7 step: 449, loss is 0.16904081404209137\n",
      "epoch: 7 step: 450, loss is 0.22197413444519043\n",
      "epoch: 7 step: 451, loss is 0.05194564536213875\n",
      "epoch: 7 step: 452, loss is 0.16966326534748077\n",
      "epoch: 7 step: 453, loss is 0.05517585948109627\n",
      "epoch: 7 step: 454, loss is 0.13967369496822357\n",
      "epoch: 7 step: 455, loss is 0.16292493045330048\n",
      "epoch: 7 step: 456, loss is 0.08558529615402222\n",
      "epoch: 7 step: 457, loss is 0.0647958442568779\n",
      "epoch: 7 step: 458, loss is 0.1704847663640976\n",
      "epoch: 7 step: 459, loss is 0.06137702986598015\n",
      "epoch: 7 step: 460, loss is 0.16183145344257355\n",
      "epoch: 7 step: 461, loss is 0.08526361733675003\n",
      "epoch: 7 step: 462, loss is 0.20104572176933289\n",
      "epoch: 7 step: 463, loss is 0.1734434962272644\n",
      "epoch: 7 step: 464, loss is 0.15560179948806763\n",
      "epoch: 7 step: 465, loss is 0.12690535187721252\n",
      "epoch: 7 step: 466, loss is 0.07029710710048676\n",
      "epoch: 7 step: 467, loss is 0.263258695602417\n",
      "epoch: 7 step: 468, loss is 0.16248370707035065\n",
      "epoch: 7 step: 469, loss is 0.09454909712076187\n",
      "epoch: 7 step: 470, loss is 0.049993500113487244\n",
      "epoch: 7 step: 471, loss is 0.18141694366931915\n",
      "epoch: 7 step: 472, loss is 0.04029164835810661\n",
      "epoch: 7 step: 473, loss is 0.10204090923070908\n",
      "epoch: 7 step: 474, loss is 0.09544061124324799\n",
      "epoch: 7 step: 475, loss is 0.13563752174377441\n",
      "epoch: 7 step: 476, loss is 0.12224508076906204\n",
      "epoch: 7 step: 477, loss is 0.09388971328735352\n",
      "epoch: 7 step: 478, loss is 0.03163733333349228\n",
      "epoch: 7 step: 479, loss is 0.14773139357566833\n",
      "epoch: 7 step: 480, loss is 0.10426799207925797\n",
      "epoch: 7 step: 481, loss is 0.0735611692070961\n",
      "epoch: 7 step: 482, loss is 0.1124253049492836\n",
      "epoch: 7 step: 483, loss is 0.11373379081487656\n",
      "epoch: 7 step: 484, loss is 0.05468164011836052\n",
      "epoch: 7 step: 485, loss is 0.11363640427589417\n",
      "epoch: 7 step: 486, loss is 0.09081858396530151\n",
      "epoch: 7 step: 487, loss is 0.3144509792327881\n",
      "epoch: 7 step: 488, loss is 0.10656560957431793\n",
      "epoch: 7 step: 489, loss is 0.09488078951835632\n",
      "epoch: 7 step: 490, loss is 0.0889904648065567\n",
      "epoch: 7 step: 491, loss is 0.14609967172145844\n",
      "epoch: 7 step: 492, loss is 0.10317590832710266\n",
      "epoch: 7 step: 493, loss is 0.043188031762838364\n",
      "epoch: 7 step: 494, loss is 0.20970501005649567\n",
      "epoch: 7 step: 495, loss is 0.1527976393699646\n",
      "epoch: 7 step: 496, loss is 0.08131299167871475\n",
      "epoch: 7 step: 497, loss is 0.14299847185611725\n",
      "epoch: 7 step: 498, loss is 0.07621084898710251\n",
      "epoch: 7 step: 499, loss is 0.027873067185282707\n",
      "epoch: 7 step: 500, loss is 0.137663334608078\n",
      "epoch: 7 step: 501, loss is 0.06508180499076843\n",
      "epoch: 7 step: 502, loss is 0.10208937525749207\n",
      "epoch: 7 step: 503, loss is 0.11053509265184402\n",
      "epoch: 7 step: 504, loss is 0.2513876259326935\n",
      "epoch: 7 step: 505, loss is 0.0900016650557518\n",
      "epoch: 7 step: 506, loss is 0.12037011981010437\n",
      "epoch: 7 step: 507, loss is 0.08193255215883255\n",
      "epoch: 7 step: 508, loss is 0.08945636451244354\n",
      "epoch: 7 step: 509, loss is 0.07189881056547165\n",
      "epoch: 7 step: 510, loss is 0.08096642047166824\n",
      "epoch: 7 step: 511, loss is 0.10644605755805969\n",
      "epoch: 7 step: 512, loss is 0.10721160471439362\n",
      "epoch: 7 step: 513, loss is 0.0627060979604721\n",
      "epoch: 7 step: 514, loss is 0.16878877580165863\n",
      "epoch: 7 step: 515, loss is 0.03931937739253044\n",
      "epoch: 7 step: 516, loss is 0.08219326287508011\n",
      "epoch: 7 step: 517, loss is 0.08653373271226883\n",
      "epoch: 7 step: 518, loss is 0.11552611738443375\n",
      "epoch: 7 step: 519, loss is 0.3167942464351654\n",
      "epoch: 7 step: 520, loss is 0.09803557395935059\n",
      "epoch: 7 step: 521, loss is 0.27208131551742554\n",
      "epoch: 7 step: 522, loss is 0.09624146670103073\n",
      "epoch: 7 step: 523, loss is 0.11650407314300537\n",
      "epoch: 7 step: 524, loss is 0.14238110184669495\n",
      "epoch: 7 step: 525, loss is 0.08133472502231598\n",
      "epoch: 7 step: 526, loss is 0.19347773492336273\n",
      "epoch: 7 step: 527, loss is 0.36106300354003906\n",
      "epoch: 7 step: 528, loss is 0.06823688745498657\n",
      "epoch: 7 step: 529, loss is 0.16307368874549866\n",
      "epoch: 7 step: 530, loss is 0.0695633515715599\n",
      "epoch: 7 step: 531, loss is 0.033155374228954315\n",
      "epoch: 7 step: 532, loss is 0.10319655388593674\n",
      "epoch: 7 step: 533, loss is 0.0544603206217289\n",
      "epoch: 7 step: 534, loss is 0.06458187103271484\n",
      "epoch: 7 step: 535, loss is 0.07910779118537903\n",
      "epoch: 7 step: 536, loss is 0.13167010247707367\n",
      "epoch: 7 step: 537, loss is 0.22066466510295868\n",
      "epoch: 7 step: 538, loss is 0.06843595951795578\n",
      "epoch: 7 step: 539, loss is 0.13379985094070435\n",
      "epoch: 7 step: 540, loss is 0.048135917633771896\n",
      "epoch: 7 step: 541, loss is 0.1527702957391739\n",
      "epoch: 7 step: 542, loss is 0.07262454926967621\n",
      "epoch: 7 step: 543, loss is 0.0623660571873188\n",
      "epoch: 7 step: 544, loss is 0.17535318434238434\n",
      "epoch: 7 step: 545, loss is 0.2932414412498474\n",
      "epoch: 7 step: 546, loss is 0.2295592725276947\n",
      "epoch: 7 step: 547, loss is 0.08538983017206192\n",
      "epoch: 7 step: 548, loss is 0.0726141482591629\n",
      "epoch: 7 step: 549, loss is 0.054451681673526764\n",
      "epoch: 7 step: 550, loss is 0.2429078221321106\n",
      "epoch: 7 step: 551, loss is 0.14963684976100922\n",
      "epoch: 7 step: 552, loss is 0.0564793236553669\n",
      "epoch: 7 step: 553, loss is 0.1108967736363411\n",
      "epoch: 7 step: 554, loss is 0.11957649141550064\n",
      "epoch: 7 step: 555, loss is 0.03483268618583679\n",
      "epoch: 7 step: 556, loss is 0.03577212616801262\n",
      "epoch: 7 step: 557, loss is 0.10998338460922241\n",
      "epoch: 7 step: 558, loss is 0.04406321048736572\n",
      "epoch: 7 step: 559, loss is 0.10038445144891739\n",
      "epoch: 7 step: 560, loss is 0.07725492864847183\n",
      "epoch: 7 step: 561, loss is 0.13237668573856354\n",
      "epoch: 7 step: 562, loss is 0.10663758963346481\n",
      "epoch: 7 step: 563, loss is 0.16057872772216797\n",
      "epoch: 7 step: 564, loss is 0.04823227971792221\n",
      "epoch: 7 step: 565, loss is 0.13343386352062225\n",
      "epoch: 7 step: 566, loss is 0.06010191887617111\n",
      "epoch: 7 step: 567, loss is 0.06766879558563232\n",
      "epoch: 7 step: 568, loss is 0.09337969869375229\n",
      "epoch: 7 step: 569, loss is 0.10072006285190582\n",
      "epoch: 7 step: 570, loss is 0.0790499597787857\n",
      "epoch: 7 step: 571, loss is 0.1395043432712555\n",
      "epoch: 7 step: 572, loss is 0.09924302995204926\n",
      "epoch: 7 step: 573, loss is 0.17248909175395966\n",
      "epoch: 7 step: 574, loss is 0.20183950662612915\n",
      "epoch: 7 step: 575, loss is 0.1652996689081192\n",
      "epoch: 7 step: 576, loss is 0.07979924231767654\n",
      "epoch: 7 step: 577, loss is 0.08033697307109833\n",
      "epoch: 7 step: 578, loss is 0.04621255397796631\n",
      "epoch: 7 step: 579, loss is 0.10808279365301132\n",
      "epoch: 7 step: 580, loss is 0.08799917250871658\n",
      "epoch: 7 step: 581, loss is 0.10438975691795349\n",
      "epoch: 7 step: 582, loss is 0.06820730865001678\n",
      "epoch: 7 step: 583, loss is 0.0694570392370224\n",
      "epoch: 7 step: 584, loss is 0.10814222693443298\n",
      "epoch: 7 step: 585, loss is 0.08890418708324432\n",
      "epoch: 7 step: 586, loss is 0.06744679063558578\n",
      "epoch: 7 step: 587, loss is 0.11639676988124847\n",
      "epoch: 7 step: 588, loss is 0.04698078706860542\n",
      "epoch: 7 step: 589, loss is 0.033201489597558975\n",
      "epoch: 7 step: 590, loss is 0.175176203250885\n",
      "epoch: 7 step: 591, loss is 0.08362798392772675\n",
      "epoch: 7 step: 592, loss is 0.012846624478697777\n",
      "epoch: 7 step: 593, loss is 0.08195681124925613\n",
      "epoch: 7 step: 594, loss is 0.07552968710660934\n",
      "epoch: 7 step: 595, loss is 0.06309016048908234\n",
      "epoch: 7 step: 596, loss is 0.05030206963419914\n",
      "epoch: 7 step: 597, loss is 0.11717897653579712\n",
      "epoch: 7 step: 598, loss is 0.1238061785697937\n",
      "epoch: 7 step: 599, loss is 0.11473730206489563\n",
      "epoch: 7 step: 600, loss is 0.20471617579460144\n",
      "epoch: 7 step: 601, loss is 0.16555418074131012\n",
      "epoch: 7 step: 602, loss is 0.10404271632432938\n",
      "epoch: 7 step: 603, loss is 0.061089642345905304\n",
      "epoch: 7 step: 604, loss is 0.10685500502586365\n",
      "epoch: 7 step: 605, loss is 0.045157477259635925\n",
      "epoch: 7 step: 606, loss is 0.038798920810222626\n",
      "epoch: 7 step: 607, loss is 0.16992954909801483\n",
      "epoch: 7 step: 608, loss is 0.041040971875190735\n",
      "epoch: 7 step: 609, loss is 0.14037172496318817\n",
      "epoch: 7 step: 610, loss is 0.08988123387098312\n",
      "epoch: 7 step: 611, loss is 0.25505614280700684\n",
      "epoch: 7 step: 612, loss is 0.1639990359544754\n",
      "epoch: 7 step: 613, loss is 0.1525079905986786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 614, loss is 0.1479784995317459\n",
      "epoch: 7 step: 615, loss is 0.10709168016910553\n",
      "epoch: 7 step: 616, loss is 0.10180500149726868\n",
      "epoch: 7 step: 617, loss is 0.13458526134490967\n",
      "epoch: 7 step: 618, loss is 0.19621634483337402\n",
      "epoch: 7 step: 619, loss is 0.06792540103197098\n",
      "epoch: 7 step: 620, loss is 0.07067716866731644\n",
      "epoch: 7 step: 621, loss is 0.04098453000187874\n",
      "epoch: 7 step: 622, loss is 0.1863546520471573\n",
      "epoch: 7 step: 623, loss is 0.2126261591911316\n",
      "epoch: 7 step: 624, loss is 0.08258987963199615\n",
      "epoch: 7 step: 625, loss is 0.08551917225122452\n",
      "epoch: 7 step: 626, loss is 0.04006831720471382\n",
      "epoch: 7 step: 627, loss is 0.07162480801343918\n",
      "epoch: 7 step: 628, loss is 0.23091164231300354\n",
      "epoch: 7 step: 629, loss is 0.19029147922992706\n",
      "epoch: 7 step: 630, loss is 0.16116032004356384\n",
      "epoch: 7 step: 631, loss is 0.03336760774254799\n",
      "epoch: 7 step: 632, loss is 0.10817406326532364\n",
      "epoch: 7 step: 633, loss is 0.15590216219425201\n",
      "epoch: 7 step: 634, loss is 0.06798014044761658\n",
      "epoch: 7 step: 635, loss is 0.13723471760749817\n",
      "epoch: 7 step: 636, loss is 0.1835738718509674\n",
      "epoch: 7 step: 637, loss is 0.11260133236646652\n",
      "epoch: 7 step: 638, loss is 0.06423637270927429\n",
      "epoch: 7 step: 639, loss is 0.19290372729301453\n",
      "epoch: 7 step: 640, loss is 0.0476248674094677\n",
      "epoch: 7 step: 641, loss is 0.17061008512973785\n",
      "epoch: 7 step: 642, loss is 0.15160280466079712\n",
      "epoch: 7 step: 643, loss is 0.14386479556560516\n",
      "epoch: 7 step: 644, loss is 0.1086956262588501\n",
      "epoch: 7 step: 645, loss is 0.10206318646669388\n",
      "epoch: 7 step: 646, loss is 0.05697691813111305\n",
      "epoch: 7 step: 647, loss is 0.13475115597248077\n",
      "epoch: 7 step: 648, loss is 0.08743155002593994\n",
      "epoch: 7 step: 649, loss is 0.12876345217227936\n",
      "epoch: 7 step: 650, loss is 0.11418388783931732\n",
      "epoch: 7 step: 651, loss is 0.12145580351352692\n",
      "epoch: 7 step: 652, loss is 0.20766492187976837\n",
      "epoch: 7 step: 653, loss is 0.08245272934436798\n",
      "epoch: 7 step: 654, loss is 0.06497253477573395\n",
      "epoch: 7 step: 655, loss is 0.16697297990322113\n",
      "epoch: 7 step: 656, loss is 0.0783199667930603\n",
      "epoch: 7 step: 657, loss is 0.03483026102185249\n",
      "epoch: 7 step: 658, loss is 0.036191847175359726\n",
      "epoch: 7 step: 659, loss is 0.1231677383184433\n",
      "epoch: 7 step: 660, loss is 0.09176984429359436\n",
      "epoch: 7 step: 661, loss is 0.1729755848646164\n",
      "epoch: 7 step: 662, loss is 0.07803944498300552\n",
      "epoch: 7 step: 663, loss is 0.3473512530326843\n",
      "epoch: 7 step: 664, loss is 0.06720200181007385\n",
      "epoch: 7 step: 665, loss is 0.10913236439228058\n",
      "epoch: 7 step: 666, loss is 0.05209735780954361\n",
      "epoch: 7 step: 667, loss is 0.12276219576597214\n",
      "epoch: 7 step: 668, loss is 0.15146848559379578\n",
      "epoch: 7 step: 669, loss is 0.03974856063723564\n",
      "epoch: 7 step: 670, loss is 0.07113903760910034\n",
      "epoch: 7 step: 671, loss is 0.12590885162353516\n",
      "epoch: 7 step: 672, loss is 0.06918653845787048\n",
      "epoch: 7 step: 673, loss is 0.1477518528699875\n",
      "epoch: 7 step: 674, loss is 0.21687807142734528\n",
      "epoch: 7 step: 675, loss is 0.17288511991500854\n",
      "epoch: 7 step: 676, loss is 0.055408086627721786\n",
      "epoch: 7 step: 677, loss is 0.1358146369457245\n",
      "epoch: 7 step: 678, loss is 0.17371560633182526\n",
      "epoch: 7 step: 679, loss is 0.11762012541294098\n",
      "epoch: 7 step: 680, loss is 0.10690382868051529\n",
      "epoch: 7 step: 681, loss is 0.08477578312158585\n",
      "epoch: 7 step: 682, loss is 0.13702283799648285\n",
      "epoch: 7 step: 683, loss is 0.1276341676712036\n",
      "epoch: 7 step: 684, loss is 0.1453532874584198\n",
      "epoch: 7 step: 685, loss is 0.1216602474451065\n",
      "epoch: 7 step: 686, loss is 0.09045883268117905\n",
      "epoch: 7 step: 687, loss is 0.09134811908006668\n",
      "epoch: 7 step: 688, loss is 0.057458847761154175\n",
      "epoch: 7 step: 689, loss is 0.10977692157030106\n",
      "epoch: 7 step: 690, loss is 0.08808855712413788\n",
      "epoch: 7 step: 691, loss is 0.21303479373455048\n",
      "epoch: 7 step: 692, loss is 0.2732764482498169\n",
      "epoch: 7 step: 693, loss is 0.11310010403394699\n",
      "epoch: 7 step: 694, loss is 0.0724586546421051\n",
      "epoch: 7 step: 695, loss is 0.08051955699920654\n",
      "epoch: 7 step: 696, loss is 0.12699437141418457\n",
      "epoch: 7 step: 697, loss is 0.19190378487110138\n",
      "epoch: 7 step: 698, loss is 0.03847651556134224\n",
      "epoch: 7 step: 699, loss is 0.07041778415441513\n",
      "epoch: 7 step: 700, loss is 0.1963430643081665\n",
      "epoch: 7 step: 701, loss is 0.23691906034946442\n",
      "epoch: 7 step: 702, loss is 0.037012744694948196\n",
      "epoch: 7 step: 703, loss is 0.043698180466890335\n",
      "epoch: 7 step: 704, loss is 0.2332415133714676\n",
      "epoch: 7 step: 705, loss is 0.06738882511854172\n",
      "epoch: 7 step: 706, loss is 0.17385244369506836\n",
      "epoch: 7 step: 707, loss is 0.13033124804496765\n",
      "epoch: 7 step: 708, loss is 0.12455697357654572\n",
      "epoch: 7 step: 709, loss is 0.03072943165898323\n",
      "epoch: 7 step: 710, loss is 0.3055395185947418\n",
      "epoch: 7 step: 711, loss is 0.08886424452066422\n",
      "epoch: 7 step: 712, loss is 0.14654141664505005\n",
      "epoch: 7 step: 713, loss is 0.1694607138633728\n",
      "epoch: 7 step: 714, loss is 0.20282888412475586\n",
      "epoch: 7 step: 715, loss is 0.057392362505197525\n",
      "epoch: 7 step: 716, loss is 0.1753574013710022\n",
      "epoch: 7 step: 717, loss is 0.05382856726646423\n",
      "epoch: 7 step: 718, loss is 0.06403937935829163\n",
      "epoch: 7 step: 719, loss is 0.21856780350208282\n",
      "epoch: 7 step: 720, loss is 0.20105309784412384\n",
      "epoch: 7 step: 721, loss is 0.16437825560569763\n",
      "epoch: 7 step: 722, loss is 0.09108886122703552\n",
      "epoch: 7 step: 723, loss is 0.11310974508523941\n",
      "epoch: 7 step: 724, loss is 0.27044984698295593\n",
      "epoch: 7 step: 725, loss is 0.11788380146026611\n",
      "epoch: 7 step: 726, loss is 0.2036871612071991\n",
      "epoch: 7 step: 727, loss is 0.21037931740283966\n",
      "epoch: 7 step: 728, loss is 0.14292290806770325\n",
      "epoch: 7 step: 729, loss is 0.06729660928249359\n",
      "epoch: 7 step: 730, loss is 0.07258662581443787\n",
      "epoch: 7 step: 731, loss is 0.13868042826652527\n",
      "epoch: 7 step: 732, loss is 0.12574051320552826\n",
      "epoch: 7 step: 733, loss is 0.057223573327064514\n",
      "epoch: 7 step: 734, loss is 0.14784999191761017\n",
      "epoch: 7 step: 735, loss is 0.1172107383608818\n",
      "epoch: 7 step: 736, loss is 0.09354734420776367\n",
      "epoch: 7 step: 737, loss is 0.1430554986000061\n",
      "epoch: 7 step: 738, loss is 0.12291663885116577\n",
      "epoch: 7 step: 739, loss is 0.12431919574737549\n",
      "epoch: 7 step: 740, loss is 0.041693318635225296\n",
      "epoch: 7 step: 741, loss is 0.2193998098373413\n",
      "epoch: 7 step: 742, loss is 0.16566070914268494\n",
      "epoch: 7 step: 743, loss is 0.2128739058971405\n",
      "epoch: 7 step: 744, loss is 0.2613900601863861\n",
      "epoch: 7 step: 745, loss is 0.1535668820142746\n",
      "epoch: 7 step: 746, loss is 0.1805390566587448\n",
      "epoch: 7 step: 747, loss is 0.12298302352428436\n",
      "epoch: 7 step: 748, loss is 0.11942067742347717\n",
      "epoch: 7 step: 749, loss is 0.1132705956697464\n",
      "epoch: 7 step: 750, loss is 0.10296419262886047\n",
      "epoch: 7 step: 751, loss is 0.08529455214738846\n",
      "epoch: 7 step: 752, loss is 0.1229570135474205\n",
      "epoch: 7 step: 753, loss is 0.05770460143685341\n",
      "epoch: 7 step: 754, loss is 0.086541548371315\n",
      "epoch: 7 step: 755, loss is 0.11151859164237976\n",
      "epoch: 7 step: 756, loss is 0.09942781925201416\n",
      "epoch: 7 step: 757, loss is 0.1948782354593277\n",
      "epoch: 7 step: 758, loss is 0.08131591975688934\n",
      "epoch: 7 step: 759, loss is 0.15675479173660278\n",
      "epoch: 7 step: 760, loss is 0.18548402190208435\n",
      "epoch: 7 step: 761, loss is 0.12582489848136902\n",
      "epoch: 7 step: 762, loss is 0.1068740263581276\n",
      "epoch: 7 step: 763, loss is 0.2511054575443268\n",
      "epoch: 7 step: 764, loss is 0.13221290707588196\n",
      "epoch: 7 step: 765, loss is 0.1218755915760994\n",
      "epoch: 7 step: 766, loss is 0.06606419384479523\n",
      "epoch: 7 step: 767, loss is 0.05425730720162392\n",
      "epoch: 7 step: 768, loss is 0.16689570248126984\n",
      "epoch: 7 step: 769, loss is 0.09352310001850128\n",
      "epoch: 7 step: 770, loss is 0.10774780064821243\n",
      "epoch: 7 step: 771, loss is 0.07628124952316284\n",
      "epoch: 7 step: 772, loss is 0.027361147105693817\n",
      "epoch: 7 step: 773, loss is 0.08960817009210587\n",
      "epoch: 7 step: 774, loss is 0.02860807068645954\n",
      "epoch: 7 step: 775, loss is 0.06086866557598114\n",
      "epoch: 7 step: 776, loss is 0.10255846381187439\n",
      "epoch: 7 step: 777, loss is 0.06595081835985184\n",
      "epoch: 7 step: 778, loss is 0.11670666933059692\n",
      "epoch: 7 step: 779, loss is 0.18988601863384247\n",
      "epoch: 7 step: 780, loss is 0.14009004831314087\n",
      "epoch: 7 step: 781, loss is 0.18937981128692627\n",
      "epoch: 7 step: 782, loss is 0.1613192856311798\n",
      "epoch: 7 step: 783, loss is 0.12932713329792023\n",
      "epoch: 7 step: 784, loss is 0.1340922713279724\n",
      "epoch: 7 step: 785, loss is 0.1365005522966385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 786, loss is 0.15147574245929718\n",
      "epoch: 7 step: 787, loss is 0.24804767966270447\n",
      "epoch: 7 step: 788, loss is 0.08951517939567566\n",
      "epoch: 7 step: 789, loss is 0.1419353038072586\n",
      "epoch: 7 step: 790, loss is 0.1447301208972931\n",
      "epoch: 7 step: 791, loss is 0.19163180887699127\n",
      "epoch: 7 step: 792, loss is 0.2175818383693695\n",
      "epoch: 7 step: 793, loss is 0.24129073321819305\n",
      "epoch: 7 step: 794, loss is 0.0980466827750206\n",
      "epoch: 7 step: 795, loss is 0.04037193953990936\n",
      "epoch: 7 step: 796, loss is 0.1463468372821808\n",
      "epoch: 7 step: 797, loss is 0.03418944031000137\n",
      "epoch: 7 step: 798, loss is 0.29139649868011475\n",
      "epoch: 7 step: 799, loss is 0.22374263405799866\n",
      "epoch: 7 step: 800, loss is 0.12467500567436218\n",
      "epoch: 7 step: 801, loss is 0.1864953339099884\n",
      "epoch: 7 step: 802, loss is 0.10361653566360474\n",
      "epoch: 7 step: 803, loss is 0.2039748579263687\n",
      "epoch: 7 step: 804, loss is 0.08163897693157196\n",
      "epoch: 7 step: 805, loss is 0.18552516400814056\n",
      "epoch: 7 step: 806, loss is 0.07198675721883774\n",
      "epoch: 7 step: 807, loss is 0.183877632021904\n",
      "epoch: 7 step: 808, loss is 0.05697333440184593\n",
      "epoch: 7 step: 809, loss is 0.1770758479833603\n",
      "epoch: 7 step: 810, loss is 0.09028751403093338\n",
      "epoch: 7 step: 811, loss is 0.12105516344308853\n",
      "epoch: 7 step: 812, loss is 0.11511332541704178\n",
      "epoch: 7 step: 813, loss is 0.15772277116775513\n",
      "epoch: 7 step: 814, loss is 0.08625857532024384\n",
      "epoch: 7 step: 815, loss is 0.12196522206068039\n",
      "epoch: 7 step: 816, loss is 0.14852991700172424\n",
      "epoch: 7 step: 817, loss is 0.0984804630279541\n",
      "epoch: 7 step: 818, loss is 0.017896004021167755\n",
      "epoch: 7 step: 819, loss is 0.1544828861951828\n",
      "epoch: 7 step: 820, loss is 0.09516636282205582\n",
      "epoch: 7 step: 821, loss is 0.10678663849830627\n",
      "epoch: 7 step: 822, loss is 0.15304310619831085\n",
      "epoch: 7 step: 823, loss is 0.2671312093734741\n",
      "epoch: 7 step: 824, loss is 0.0877998024225235\n",
      "epoch: 7 step: 825, loss is 0.09151309728622437\n",
      "epoch: 7 step: 826, loss is 0.07382737100124359\n",
      "epoch: 7 step: 827, loss is 0.3247477412223816\n",
      "epoch: 7 step: 828, loss is 0.1558307558298111\n",
      "epoch: 7 step: 829, loss is 0.124991774559021\n",
      "epoch: 7 step: 830, loss is 0.02378660812973976\n",
      "epoch: 7 step: 831, loss is 0.18307045102119446\n",
      "epoch: 7 step: 832, loss is 0.16769324243068695\n",
      "epoch: 7 step: 833, loss is 0.15285655856132507\n",
      "epoch: 7 step: 834, loss is 0.15467600524425507\n",
      "epoch: 7 step: 835, loss is 0.11165168136358261\n",
      "epoch: 7 step: 836, loss is 0.1644233763217926\n",
      "epoch: 7 step: 837, loss is 0.12189973890781403\n",
      "epoch: 7 step: 838, loss is 0.20416872203350067\n",
      "epoch: 7 step: 839, loss is 0.1020742803812027\n",
      "epoch: 7 step: 840, loss is 0.1328313797712326\n",
      "epoch: 7 step: 841, loss is 0.103129081428051\n",
      "epoch: 7 step: 842, loss is 0.07745686173439026\n",
      "epoch: 7 step: 843, loss is 0.034716393798589706\n",
      "epoch: 7 step: 844, loss is 0.22860251367092133\n",
      "epoch: 7 step: 845, loss is 0.05842592567205429\n",
      "epoch: 7 step: 846, loss is 0.062216803431510925\n",
      "epoch: 7 step: 847, loss is 0.11155802011489868\n",
      "epoch: 7 step: 848, loss is 0.15742753446102142\n",
      "epoch: 7 step: 849, loss is 0.198086217045784\n",
      "epoch: 7 step: 850, loss is 0.13502846658229828\n",
      "epoch: 7 step: 851, loss is 0.17367544770240784\n",
      "epoch: 7 step: 852, loss is 0.1263161301612854\n",
      "epoch: 7 step: 853, loss is 0.09155498445034027\n",
      "epoch: 7 step: 854, loss is 0.08852137625217438\n",
      "epoch: 7 step: 855, loss is 0.03856657072901726\n",
      "epoch: 7 step: 856, loss is 0.12980616092681885\n",
      "epoch: 7 step: 857, loss is 0.1695312261581421\n",
      "epoch: 7 step: 858, loss is 0.21829062700271606\n",
      "epoch: 7 step: 859, loss is 0.13362054526805878\n",
      "epoch: 7 step: 860, loss is 0.05958814546465874\n",
      "epoch: 7 step: 861, loss is 0.06787952035665512\n",
      "epoch: 7 step: 862, loss is 0.027134668081998825\n",
      "epoch: 7 step: 863, loss is 0.07116616517305374\n",
      "epoch: 7 step: 864, loss is 0.1335437148809433\n",
      "epoch: 7 step: 865, loss is 0.135298952460289\n",
      "epoch: 7 step: 866, loss is 0.07355446368455887\n",
      "epoch: 7 step: 867, loss is 0.10922349989414215\n",
      "epoch: 7 step: 868, loss is 0.13231804966926575\n",
      "epoch: 7 step: 869, loss is 0.0661226287484169\n",
      "epoch: 7 step: 870, loss is 0.23121702671051025\n",
      "epoch: 7 step: 871, loss is 0.05185883864760399\n",
      "epoch: 7 step: 872, loss is 0.15581141412258148\n",
      "epoch: 7 step: 873, loss is 0.10982122272253036\n",
      "epoch: 7 step: 874, loss is 0.18834351003170013\n",
      "epoch: 7 step: 875, loss is 0.0837666243314743\n",
      "epoch: 7 step: 876, loss is 0.16698497533798218\n",
      "epoch: 7 step: 877, loss is 0.04958733171224594\n",
      "epoch: 7 step: 878, loss is 0.10380779951810837\n",
      "epoch: 7 step: 879, loss is 0.27876001596450806\n",
      "epoch: 7 step: 880, loss is 0.059421129524707794\n",
      "epoch: 7 step: 881, loss is 0.11264683306217194\n",
      "epoch: 7 step: 882, loss is 0.08892074227333069\n",
      "epoch: 7 step: 883, loss is 0.1461847871541977\n",
      "epoch: 7 step: 884, loss is 0.17011244595050812\n",
      "epoch: 7 step: 885, loss is 0.0828406810760498\n",
      "epoch: 7 step: 886, loss is 0.08933412283658981\n",
      "epoch: 7 step: 887, loss is 0.06492031365633011\n",
      "epoch: 7 step: 888, loss is 0.04056515172123909\n",
      "epoch: 7 step: 889, loss is 0.15219052135944366\n",
      "epoch: 7 step: 890, loss is 0.14769160747528076\n",
      "epoch: 7 step: 891, loss is 0.0920693650841713\n",
      "epoch: 7 step: 892, loss is 0.19230081140995026\n",
      "epoch: 7 step: 893, loss is 0.07288537174463272\n",
      "epoch: 7 step: 894, loss is 0.11681258678436279\n",
      "epoch: 7 step: 895, loss is 0.12456101924180984\n",
      "epoch: 7 step: 896, loss is 0.05416838824748993\n",
      "epoch: 7 step: 897, loss is 0.15089988708496094\n",
      "epoch: 7 step: 898, loss is 0.07020486146211624\n",
      "epoch: 7 step: 899, loss is 0.09443633258342743\n",
      "epoch: 7 step: 900, loss is 0.06291285902261734\n",
      "epoch: 7 step: 901, loss is 0.15318991243839264\n",
      "epoch: 7 step: 902, loss is 0.113681860268116\n",
      "epoch: 7 step: 903, loss is 0.14341501891613007\n",
      "epoch: 7 step: 904, loss is 0.09845611453056335\n",
      "epoch: 7 step: 905, loss is 0.040876321494579315\n",
      "epoch: 7 step: 906, loss is 0.07785377651453018\n",
      "epoch: 7 step: 907, loss is 0.1874312162399292\n",
      "epoch: 7 step: 908, loss is 0.10822939872741699\n",
      "epoch: 7 step: 909, loss is 0.15731538832187653\n",
      "epoch: 7 step: 910, loss is 0.07368365675210953\n",
      "epoch: 7 step: 911, loss is 0.3150630593299866\n",
      "epoch: 7 step: 912, loss is 0.06126907840371132\n",
      "epoch: 7 step: 913, loss is 0.14407002925872803\n",
      "epoch: 7 step: 914, loss is 0.12439597398042679\n",
      "epoch: 7 step: 915, loss is 0.15605342388153076\n",
      "epoch: 7 step: 916, loss is 0.09238405525684357\n",
      "epoch: 7 step: 917, loss is 0.21746216714382172\n",
      "epoch: 7 step: 918, loss is 0.10623881220817566\n",
      "epoch: 7 step: 919, loss is 0.11634669452905655\n",
      "epoch: 7 step: 920, loss is 0.21142299473285675\n",
      "epoch: 7 step: 921, loss is 0.17119453847408295\n",
      "epoch: 7 step: 922, loss is 0.09472744166851044\n",
      "epoch: 7 step: 923, loss is 0.09513507783412933\n",
      "epoch: 7 step: 924, loss is 0.14043734967708588\n",
      "epoch: 7 step: 925, loss is 0.11093828082084656\n",
      "epoch: 7 step: 926, loss is 0.24956195056438446\n",
      "epoch: 7 step: 927, loss is 0.1092161238193512\n",
      "epoch: 7 step: 928, loss is 0.05616932734847069\n",
      "epoch: 7 step: 929, loss is 0.06022164598107338\n",
      "epoch: 7 step: 930, loss is 0.1208554208278656\n",
      "epoch: 7 step: 931, loss is 0.3150671124458313\n",
      "epoch: 7 step: 932, loss is 0.18219435214996338\n",
      "epoch: 7 step: 933, loss is 0.19583794474601746\n",
      "epoch: 7 step: 934, loss is 0.13113823533058167\n",
      "epoch: 7 step: 935, loss is 0.08773039281368256\n",
      "epoch: 7 step: 936, loss is 0.19079680740833282\n",
      "epoch: 7 step: 937, loss is 0.1892513930797577\n",
      "epoch: 8 step: 1, loss is 0.10828583687543869\n",
      "epoch: 8 step: 2, loss is 0.07089114189147949\n",
      "epoch: 8 step: 3, loss is 0.13387374579906464\n",
      "epoch: 8 step: 4, loss is 0.04707411304116249\n",
      "epoch: 8 step: 5, loss is 0.12371629476547241\n",
      "epoch: 8 step: 6, loss is 0.06764263659715652\n",
      "epoch: 8 step: 7, loss is 0.14530125260353088\n",
      "epoch: 8 step: 8, loss is 0.05781682953238487\n",
      "epoch: 8 step: 9, loss is 0.08149103075265884\n",
      "epoch: 8 step: 10, loss is 0.0904705673456192\n",
      "epoch: 8 step: 11, loss is 0.10116647928953171\n",
      "epoch: 8 step: 12, loss is 0.0580509789288044\n",
      "epoch: 8 step: 13, loss is 0.18977923691272736\n",
      "epoch: 8 step: 14, loss is 0.09744004905223846\n",
      "epoch: 8 step: 15, loss is 0.09649215638637543\n",
      "epoch: 8 step: 16, loss is 0.06412231922149658\n",
      "epoch: 8 step: 17, loss is 0.04983390122652054\n",
      "epoch: 8 step: 18, loss is 0.10448028147220612\n",
      "epoch: 8 step: 19, loss is 0.10235461592674255\n",
      "epoch: 8 step: 20, loss is 0.05173289403319359\n",
      "epoch: 8 step: 21, loss is 0.043574828654527664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 22, loss is 0.06918828189373016\n",
      "epoch: 8 step: 23, loss is 0.06520018726587296\n",
      "epoch: 8 step: 24, loss is 0.05882769450545311\n",
      "epoch: 8 step: 25, loss is 0.0754665806889534\n",
      "epoch: 8 step: 26, loss is 0.1635696142911911\n",
      "epoch: 8 step: 27, loss is 0.1167927235364914\n",
      "epoch: 8 step: 28, loss is 0.03531867265701294\n",
      "epoch: 8 step: 29, loss is 0.10211361199617386\n",
      "epoch: 8 step: 30, loss is 0.06799688190221786\n",
      "epoch: 8 step: 31, loss is 0.13446375727653503\n",
      "epoch: 8 step: 32, loss is 0.14881229400634766\n",
      "epoch: 8 step: 33, loss is 0.18057020008563995\n",
      "epoch: 8 step: 34, loss is 0.05141884833574295\n",
      "epoch: 8 step: 35, loss is 0.10734857618808746\n",
      "epoch: 8 step: 36, loss is 0.046522971242666245\n",
      "epoch: 8 step: 37, loss is 0.07384151220321655\n",
      "epoch: 8 step: 38, loss is 0.06233711168169975\n",
      "epoch: 8 step: 39, loss is 0.04616193473339081\n",
      "epoch: 8 step: 40, loss is 0.0590704046189785\n",
      "epoch: 8 step: 41, loss is 0.11321515589952469\n",
      "epoch: 8 step: 42, loss is 0.08056847751140594\n",
      "epoch: 8 step: 43, loss is 0.06128540635108948\n",
      "epoch: 8 step: 44, loss is 0.10925579816102982\n",
      "epoch: 8 step: 45, loss is 0.06831306964159012\n",
      "epoch: 8 step: 46, loss is 0.09344201534986496\n",
      "epoch: 8 step: 47, loss is 0.026918718591332436\n",
      "epoch: 8 step: 48, loss is 0.05312763899564743\n",
      "epoch: 8 step: 49, loss is 0.05016293749213219\n",
      "epoch: 8 step: 50, loss is 0.11406572163105011\n",
      "epoch: 8 step: 51, loss is 0.12257485091686249\n",
      "epoch: 8 step: 52, loss is 0.05896418169140816\n",
      "epoch: 8 step: 53, loss is 0.027801984921097755\n",
      "epoch: 8 step: 54, loss is 0.07627900689840317\n",
      "epoch: 8 step: 55, loss is 0.13171140849590302\n",
      "epoch: 8 step: 56, loss is 0.03977273032069206\n",
      "epoch: 8 step: 57, loss is 0.06678186357021332\n",
      "epoch: 8 step: 58, loss is 0.07228092849254608\n",
      "epoch: 8 step: 59, loss is 0.052182648330926895\n",
      "epoch: 8 step: 60, loss is 0.0738348662853241\n",
      "epoch: 8 step: 61, loss is 0.023934831842780113\n",
      "epoch: 8 step: 62, loss is 0.06203385815024376\n",
      "epoch: 8 step: 63, loss is 0.06703418493270874\n",
      "epoch: 8 step: 64, loss is 0.07388009130954742\n",
      "epoch: 8 step: 65, loss is 0.18571476638317108\n",
      "epoch: 8 step: 66, loss is 0.0829763263463974\n",
      "epoch: 8 step: 67, loss is 0.11277662962675095\n",
      "epoch: 8 step: 68, loss is 0.18542088568210602\n",
      "epoch: 8 step: 69, loss is 0.07584057748317719\n",
      "epoch: 8 step: 70, loss is 0.04732486605644226\n",
      "epoch: 8 step: 71, loss is 0.04267595335841179\n",
      "epoch: 8 step: 72, loss is 0.07664893567562103\n",
      "epoch: 8 step: 73, loss is 0.11140631884336472\n",
      "epoch: 8 step: 74, loss is 0.12308961898088455\n",
      "epoch: 8 step: 75, loss is 0.20769847929477692\n",
      "epoch: 8 step: 76, loss is 0.13726815581321716\n",
      "epoch: 8 step: 77, loss is 0.21084515750408173\n",
      "epoch: 8 step: 78, loss is 0.07047660648822784\n",
      "epoch: 8 step: 79, loss is 0.21526336669921875\n",
      "epoch: 8 step: 80, loss is 0.055611029267311096\n",
      "epoch: 8 step: 81, loss is 0.09204214066267014\n",
      "epoch: 8 step: 82, loss is 0.06935590505599976\n",
      "epoch: 8 step: 83, loss is 0.1514725238084793\n",
      "epoch: 8 step: 84, loss is 0.13189849257469177\n",
      "epoch: 8 step: 85, loss is 0.12573102116584778\n",
      "epoch: 8 step: 86, loss is 0.09773153811693192\n",
      "epoch: 8 step: 87, loss is 0.06531590968370438\n",
      "epoch: 8 step: 88, loss is 0.08254963159561157\n",
      "epoch: 8 step: 89, loss is 0.047993529587984085\n",
      "epoch: 8 step: 90, loss is 0.147712841629982\n",
      "epoch: 8 step: 91, loss is 0.06334054470062256\n",
      "epoch: 8 step: 92, loss is 0.053081270307302475\n",
      "epoch: 8 step: 93, loss is 0.1042851135134697\n",
      "epoch: 8 step: 94, loss is 0.045460544526576996\n",
      "epoch: 8 step: 95, loss is 0.0759500116109848\n",
      "epoch: 8 step: 96, loss is 0.110092893242836\n",
      "epoch: 8 step: 97, loss is 0.04253583401441574\n",
      "epoch: 8 step: 98, loss is 0.1673758625984192\n",
      "epoch: 8 step: 99, loss is 0.050662752240896225\n",
      "epoch: 8 step: 100, loss is 0.022947091609239578\n",
      "epoch: 8 step: 101, loss is 0.0852886214852333\n",
      "epoch: 8 step: 102, loss is 0.12127478420734406\n",
      "epoch: 8 step: 103, loss is 0.15579605102539062\n",
      "epoch: 8 step: 104, loss is 0.11059891432523727\n",
      "epoch: 8 step: 105, loss is 0.04213913157582283\n",
      "epoch: 8 step: 106, loss is 0.038770891726017\n",
      "epoch: 8 step: 107, loss is 0.11093949526548386\n",
      "epoch: 8 step: 108, loss is 0.0689287781715393\n",
      "epoch: 8 step: 109, loss is 0.07467219978570938\n",
      "epoch: 8 step: 110, loss is 0.16240790486335754\n",
      "epoch: 8 step: 111, loss is 0.039888035506010056\n",
      "epoch: 8 step: 112, loss is 0.16627813875675201\n",
      "epoch: 8 step: 113, loss is 0.1818225234746933\n",
      "epoch: 8 step: 114, loss is 0.07641231268644333\n",
      "epoch: 8 step: 115, loss is 0.14433205127716064\n",
      "epoch: 8 step: 116, loss is 0.03265483304858208\n",
      "epoch: 8 step: 117, loss is 0.04901033639907837\n",
      "epoch: 8 step: 118, loss is 0.15142764151096344\n",
      "epoch: 8 step: 119, loss is 0.11717412620782852\n",
      "epoch: 8 step: 120, loss is 0.05390813946723938\n",
      "epoch: 8 step: 121, loss is 0.09964534640312195\n",
      "epoch: 8 step: 122, loss is 0.055818524211645126\n",
      "epoch: 8 step: 123, loss is 0.08061140030622482\n",
      "epoch: 8 step: 124, loss is 0.0607043094933033\n",
      "epoch: 8 step: 125, loss is 0.08305803686380386\n",
      "epoch: 8 step: 126, loss is 0.07713644206523895\n",
      "epoch: 8 step: 127, loss is 0.046360377222299576\n",
      "epoch: 8 step: 128, loss is 0.05444350838661194\n",
      "epoch: 8 step: 129, loss is 0.05307823419570923\n",
      "epoch: 8 step: 130, loss is 0.12717290222644806\n",
      "epoch: 8 step: 131, loss is 0.18827198445796967\n",
      "epoch: 8 step: 132, loss is 0.055440861731767654\n",
      "epoch: 8 step: 133, loss is 0.07539863884449005\n",
      "epoch: 8 step: 134, loss is 0.062122050672769547\n",
      "epoch: 8 step: 135, loss is 0.03920689597725868\n",
      "epoch: 8 step: 136, loss is 0.10195811837911606\n",
      "epoch: 8 step: 137, loss is 0.3043648898601532\n",
      "epoch: 8 step: 138, loss is 0.036370813846588135\n",
      "epoch: 8 step: 139, loss is 0.08682138472795486\n",
      "epoch: 8 step: 140, loss is 0.12670183181762695\n",
      "epoch: 8 step: 141, loss is 0.10670851171016693\n",
      "epoch: 8 step: 142, loss is 0.026140909641981125\n",
      "epoch: 8 step: 143, loss is 0.13022013008594513\n",
      "epoch: 8 step: 144, loss is 0.059793099761009216\n",
      "epoch: 8 step: 145, loss is 0.039255160838365555\n",
      "epoch: 8 step: 146, loss is 0.1813899725675583\n",
      "epoch: 8 step: 147, loss is 0.14932632446289062\n",
      "epoch: 8 step: 148, loss is 0.07441845536231995\n",
      "epoch: 8 step: 149, loss is 0.052776698023080826\n",
      "epoch: 8 step: 150, loss is 0.07263605296611786\n",
      "epoch: 8 step: 151, loss is 0.0247770044952631\n",
      "epoch: 8 step: 152, loss is 0.2075497955083847\n",
      "epoch: 8 step: 153, loss is 0.04731379821896553\n",
      "epoch: 8 step: 154, loss is 0.07458414137363434\n",
      "epoch: 8 step: 155, loss is 0.09435971081256866\n",
      "epoch: 8 step: 156, loss is 0.10451030731201172\n",
      "epoch: 8 step: 157, loss is 0.06619297713041306\n",
      "epoch: 8 step: 158, loss is 0.09550859779119492\n",
      "epoch: 8 step: 159, loss is 0.06981293857097626\n",
      "epoch: 8 step: 160, loss is 0.04918203130364418\n",
      "epoch: 8 step: 161, loss is 0.03182211518287659\n",
      "epoch: 8 step: 162, loss is 0.023935794830322266\n",
      "epoch: 8 step: 163, loss is 0.037730760872364044\n",
      "epoch: 8 step: 164, loss is 0.10023073107004166\n",
      "epoch: 8 step: 165, loss is 0.10062537342309952\n",
      "epoch: 8 step: 166, loss is 0.15170663595199585\n",
      "epoch: 8 step: 167, loss is 0.11103016883134842\n",
      "epoch: 8 step: 168, loss is 0.2640102803707123\n",
      "epoch: 8 step: 169, loss is 0.07195894420146942\n",
      "epoch: 8 step: 170, loss is 0.04503249749541283\n",
      "epoch: 8 step: 171, loss is 0.1277647763490677\n",
      "epoch: 8 step: 172, loss is 0.07891745865345001\n",
      "epoch: 8 step: 173, loss is 0.13421130180358887\n",
      "epoch: 8 step: 174, loss is 0.1602422595024109\n",
      "epoch: 8 step: 175, loss is 0.0749817043542862\n",
      "epoch: 8 step: 176, loss is 0.08176694810390472\n",
      "epoch: 8 step: 177, loss is 0.05536945164203644\n",
      "epoch: 8 step: 178, loss is 0.06531943380832672\n",
      "epoch: 8 step: 179, loss is 0.05508199706673622\n",
      "epoch: 8 step: 180, loss is 0.11675594002008438\n",
      "epoch: 8 step: 181, loss is 0.050316620618104935\n",
      "epoch: 8 step: 182, loss is 0.03477025032043457\n",
      "epoch: 8 step: 183, loss is 0.03012946993112564\n",
      "epoch: 8 step: 184, loss is 0.08591798692941666\n",
      "epoch: 8 step: 185, loss is 0.09890526533126831\n",
      "epoch: 8 step: 186, loss is 0.04423914849758148\n",
      "epoch: 8 step: 187, loss is 0.03935541585087776\n",
      "epoch: 8 step: 188, loss is 0.22280851006507874\n",
      "epoch: 8 step: 189, loss is 0.05588017776608467\n",
      "epoch: 8 step: 190, loss is 0.08273530006408691\n",
      "epoch: 8 step: 191, loss is 0.06340506672859192\n",
      "epoch: 8 step: 192, loss is 0.07534980028867722\n",
      "epoch: 8 step: 193, loss is 0.10664283484220505\n",
      "epoch: 8 step: 194, loss is 0.2516639232635498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 195, loss is 0.045589182525873184\n",
      "epoch: 8 step: 196, loss is 0.17432859539985657\n",
      "epoch: 8 step: 197, loss is 0.22488920390605927\n",
      "epoch: 8 step: 198, loss is 0.13408653438091278\n",
      "epoch: 8 step: 199, loss is 0.07765311747789383\n",
      "epoch: 8 step: 200, loss is 0.08270476758480072\n",
      "epoch: 8 step: 201, loss is 0.04496587812900543\n",
      "epoch: 8 step: 202, loss is 0.10706090927124023\n",
      "epoch: 8 step: 203, loss is 0.02758166752755642\n",
      "epoch: 8 step: 204, loss is 0.05777931585907936\n",
      "epoch: 8 step: 205, loss is 0.17096875607967377\n",
      "epoch: 8 step: 206, loss is 0.06917543709278107\n",
      "epoch: 8 step: 207, loss is 0.09296849370002747\n",
      "epoch: 8 step: 208, loss is 0.0366920530796051\n",
      "epoch: 8 step: 209, loss is 0.07472973316907883\n",
      "epoch: 8 step: 210, loss is 0.07320728898048401\n",
      "epoch: 8 step: 211, loss is 0.21895626187324524\n",
      "epoch: 8 step: 212, loss is 0.1967281997203827\n",
      "epoch: 8 step: 213, loss is 0.12075245380401611\n",
      "epoch: 8 step: 214, loss is 0.13641253113746643\n",
      "epoch: 8 step: 215, loss is 0.07651281356811523\n",
      "epoch: 8 step: 216, loss is 0.045422960072755814\n",
      "epoch: 8 step: 217, loss is 0.07519251853227615\n",
      "epoch: 8 step: 218, loss is 0.10031997412443161\n",
      "epoch: 8 step: 219, loss is 0.09257816523313522\n",
      "epoch: 8 step: 220, loss is 0.11942549049854279\n",
      "epoch: 8 step: 221, loss is 0.05127180367708206\n",
      "epoch: 8 step: 222, loss is 0.06114770472049713\n",
      "epoch: 8 step: 223, loss is 0.03592957556247711\n",
      "epoch: 8 step: 224, loss is 0.0964370146393776\n",
      "epoch: 8 step: 225, loss is 0.05452346056699753\n",
      "epoch: 8 step: 226, loss is 0.024323537945747375\n",
      "epoch: 8 step: 227, loss is 0.06855130195617676\n",
      "epoch: 8 step: 228, loss is 0.023851405829191208\n",
      "epoch: 8 step: 229, loss is 0.16414819657802582\n",
      "epoch: 8 step: 230, loss is 0.13949935138225555\n",
      "epoch: 8 step: 231, loss is 0.06176804006099701\n",
      "epoch: 8 step: 232, loss is 0.02987191639840603\n",
      "epoch: 8 step: 233, loss is 0.061016496270895004\n",
      "epoch: 8 step: 234, loss is 0.10933088511228561\n",
      "epoch: 8 step: 235, loss is 0.09655763953924179\n",
      "epoch: 8 step: 236, loss is 0.09356049448251724\n",
      "epoch: 8 step: 237, loss is 0.0701960101723671\n",
      "epoch: 8 step: 238, loss is 0.06608607620000839\n",
      "epoch: 8 step: 239, loss is 0.08770912140607834\n",
      "epoch: 8 step: 240, loss is 0.09515523910522461\n",
      "epoch: 8 step: 241, loss is 0.09499810636043549\n",
      "epoch: 8 step: 242, loss is 0.1195550262928009\n",
      "epoch: 8 step: 243, loss is 0.06448376923799515\n",
      "epoch: 8 step: 244, loss is 0.14049531519412994\n",
      "epoch: 8 step: 245, loss is 0.19078116118907928\n",
      "epoch: 8 step: 246, loss is 0.04792281240224838\n",
      "epoch: 8 step: 247, loss is 0.1005023643374443\n",
      "epoch: 8 step: 248, loss is 0.11412203311920166\n",
      "epoch: 8 step: 249, loss is 0.09798035770654678\n",
      "epoch: 8 step: 250, loss is 0.08836598694324493\n",
      "epoch: 8 step: 251, loss is 0.157684326171875\n",
      "epoch: 8 step: 252, loss is 0.06330599635839462\n",
      "epoch: 8 step: 253, loss is 0.07749125361442566\n",
      "epoch: 8 step: 254, loss is 0.15162749588489532\n",
      "epoch: 8 step: 255, loss is 0.08823161572217941\n",
      "epoch: 8 step: 256, loss is 0.10459445416927338\n",
      "epoch: 8 step: 257, loss is 0.0721951574087143\n",
      "epoch: 8 step: 258, loss is 0.04346515238285065\n",
      "epoch: 8 step: 259, loss is 0.14631950855255127\n",
      "epoch: 8 step: 260, loss is 0.05276520550251007\n",
      "epoch: 8 step: 261, loss is 0.04790138080716133\n",
      "epoch: 8 step: 262, loss is 0.06314367055892944\n",
      "epoch: 8 step: 263, loss is 0.1652325540781021\n",
      "epoch: 8 step: 264, loss is 0.180429145693779\n",
      "epoch: 8 step: 265, loss is 0.035374708473682404\n",
      "epoch: 8 step: 266, loss is 0.12322206050157547\n",
      "epoch: 8 step: 267, loss is 0.08770695328712463\n",
      "epoch: 8 step: 268, loss is 0.04500651732087135\n",
      "epoch: 8 step: 269, loss is 0.12660369277000427\n",
      "epoch: 8 step: 270, loss is 0.07414592057466507\n",
      "epoch: 8 step: 271, loss is 0.14984239637851715\n",
      "epoch: 8 step: 272, loss is 0.0396379679441452\n",
      "epoch: 8 step: 273, loss is 0.1664617657661438\n",
      "epoch: 8 step: 274, loss is 0.13066554069519043\n",
      "epoch: 8 step: 275, loss is 0.16124524176120758\n",
      "epoch: 8 step: 276, loss is 0.09416965395212173\n",
      "epoch: 8 step: 277, loss is 0.13351313769817352\n",
      "epoch: 8 step: 278, loss is 0.09553740173578262\n",
      "epoch: 8 step: 279, loss is 0.1323927640914917\n",
      "epoch: 8 step: 280, loss is 0.04384474456310272\n",
      "epoch: 8 step: 281, loss is 0.1077023521065712\n",
      "epoch: 8 step: 282, loss is 0.11587771773338318\n",
      "epoch: 8 step: 283, loss is 0.047229740768671036\n",
      "epoch: 8 step: 284, loss is 0.06540526449680328\n",
      "epoch: 8 step: 285, loss is 0.09452152252197266\n",
      "epoch: 8 step: 286, loss is 0.1345587819814682\n",
      "epoch: 8 step: 287, loss is 0.08112120628356934\n",
      "epoch: 8 step: 288, loss is 0.10950929671525955\n",
      "epoch: 8 step: 289, loss is 0.06756997108459473\n",
      "epoch: 8 step: 290, loss is 0.0821518748998642\n",
      "epoch: 8 step: 291, loss is 0.09358453005552292\n",
      "epoch: 8 step: 292, loss is 0.10510345548391342\n",
      "epoch: 8 step: 293, loss is 0.14691294729709625\n",
      "epoch: 8 step: 294, loss is 0.0626937672495842\n",
      "epoch: 8 step: 295, loss is 0.17278094589710236\n",
      "epoch: 8 step: 296, loss is 0.02936246618628502\n",
      "epoch: 8 step: 297, loss is 0.22615912556648254\n",
      "epoch: 8 step: 298, loss is 0.07467318326234818\n",
      "epoch: 8 step: 299, loss is 0.13346625864505768\n",
      "epoch: 8 step: 300, loss is 0.11322879046201706\n",
      "epoch: 8 step: 301, loss is 0.07965894043445587\n",
      "epoch: 8 step: 302, loss is 0.09588198363780975\n",
      "epoch: 8 step: 303, loss is 0.055487100034952164\n",
      "epoch: 8 step: 304, loss is 0.10694130510091782\n",
      "epoch: 8 step: 305, loss is 0.09516257792711258\n",
      "epoch: 8 step: 306, loss is 0.1057424321770668\n",
      "epoch: 8 step: 307, loss is 0.029261358082294464\n",
      "epoch: 8 step: 308, loss is 0.08009535819292068\n",
      "epoch: 8 step: 309, loss is 0.060101959854364395\n",
      "epoch: 8 step: 310, loss is 0.2167167365550995\n",
      "epoch: 8 step: 311, loss is 0.1455150693655014\n",
      "epoch: 8 step: 312, loss is 0.10547924786806107\n",
      "epoch: 8 step: 313, loss is 0.06106064096093178\n",
      "epoch: 8 step: 314, loss is 0.07967358827590942\n",
      "epoch: 8 step: 315, loss is 0.06104906275868416\n",
      "epoch: 8 step: 316, loss is 0.08648756891489029\n",
      "epoch: 8 step: 317, loss is 0.13944478332996368\n",
      "epoch: 8 step: 318, loss is 0.14171373844146729\n",
      "epoch: 8 step: 319, loss is 0.09631486237049103\n",
      "epoch: 8 step: 320, loss is 0.06317507475614548\n",
      "epoch: 8 step: 321, loss is 0.15015020966529846\n",
      "epoch: 8 step: 322, loss is 0.051228757947683334\n",
      "epoch: 8 step: 323, loss is 0.09692232310771942\n",
      "epoch: 8 step: 324, loss is 0.0943111702799797\n",
      "epoch: 8 step: 325, loss is 0.10313186794519424\n",
      "epoch: 8 step: 326, loss is 0.08638807386159897\n",
      "epoch: 8 step: 327, loss is 0.0867505744099617\n",
      "epoch: 8 step: 328, loss is 0.05842413008213043\n",
      "epoch: 8 step: 329, loss is 0.18200886249542236\n",
      "epoch: 8 step: 330, loss is 0.058325622230768204\n",
      "epoch: 8 step: 331, loss is 0.05731004476547241\n",
      "epoch: 8 step: 332, loss is 0.13950979709625244\n",
      "epoch: 8 step: 333, loss is 0.12529942393302917\n",
      "epoch: 8 step: 334, loss is 0.09781498461961746\n",
      "epoch: 8 step: 335, loss is 0.08582748472690582\n",
      "epoch: 8 step: 336, loss is 0.07850567251443863\n",
      "epoch: 8 step: 337, loss is 0.0724332183599472\n",
      "epoch: 8 step: 338, loss is 0.13621222972869873\n",
      "epoch: 8 step: 339, loss is 0.0956784337759018\n",
      "epoch: 8 step: 340, loss is 0.18838529288768768\n",
      "epoch: 8 step: 341, loss is 0.12635962665081024\n",
      "epoch: 8 step: 342, loss is 0.10140947997570038\n",
      "epoch: 8 step: 343, loss is 0.26875409483909607\n",
      "epoch: 8 step: 344, loss is 0.06575226038694382\n",
      "epoch: 8 step: 345, loss is 0.05811876058578491\n",
      "epoch: 8 step: 346, loss is 0.0677865594625473\n",
      "epoch: 8 step: 347, loss is 0.1210651770234108\n",
      "epoch: 8 step: 348, loss is 0.15078479051589966\n",
      "epoch: 8 step: 349, loss is 0.14013051986694336\n",
      "epoch: 8 step: 350, loss is 0.058180809020996094\n",
      "epoch: 8 step: 351, loss is 0.07027245312929153\n",
      "epoch: 8 step: 352, loss is 0.09911251813173294\n",
      "epoch: 8 step: 353, loss is 0.059698257595300674\n",
      "epoch: 8 step: 354, loss is 0.04746933653950691\n",
      "epoch: 8 step: 355, loss is 0.07502564042806625\n",
      "epoch: 8 step: 356, loss is 0.12884728610515594\n",
      "epoch: 8 step: 357, loss is 0.19215747714042664\n",
      "epoch: 8 step: 358, loss is 0.14447252452373505\n",
      "epoch: 8 step: 359, loss is 0.05337528884410858\n",
      "epoch: 8 step: 360, loss is 0.06963568925857544\n",
      "epoch: 8 step: 361, loss is 0.0651576966047287\n",
      "epoch: 8 step: 362, loss is 0.03319672495126724\n",
      "epoch: 8 step: 363, loss is 0.0915365219116211\n",
      "epoch: 8 step: 364, loss is 0.029609370976686478\n",
      "epoch: 8 step: 365, loss is 0.02668595500290394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 366, loss is 0.06875403225421906\n",
      "epoch: 8 step: 367, loss is 0.07590926438570023\n",
      "epoch: 8 step: 368, loss is 0.08430223166942596\n",
      "epoch: 8 step: 369, loss is 0.12290115654468536\n",
      "epoch: 8 step: 370, loss is 0.08792964369058609\n",
      "epoch: 8 step: 371, loss is 0.10062506049871445\n",
      "epoch: 8 step: 372, loss is 0.17642584443092346\n",
      "epoch: 8 step: 373, loss is 0.05860108509659767\n",
      "epoch: 8 step: 374, loss is 0.04826614633202553\n",
      "epoch: 8 step: 375, loss is 0.08421412855386734\n",
      "epoch: 8 step: 376, loss is 0.25628313422203064\n",
      "epoch: 8 step: 377, loss is 0.12684266269207\n",
      "epoch: 8 step: 378, loss is 0.05818353220820427\n",
      "epoch: 8 step: 379, loss is 0.21806414425373077\n",
      "epoch: 8 step: 380, loss is 0.026176221668720245\n",
      "epoch: 8 step: 381, loss is 0.15681830048561096\n",
      "epoch: 8 step: 382, loss is 0.06090911477804184\n",
      "epoch: 8 step: 383, loss is 0.04521650820970535\n",
      "epoch: 8 step: 384, loss is 0.1485147327184677\n",
      "epoch: 8 step: 385, loss is 0.008831876330077648\n",
      "epoch: 8 step: 386, loss is 0.1834820657968521\n",
      "epoch: 8 step: 387, loss is 0.050849489867687225\n",
      "epoch: 8 step: 388, loss is 0.14968255162239075\n",
      "epoch: 8 step: 389, loss is 0.06982321292161942\n",
      "epoch: 8 step: 390, loss is 0.06952735036611557\n",
      "epoch: 8 step: 391, loss is 0.1116485595703125\n",
      "epoch: 8 step: 392, loss is 0.05383714288473129\n",
      "epoch: 8 step: 393, loss is 0.15434814989566803\n",
      "epoch: 8 step: 394, loss is 0.07164233922958374\n",
      "epoch: 8 step: 395, loss is 0.036410119384527206\n",
      "epoch: 8 step: 396, loss is 0.07384321093559265\n",
      "epoch: 8 step: 397, loss is 0.13749085366725922\n",
      "epoch: 8 step: 398, loss is 0.05560406297445297\n",
      "epoch: 8 step: 399, loss is 0.016540179029107094\n",
      "epoch: 8 step: 400, loss is 0.03672635555267334\n",
      "epoch: 8 step: 401, loss is 0.09390909224748611\n",
      "epoch: 8 step: 402, loss is 0.060346685349941254\n",
      "epoch: 8 step: 403, loss is 0.02630363218486309\n",
      "epoch: 8 step: 404, loss is 0.10728540271520615\n",
      "epoch: 8 step: 405, loss is 0.1447712928056717\n",
      "epoch: 8 step: 406, loss is 0.0993996262550354\n",
      "epoch: 8 step: 407, loss is 0.053998906165361404\n",
      "epoch: 8 step: 408, loss is 0.02160893753170967\n",
      "epoch: 8 step: 409, loss is 0.053610023111104965\n",
      "epoch: 8 step: 410, loss is 0.048601988703012466\n",
      "epoch: 8 step: 411, loss is 0.06600284576416016\n",
      "epoch: 8 step: 412, loss is 0.0698162317276001\n",
      "epoch: 8 step: 413, loss is 0.07687568664550781\n",
      "epoch: 8 step: 414, loss is 0.12094832211732864\n",
      "epoch: 8 step: 415, loss is 0.11551518738269806\n",
      "epoch: 8 step: 416, loss is 0.0563528947532177\n",
      "epoch: 8 step: 417, loss is 0.0701126679778099\n",
      "epoch: 8 step: 418, loss is 0.1821383833885193\n",
      "epoch: 8 step: 419, loss is 0.059418901801109314\n",
      "epoch: 8 step: 420, loss is 0.03258993849158287\n",
      "epoch: 8 step: 421, loss is 0.08522669225931168\n",
      "epoch: 8 step: 422, loss is 0.05127140134572983\n",
      "epoch: 8 step: 423, loss is 0.03171318396925926\n",
      "epoch: 8 step: 424, loss is 0.06065031513571739\n",
      "epoch: 8 step: 425, loss is 0.06643453240394592\n",
      "epoch: 8 step: 426, loss is 0.09954098612070084\n",
      "epoch: 8 step: 427, loss is 0.023587064817547798\n",
      "epoch: 8 step: 428, loss is 0.11228395253419876\n",
      "epoch: 8 step: 429, loss is 0.12629790604114532\n",
      "epoch: 8 step: 430, loss is 0.14685598015785217\n",
      "epoch: 8 step: 431, loss is 0.010374706238508224\n",
      "epoch: 8 step: 432, loss is 0.04971542954444885\n",
      "epoch: 8 step: 433, loss is 0.10948852449655533\n",
      "epoch: 8 step: 434, loss is 0.021925589069724083\n",
      "epoch: 8 step: 435, loss is 0.08606749773025513\n",
      "epoch: 8 step: 436, loss is 0.0667901411652565\n",
      "epoch: 8 step: 437, loss is 0.1289067268371582\n",
      "epoch: 8 step: 438, loss is 0.09923580288887024\n",
      "epoch: 8 step: 439, loss is 0.09506022930145264\n",
      "epoch: 8 step: 440, loss is 0.08677133917808533\n",
      "epoch: 8 step: 441, loss is 0.09269913285970688\n",
      "epoch: 8 step: 442, loss is 0.19008108973503113\n",
      "epoch: 8 step: 443, loss is 0.08755216747522354\n",
      "epoch: 8 step: 444, loss is 0.07867634296417236\n",
      "epoch: 8 step: 445, loss is 0.13021953403949738\n",
      "epoch: 8 step: 446, loss is 0.04721236228942871\n",
      "epoch: 8 step: 447, loss is 0.09217195212841034\n",
      "epoch: 8 step: 448, loss is 0.013407143764197826\n",
      "epoch: 8 step: 449, loss is 0.1315966099500656\n",
      "epoch: 8 step: 450, loss is 0.019344424828886986\n",
      "epoch: 8 step: 451, loss is 0.18145345151424408\n",
      "epoch: 8 step: 452, loss is 0.07497069239616394\n",
      "epoch: 8 step: 453, loss is 0.05535474792122841\n",
      "epoch: 8 step: 454, loss is 0.08551827818155289\n",
      "epoch: 8 step: 455, loss is 0.059848297387361526\n",
      "epoch: 8 step: 456, loss is 0.0506775826215744\n",
      "epoch: 8 step: 457, loss is 0.06573457270860672\n",
      "epoch: 8 step: 458, loss is 0.07728152722120285\n",
      "epoch: 8 step: 459, loss is 0.08764569461345673\n",
      "epoch: 8 step: 460, loss is 0.044237010180950165\n",
      "epoch: 8 step: 461, loss is 0.012503194622695446\n",
      "epoch: 8 step: 462, loss is 0.07270391285419464\n",
      "epoch: 8 step: 463, loss is 0.1393251270055771\n",
      "epoch: 8 step: 464, loss is 0.15741850435733795\n",
      "epoch: 8 step: 465, loss is 0.07551180571317673\n",
      "epoch: 8 step: 466, loss is 0.18053150177001953\n",
      "epoch: 8 step: 467, loss is 0.14914926886558533\n",
      "epoch: 8 step: 468, loss is 0.09739195555448532\n",
      "epoch: 8 step: 469, loss is 0.08170218765735626\n",
      "epoch: 8 step: 470, loss is 0.16870149970054626\n",
      "epoch: 8 step: 471, loss is 0.04769758880138397\n",
      "epoch: 8 step: 472, loss is 0.09403560310602188\n",
      "epoch: 8 step: 473, loss is 0.06912053376436234\n",
      "epoch: 8 step: 474, loss is 0.028332259505987167\n",
      "epoch: 8 step: 475, loss is 0.10286720842123032\n",
      "epoch: 8 step: 476, loss is 0.13050293922424316\n",
      "epoch: 8 step: 477, loss is 0.04101363196969032\n",
      "epoch: 8 step: 478, loss is 0.12375513464212418\n",
      "epoch: 8 step: 479, loss is 0.1389055699110031\n",
      "epoch: 8 step: 480, loss is 0.08916124701499939\n",
      "epoch: 8 step: 481, loss is 0.21418260037899017\n",
      "epoch: 8 step: 482, loss is 0.10896503180265427\n",
      "epoch: 8 step: 483, loss is 0.08778154104948044\n",
      "epoch: 8 step: 484, loss is 0.0995812639594078\n",
      "epoch: 8 step: 485, loss is 0.04580549895763397\n",
      "epoch: 8 step: 486, loss is 0.07241226732730865\n",
      "epoch: 8 step: 487, loss is 0.03385794907808304\n",
      "epoch: 8 step: 488, loss is 0.0763278380036354\n",
      "epoch: 8 step: 489, loss is 0.2375391572713852\n",
      "epoch: 8 step: 490, loss is 0.07683494687080383\n",
      "epoch: 8 step: 491, loss is 0.16594819724559784\n",
      "epoch: 8 step: 492, loss is 0.09276539087295532\n",
      "epoch: 8 step: 493, loss is 0.047959212213754654\n",
      "epoch: 8 step: 494, loss is 0.147788405418396\n",
      "epoch: 8 step: 495, loss is 0.12770973145961761\n",
      "epoch: 8 step: 496, loss is 0.2239246517419815\n",
      "epoch: 8 step: 497, loss is 0.12204407155513763\n",
      "epoch: 8 step: 498, loss is 0.05125853791832924\n",
      "epoch: 8 step: 499, loss is 0.06076967343688011\n",
      "epoch: 8 step: 500, loss is 0.17458707094192505\n",
      "epoch: 8 step: 501, loss is 0.06098675727844238\n",
      "epoch: 8 step: 502, loss is 0.055568769574165344\n",
      "epoch: 8 step: 503, loss is 0.04474996030330658\n",
      "epoch: 8 step: 504, loss is 0.14974553883075714\n",
      "epoch: 8 step: 505, loss is 0.14345510303974152\n",
      "epoch: 8 step: 506, loss is 0.1293754130601883\n",
      "epoch: 8 step: 507, loss is 0.09351181238889694\n",
      "epoch: 8 step: 508, loss is 0.051991336047649384\n",
      "epoch: 8 step: 509, loss is 0.04266287013888359\n",
      "epoch: 8 step: 510, loss is 0.054076485335826874\n",
      "epoch: 8 step: 511, loss is 0.05306478962302208\n",
      "epoch: 8 step: 512, loss is 0.11775848269462585\n",
      "epoch: 8 step: 513, loss is 0.06588810682296753\n",
      "epoch: 8 step: 514, loss is 0.15393531322479248\n",
      "epoch: 8 step: 515, loss is 0.08793376386165619\n",
      "epoch: 8 step: 516, loss is 0.09702076017856598\n",
      "epoch: 8 step: 517, loss is 0.08539923280477524\n",
      "epoch: 8 step: 518, loss is 0.09795691072940826\n",
      "epoch: 8 step: 519, loss is 0.08349306881427765\n",
      "epoch: 8 step: 520, loss is 0.05338140204548836\n",
      "epoch: 8 step: 521, loss is 0.13421253859996796\n",
      "epoch: 8 step: 522, loss is 0.09132073819637299\n",
      "epoch: 8 step: 523, loss is 0.07235652953386307\n",
      "epoch: 8 step: 524, loss is 0.08654703944921494\n",
      "epoch: 8 step: 525, loss is 0.09269099682569504\n",
      "epoch: 8 step: 526, loss is 0.13813023269176483\n",
      "epoch: 8 step: 527, loss is 0.11611968278884888\n",
      "epoch: 8 step: 528, loss is 0.11310766637325287\n",
      "epoch: 8 step: 529, loss is 0.03076242469251156\n",
      "epoch: 8 step: 530, loss is 0.04794275015592575\n",
      "epoch: 8 step: 531, loss is 0.08556991815567017\n",
      "epoch: 8 step: 532, loss is 0.026949122548103333\n",
      "epoch: 8 step: 533, loss is 0.08189983665943146\n",
      "epoch: 8 step: 534, loss is 0.053586944937705994\n",
      "epoch: 8 step: 535, loss is 0.19342336058616638\n",
      "epoch: 8 step: 536, loss is 0.2170877605676651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 537, loss is 0.21649891138076782\n",
      "epoch: 8 step: 538, loss is 0.044487737119197845\n",
      "epoch: 8 step: 539, loss is 0.04809122160077095\n",
      "epoch: 8 step: 540, loss is 0.008788716979324818\n",
      "epoch: 8 step: 541, loss is 0.08811430633068085\n",
      "epoch: 8 step: 542, loss is 0.22106815874576569\n",
      "epoch: 8 step: 543, loss is 0.05105292424559593\n",
      "epoch: 8 step: 544, loss is 0.03052629716694355\n",
      "epoch: 8 step: 545, loss is 0.13930238783359528\n",
      "epoch: 8 step: 546, loss is 0.08012359589338303\n",
      "epoch: 8 step: 547, loss is 0.1316029131412506\n",
      "epoch: 8 step: 548, loss is 0.224068745970726\n",
      "epoch: 8 step: 549, loss is 0.042619310319423676\n",
      "epoch: 8 step: 550, loss is 0.06651820242404938\n",
      "epoch: 8 step: 551, loss is 0.11015256494283676\n",
      "epoch: 8 step: 552, loss is 0.0733766108751297\n",
      "epoch: 8 step: 553, loss is 0.1042068675160408\n",
      "epoch: 8 step: 554, loss is 0.15454627573490143\n",
      "epoch: 8 step: 555, loss is 0.1056361272931099\n",
      "epoch: 8 step: 556, loss is 0.1548064947128296\n",
      "epoch: 8 step: 557, loss is 0.04979787394404411\n",
      "epoch: 8 step: 558, loss is 0.13557769358158112\n",
      "epoch: 8 step: 559, loss is 0.09648400545120239\n",
      "epoch: 8 step: 560, loss is 0.08056671917438507\n",
      "epoch: 8 step: 561, loss is 0.1513325721025467\n",
      "epoch: 8 step: 562, loss is 0.09489533305168152\n",
      "epoch: 8 step: 563, loss is 0.12231438606977463\n",
      "epoch: 8 step: 564, loss is 0.07942141592502594\n",
      "epoch: 8 step: 565, loss is 0.09731943905353546\n",
      "epoch: 8 step: 566, loss is 0.07754835486412048\n",
      "epoch: 8 step: 567, loss is 0.06065138429403305\n",
      "epoch: 8 step: 568, loss is 0.026200512424111366\n",
      "epoch: 8 step: 569, loss is 0.13863861560821533\n",
      "epoch: 8 step: 570, loss is 0.0471261627972126\n",
      "epoch: 8 step: 571, loss is 0.06280869245529175\n",
      "epoch: 8 step: 572, loss is 0.047429874539375305\n",
      "epoch: 8 step: 573, loss is 0.06108995899558067\n",
      "epoch: 8 step: 574, loss is 0.06798548251390457\n",
      "epoch: 8 step: 575, loss is 0.05330895632505417\n",
      "epoch: 8 step: 576, loss is 0.05704408884048462\n",
      "epoch: 8 step: 577, loss is 0.041054558008909225\n",
      "epoch: 8 step: 578, loss is 0.1050235703587532\n",
      "epoch: 8 step: 579, loss is 0.0613657683134079\n",
      "epoch: 8 step: 580, loss is 0.30836236476898193\n",
      "epoch: 8 step: 581, loss is 0.03882794827222824\n",
      "epoch: 8 step: 582, loss is 0.1099042296409607\n",
      "epoch: 8 step: 583, loss is 0.26054075360298157\n",
      "epoch: 8 step: 584, loss is 0.13823764026165009\n",
      "epoch: 8 step: 585, loss is 0.07808006554841995\n",
      "epoch: 8 step: 586, loss is 0.12864641845226288\n",
      "epoch: 8 step: 587, loss is 0.05034681037068367\n",
      "epoch: 8 step: 588, loss is 0.15866820514202118\n",
      "epoch: 8 step: 589, loss is 0.08366222679615021\n",
      "epoch: 8 step: 590, loss is 0.04626966267824173\n",
      "epoch: 8 step: 591, loss is 0.07219673693180084\n",
      "epoch: 8 step: 592, loss is 0.03275424987077713\n",
      "epoch: 8 step: 593, loss is 0.14472082257270813\n",
      "epoch: 8 step: 594, loss is 0.07091177254915237\n",
      "epoch: 8 step: 595, loss is 0.0703255906701088\n",
      "epoch: 8 step: 596, loss is 0.09110405296087265\n",
      "epoch: 8 step: 597, loss is 0.08716511726379395\n",
      "epoch: 8 step: 598, loss is 0.04841255769133568\n",
      "epoch: 8 step: 599, loss is 0.05679018050432205\n",
      "epoch: 8 step: 600, loss is 0.1602879762649536\n",
      "epoch: 8 step: 601, loss is 0.1542348861694336\n",
      "epoch: 8 step: 602, loss is 0.028901806101202965\n",
      "epoch: 8 step: 603, loss is 0.0799640342593193\n",
      "epoch: 8 step: 604, loss is 0.14617717266082764\n",
      "epoch: 8 step: 605, loss is 0.07854658365249634\n",
      "epoch: 8 step: 606, loss is 0.10893138498067856\n",
      "epoch: 8 step: 607, loss is 0.0992482528090477\n",
      "epoch: 8 step: 608, loss is 0.12233872711658478\n",
      "epoch: 8 step: 609, loss is 0.2040925920009613\n",
      "epoch: 8 step: 610, loss is 0.1022319495677948\n",
      "epoch: 8 step: 611, loss is 0.11151529848575592\n",
      "epoch: 8 step: 612, loss is 0.09366229921579361\n",
      "epoch: 8 step: 613, loss is 0.13137835264205933\n",
      "epoch: 8 step: 614, loss is 0.09330468624830246\n",
      "epoch: 8 step: 615, loss is 0.124669149518013\n",
      "epoch: 8 step: 616, loss is 0.0415644533932209\n",
      "epoch: 8 step: 617, loss is 0.2806873917579651\n",
      "epoch: 8 step: 618, loss is 0.0816539004445076\n",
      "epoch: 8 step: 619, loss is 0.10118929296731949\n",
      "epoch: 8 step: 620, loss is 0.03148287534713745\n",
      "epoch: 8 step: 621, loss is 0.23753264546394348\n",
      "epoch: 8 step: 622, loss is 0.07998105883598328\n",
      "epoch: 8 step: 623, loss is 0.12233268469572067\n",
      "epoch: 8 step: 624, loss is 0.09141852706670761\n",
      "epoch: 8 step: 625, loss is 0.11562682688236237\n",
      "epoch: 8 step: 626, loss is 0.1786900907754898\n",
      "epoch: 8 step: 627, loss is 0.1590414196252823\n",
      "epoch: 8 step: 628, loss is 0.028721429407596588\n",
      "epoch: 8 step: 629, loss is 0.08960963040590286\n",
      "epoch: 8 step: 630, loss is 0.0766819566488266\n",
      "epoch: 8 step: 631, loss is 0.06193400174379349\n",
      "epoch: 8 step: 632, loss is 0.19438129663467407\n",
      "epoch: 8 step: 633, loss is 0.16835054755210876\n",
      "epoch: 8 step: 634, loss is 0.21670173108577728\n",
      "epoch: 8 step: 635, loss is 0.06483447551727295\n",
      "epoch: 8 step: 636, loss is 0.05155466869473457\n",
      "epoch: 8 step: 637, loss is 0.26001545786857605\n",
      "epoch: 8 step: 638, loss is 0.17230041325092316\n",
      "epoch: 8 step: 639, loss is 0.17349126935005188\n",
      "epoch: 8 step: 640, loss is 0.04366304352879524\n",
      "epoch: 8 step: 641, loss is 0.10888584703207016\n",
      "epoch: 8 step: 642, loss is 0.07605413347482681\n",
      "epoch: 8 step: 643, loss is 0.063222236931324\n",
      "epoch: 8 step: 644, loss is 0.1534431129693985\n",
      "epoch: 8 step: 645, loss is 0.03269758075475693\n",
      "epoch: 8 step: 646, loss is 0.07781122624874115\n",
      "epoch: 8 step: 647, loss is 0.034507304430007935\n",
      "epoch: 8 step: 648, loss is 0.09993370622396469\n",
      "epoch: 8 step: 649, loss is 0.1468370109796524\n",
      "epoch: 8 step: 650, loss is 0.11452297121286392\n",
      "epoch: 8 step: 651, loss is 0.11394083499908447\n",
      "epoch: 8 step: 652, loss is 0.1161520928144455\n",
      "epoch: 8 step: 653, loss is 0.11801634728908539\n",
      "epoch: 8 step: 654, loss is 0.05316806957125664\n",
      "epoch: 8 step: 655, loss is 0.08538492769002914\n",
      "epoch: 8 step: 656, loss is 0.08389091491699219\n",
      "epoch: 8 step: 657, loss is 0.05686638504266739\n",
      "epoch: 8 step: 658, loss is 0.059104423969984055\n",
      "epoch: 8 step: 659, loss is 0.06708719581365585\n",
      "epoch: 8 step: 660, loss is 0.055552888661623\n",
      "epoch: 8 step: 661, loss is 0.1193845197558403\n",
      "epoch: 8 step: 662, loss is 0.08076443523168564\n",
      "epoch: 8 step: 663, loss is 0.0486711747944355\n",
      "epoch: 8 step: 664, loss is 0.056166499853134155\n",
      "epoch: 8 step: 665, loss is 0.061951007694005966\n",
      "epoch: 8 step: 666, loss is 0.03570013865828514\n",
      "epoch: 8 step: 667, loss is 0.15200532972812653\n",
      "epoch: 8 step: 668, loss is 0.08496823161840439\n",
      "epoch: 8 step: 669, loss is 0.10094499588012695\n",
      "epoch: 8 step: 670, loss is 0.044801678508520126\n",
      "epoch: 8 step: 671, loss is 0.03569638356566429\n",
      "epoch: 8 step: 672, loss is 0.05887685343623161\n",
      "epoch: 8 step: 673, loss is 0.04701199382543564\n",
      "epoch: 8 step: 674, loss is 0.08161834627389908\n",
      "epoch: 8 step: 675, loss is 0.04546937346458435\n",
      "epoch: 8 step: 676, loss is 0.0665542408823967\n",
      "epoch: 8 step: 677, loss is 0.11232519894838333\n",
      "epoch: 8 step: 678, loss is 0.13676141202449799\n",
      "epoch: 8 step: 679, loss is 0.01698574423789978\n",
      "epoch: 8 step: 680, loss is 0.05370473489165306\n",
      "epoch: 8 step: 681, loss is 0.10234019160270691\n",
      "epoch: 8 step: 682, loss is 0.06081685796380043\n",
      "epoch: 8 step: 683, loss is 0.1311633437871933\n",
      "epoch: 8 step: 684, loss is 0.0561116561293602\n",
      "epoch: 8 step: 685, loss is 0.05820663645863533\n",
      "epoch: 8 step: 686, loss is 0.05328148230910301\n",
      "epoch: 8 step: 687, loss is 0.05642715469002724\n",
      "epoch: 8 step: 688, loss is 0.05848504975438118\n",
      "epoch: 8 step: 689, loss is 0.11521458625793457\n",
      "epoch: 8 step: 690, loss is 0.0757519006729126\n",
      "epoch: 8 step: 691, loss is 0.0668318122625351\n",
      "epoch: 8 step: 692, loss is 0.12599557638168335\n",
      "epoch: 8 step: 693, loss is 0.08475983887910843\n",
      "epoch: 8 step: 694, loss is 0.12873609364032745\n",
      "epoch: 8 step: 695, loss is 0.053576599806547165\n",
      "epoch: 8 step: 696, loss is 0.1091708391904831\n",
      "epoch: 8 step: 697, loss is 0.06328035145998001\n",
      "epoch: 8 step: 698, loss is 0.06307525932788849\n",
      "epoch: 8 step: 699, loss is 0.09873498976230621\n",
      "epoch: 8 step: 700, loss is 0.17216281592845917\n",
      "epoch: 8 step: 701, loss is 0.0451657809317112\n",
      "epoch: 8 step: 702, loss is 0.12908197939395905\n",
      "epoch: 8 step: 703, loss is 0.10241170972585678\n",
      "epoch: 8 step: 704, loss is 0.04342407360672951\n",
      "epoch: 8 step: 705, loss is 0.10584203898906708\n",
      "epoch: 8 step: 706, loss is 0.08446487784385681\n",
      "epoch: 8 step: 707, loss is 0.06194106116890907\n",
      "epoch: 8 step: 708, loss is 0.2191554754972458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 709, loss is 0.05161483585834503\n",
      "epoch: 8 step: 710, loss is 0.03840943053364754\n",
      "epoch: 8 step: 711, loss is 0.14550918340682983\n",
      "epoch: 8 step: 712, loss is 0.13812746107578278\n",
      "epoch: 8 step: 713, loss is 0.18285058438777924\n",
      "epoch: 8 step: 714, loss is 0.040255673229694366\n",
      "epoch: 8 step: 715, loss is 0.14353154599666595\n",
      "epoch: 8 step: 716, loss is 0.07155730575323105\n",
      "epoch: 8 step: 717, loss is 0.07339423894882202\n",
      "epoch: 8 step: 718, loss is 0.21581192314624786\n",
      "epoch: 8 step: 719, loss is 0.14959535002708435\n",
      "epoch: 8 step: 720, loss is 0.12689396739006042\n",
      "epoch: 8 step: 721, loss is 0.12971383333206177\n",
      "epoch: 8 step: 722, loss is 0.09835443645715714\n",
      "epoch: 8 step: 723, loss is 0.05801081657409668\n",
      "epoch: 8 step: 724, loss is 0.29404884576797485\n",
      "epoch: 8 step: 725, loss is 0.10730487108230591\n",
      "epoch: 8 step: 726, loss is 0.08044587075710297\n",
      "epoch: 8 step: 727, loss is 0.08808565884828568\n",
      "epoch: 8 step: 728, loss is 0.1254861056804657\n",
      "epoch: 8 step: 729, loss is 0.08055701106786728\n",
      "epoch: 8 step: 730, loss is 0.07238217443227768\n",
      "epoch: 8 step: 731, loss is 0.015409590676426888\n",
      "epoch: 8 step: 732, loss is 0.044270485639572144\n",
      "epoch: 8 step: 733, loss is 0.02654987759888172\n",
      "epoch: 8 step: 734, loss is 0.08551992475986481\n",
      "epoch: 8 step: 735, loss is 0.07869377732276917\n",
      "epoch: 8 step: 736, loss is 0.06626226007938385\n",
      "epoch: 8 step: 737, loss is 0.2362043857574463\n",
      "epoch: 8 step: 738, loss is 0.12637947499752045\n",
      "epoch: 8 step: 739, loss is 0.09149426221847534\n",
      "epoch: 8 step: 740, loss is 0.11497674137353897\n",
      "epoch: 8 step: 741, loss is 0.09061744809150696\n",
      "epoch: 8 step: 742, loss is 0.1105431392788887\n",
      "epoch: 8 step: 743, loss is 0.09868384152650833\n",
      "epoch: 8 step: 744, loss is 0.08020217716693878\n",
      "epoch: 8 step: 745, loss is 0.1263325959444046\n",
      "epoch: 8 step: 746, loss is 0.17641764879226685\n",
      "epoch: 8 step: 747, loss is 0.026808930560946465\n",
      "epoch: 8 step: 748, loss is 0.14137209951877594\n",
      "epoch: 8 step: 749, loss is 0.09639718383550644\n",
      "epoch: 8 step: 750, loss is 0.0952366292476654\n",
      "epoch: 8 step: 751, loss is 0.08683166652917862\n",
      "epoch: 8 step: 752, loss is 0.2515438497066498\n",
      "epoch: 8 step: 753, loss is 0.20706525444984436\n",
      "epoch: 8 step: 754, loss is 0.1354370266199112\n",
      "epoch: 8 step: 755, loss is 0.12345191091299057\n",
      "epoch: 8 step: 756, loss is 0.19189435243606567\n",
      "epoch: 8 step: 757, loss is 0.17922963201999664\n",
      "epoch: 8 step: 758, loss is 0.11186809092760086\n",
      "epoch: 8 step: 759, loss is 0.09803330153226852\n",
      "epoch: 8 step: 760, loss is 0.11237088590860367\n",
      "epoch: 8 step: 761, loss is 0.10844375938177109\n",
      "epoch: 8 step: 762, loss is 0.15201126039028168\n",
      "epoch: 8 step: 763, loss is 0.13760890066623688\n",
      "epoch: 8 step: 764, loss is 0.09196560829877853\n",
      "epoch: 8 step: 765, loss is 0.2003261297941208\n",
      "epoch: 8 step: 766, loss is 0.1260768324136734\n",
      "epoch: 8 step: 767, loss is 0.12692753970623016\n",
      "epoch: 8 step: 768, loss is 0.08016013354063034\n",
      "epoch: 8 step: 769, loss is 0.0503406897187233\n",
      "epoch: 8 step: 770, loss is 0.1483582705259323\n",
      "epoch: 8 step: 771, loss is 0.13969342410564423\n",
      "epoch: 8 step: 772, loss is 0.10703332722187042\n",
      "epoch: 8 step: 773, loss is 0.03456123173236847\n",
      "epoch: 8 step: 774, loss is 0.049663007259368896\n",
      "epoch: 8 step: 775, loss is 0.09002530574798584\n",
      "epoch: 8 step: 776, loss is 0.09372425824403763\n",
      "epoch: 8 step: 777, loss is 0.11786551773548126\n",
      "epoch: 8 step: 778, loss is 0.14144399762153625\n",
      "epoch: 8 step: 779, loss is 0.09794257581233978\n",
      "epoch: 8 step: 780, loss is 0.32173019647598267\n",
      "epoch: 8 step: 781, loss is 0.13790614902973175\n",
      "epoch: 8 step: 782, loss is 0.03681324049830437\n",
      "epoch: 8 step: 783, loss is 0.12749116122722626\n",
      "epoch: 8 step: 784, loss is 0.15818290412425995\n",
      "epoch: 8 step: 785, loss is 0.08768785744905472\n",
      "epoch: 8 step: 786, loss is 0.04672600328922272\n",
      "epoch: 8 step: 787, loss is 0.10533121228218079\n",
      "epoch: 8 step: 788, loss is 0.07633128017187119\n",
      "epoch: 8 step: 789, loss is 0.10942252725362778\n",
      "epoch: 8 step: 790, loss is 0.2679651379585266\n",
      "epoch: 8 step: 791, loss is 0.042832404375076294\n",
      "epoch: 8 step: 792, loss is 0.10741928964853287\n",
      "epoch: 8 step: 793, loss is 0.13556940853595734\n",
      "epoch: 8 step: 794, loss is 0.056105080991983414\n",
      "epoch: 8 step: 795, loss is 0.10880142450332642\n",
      "epoch: 8 step: 796, loss is 0.12960758805274963\n",
      "epoch: 8 step: 797, loss is 0.1313668042421341\n",
      "epoch: 8 step: 798, loss is 0.07936838269233704\n",
      "epoch: 8 step: 799, loss is 0.04936408996582031\n",
      "epoch: 8 step: 800, loss is 0.06541445851325989\n",
      "epoch: 8 step: 801, loss is 0.08361302316188812\n",
      "epoch: 8 step: 802, loss is 0.17330393195152283\n",
      "epoch: 8 step: 803, loss is 0.048231158405542374\n",
      "epoch: 8 step: 804, loss is 0.09462390094995499\n",
      "epoch: 8 step: 805, loss is 0.0877642035484314\n",
      "epoch: 8 step: 806, loss is 0.1352197229862213\n",
      "epoch: 8 step: 807, loss is 0.0433853454887867\n",
      "epoch: 8 step: 808, loss is 0.02849237620830536\n",
      "epoch: 8 step: 809, loss is 0.05748043581843376\n",
      "epoch: 8 step: 810, loss is 0.19500871002674103\n",
      "epoch: 8 step: 811, loss is 0.1015804186463356\n",
      "epoch: 8 step: 812, loss is 0.11362331360578537\n",
      "epoch: 8 step: 813, loss is 0.04509621486067772\n",
      "epoch: 8 step: 814, loss is 0.15737204253673553\n",
      "epoch: 8 step: 815, loss is 0.10431402176618576\n",
      "epoch: 8 step: 816, loss is 0.12576405704021454\n",
      "epoch: 8 step: 817, loss is 0.07739731669425964\n",
      "epoch: 8 step: 818, loss is 0.044621825218200684\n",
      "epoch: 8 step: 819, loss is 0.08353828638792038\n",
      "epoch: 8 step: 820, loss is 0.06153280660510063\n",
      "epoch: 8 step: 821, loss is 0.10474367439746857\n",
      "epoch: 8 step: 822, loss is 0.10729142278432846\n",
      "epoch: 8 step: 823, loss is 0.10301540791988373\n",
      "epoch: 8 step: 824, loss is 0.10319962352514267\n",
      "epoch: 8 step: 825, loss is 0.11089453846216202\n",
      "epoch: 8 step: 826, loss is 0.060332778841257095\n",
      "epoch: 8 step: 827, loss is 0.05037512630224228\n",
      "epoch: 8 step: 828, loss is 0.13451476395130157\n",
      "epoch: 8 step: 829, loss is 0.0859021246433258\n",
      "epoch: 8 step: 830, loss is 0.051719117909669876\n",
      "epoch: 8 step: 831, loss is 0.11399729549884796\n",
      "epoch: 8 step: 832, loss is 0.11580245196819305\n",
      "epoch: 8 step: 833, loss is 0.04830346256494522\n",
      "epoch: 8 step: 834, loss is 0.05389709025621414\n",
      "epoch: 8 step: 835, loss is 0.06719809770584106\n",
      "epoch: 8 step: 836, loss is 0.05454364791512489\n",
      "epoch: 8 step: 837, loss is 0.08547185361385345\n",
      "epoch: 8 step: 838, loss is 0.251124769449234\n",
      "epoch: 8 step: 839, loss is 0.20470860600471497\n",
      "epoch: 8 step: 840, loss is 0.016324911266565323\n",
      "epoch: 8 step: 841, loss is 0.019804226234555244\n",
      "epoch: 8 step: 842, loss is 0.15745477378368378\n",
      "epoch: 8 step: 843, loss is 0.09649378806352615\n",
      "epoch: 8 step: 844, loss is 0.06544223427772522\n",
      "epoch: 8 step: 845, loss is 0.06066664680838585\n",
      "epoch: 8 step: 846, loss is 0.11195804178714752\n",
      "epoch: 8 step: 847, loss is 0.027660921216011047\n",
      "epoch: 8 step: 848, loss is 0.06902879476547241\n",
      "epoch: 8 step: 849, loss is 0.23129427433013916\n",
      "epoch: 8 step: 850, loss is 0.18781600892543793\n",
      "epoch: 8 step: 851, loss is 0.08986878395080566\n",
      "epoch: 8 step: 852, loss is 0.10079309344291687\n",
      "epoch: 8 step: 853, loss is 0.12952381372451782\n",
      "epoch: 8 step: 854, loss is 0.12677989900112152\n",
      "epoch: 8 step: 855, loss is 0.07053323835134506\n",
      "epoch: 8 step: 856, loss is 0.044588375836610794\n",
      "epoch: 8 step: 857, loss is 0.10165014863014221\n",
      "epoch: 8 step: 858, loss is 0.10226324200630188\n",
      "epoch: 8 step: 859, loss is 0.26782575249671936\n",
      "epoch: 8 step: 860, loss is 0.05381151661276817\n",
      "epoch: 8 step: 861, loss is 0.14309431612491608\n",
      "epoch: 8 step: 862, loss is 0.10681584477424622\n",
      "epoch: 8 step: 863, loss is 0.10879615694284439\n",
      "epoch: 8 step: 864, loss is 0.03339632228016853\n",
      "epoch: 8 step: 865, loss is 0.09145282953977585\n",
      "epoch: 8 step: 866, loss is 0.13885517418384552\n",
      "epoch: 8 step: 867, loss is 0.1105305552482605\n",
      "epoch: 8 step: 868, loss is 0.26002925634384155\n",
      "epoch: 8 step: 869, loss is 0.11391787976026535\n",
      "epoch: 8 step: 870, loss is 0.15240752696990967\n",
      "epoch: 8 step: 871, loss is 0.07131464034318924\n",
      "epoch: 8 step: 872, loss is 0.10068780183792114\n",
      "epoch: 8 step: 873, loss is 0.07948589324951172\n",
      "epoch: 8 step: 874, loss is 0.1718968003988266\n",
      "epoch: 8 step: 875, loss is 0.0775206908583641\n",
      "epoch: 8 step: 876, loss is 0.06535347551107407\n",
      "epoch: 8 step: 877, loss is 0.10305925458669662\n",
      "epoch: 8 step: 878, loss is 0.05603248253464699\n",
      "epoch: 8 step: 879, loss is 0.15280809998512268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 880, loss is 0.05891825631260872\n",
      "epoch: 8 step: 881, loss is 0.14119207859039307\n",
      "epoch: 8 step: 882, loss is 0.1431579887866974\n",
      "epoch: 8 step: 883, loss is 0.05340435728430748\n",
      "epoch: 8 step: 884, loss is 0.12725332379341125\n",
      "epoch: 8 step: 885, loss is 0.022045601159334183\n",
      "epoch: 8 step: 886, loss is 0.0220734104514122\n",
      "epoch: 8 step: 887, loss is 0.21026892960071564\n",
      "epoch: 8 step: 888, loss is 0.07004166394472122\n",
      "epoch: 8 step: 889, loss is 0.10285302251577377\n",
      "epoch: 8 step: 890, loss is 0.1312815248966217\n",
      "epoch: 8 step: 891, loss is 0.2013719379901886\n",
      "epoch: 8 step: 892, loss is 0.023495525121688843\n",
      "epoch: 8 step: 893, loss is 0.10386736690998077\n",
      "epoch: 8 step: 894, loss is 0.11196417361497879\n",
      "epoch: 8 step: 895, loss is 0.2688540816307068\n",
      "epoch: 8 step: 896, loss is 0.06908564269542694\n",
      "epoch: 8 step: 897, loss is 0.051434777677059174\n",
      "epoch: 8 step: 898, loss is 0.0833013653755188\n",
      "epoch: 8 step: 899, loss is 0.0806732103228569\n",
      "epoch: 8 step: 900, loss is 0.08282782882452011\n",
      "epoch: 8 step: 901, loss is 0.26371651887893677\n",
      "epoch: 8 step: 902, loss is 0.10042562335729599\n",
      "epoch: 8 step: 903, loss is 0.1304936408996582\n",
      "epoch: 8 step: 904, loss is 0.2912098467350006\n",
      "epoch: 8 step: 905, loss is 0.2702617347240448\n",
      "epoch: 8 step: 906, loss is 0.10067333281040192\n",
      "epoch: 8 step: 907, loss is 0.03217018395662308\n",
      "epoch: 8 step: 908, loss is 0.17130854725837708\n",
      "epoch: 8 step: 909, loss is 0.09014777094125748\n",
      "epoch: 8 step: 910, loss is 0.1061621755361557\n",
      "epoch: 8 step: 911, loss is 0.03006306290626526\n",
      "epoch: 8 step: 912, loss is 0.08216027170419693\n",
      "epoch: 8 step: 913, loss is 0.09192463755607605\n",
      "epoch: 8 step: 914, loss is 0.08662353456020355\n",
      "epoch: 8 step: 915, loss is 0.04562575742602348\n",
      "epoch: 8 step: 916, loss is 0.12427295744419098\n",
      "epoch: 8 step: 917, loss is 0.0917602926492691\n",
      "epoch: 8 step: 918, loss is 0.033910807222127914\n",
      "epoch: 8 step: 919, loss is 0.033792492002248764\n",
      "epoch: 8 step: 920, loss is 0.13146865367889404\n",
      "epoch: 8 step: 921, loss is 0.047227442264556885\n",
      "epoch: 8 step: 922, loss is 0.0333843007683754\n",
      "epoch: 8 step: 923, loss is 0.21845142543315887\n",
      "epoch: 8 step: 924, loss is 0.2109289914369583\n",
      "epoch: 8 step: 925, loss is 0.04594503715634346\n",
      "epoch: 8 step: 926, loss is 0.09338858723640442\n",
      "epoch: 8 step: 927, loss is 0.04048914462327957\n",
      "epoch: 8 step: 928, loss is 0.06162857636809349\n",
      "epoch: 8 step: 929, loss is 0.03809501603245735\n",
      "epoch: 8 step: 930, loss is 0.050834838300943375\n",
      "epoch: 8 step: 931, loss is 0.06882820278406143\n",
      "epoch: 8 step: 932, loss is 0.03640420362353325\n",
      "epoch: 8 step: 933, loss is 0.09104140847921371\n",
      "epoch: 8 step: 934, loss is 0.06999143213033676\n",
      "epoch: 8 step: 935, loss is 0.06277017295360565\n",
      "epoch: 8 step: 936, loss is 0.1429649144411087\n",
      "epoch: 8 step: 937, loss is 0.11710338294506073\n",
      "epoch: 9 step: 1, loss is 0.17472703754901886\n",
      "epoch: 9 step: 2, loss is 0.04778370261192322\n",
      "epoch: 9 step: 3, loss is 0.0848781168460846\n",
      "epoch: 9 step: 4, loss is 0.10652564465999603\n",
      "epoch: 9 step: 5, loss is 0.05635907128453255\n",
      "epoch: 9 step: 6, loss is 0.07702525705099106\n",
      "epoch: 9 step: 7, loss is 0.04738590493798256\n",
      "epoch: 9 step: 8, loss is 0.09773077815771103\n",
      "epoch: 9 step: 9, loss is 0.07313751429319382\n",
      "epoch: 9 step: 10, loss is 0.08802862465381622\n",
      "epoch: 9 step: 11, loss is 0.023128774017095566\n",
      "epoch: 9 step: 12, loss is 0.04446444287896156\n",
      "epoch: 9 step: 13, loss is 0.05897080525755882\n",
      "epoch: 9 step: 14, loss is 0.11239711195230484\n",
      "epoch: 9 step: 15, loss is 0.024985209107398987\n",
      "epoch: 9 step: 16, loss is 0.03971428424119949\n",
      "epoch: 9 step: 17, loss is 0.09413272142410278\n",
      "epoch: 9 step: 18, loss is 0.04963982105255127\n",
      "epoch: 9 step: 19, loss is 0.013195022009313107\n",
      "epoch: 9 step: 20, loss is 0.03157535195350647\n",
      "epoch: 9 step: 21, loss is 0.05775804445147514\n",
      "epoch: 9 step: 22, loss is 0.023513546213507652\n",
      "epoch: 9 step: 23, loss is 0.02900673635303974\n",
      "epoch: 9 step: 24, loss is 0.024918220937252045\n",
      "epoch: 9 step: 25, loss is 0.026212375611066818\n",
      "epoch: 9 step: 26, loss is 0.0700930505990982\n",
      "epoch: 9 step: 27, loss is 0.02682514861226082\n",
      "epoch: 9 step: 28, loss is 0.029169837012887\n",
      "epoch: 9 step: 29, loss is 0.05379835516214371\n",
      "epoch: 9 step: 30, loss is 0.07560703158378601\n",
      "epoch: 9 step: 31, loss is 0.06089159846305847\n",
      "epoch: 9 step: 32, loss is 0.016760194674134254\n",
      "epoch: 9 step: 33, loss is 0.06383960694074631\n",
      "epoch: 9 step: 34, loss is 0.10168950259685516\n",
      "epoch: 9 step: 35, loss is 0.05680122971534729\n",
      "epoch: 9 step: 36, loss is 0.015026402659714222\n",
      "epoch: 9 step: 37, loss is 0.04913262277841568\n",
      "epoch: 9 step: 38, loss is 0.04005103558301926\n",
      "epoch: 9 step: 39, loss is 0.05859684199094772\n",
      "epoch: 9 step: 40, loss is 0.06948868185281754\n",
      "epoch: 9 step: 41, loss is 0.10456276684999466\n",
      "epoch: 9 step: 42, loss is 0.0731387585401535\n",
      "epoch: 9 step: 43, loss is 0.10673157870769501\n",
      "epoch: 9 step: 44, loss is 0.07130544632673264\n",
      "epoch: 9 step: 45, loss is 0.08225556463003159\n",
      "epoch: 9 step: 46, loss is 0.03344009816646576\n",
      "epoch: 9 step: 47, loss is 0.0413215346634388\n",
      "epoch: 9 step: 48, loss is 0.013708945363759995\n",
      "epoch: 9 step: 49, loss is 0.04491902515292168\n",
      "epoch: 9 step: 50, loss is 0.05428249388933182\n",
      "epoch: 9 step: 51, loss is 0.05197085812687874\n",
      "epoch: 9 step: 52, loss is 0.03200025483965874\n",
      "epoch: 9 step: 53, loss is 0.11711198091506958\n",
      "epoch: 9 step: 54, loss is 0.06451975554227829\n",
      "epoch: 9 step: 55, loss is 0.08373074233531952\n",
      "epoch: 9 step: 56, loss is 0.08578715473413467\n",
      "epoch: 9 step: 57, loss is 0.0744849443435669\n",
      "epoch: 9 step: 58, loss is 0.06085343286395073\n",
      "epoch: 9 step: 59, loss is 0.05117933079600334\n",
      "epoch: 9 step: 60, loss is 0.08598089963197708\n",
      "epoch: 9 step: 61, loss is 0.028339803218841553\n",
      "epoch: 9 step: 62, loss is 0.05379793420433998\n",
      "epoch: 9 step: 63, loss is 0.02246709167957306\n",
      "epoch: 9 step: 64, loss is 0.027847055345773697\n",
      "epoch: 9 step: 65, loss is 0.07343042641878128\n",
      "epoch: 9 step: 66, loss is 0.03060344234108925\n",
      "epoch: 9 step: 67, loss is 0.07865512371063232\n",
      "epoch: 9 step: 68, loss is 0.07324612885713577\n",
      "epoch: 9 step: 69, loss is 0.0332615040242672\n",
      "epoch: 9 step: 70, loss is 0.05713488534092903\n",
      "epoch: 9 step: 71, loss is 0.09330528229475021\n",
      "epoch: 9 step: 72, loss is 0.030328471213579178\n",
      "epoch: 9 step: 73, loss is 0.09196390956640244\n",
      "epoch: 9 step: 74, loss is 0.053371917456388474\n",
      "epoch: 9 step: 75, loss is 0.06913641840219498\n",
      "epoch: 9 step: 76, loss is 0.02706502377986908\n",
      "epoch: 9 step: 77, loss is 0.023655230179429054\n",
      "epoch: 9 step: 78, loss is 0.05202199146151543\n",
      "epoch: 9 step: 79, loss is 0.07037653774023056\n",
      "epoch: 9 step: 80, loss is 0.017897536978125572\n",
      "epoch: 9 step: 81, loss is 0.029214156791567802\n",
      "epoch: 9 step: 82, loss is 0.10511383414268494\n",
      "epoch: 9 step: 83, loss is 0.07962212711572647\n",
      "epoch: 9 step: 84, loss is 0.06551013141870499\n",
      "epoch: 9 step: 85, loss is 0.089190274477005\n",
      "epoch: 9 step: 86, loss is 0.096550352871418\n",
      "epoch: 9 step: 87, loss is 0.01466537918895483\n",
      "epoch: 9 step: 88, loss is 0.06368459761142731\n",
      "epoch: 9 step: 89, loss is 0.0713585615158081\n",
      "epoch: 9 step: 90, loss is 0.0648525208234787\n",
      "epoch: 9 step: 91, loss is 0.08049837499856949\n",
      "epoch: 9 step: 92, loss is 0.03606762737035751\n",
      "epoch: 9 step: 93, loss is 0.1491645723581314\n",
      "epoch: 9 step: 94, loss is 0.08894370496273041\n",
      "epoch: 9 step: 95, loss is 0.017354821786284447\n",
      "epoch: 9 step: 96, loss is 0.15228186547756195\n",
      "epoch: 9 step: 97, loss is 0.025942476466298103\n",
      "epoch: 9 step: 98, loss is 0.061701852828264236\n",
      "epoch: 9 step: 99, loss is 0.047682225704193115\n",
      "epoch: 9 step: 100, loss is 0.0542450025677681\n",
      "epoch: 9 step: 101, loss is 0.023281462490558624\n",
      "epoch: 9 step: 102, loss is 0.019794395193457603\n",
      "epoch: 9 step: 103, loss is 0.11714591830968857\n",
      "epoch: 9 step: 104, loss is 0.2031078189611435\n",
      "epoch: 9 step: 105, loss is 0.127808079123497\n",
      "epoch: 9 step: 106, loss is 0.04493028670549393\n",
      "epoch: 9 step: 107, loss is 0.03743123635649681\n",
      "epoch: 9 step: 108, loss is 0.10950527340173721\n",
      "epoch: 9 step: 109, loss is 0.1020393893122673\n",
      "epoch: 9 step: 110, loss is 0.06248388811945915\n",
      "epoch: 9 step: 111, loss is 0.07324732095003128\n",
      "epoch: 9 step: 112, loss is 0.14176315069198608\n",
      "epoch: 9 step: 113, loss is 0.0726834237575531\n",
      "epoch: 9 step: 114, loss is 0.06181444972753525\n",
      "epoch: 9 step: 115, loss is 0.0188944973051548\n",
      "epoch: 9 step: 116, loss is 0.054356858134269714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 117, loss is 0.1464918553829193\n",
      "epoch: 9 step: 118, loss is 0.09292711317539215\n",
      "epoch: 9 step: 119, loss is 0.06609567254781723\n",
      "epoch: 9 step: 120, loss is 0.052896466106176376\n",
      "epoch: 9 step: 121, loss is 0.050380073487758636\n",
      "epoch: 9 step: 122, loss is 0.0570341981947422\n",
      "epoch: 9 step: 123, loss is 0.016069544479250908\n",
      "epoch: 9 step: 124, loss is 0.06032400578260422\n",
      "epoch: 9 step: 125, loss is 0.10264425724744797\n",
      "epoch: 9 step: 126, loss is 0.1439472734928131\n",
      "epoch: 9 step: 127, loss is 0.07134784013032913\n",
      "epoch: 9 step: 128, loss is 0.0659731775522232\n",
      "epoch: 9 step: 129, loss is 0.050222162157297134\n",
      "epoch: 9 step: 130, loss is 0.03245055302977562\n",
      "epoch: 9 step: 131, loss is 0.04603943973779678\n",
      "epoch: 9 step: 132, loss is 0.07414329051971436\n",
      "epoch: 9 step: 133, loss is 0.020958762615919113\n",
      "epoch: 9 step: 134, loss is 0.13167688250541687\n",
      "epoch: 9 step: 135, loss is 0.07421055436134338\n",
      "epoch: 9 step: 136, loss is 0.03746652230620384\n",
      "epoch: 9 step: 137, loss is 0.021639226004481316\n",
      "epoch: 9 step: 138, loss is 0.039247170090675354\n",
      "epoch: 9 step: 139, loss is 0.01555160153657198\n",
      "epoch: 9 step: 140, loss is 0.10122044384479523\n",
      "epoch: 9 step: 141, loss is 0.045482803136110306\n",
      "epoch: 9 step: 142, loss is 0.10617164522409439\n",
      "epoch: 9 step: 143, loss is 0.07041343301534653\n",
      "epoch: 9 step: 144, loss is 0.20118579268455505\n",
      "epoch: 9 step: 145, loss is 0.020014312118291855\n",
      "epoch: 9 step: 146, loss is 0.036466632038354874\n",
      "epoch: 9 step: 147, loss is 0.04386035352945328\n",
      "epoch: 9 step: 148, loss is 0.016346167773008347\n",
      "epoch: 9 step: 149, loss is 0.09926670789718628\n",
      "epoch: 9 step: 150, loss is 0.0579967238008976\n",
      "epoch: 9 step: 151, loss is 0.04919552057981491\n",
      "epoch: 9 step: 152, loss is 0.06638315320014954\n",
      "epoch: 9 step: 153, loss is 0.08253052830696106\n",
      "epoch: 9 step: 154, loss is 0.03725196421146393\n",
      "epoch: 9 step: 155, loss is 0.09731167554855347\n",
      "epoch: 9 step: 156, loss is 0.09471765905618668\n",
      "epoch: 9 step: 157, loss is 0.1971225142478943\n",
      "epoch: 9 step: 158, loss is 0.11325075477361679\n",
      "epoch: 9 step: 159, loss is 0.013300682418048382\n",
      "epoch: 9 step: 160, loss is 0.07265504449605942\n",
      "epoch: 9 step: 161, loss is 0.05306580662727356\n",
      "epoch: 9 step: 162, loss is 0.04289636015892029\n",
      "epoch: 9 step: 163, loss is 0.02280072495341301\n",
      "epoch: 9 step: 164, loss is 0.14090444147586823\n",
      "epoch: 9 step: 165, loss is 0.04691708832979202\n",
      "epoch: 9 step: 166, loss is 0.09073177725076675\n",
      "epoch: 9 step: 167, loss is 0.026185642927885056\n",
      "epoch: 9 step: 168, loss is 0.11525203287601471\n",
      "epoch: 9 step: 169, loss is 0.05798136815428734\n",
      "epoch: 9 step: 170, loss is 0.06976950168609619\n",
      "epoch: 9 step: 171, loss is 0.14945267140865326\n",
      "epoch: 9 step: 172, loss is 0.10219433903694153\n",
      "epoch: 9 step: 173, loss is 0.022717101499438286\n",
      "epoch: 9 step: 174, loss is 0.04242920130491257\n",
      "epoch: 9 step: 175, loss is 0.03753954917192459\n",
      "epoch: 9 step: 176, loss is 0.09877477586269379\n",
      "epoch: 9 step: 177, loss is 0.07531403750181198\n",
      "epoch: 9 step: 178, loss is 0.023498166352510452\n",
      "epoch: 9 step: 179, loss is 0.09226635843515396\n",
      "epoch: 9 step: 180, loss is 0.0501251257956028\n",
      "epoch: 9 step: 181, loss is 0.0506608821451664\n",
      "epoch: 9 step: 182, loss is 0.06727747619152069\n",
      "epoch: 9 step: 183, loss is 0.09925732016563416\n",
      "epoch: 9 step: 184, loss is 0.028051583096385002\n",
      "epoch: 9 step: 185, loss is 0.05986651033163071\n",
      "epoch: 9 step: 186, loss is 0.03884561359882355\n",
      "epoch: 9 step: 187, loss is 0.027351928874850273\n",
      "epoch: 9 step: 188, loss is 0.02946517989039421\n",
      "epoch: 9 step: 189, loss is 0.0863020271062851\n",
      "epoch: 9 step: 190, loss is 0.014630166813731194\n",
      "epoch: 9 step: 191, loss is 0.016112588346004486\n",
      "epoch: 9 step: 192, loss is 0.06481657177209854\n",
      "epoch: 9 step: 193, loss is 0.04481305181980133\n",
      "epoch: 9 step: 194, loss is 0.07547362148761749\n",
      "epoch: 9 step: 195, loss is 0.03954385593533516\n",
      "epoch: 9 step: 196, loss is 0.030110683292150497\n",
      "epoch: 9 step: 197, loss is 0.14616860449314117\n",
      "epoch: 9 step: 198, loss is 0.12512335181236267\n",
      "epoch: 9 step: 199, loss is 0.01921643503010273\n",
      "epoch: 9 step: 200, loss is 0.013547024689614773\n",
      "epoch: 9 step: 201, loss is 0.1389877200126648\n",
      "epoch: 9 step: 202, loss is 0.06021527200937271\n",
      "epoch: 9 step: 203, loss is 0.08336757868528366\n",
      "epoch: 9 step: 204, loss is 0.027876686304807663\n",
      "epoch: 9 step: 205, loss is 0.11309405416250229\n",
      "epoch: 9 step: 206, loss is 0.022124597802758217\n",
      "epoch: 9 step: 207, loss is 0.061938077211380005\n",
      "epoch: 9 step: 208, loss is 0.14284656941890717\n",
      "epoch: 9 step: 209, loss is 0.022337330505251884\n",
      "epoch: 9 step: 210, loss is 0.0839727446436882\n",
      "epoch: 9 step: 211, loss is 0.043774619698524475\n",
      "epoch: 9 step: 212, loss is 0.06238054484128952\n",
      "epoch: 9 step: 213, loss is 0.08079280704259872\n",
      "epoch: 9 step: 214, loss is 0.0856763944029808\n",
      "epoch: 9 step: 215, loss is 0.022924816235899925\n",
      "epoch: 9 step: 216, loss is 0.03896940499544144\n",
      "epoch: 9 step: 217, loss is 0.07683521509170532\n",
      "epoch: 9 step: 218, loss is 0.07012154906988144\n",
      "epoch: 9 step: 219, loss is 0.057810716331005096\n",
      "epoch: 9 step: 220, loss is 0.06843689829111099\n",
      "epoch: 9 step: 221, loss is 0.05901138111948967\n",
      "epoch: 9 step: 222, loss is 0.09290480613708496\n",
      "epoch: 9 step: 223, loss is 0.09336050599813461\n",
      "epoch: 9 step: 224, loss is 0.11150291562080383\n",
      "epoch: 9 step: 225, loss is 0.07939163595438004\n",
      "epoch: 9 step: 226, loss is 0.004932158160954714\n",
      "epoch: 9 step: 227, loss is 0.07704658061265945\n",
      "epoch: 9 step: 228, loss is 0.04991050437092781\n",
      "epoch: 9 step: 229, loss is 0.03559315949678421\n",
      "epoch: 9 step: 230, loss is 0.07639402896165848\n",
      "epoch: 9 step: 231, loss is 0.026766013354063034\n",
      "epoch: 9 step: 232, loss is 0.06790981441736221\n",
      "epoch: 9 step: 233, loss is 0.022973714396357536\n",
      "epoch: 9 step: 234, loss is 0.03551938012242317\n",
      "epoch: 9 step: 235, loss is 0.1270962357521057\n",
      "epoch: 9 step: 236, loss is 0.09489019215106964\n",
      "epoch: 9 step: 237, loss is 0.023769596591591835\n",
      "epoch: 9 step: 238, loss is 0.08305790275335312\n",
      "epoch: 9 step: 239, loss is 0.03142588958144188\n",
      "epoch: 9 step: 240, loss is 0.07621067017316818\n",
      "epoch: 9 step: 241, loss is 0.15260738134384155\n",
      "epoch: 9 step: 242, loss is 0.033531807363033295\n",
      "epoch: 9 step: 243, loss is 0.04694738611578941\n",
      "epoch: 9 step: 244, loss is 0.07077588886022568\n",
      "epoch: 9 step: 245, loss is 0.16131553053855896\n",
      "epoch: 9 step: 246, loss is 0.10625753551721573\n",
      "epoch: 9 step: 247, loss is 0.08460013568401337\n",
      "epoch: 9 step: 248, loss is 0.05512325093150139\n",
      "epoch: 9 step: 249, loss is 0.13244959712028503\n",
      "epoch: 9 step: 250, loss is 0.04650891199707985\n",
      "epoch: 9 step: 251, loss is 0.034006908535957336\n",
      "epoch: 9 step: 252, loss is 0.09807205200195312\n",
      "epoch: 9 step: 253, loss is 0.04262644052505493\n",
      "epoch: 9 step: 254, loss is 0.11802569031715393\n",
      "epoch: 9 step: 255, loss is 0.07337974011898041\n",
      "epoch: 9 step: 256, loss is 0.07006781548261642\n",
      "epoch: 9 step: 257, loss is 0.04423936456441879\n",
      "epoch: 9 step: 258, loss is 0.05799539014697075\n",
      "epoch: 9 step: 259, loss is 0.012012000195682049\n",
      "epoch: 9 step: 260, loss is 0.06519612669944763\n",
      "epoch: 9 step: 261, loss is 0.03772444278001785\n",
      "epoch: 9 step: 262, loss is 0.0501323938369751\n",
      "epoch: 9 step: 263, loss is 0.04961206391453743\n",
      "epoch: 9 step: 264, loss is 0.07907955348491669\n",
      "epoch: 9 step: 265, loss is 0.08162498474121094\n",
      "epoch: 9 step: 266, loss is 0.13954946398735046\n",
      "epoch: 9 step: 267, loss is 0.07060378789901733\n",
      "epoch: 9 step: 268, loss is 0.062441401183605194\n",
      "epoch: 9 step: 269, loss is 0.10915809124708176\n",
      "epoch: 9 step: 270, loss is 0.08688285946846008\n",
      "epoch: 9 step: 271, loss is 0.03946179524064064\n",
      "epoch: 9 step: 272, loss is 0.06212002411484718\n",
      "epoch: 9 step: 273, loss is 0.12541083991527557\n",
      "epoch: 9 step: 274, loss is 0.03262481838464737\n",
      "epoch: 9 step: 275, loss is 0.032947082072496414\n",
      "epoch: 9 step: 276, loss is 0.032860733568668365\n",
      "epoch: 9 step: 277, loss is 0.12467797845602036\n",
      "epoch: 9 step: 278, loss is 0.08299603313207626\n",
      "epoch: 9 step: 279, loss is 0.08695539832115173\n",
      "epoch: 9 step: 280, loss is 0.03879844397306442\n",
      "epoch: 9 step: 281, loss is 0.06568767875432968\n",
      "epoch: 9 step: 282, loss is 0.06667745858430862\n",
      "epoch: 9 step: 283, loss is 0.13453587889671326\n",
      "epoch: 9 step: 284, loss is 0.05573709309101105\n",
      "epoch: 9 step: 285, loss is 0.06028660759329796\n",
      "epoch: 9 step: 286, loss is 0.015489076264202595\n",
      "epoch: 9 step: 287, loss is 0.042114946991205215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 288, loss is 0.04252840206027031\n",
      "epoch: 9 step: 289, loss is 0.069779172539711\n",
      "epoch: 9 step: 290, loss is 0.16381503641605377\n",
      "epoch: 9 step: 291, loss is 0.034764643758535385\n",
      "epoch: 9 step: 292, loss is 0.08947999775409698\n",
      "epoch: 9 step: 293, loss is 0.09413904696702957\n",
      "epoch: 9 step: 294, loss is 0.09851763397455215\n",
      "epoch: 9 step: 295, loss is 0.04866735637187958\n",
      "epoch: 9 step: 296, loss is 0.09310449659824371\n",
      "epoch: 9 step: 297, loss is 0.09944546967744827\n",
      "epoch: 9 step: 298, loss is 0.014246477745473385\n",
      "epoch: 9 step: 299, loss is 0.10016635060310364\n",
      "epoch: 9 step: 300, loss is 0.06978576630353928\n",
      "epoch: 9 step: 301, loss is 0.06466540694236755\n",
      "epoch: 9 step: 302, loss is 0.07913800328969955\n",
      "epoch: 9 step: 303, loss is 0.09720336645841599\n",
      "epoch: 9 step: 304, loss is 0.03013182431459427\n",
      "epoch: 9 step: 305, loss is 0.11436159163713455\n",
      "epoch: 9 step: 306, loss is 0.05512396991252899\n",
      "epoch: 9 step: 307, loss is 0.07311492413282394\n",
      "epoch: 9 step: 308, loss is 0.10201644897460938\n",
      "epoch: 9 step: 309, loss is 0.048756640404462814\n",
      "epoch: 9 step: 310, loss is 0.07730147242546082\n",
      "epoch: 9 step: 311, loss is 0.09352190792560577\n",
      "epoch: 9 step: 312, loss is 0.06504852324724197\n",
      "epoch: 9 step: 313, loss is 0.14635354280471802\n",
      "epoch: 9 step: 314, loss is 0.059087831526994705\n",
      "epoch: 9 step: 315, loss is 0.07633370161056519\n",
      "epoch: 9 step: 316, loss is 0.12275569140911102\n",
      "epoch: 9 step: 317, loss is 0.010394866578280926\n",
      "epoch: 9 step: 318, loss is 0.10464288294315338\n",
      "epoch: 9 step: 319, loss is 0.07572225481271744\n",
      "epoch: 9 step: 320, loss is 0.0760943740606308\n",
      "epoch: 9 step: 321, loss is 0.07904993742704391\n",
      "epoch: 9 step: 322, loss is 0.07486448436975479\n",
      "epoch: 9 step: 323, loss is 0.09576088935136795\n",
      "epoch: 9 step: 324, loss is 0.1811358481645584\n",
      "epoch: 9 step: 325, loss is 0.07393117249011993\n",
      "epoch: 9 step: 326, loss is 0.04184707999229431\n",
      "epoch: 9 step: 327, loss is 0.19054670631885529\n",
      "epoch: 9 step: 328, loss is 0.03027351014316082\n",
      "epoch: 9 step: 329, loss is 0.11278913915157318\n",
      "epoch: 9 step: 330, loss is 0.27658286690711975\n",
      "epoch: 9 step: 331, loss is 0.07834015786647797\n",
      "epoch: 9 step: 332, loss is 0.07496745884418488\n",
      "epoch: 9 step: 333, loss is 0.15032146871089935\n",
      "epoch: 9 step: 334, loss is 0.09676137566566467\n",
      "epoch: 9 step: 335, loss is 0.10342936217784882\n",
      "epoch: 9 step: 336, loss is 0.07766444236040115\n",
      "epoch: 9 step: 337, loss is 0.0478491336107254\n",
      "epoch: 9 step: 338, loss is 0.036818794906139374\n",
      "epoch: 9 step: 339, loss is 0.081138476729393\n",
      "epoch: 9 step: 340, loss is 0.053031884133815765\n",
      "epoch: 9 step: 341, loss is 0.09948038309812546\n",
      "epoch: 9 step: 342, loss is 0.03514714166522026\n",
      "epoch: 9 step: 343, loss is 0.14930325746536255\n",
      "epoch: 9 step: 344, loss is 0.08233093470335007\n",
      "epoch: 9 step: 345, loss is 0.020159780979156494\n",
      "epoch: 9 step: 346, loss is 0.06708735972642899\n",
      "epoch: 9 step: 347, loss is 0.13414520025253296\n",
      "epoch: 9 step: 348, loss is 0.03291955217719078\n",
      "epoch: 9 step: 349, loss is 0.03572632744908333\n",
      "epoch: 9 step: 350, loss is 0.10626908391714096\n",
      "epoch: 9 step: 351, loss is 0.057757966220378876\n",
      "epoch: 9 step: 352, loss is 0.060687556862831116\n",
      "epoch: 9 step: 353, loss is 0.16446299850940704\n",
      "epoch: 9 step: 354, loss is 0.058420877903699875\n",
      "epoch: 9 step: 355, loss is 0.12122402340173721\n",
      "epoch: 9 step: 356, loss is 0.07122864574193954\n",
      "epoch: 9 step: 357, loss is 0.06054845452308655\n",
      "epoch: 9 step: 358, loss is 0.077663354575634\n",
      "epoch: 9 step: 359, loss is 0.03262010216712952\n",
      "epoch: 9 step: 360, loss is 0.10784181952476501\n",
      "epoch: 9 step: 361, loss is 0.13804121315479279\n",
      "epoch: 9 step: 362, loss is 0.062035877257585526\n",
      "epoch: 9 step: 363, loss is 0.06367532163858414\n",
      "epoch: 9 step: 364, loss is 0.047996219247579575\n",
      "epoch: 9 step: 365, loss is 0.04163661599159241\n",
      "epoch: 9 step: 366, loss is 0.05869056656956673\n",
      "epoch: 9 step: 367, loss is 0.023000570014119148\n",
      "epoch: 9 step: 368, loss is 0.06430743634700775\n",
      "epoch: 9 step: 369, loss is 0.1169961616396904\n",
      "epoch: 9 step: 370, loss is 0.14092785120010376\n",
      "epoch: 9 step: 371, loss is 0.08560127764940262\n",
      "epoch: 9 step: 372, loss is 0.07649876922369003\n",
      "epoch: 9 step: 373, loss is 0.05805937573313713\n",
      "epoch: 9 step: 374, loss is 0.06297849863767624\n",
      "epoch: 9 step: 375, loss is 0.052507102489471436\n",
      "epoch: 9 step: 376, loss is 0.20686233043670654\n",
      "epoch: 9 step: 377, loss is 0.04870038852095604\n",
      "epoch: 9 step: 378, loss is 0.08026143163442612\n",
      "epoch: 9 step: 379, loss is 0.04513116180896759\n",
      "epoch: 9 step: 380, loss is 0.02997075393795967\n",
      "epoch: 9 step: 381, loss is 0.08814582973718643\n",
      "epoch: 9 step: 382, loss is 0.06726235896348953\n",
      "epoch: 9 step: 383, loss is 0.08544743806123734\n",
      "epoch: 9 step: 384, loss is 0.06996717303991318\n",
      "epoch: 9 step: 385, loss is 0.13500012457370758\n",
      "epoch: 9 step: 386, loss is 0.031241614371538162\n",
      "epoch: 9 step: 387, loss is 0.09946458041667938\n",
      "epoch: 9 step: 388, loss is 0.03967960178852081\n",
      "epoch: 9 step: 389, loss is 0.06831053644418716\n",
      "epoch: 9 step: 390, loss is 0.08830105513334274\n",
      "epoch: 9 step: 391, loss is 0.07772928476333618\n",
      "epoch: 9 step: 392, loss is 0.08352290093898773\n",
      "epoch: 9 step: 393, loss is 0.04680594056844711\n",
      "epoch: 9 step: 394, loss is 0.029548022896051407\n",
      "epoch: 9 step: 395, loss is 0.029463347047567368\n",
      "epoch: 9 step: 396, loss is 0.14685781300067902\n",
      "epoch: 9 step: 397, loss is 0.04025626182556152\n",
      "epoch: 9 step: 398, loss is 0.024964524433016777\n",
      "epoch: 9 step: 399, loss is 0.0831076055765152\n",
      "epoch: 9 step: 400, loss is 0.03378748521208763\n",
      "epoch: 9 step: 401, loss is 0.12660454213619232\n",
      "epoch: 9 step: 402, loss is 0.11804121732711792\n",
      "epoch: 9 step: 403, loss is 0.07324642688035965\n",
      "epoch: 9 step: 404, loss is 0.08952198177576065\n",
      "epoch: 9 step: 405, loss is 0.09480465203523636\n",
      "epoch: 9 step: 406, loss is 0.17154476046562195\n",
      "epoch: 9 step: 407, loss is 0.09850390255451202\n",
      "epoch: 9 step: 408, loss is 0.1719799041748047\n",
      "epoch: 9 step: 409, loss is 0.14145377278327942\n",
      "epoch: 9 step: 410, loss is 0.06270717829465866\n",
      "epoch: 9 step: 411, loss is 0.060906894505023956\n",
      "epoch: 9 step: 412, loss is 0.09836675226688385\n",
      "epoch: 9 step: 413, loss is 0.09213509410619736\n",
      "epoch: 9 step: 414, loss is 0.15700899064540863\n",
      "epoch: 9 step: 415, loss is 0.02668711729347706\n",
      "epoch: 9 step: 416, loss is 0.08168020099401474\n",
      "epoch: 9 step: 417, loss is 0.05552472546696663\n",
      "epoch: 9 step: 418, loss is 0.03666210174560547\n",
      "epoch: 9 step: 419, loss is 0.10289961844682693\n",
      "epoch: 9 step: 420, loss is 0.06663000583648682\n",
      "epoch: 9 step: 421, loss is 0.06892533600330353\n",
      "epoch: 9 step: 422, loss is 0.04835725948214531\n",
      "epoch: 9 step: 423, loss is 0.21688970923423767\n",
      "epoch: 9 step: 424, loss is 0.061883244663476944\n",
      "epoch: 9 step: 425, loss is 0.07705529034137726\n",
      "epoch: 9 step: 426, loss is 0.04776391014456749\n",
      "epoch: 9 step: 427, loss is 0.027648819610476494\n",
      "epoch: 9 step: 428, loss is 0.25726118683815\n",
      "epoch: 9 step: 429, loss is 0.0291953943669796\n",
      "epoch: 9 step: 430, loss is 0.04914271458983421\n",
      "epoch: 9 step: 431, loss is 0.0498727448284626\n",
      "epoch: 9 step: 432, loss is 0.01992041803896427\n",
      "epoch: 9 step: 433, loss is 0.018996478989720345\n",
      "epoch: 9 step: 434, loss is 0.05569072440266609\n",
      "epoch: 9 step: 435, loss is 0.1294485628604889\n",
      "epoch: 9 step: 436, loss is 0.12477157264947891\n",
      "epoch: 9 step: 437, loss is 0.31487417221069336\n",
      "epoch: 9 step: 438, loss is 0.15608543157577515\n",
      "epoch: 9 step: 439, loss is 0.04381565749645233\n",
      "epoch: 9 step: 440, loss is 0.07066508382558823\n",
      "epoch: 9 step: 441, loss is 0.011360875330865383\n",
      "epoch: 9 step: 442, loss is 0.06984919309616089\n",
      "epoch: 9 step: 443, loss is 0.10819645971059799\n",
      "epoch: 9 step: 444, loss is 0.1283591240644455\n",
      "epoch: 9 step: 445, loss is 0.06783042848110199\n",
      "epoch: 9 step: 446, loss is 0.12508532404899597\n",
      "epoch: 9 step: 447, loss is 0.082310751080513\n",
      "epoch: 9 step: 448, loss is 0.06734419614076614\n",
      "epoch: 9 step: 449, loss is 0.048740338534116745\n",
      "epoch: 9 step: 450, loss is 0.061441801488399506\n",
      "epoch: 9 step: 451, loss is 0.1394510120153427\n",
      "epoch: 9 step: 452, loss is 0.0993109792470932\n",
      "epoch: 9 step: 453, loss is 0.07024756819009781\n",
      "epoch: 9 step: 454, loss is 0.03565971553325653\n",
      "epoch: 9 step: 455, loss is 0.04278666153550148\n",
      "epoch: 9 step: 456, loss is 0.15312933921813965\n",
      "epoch: 9 step: 457, loss is 0.06389728933572769\n",
      "epoch: 9 step: 458, loss is 0.12461316585540771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 459, loss is 0.0438619963824749\n",
      "epoch: 9 step: 460, loss is 0.034092310816049576\n",
      "epoch: 9 step: 461, loss is 0.06194818764925003\n",
      "epoch: 9 step: 462, loss is 0.07472168654203415\n",
      "epoch: 9 step: 463, loss is 0.137846902012825\n",
      "epoch: 9 step: 464, loss is 0.12366438657045364\n",
      "epoch: 9 step: 465, loss is 0.06956416368484497\n",
      "epoch: 9 step: 466, loss is 0.061627574265003204\n",
      "epoch: 9 step: 467, loss is 0.056035514920949936\n",
      "epoch: 9 step: 468, loss is 0.07758064568042755\n",
      "epoch: 9 step: 469, loss is 0.09768471121788025\n",
      "epoch: 9 step: 470, loss is 0.060080450028181076\n",
      "epoch: 9 step: 471, loss is 0.052548598498106\n",
      "epoch: 9 step: 472, loss is 0.03895755112171173\n",
      "epoch: 9 step: 473, loss is 0.018222859129309654\n",
      "epoch: 9 step: 474, loss is 0.023899810388684273\n",
      "epoch: 9 step: 475, loss is 0.0986572802066803\n",
      "epoch: 9 step: 476, loss is 0.07153777033090591\n",
      "epoch: 9 step: 477, loss is 0.07994305342435837\n",
      "epoch: 9 step: 478, loss is 0.08994413167238235\n",
      "epoch: 9 step: 479, loss is 0.17262451350688934\n",
      "epoch: 9 step: 480, loss is 0.04000481218099594\n",
      "epoch: 9 step: 481, loss is 0.09809397161006927\n",
      "epoch: 9 step: 482, loss is 0.18235844373703003\n",
      "epoch: 9 step: 483, loss is 0.09712924808263779\n",
      "epoch: 9 step: 484, loss is 0.01302423607558012\n",
      "epoch: 9 step: 485, loss is 0.08744750916957855\n",
      "epoch: 9 step: 486, loss is 0.07865290343761444\n",
      "epoch: 9 step: 487, loss is 0.038174837827682495\n",
      "epoch: 9 step: 488, loss is 0.023638121783733368\n",
      "epoch: 9 step: 489, loss is 0.023653604090213776\n",
      "epoch: 9 step: 490, loss is 0.02610614337027073\n",
      "epoch: 9 step: 491, loss is 0.11072070896625519\n",
      "epoch: 9 step: 492, loss is 0.02764013223350048\n",
      "epoch: 9 step: 493, loss is 0.037969063967466354\n",
      "epoch: 9 step: 494, loss is 0.03181330859661102\n",
      "epoch: 9 step: 495, loss is 0.06157050281763077\n",
      "epoch: 9 step: 496, loss is 0.03064228780567646\n",
      "epoch: 9 step: 497, loss is 0.09810790419578552\n",
      "epoch: 9 step: 498, loss is 0.08044729381799698\n",
      "epoch: 9 step: 499, loss is 0.03380340710282326\n",
      "epoch: 9 step: 500, loss is 0.06310731917619705\n",
      "epoch: 9 step: 501, loss is 0.0354304276406765\n",
      "epoch: 9 step: 502, loss is 0.19683198630809784\n",
      "epoch: 9 step: 503, loss is 0.19623853266239166\n",
      "epoch: 9 step: 504, loss is 0.059271231293678284\n",
      "epoch: 9 step: 505, loss is 0.07268589735031128\n",
      "epoch: 9 step: 506, loss is 0.06316684186458588\n",
      "epoch: 9 step: 507, loss is 0.03349367901682854\n",
      "epoch: 9 step: 508, loss is 0.02621590904891491\n",
      "epoch: 9 step: 509, loss is 0.032401442527770996\n",
      "epoch: 9 step: 510, loss is 0.03342818841338158\n",
      "epoch: 9 step: 511, loss is 0.053203679621219635\n",
      "epoch: 9 step: 512, loss is 0.03465307503938675\n",
      "epoch: 9 step: 513, loss is 0.01054108515381813\n",
      "epoch: 9 step: 514, loss is 0.12947292625904083\n",
      "epoch: 9 step: 515, loss is 0.07604493945837021\n",
      "epoch: 9 step: 516, loss is 0.019475864246487617\n",
      "epoch: 9 step: 517, loss is 0.08874490857124329\n",
      "epoch: 9 step: 518, loss is 0.08031634986400604\n",
      "epoch: 9 step: 519, loss is 0.030472801998257637\n",
      "epoch: 9 step: 520, loss is 0.04018533229827881\n",
      "epoch: 9 step: 521, loss is 0.014834856614470482\n",
      "epoch: 9 step: 522, loss is 0.11017538607120514\n",
      "epoch: 9 step: 523, loss is 0.025343261659145355\n",
      "epoch: 9 step: 524, loss is 0.09525325149297714\n",
      "epoch: 9 step: 525, loss is 0.15667249262332916\n",
      "epoch: 9 step: 526, loss is 0.07428213953971863\n",
      "epoch: 9 step: 527, loss is 0.031876638531684875\n",
      "epoch: 9 step: 528, loss is 0.14928703010082245\n",
      "epoch: 9 step: 529, loss is 0.02703229710459709\n",
      "epoch: 9 step: 530, loss is 0.07182805985212326\n",
      "epoch: 9 step: 531, loss is 0.04992319270968437\n",
      "epoch: 9 step: 532, loss is 0.03811109438538551\n",
      "epoch: 9 step: 533, loss is 0.09465615451335907\n",
      "epoch: 9 step: 534, loss is 0.13808086514472961\n",
      "epoch: 9 step: 535, loss is 0.09907832741737366\n",
      "epoch: 9 step: 536, loss is 0.061903126537799835\n",
      "epoch: 9 step: 537, loss is 0.12933309376239777\n",
      "epoch: 9 step: 538, loss is 0.09847259521484375\n",
      "epoch: 9 step: 539, loss is 0.10501979291439056\n",
      "epoch: 9 step: 540, loss is 0.06787169724702835\n",
      "epoch: 9 step: 541, loss is 0.14074550569057465\n",
      "epoch: 9 step: 542, loss is 0.022203242406249046\n",
      "epoch: 9 step: 543, loss is 0.04858631268143654\n",
      "epoch: 9 step: 544, loss is 0.3121601641178131\n",
      "epoch: 9 step: 545, loss is 0.04265372082591057\n",
      "epoch: 9 step: 546, loss is 0.08802006393671036\n",
      "epoch: 9 step: 547, loss is 0.024592291563749313\n",
      "epoch: 9 step: 548, loss is 0.014501064084470272\n",
      "epoch: 9 step: 549, loss is 0.029691090807318687\n",
      "epoch: 9 step: 550, loss is 0.11657080054283142\n",
      "epoch: 9 step: 551, loss is 0.10746265947818756\n",
      "epoch: 9 step: 552, loss is 0.1254354864358902\n",
      "epoch: 9 step: 553, loss is 0.018436212092638016\n",
      "epoch: 9 step: 554, loss is 0.027155844494700432\n",
      "epoch: 9 step: 555, loss is 0.0587124228477478\n",
      "epoch: 9 step: 556, loss is 0.09107130020856857\n",
      "epoch: 9 step: 557, loss is 0.06561605632305145\n",
      "epoch: 9 step: 558, loss is 0.06563355773687363\n",
      "epoch: 9 step: 559, loss is 0.09803657978773117\n",
      "epoch: 9 step: 560, loss is 0.061815232038497925\n",
      "epoch: 9 step: 561, loss is 0.09692303836345673\n",
      "epoch: 9 step: 562, loss is 0.0434175543487072\n",
      "epoch: 9 step: 563, loss is 0.31483545899391174\n",
      "epoch: 9 step: 564, loss is 0.04984508827328682\n",
      "epoch: 9 step: 565, loss is 0.12129317224025726\n",
      "epoch: 9 step: 566, loss is 0.05971763655543327\n",
      "epoch: 9 step: 567, loss is 0.13810782134532928\n",
      "epoch: 9 step: 568, loss is 0.08719082176685333\n",
      "epoch: 9 step: 569, loss is 0.012373266741633415\n",
      "epoch: 9 step: 570, loss is 0.16363240778446198\n",
      "epoch: 9 step: 571, loss is 0.18534456193447113\n",
      "epoch: 9 step: 572, loss is 0.08767382800579071\n",
      "epoch: 9 step: 573, loss is 0.04245482385158539\n",
      "epoch: 9 step: 574, loss is 0.05347229912877083\n",
      "epoch: 9 step: 575, loss is 0.06553500890731812\n",
      "epoch: 9 step: 576, loss is 0.11518945544958115\n",
      "epoch: 9 step: 577, loss is 0.06532509624958038\n",
      "epoch: 9 step: 578, loss is 0.17406097054481506\n",
      "epoch: 9 step: 579, loss is 0.04986461251974106\n",
      "epoch: 9 step: 580, loss is 0.05443261191248894\n",
      "epoch: 9 step: 581, loss is 0.09872523695230484\n",
      "epoch: 9 step: 582, loss is 0.17695288360118866\n",
      "epoch: 9 step: 583, loss is 0.09182348847389221\n",
      "epoch: 9 step: 584, loss is 0.0963050052523613\n",
      "epoch: 9 step: 585, loss is 0.07776685804128647\n",
      "epoch: 9 step: 586, loss is 0.03484974801540375\n",
      "epoch: 9 step: 587, loss is 0.06470784544944763\n",
      "epoch: 9 step: 588, loss is 0.036117073148489\n",
      "epoch: 9 step: 589, loss is 0.08675685524940491\n",
      "epoch: 9 step: 590, loss is 0.011419746093451977\n",
      "epoch: 9 step: 591, loss is 0.04182540625333786\n",
      "epoch: 9 step: 592, loss is 0.0266081802546978\n",
      "epoch: 9 step: 593, loss is 0.060255713760852814\n",
      "epoch: 9 step: 594, loss is 0.13325808942317963\n",
      "epoch: 9 step: 595, loss is 0.06986214965581894\n",
      "epoch: 9 step: 596, loss is 0.0697798877954483\n",
      "epoch: 9 step: 597, loss is 0.05379168316721916\n",
      "epoch: 9 step: 598, loss is 0.09147468209266663\n",
      "epoch: 9 step: 599, loss is 0.10276824235916138\n",
      "epoch: 9 step: 600, loss is 0.1082926094532013\n",
      "epoch: 9 step: 601, loss is 0.05462295189499855\n",
      "epoch: 9 step: 602, loss is 0.057746388018131256\n",
      "epoch: 9 step: 603, loss is 0.07734677940607071\n",
      "epoch: 9 step: 604, loss is 0.03342250734567642\n",
      "epoch: 9 step: 605, loss is 0.20588670670986176\n",
      "epoch: 9 step: 606, loss is 0.02394901216030121\n",
      "epoch: 9 step: 607, loss is 0.09197434037923813\n",
      "epoch: 9 step: 608, loss is 0.07009683549404144\n",
      "epoch: 9 step: 609, loss is 0.05497848987579346\n",
      "epoch: 9 step: 610, loss is 0.04658080264925957\n",
      "epoch: 9 step: 611, loss is 0.05415211245417595\n",
      "epoch: 9 step: 612, loss is 0.14092309772968292\n",
      "epoch: 9 step: 613, loss is 0.05612063407897949\n",
      "epoch: 9 step: 614, loss is 0.07735656201839447\n",
      "epoch: 9 step: 615, loss is 0.039093513041734695\n",
      "epoch: 9 step: 616, loss is 0.037584640085697174\n",
      "epoch: 9 step: 617, loss is 0.07659528404474258\n",
      "epoch: 9 step: 618, loss is 0.07808342576026917\n",
      "epoch: 9 step: 619, loss is 0.12170387804508209\n",
      "epoch: 9 step: 620, loss is 0.07995884120464325\n",
      "epoch: 9 step: 621, loss is 0.12368856370449066\n",
      "epoch: 9 step: 622, loss is 0.09369334578514099\n",
      "epoch: 9 step: 623, loss is 0.13961099088191986\n",
      "epoch: 9 step: 624, loss is 0.14394858479499817\n",
      "epoch: 9 step: 625, loss is 0.1695757955312729\n",
      "epoch: 9 step: 626, loss is 0.015987424179911613\n",
      "epoch: 9 step: 627, loss is 0.008172851987183094\n",
      "epoch: 9 step: 628, loss is 0.016985364258289337\n",
      "epoch: 9 step: 629, loss is 0.07991748303174973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 630, loss is 0.021237079054117203\n",
      "epoch: 9 step: 631, loss is 0.15388548374176025\n",
      "epoch: 9 step: 632, loss is 0.09045600146055222\n",
      "epoch: 9 step: 633, loss is 0.09882191568613052\n",
      "epoch: 9 step: 634, loss is 0.0659177228808403\n",
      "epoch: 9 step: 635, loss is 0.0490150973200798\n",
      "epoch: 9 step: 636, loss is 0.11423034965991974\n",
      "epoch: 9 step: 637, loss is 0.07468820363283157\n",
      "epoch: 9 step: 638, loss is 0.036113105714321136\n",
      "epoch: 9 step: 639, loss is 0.04853476583957672\n",
      "epoch: 9 step: 640, loss is 0.07332466542720795\n",
      "epoch: 9 step: 641, loss is 0.19792351126670837\n",
      "epoch: 9 step: 642, loss is 0.1023525819182396\n",
      "epoch: 9 step: 643, loss is 0.08775745332241058\n",
      "epoch: 9 step: 644, loss is 0.15313804149627686\n",
      "epoch: 9 step: 645, loss is 0.03193367272615433\n",
      "epoch: 9 step: 646, loss is 0.08699898421764374\n",
      "epoch: 9 step: 647, loss is 0.10672558844089508\n",
      "epoch: 9 step: 648, loss is 0.028506945818662643\n",
      "epoch: 9 step: 649, loss is 0.0423494391143322\n",
      "epoch: 9 step: 650, loss is 0.10808281600475311\n",
      "epoch: 9 step: 651, loss is 0.08101414144039154\n",
      "epoch: 9 step: 652, loss is 0.01740141585469246\n",
      "epoch: 9 step: 653, loss is 0.062349673360586166\n",
      "epoch: 9 step: 654, loss is 0.034320808947086334\n",
      "epoch: 9 step: 655, loss is 0.1350862681865692\n",
      "epoch: 9 step: 656, loss is 0.11078706383705139\n",
      "epoch: 9 step: 657, loss is 0.05879410356283188\n",
      "epoch: 9 step: 658, loss is 0.03240129351615906\n",
      "epoch: 9 step: 659, loss is 0.011675027199089527\n",
      "epoch: 9 step: 660, loss is 0.03319114074110985\n",
      "epoch: 9 step: 661, loss is 0.0850692167878151\n",
      "epoch: 9 step: 662, loss is 0.05780792236328125\n",
      "epoch: 9 step: 663, loss is 0.09319834411144257\n",
      "epoch: 9 step: 664, loss is 0.01756824366748333\n",
      "epoch: 9 step: 665, loss is 0.04588373377919197\n",
      "epoch: 9 step: 666, loss is 0.17684035003185272\n",
      "epoch: 9 step: 667, loss is 0.03260732442140579\n",
      "epoch: 9 step: 668, loss is 0.06760718673467636\n",
      "epoch: 9 step: 669, loss is 0.06793238967657089\n",
      "epoch: 9 step: 670, loss is 0.10825733095407486\n",
      "epoch: 9 step: 671, loss is 0.11189796775579453\n",
      "epoch: 9 step: 672, loss is 0.02163262851536274\n",
      "epoch: 9 step: 673, loss is 0.016498012468218803\n",
      "epoch: 9 step: 674, loss is 0.02243356965482235\n",
      "epoch: 9 step: 675, loss is 0.057394009083509445\n",
      "epoch: 9 step: 676, loss is 0.09243452548980713\n",
      "epoch: 9 step: 677, loss is 0.06030331924557686\n",
      "epoch: 9 step: 678, loss is 0.023923462256789207\n",
      "epoch: 9 step: 679, loss is 0.03695501387119293\n",
      "epoch: 9 step: 680, loss is 0.2799746096134186\n",
      "epoch: 9 step: 681, loss is 0.025148695334792137\n",
      "epoch: 9 step: 682, loss is 0.017153656110167503\n",
      "epoch: 9 step: 683, loss is 0.019117768853902817\n",
      "epoch: 9 step: 684, loss is 0.15674792230129242\n",
      "epoch: 9 step: 685, loss is 0.0634882003068924\n",
      "epoch: 9 step: 686, loss is 0.11859158426523209\n",
      "epoch: 9 step: 687, loss is 0.02591482177376747\n",
      "epoch: 9 step: 688, loss is 0.05476485192775726\n",
      "epoch: 9 step: 689, loss is 0.10946473479270935\n",
      "epoch: 9 step: 690, loss is 0.025211893022060394\n",
      "epoch: 9 step: 691, loss is 0.16133032739162445\n",
      "epoch: 9 step: 692, loss is 0.14338566362857819\n",
      "epoch: 9 step: 693, loss is 0.030390501022338867\n",
      "epoch: 9 step: 694, loss is 0.08736081421375275\n",
      "epoch: 9 step: 695, loss is 0.03546559065580368\n",
      "epoch: 9 step: 696, loss is 0.15797235071659088\n",
      "epoch: 9 step: 697, loss is 0.04109827056527138\n",
      "epoch: 9 step: 698, loss is 0.030976010486483574\n",
      "epoch: 9 step: 699, loss is 0.04056503251194954\n",
      "epoch: 9 step: 700, loss is 0.09164462983608246\n",
      "epoch: 9 step: 701, loss is 0.07096762210130692\n",
      "epoch: 9 step: 702, loss is 0.06427870690822601\n",
      "epoch: 9 step: 703, loss is 0.2787744402885437\n",
      "epoch: 9 step: 704, loss is 0.1882404088973999\n",
      "epoch: 9 step: 705, loss is 0.1087569147348404\n",
      "epoch: 9 step: 706, loss is 0.0472865104675293\n",
      "epoch: 9 step: 707, loss is 0.060702063143253326\n",
      "epoch: 9 step: 708, loss is 0.033974211663007736\n",
      "epoch: 9 step: 709, loss is 0.11604700237512589\n",
      "epoch: 9 step: 710, loss is 0.06838469952344894\n",
      "epoch: 9 step: 711, loss is 0.023411991074681282\n",
      "epoch: 9 step: 712, loss is 0.03948860615491867\n",
      "epoch: 9 step: 713, loss is 0.03651455044746399\n",
      "epoch: 9 step: 714, loss is 0.07168121635913849\n",
      "epoch: 9 step: 715, loss is 0.08172369748353958\n",
      "epoch: 9 step: 716, loss is 0.0603964738547802\n",
      "epoch: 9 step: 717, loss is 0.06905670464038849\n",
      "epoch: 9 step: 718, loss is 0.07253362238407135\n",
      "epoch: 9 step: 719, loss is 0.038369301706552505\n",
      "epoch: 9 step: 720, loss is 0.10245837271213531\n",
      "epoch: 9 step: 721, loss is 0.1338215172290802\n",
      "epoch: 9 step: 722, loss is 0.05598496273159981\n",
      "epoch: 9 step: 723, loss is 0.06176837533712387\n",
      "epoch: 9 step: 724, loss is 0.01734638214111328\n",
      "epoch: 9 step: 725, loss is 0.09777370095252991\n",
      "epoch: 9 step: 726, loss is 0.031411927193403244\n",
      "epoch: 9 step: 727, loss is 0.04319281876087189\n",
      "epoch: 9 step: 728, loss is 0.026300406083464622\n",
      "epoch: 9 step: 729, loss is 0.12000835686922073\n",
      "epoch: 9 step: 730, loss is 0.08464419841766357\n",
      "epoch: 9 step: 731, loss is 0.06465914100408554\n",
      "epoch: 9 step: 732, loss is 0.08930002152919769\n",
      "epoch: 9 step: 733, loss is 0.08420766144990921\n",
      "epoch: 9 step: 734, loss is 0.1374817043542862\n",
      "epoch: 9 step: 735, loss is 0.04817698150873184\n",
      "epoch: 9 step: 736, loss is 0.1047518253326416\n",
      "epoch: 9 step: 737, loss is 0.08327080309391022\n",
      "epoch: 9 step: 738, loss is 0.10071282833814621\n",
      "epoch: 9 step: 739, loss is 0.11984141170978546\n",
      "epoch: 9 step: 740, loss is 0.06409479677677155\n",
      "epoch: 9 step: 741, loss is 0.05470432713627815\n",
      "epoch: 9 step: 742, loss is 0.04627254977822304\n",
      "epoch: 9 step: 743, loss is 0.05857573449611664\n",
      "epoch: 9 step: 744, loss is 0.02877645380795002\n",
      "epoch: 9 step: 745, loss is 0.08113805949687958\n",
      "epoch: 9 step: 746, loss is 0.15993714332580566\n",
      "epoch: 9 step: 747, loss is 0.054495919495821\n",
      "epoch: 9 step: 748, loss is 0.07448825240135193\n",
      "epoch: 9 step: 749, loss is 0.09680884331464767\n",
      "epoch: 9 step: 750, loss is 0.06655597686767578\n",
      "epoch: 9 step: 751, loss is 0.04354880005121231\n",
      "epoch: 9 step: 752, loss is 0.11711834371089935\n",
      "epoch: 9 step: 753, loss is 0.1411268413066864\n",
      "epoch: 9 step: 754, loss is 0.05646582320332527\n",
      "epoch: 9 step: 755, loss is 0.054952774196863174\n",
      "epoch: 9 step: 756, loss is 0.08891672641038895\n",
      "epoch: 9 step: 757, loss is 0.02679404802620411\n",
      "epoch: 9 step: 758, loss is 0.009112757630646229\n",
      "epoch: 9 step: 759, loss is 0.021763792261481285\n",
      "epoch: 9 step: 760, loss is 0.01987145096063614\n",
      "epoch: 9 step: 761, loss is 0.047908179461956024\n",
      "epoch: 9 step: 762, loss is 0.06659761071205139\n",
      "epoch: 9 step: 763, loss is 0.04318739473819733\n",
      "epoch: 9 step: 764, loss is 0.023492561653256416\n",
      "epoch: 9 step: 765, loss is 0.07471363246440887\n",
      "epoch: 9 step: 766, loss is 0.0936405286192894\n",
      "epoch: 9 step: 767, loss is 0.047022633254528046\n",
      "epoch: 9 step: 768, loss is 0.13628925383090973\n",
      "epoch: 9 step: 769, loss is 0.052954234182834625\n",
      "epoch: 9 step: 770, loss is 0.046922992914915085\n",
      "epoch: 9 step: 771, loss is 0.08892279118299484\n",
      "epoch: 9 step: 772, loss is 0.10175131261348724\n",
      "epoch: 9 step: 773, loss is 0.07875512540340424\n",
      "epoch: 9 step: 774, loss is 0.1427004188299179\n",
      "epoch: 9 step: 775, loss is 0.070339635014534\n",
      "epoch: 9 step: 776, loss is 0.010887338779866695\n",
      "epoch: 9 step: 777, loss is 0.03701655939221382\n",
      "epoch: 9 step: 778, loss is 0.06031610816717148\n",
      "epoch: 9 step: 779, loss is 0.11932295560836792\n",
      "epoch: 9 step: 780, loss is 0.016750531271100044\n",
      "epoch: 9 step: 781, loss is 0.05397095903754234\n",
      "epoch: 9 step: 782, loss is 0.06364480406045914\n",
      "epoch: 9 step: 783, loss is 0.09704941511154175\n",
      "epoch: 9 step: 784, loss is 0.06596257537603378\n",
      "epoch: 9 step: 785, loss is 0.11986612528562546\n",
      "epoch: 9 step: 786, loss is 0.11200884729623795\n",
      "epoch: 9 step: 787, loss is 0.012712514027953148\n",
      "epoch: 9 step: 788, loss is 0.0647793859243393\n",
      "epoch: 9 step: 789, loss is 0.053539954125881195\n",
      "epoch: 9 step: 790, loss is 0.12795554101467133\n",
      "epoch: 9 step: 791, loss is 0.22783181071281433\n",
      "epoch: 9 step: 792, loss is 0.10959122329950333\n",
      "epoch: 9 step: 793, loss is 0.0873023048043251\n",
      "epoch: 9 step: 794, loss is 0.04192669317126274\n",
      "epoch: 9 step: 795, loss is 0.04444732144474983\n",
      "epoch: 9 step: 796, loss is 0.11545998603105545\n",
      "epoch: 9 step: 797, loss is 0.1494734138250351\n",
      "epoch: 9 step: 798, loss is 0.05588236823678017\n",
      "epoch: 9 step: 799, loss is 0.0226872731000185\n",
      "epoch: 9 step: 800, loss is 0.08154170215129852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 801, loss is 0.1604771912097931\n",
      "epoch: 9 step: 802, loss is 0.06689870357513428\n",
      "epoch: 9 step: 803, loss is 0.11027104407548904\n",
      "epoch: 9 step: 804, loss is 0.07300376147031784\n",
      "epoch: 9 step: 805, loss is 0.1386919617652893\n",
      "epoch: 9 step: 806, loss is 0.06491923332214355\n",
      "epoch: 9 step: 807, loss is 0.10281144827604294\n",
      "epoch: 9 step: 808, loss is 0.03358246386051178\n",
      "epoch: 9 step: 809, loss is 0.033190011978149414\n",
      "epoch: 9 step: 810, loss is 0.1304282397031784\n",
      "epoch: 9 step: 811, loss is 0.07085014134645462\n",
      "epoch: 9 step: 812, loss is 0.01576683111488819\n",
      "epoch: 9 step: 813, loss is 0.04301813989877701\n",
      "epoch: 9 step: 814, loss is 0.08806217461824417\n",
      "epoch: 9 step: 815, loss is 0.06652840226888657\n",
      "epoch: 9 step: 816, loss is 0.09763229638338089\n",
      "epoch: 9 step: 817, loss is 0.04973280057311058\n",
      "epoch: 9 step: 818, loss is 0.03530590981245041\n",
      "epoch: 9 step: 819, loss is 0.08718857914209366\n",
      "epoch: 9 step: 820, loss is 0.020790215581655502\n",
      "epoch: 9 step: 821, loss is 0.08269663900136948\n",
      "epoch: 9 step: 822, loss is 0.048864156007766724\n",
      "epoch: 9 step: 823, loss is 0.025862814858555794\n",
      "epoch: 9 step: 824, loss is 0.05081504210829735\n",
      "epoch: 9 step: 825, loss is 0.08834370970726013\n",
      "epoch: 9 step: 826, loss is 0.06818544864654541\n",
      "epoch: 9 step: 827, loss is 0.07386217266321182\n",
      "epoch: 9 step: 828, loss is 0.048607487231492996\n",
      "epoch: 9 step: 829, loss is 0.054169535636901855\n",
      "epoch: 9 step: 830, loss is 0.04954570531845093\n",
      "epoch: 9 step: 831, loss is 0.008605599403381348\n",
      "epoch: 9 step: 832, loss is 0.05840059369802475\n",
      "epoch: 9 step: 833, loss is 0.018458528444170952\n",
      "epoch: 9 step: 834, loss is 0.03874250128865242\n",
      "epoch: 9 step: 835, loss is 0.18071798980236053\n",
      "epoch: 9 step: 836, loss is 0.05311686918139458\n",
      "epoch: 9 step: 837, loss is 0.04221542924642563\n",
      "epoch: 9 step: 838, loss is 0.03687115013599396\n",
      "epoch: 9 step: 839, loss is 0.13589200377464294\n",
      "epoch: 9 step: 840, loss is 0.03935493528842926\n",
      "epoch: 9 step: 841, loss is 0.05112612619996071\n",
      "epoch: 9 step: 842, loss is 0.045423537492752075\n",
      "epoch: 9 step: 843, loss is 0.036293353885412216\n",
      "epoch: 9 step: 844, loss is 0.22249989211559296\n",
      "epoch: 9 step: 845, loss is 0.13211573660373688\n",
      "epoch: 9 step: 846, loss is 0.05620495602488518\n",
      "epoch: 9 step: 847, loss is 0.12943997979164124\n",
      "epoch: 9 step: 848, loss is 0.14246267080307007\n",
      "epoch: 9 step: 849, loss is 0.10273873805999756\n",
      "epoch: 9 step: 850, loss is 0.05126892030239105\n",
      "epoch: 9 step: 851, loss is 0.045725006610155106\n",
      "epoch: 9 step: 852, loss is 0.10055642575025558\n",
      "epoch: 9 step: 853, loss is 0.06665435433387756\n",
      "epoch: 9 step: 854, loss is 0.030219392850995064\n",
      "epoch: 9 step: 855, loss is 0.11543066054582596\n",
      "epoch: 9 step: 856, loss is 0.1101953312754631\n",
      "epoch: 9 step: 857, loss is 0.17455361783504486\n",
      "epoch: 9 step: 858, loss is 0.04116079583764076\n",
      "epoch: 9 step: 859, loss is 0.08635754138231277\n",
      "epoch: 9 step: 860, loss is 0.06023452430963516\n",
      "epoch: 9 step: 861, loss is 0.06601019948720932\n",
      "epoch: 9 step: 862, loss is 0.07618822157382965\n",
      "epoch: 9 step: 863, loss is 0.11333303153514862\n",
      "epoch: 9 step: 864, loss is 0.03648516535758972\n",
      "epoch: 9 step: 865, loss is 0.05495588481426239\n",
      "epoch: 9 step: 866, loss is 0.062075499445199966\n",
      "epoch: 9 step: 867, loss is 0.11979541182518005\n",
      "epoch: 9 step: 868, loss is 0.07993678003549576\n",
      "epoch: 9 step: 869, loss is 0.17209477722644806\n",
      "epoch: 9 step: 870, loss is 0.11347898840904236\n",
      "epoch: 9 step: 871, loss is 0.1594298779964447\n",
      "epoch: 9 step: 872, loss is 0.13228456676006317\n",
      "epoch: 9 step: 873, loss is 0.14984111487865448\n",
      "epoch: 9 step: 874, loss is 0.05591766536235809\n",
      "epoch: 9 step: 875, loss is 0.10241419076919556\n",
      "epoch: 9 step: 876, loss is 0.04121243581175804\n",
      "epoch: 9 step: 877, loss is 0.09376474469900131\n",
      "epoch: 9 step: 878, loss is 0.11044678092002869\n",
      "epoch: 9 step: 879, loss is 0.10406142473220825\n",
      "epoch: 9 step: 880, loss is 0.06205880269408226\n",
      "epoch: 9 step: 881, loss is 0.06339168548583984\n",
      "epoch: 9 step: 882, loss is 0.10923480987548828\n",
      "epoch: 9 step: 883, loss is 0.09480464458465576\n",
      "epoch: 9 step: 884, loss is 0.06741475313901901\n",
      "epoch: 9 step: 885, loss is 0.06044168770313263\n",
      "epoch: 9 step: 886, loss is 0.11538774520158768\n",
      "epoch: 9 step: 887, loss is 0.04853970184922218\n",
      "epoch: 9 step: 888, loss is 0.07935842126607895\n",
      "epoch: 9 step: 889, loss is 0.09615860134363174\n",
      "epoch: 9 step: 890, loss is 0.15093280375003815\n",
      "epoch: 9 step: 891, loss is 0.04130320996046066\n",
      "epoch: 9 step: 892, loss is 0.03794604167342186\n",
      "epoch: 9 step: 893, loss is 0.12282724678516388\n",
      "epoch: 9 step: 894, loss is 0.12962554395198822\n",
      "epoch: 9 step: 895, loss is 0.14493915438652039\n",
      "epoch: 9 step: 896, loss is 0.07539443671703339\n",
      "epoch: 9 step: 897, loss is 0.0716073140501976\n",
      "epoch: 9 step: 898, loss is 0.08599822223186493\n",
      "epoch: 9 step: 899, loss is 0.20543138682842255\n",
      "epoch: 9 step: 900, loss is 0.06982267647981644\n",
      "epoch: 9 step: 901, loss is 0.1709570288658142\n",
      "epoch: 9 step: 902, loss is 0.14373965561389923\n",
      "epoch: 9 step: 903, loss is 0.07667248696088791\n",
      "epoch: 9 step: 904, loss is 0.11348307877779007\n",
      "epoch: 9 step: 905, loss is 0.08381129801273346\n",
      "epoch: 9 step: 906, loss is 0.039174240082502365\n",
      "epoch: 9 step: 907, loss is 0.11992344260215759\n",
      "epoch: 9 step: 908, loss is 0.04734727367758751\n",
      "epoch: 9 step: 909, loss is 0.07717951387166977\n",
      "epoch: 9 step: 910, loss is 0.08106423169374466\n",
      "epoch: 9 step: 911, loss is 0.09259288758039474\n",
      "epoch: 9 step: 912, loss is 0.10696370899677277\n",
      "epoch: 9 step: 913, loss is 0.07592667639255524\n",
      "epoch: 9 step: 914, loss is 0.09625767171382904\n",
      "epoch: 9 step: 915, loss is 0.09178736060857773\n",
      "epoch: 9 step: 916, loss is 0.12030035257339478\n",
      "epoch: 9 step: 917, loss is 0.02493397891521454\n",
      "epoch: 9 step: 918, loss is 0.05055828019976616\n",
      "epoch: 9 step: 919, loss is 0.03420427814126015\n",
      "epoch: 9 step: 920, loss is 0.10887743532657623\n",
      "epoch: 9 step: 921, loss is 0.17660100758075714\n",
      "epoch: 9 step: 922, loss is 0.019552526995539665\n",
      "epoch: 9 step: 923, loss is 0.09157714247703552\n",
      "epoch: 9 step: 924, loss is 0.16220790147781372\n",
      "epoch: 9 step: 925, loss is 0.0355742983520031\n",
      "epoch: 9 step: 926, loss is 0.13739444315433502\n",
      "epoch: 9 step: 927, loss is 0.026666507124900818\n",
      "epoch: 9 step: 928, loss is 0.07675594836473465\n",
      "epoch: 9 step: 929, loss is 0.12190316617488861\n",
      "epoch: 9 step: 930, loss is 0.05758201703429222\n",
      "epoch: 9 step: 931, loss is 0.0685126781463623\n",
      "epoch: 9 step: 932, loss is 0.02162358909845352\n",
      "epoch: 9 step: 933, loss is 0.03424621373414993\n",
      "epoch: 9 step: 934, loss is 0.14918014407157898\n",
      "epoch: 9 step: 935, loss is 0.03700224682688713\n",
      "epoch: 9 step: 936, loss is 0.08273642510175705\n",
      "epoch: 9 step: 937, loss is 0.07909741252660751\n",
      "epoch: 10 step: 1, loss is 0.240911066532135\n",
      "epoch: 10 step: 2, loss is 0.1652461290359497\n",
      "epoch: 10 step: 3, loss is 0.05352216213941574\n",
      "epoch: 10 step: 4, loss is 0.03340703621506691\n",
      "epoch: 10 step: 5, loss is 0.030471570789813995\n",
      "epoch: 10 step: 6, loss is 0.02211812511086464\n",
      "epoch: 10 step: 7, loss is 0.07662533223628998\n",
      "epoch: 10 step: 8, loss is 0.017631670460104942\n",
      "epoch: 10 step: 9, loss is 0.010997422970831394\n",
      "epoch: 10 step: 10, loss is 0.11120018362998962\n",
      "epoch: 10 step: 11, loss is 0.06785103678703308\n",
      "epoch: 10 step: 12, loss is 0.06617840379476547\n",
      "epoch: 10 step: 13, loss is 0.03105405904352665\n",
      "epoch: 10 step: 14, loss is 0.02624957263469696\n",
      "epoch: 10 step: 15, loss is 0.05829090625047684\n",
      "epoch: 10 step: 16, loss is 0.09868847578763962\n",
      "epoch: 10 step: 17, loss is 0.01954406127333641\n",
      "epoch: 10 step: 18, loss is 0.04303782060742378\n",
      "epoch: 10 step: 19, loss is 0.07730168849229813\n",
      "epoch: 10 step: 20, loss is 0.065192811191082\n",
      "epoch: 10 step: 21, loss is 0.049526751041412354\n",
      "epoch: 10 step: 22, loss is 0.10209333896636963\n",
      "epoch: 10 step: 23, loss is 0.14347606897354126\n",
      "epoch: 10 step: 24, loss is 0.08406700193881989\n",
      "epoch: 10 step: 25, loss is 0.09623358398675919\n",
      "epoch: 10 step: 26, loss is 0.013735332526266575\n",
      "epoch: 10 step: 27, loss is 0.024495793506503105\n",
      "epoch: 10 step: 28, loss is 0.012951252050697803\n",
      "epoch: 10 step: 29, loss is 0.09178285300731659\n",
      "epoch: 10 step: 30, loss is 0.03010752610862255\n",
      "epoch: 10 step: 31, loss is 0.10972927510738373\n",
      "epoch: 10 step: 32, loss is 0.019657136872410774\n",
      "epoch: 10 step: 33, loss is 0.05999697744846344\n",
      "epoch: 10 step: 34, loss is 0.042929474264383316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 35, loss is 0.044212691485881805\n",
      "epoch: 10 step: 36, loss is 0.07115833461284637\n",
      "epoch: 10 step: 37, loss is 0.02590850740671158\n",
      "epoch: 10 step: 38, loss is 0.04452231526374817\n",
      "epoch: 10 step: 39, loss is 0.026033001020550728\n",
      "epoch: 10 step: 40, loss is 0.03918585926294327\n",
      "epoch: 10 step: 41, loss is 0.0255954060703516\n",
      "epoch: 10 step: 42, loss is 0.03715462237596512\n",
      "epoch: 10 step: 43, loss is 0.049927182495594025\n",
      "epoch: 10 step: 44, loss is 0.0062920106574893\n",
      "epoch: 10 step: 45, loss is 0.017988938838243484\n",
      "epoch: 10 step: 46, loss is 0.009000664576888084\n",
      "epoch: 10 step: 47, loss is 0.039250731468200684\n",
      "epoch: 10 step: 48, loss is 0.051626045256853104\n",
      "epoch: 10 step: 49, loss is 0.10390050709247589\n",
      "epoch: 10 step: 50, loss is 0.028308603912591934\n",
      "epoch: 10 step: 51, loss is 0.03233665972948074\n",
      "epoch: 10 step: 52, loss is 0.014419635757803917\n",
      "epoch: 10 step: 53, loss is 0.027175458148121834\n",
      "epoch: 10 step: 54, loss is 0.059315118938684464\n",
      "epoch: 10 step: 55, loss is 0.06808391958475113\n",
      "epoch: 10 step: 56, loss is 0.10046815872192383\n",
      "epoch: 10 step: 57, loss is 0.022322380915284157\n",
      "epoch: 10 step: 58, loss is 0.03611406311392784\n",
      "epoch: 10 step: 59, loss is 0.06020351126790047\n",
      "epoch: 10 step: 60, loss is 0.07329966872930527\n",
      "epoch: 10 step: 61, loss is 0.003990649711340666\n",
      "epoch: 10 step: 62, loss is 0.06976243108510971\n",
      "epoch: 10 step: 63, loss is 0.01453211810439825\n",
      "epoch: 10 step: 64, loss is 0.07099144160747528\n",
      "epoch: 10 step: 65, loss is 0.053281452506780624\n",
      "epoch: 10 step: 66, loss is 0.029097970575094223\n",
      "epoch: 10 step: 67, loss is 0.12225554138422012\n",
      "epoch: 10 step: 68, loss is 0.06716258078813553\n",
      "epoch: 10 step: 69, loss is 0.03779907524585724\n",
      "epoch: 10 step: 70, loss is 0.021215008571743965\n",
      "epoch: 10 step: 71, loss is 0.05347362905740738\n",
      "epoch: 10 step: 72, loss is 0.00993412546813488\n",
      "epoch: 10 step: 73, loss is 0.07655216753482819\n",
      "epoch: 10 step: 74, loss is 0.035376038402318954\n",
      "epoch: 10 step: 75, loss is 0.07278070598840714\n",
      "epoch: 10 step: 76, loss is 0.07234181463718414\n",
      "epoch: 10 step: 77, loss is 0.03809058666229248\n",
      "epoch: 10 step: 78, loss is 0.0662657618522644\n",
      "epoch: 10 step: 79, loss is 0.05768289044499397\n",
      "epoch: 10 step: 80, loss is 0.06534811109304428\n",
      "epoch: 10 step: 81, loss is 0.010439621284604073\n",
      "epoch: 10 step: 82, loss is 0.02246047556400299\n",
      "epoch: 10 step: 83, loss is 0.08236318081617355\n",
      "epoch: 10 step: 84, loss is 0.03278101608157158\n",
      "epoch: 10 step: 85, loss is 0.09673574566841125\n",
      "epoch: 10 step: 86, loss is 0.009245742112398148\n",
      "epoch: 10 step: 87, loss is 0.024368710815906525\n",
      "epoch: 10 step: 88, loss is 0.13502925634384155\n",
      "epoch: 10 step: 89, loss is 0.027795961126685143\n",
      "epoch: 10 step: 90, loss is 0.03192475438117981\n",
      "epoch: 10 step: 91, loss is 0.01933269388973713\n",
      "epoch: 10 step: 92, loss is 0.12707364559173584\n",
      "epoch: 10 step: 93, loss is 0.07554508000612259\n",
      "epoch: 10 step: 94, loss is 0.012642077170312405\n",
      "epoch: 10 step: 95, loss is 0.10406573861837387\n",
      "epoch: 10 step: 96, loss is 0.06577471643686295\n",
      "epoch: 10 step: 97, loss is 0.00696446280926466\n",
      "epoch: 10 step: 98, loss is 0.05030090734362602\n",
      "epoch: 10 step: 99, loss is 0.04068892449140549\n",
      "epoch: 10 step: 100, loss is 0.05105232819914818\n",
      "epoch: 10 step: 101, loss is 0.1450253278017044\n",
      "epoch: 10 step: 102, loss is 0.037323370575904846\n",
      "epoch: 10 step: 103, loss is 0.018533123657107353\n",
      "epoch: 10 step: 104, loss is 0.023960620164871216\n",
      "epoch: 10 step: 105, loss is 0.003644611919298768\n",
      "epoch: 10 step: 106, loss is 0.01604735478758812\n",
      "epoch: 10 step: 107, loss is 0.022121474146842957\n",
      "epoch: 10 step: 108, loss is 0.026270287111401558\n",
      "epoch: 10 step: 109, loss is 0.020886987447738647\n",
      "epoch: 10 step: 110, loss is 0.08668382465839386\n",
      "epoch: 10 step: 111, loss is 0.036934349685907364\n",
      "epoch: 10 step: 112, loss is 0.01886690966784954\n",
      "epoch: 10 step: 113, loss is 0.029910914599895477\n",
      "epoch: 10 step: 114, loss is 0.08442750573158264\n",
      "epoch: 10 step: 115, loss is 0.09629824012517929\n",
      "epoch: 10 step: 116, loss is 0.009529666975140572\n",
      "epoch: 10 step: 117, loss is 0.0787479430437088\n",
      "epoch: 10 step: 118, loss is 0.0719684436917305\n",
      "epoch: 10 step: 119, loss is 0.010688969865441322\n",
      "epoch: 10 step: 120, loss is 0.009402129799127579\n",
      "epoch: 10 step: 121, loss is 0.01717735454440117\n",
      "epoch: 10 step: 122, loss is 0.03704014793038368\n",
      "epoch: 10 step: 123, loss is 0.11202845722436905\n",
      "epoch: 10 step: 124, loss is 0.04832472652196884\n",
      "epoch: 10 step: 125, loss is 0.03606143593788147\n",
      "epoch: 10 step: 126, loss is 0.10254696011543274\n",
      "epoch: 10 step: 127, loss is 0.0959927961230278\n",
      "epoch: 10 step: 128, loss is 0.02902272529900074\n",
      "epoch: 10 step: 129, loss is 0.0517345555126667\n",
      "epoch: 10 step: 130, loss is 0.015521981753408909\n",
      "epoch: 10 step: 131, loss is 0.020495671778917313\n",
      "epoch: 10 step: 132, loss is 0.06616485863924026\n",
      "epoch: 10 step: 133, loss is 0.04728517681360245\n",
      "epoch: 10 step: 134, loss is 0.018964190036058426\n",
      "epoch: 10 step: 135, loss is 0.11735475808382034\n",
      "epoch: 10 step: 136, loss is 0.03535165265202522\n",
      "epoch: 10 step: 137, loss is 0.018165215849876404\n",
      "epoch: 10 step: 138, loss is 0.20569631457328796\n",
      "epoch: 10 step: 139, loss is 0.06188022717833519\n",
      "epoch: 10 step: 140, loss is 0.045582279562950134\n",
      "epoch: 10 step: 141, loss is 0.10022324323654175\n",
      "epoch: 10 step: 142, loss is 0.028820741921663284\n",
      "epoch: 10 step: 143, loss is 0.03303590789437294\n",
      "epoch: 10 step: 144, loss is 0.08321115374565125\n",
      "epoch: 10 step: 145, loss is 0.029218332841992378\n",
      "epoch: 10 step: 146, loss is 0.003458756487816572\n",
      "epoch: 10 step: 147, loss is 0.01335193868726492\n",
      "epoch: 10 step: 148, loss is 0.01924596168100834\n",
      "epoch: 10 step: 149, loss is 0.0403815433382988\n",
      "epoch: 10 step: 150, loss is 0.03992883861064911\n",
      "epoch: 10 step: 151, loss is 0.046929098665714264\n",
      "epoch: 10 step: 152, loss is 0.03122439980506897\n",
      "epoch: 10 step: 153, loss is 0.026338467374444008\n",
      "epoch: 10 step: 154, loss is 0.03459451347589493\n",
      "epoch: 10 step: 155, loss is 0.01623290404677391\n",
      "epoch: 10 step: 156, loss is 0.07802564650774002\n",
      "epoch: 10 step: 157, loss is 0.11806845664978027\n",
      "epoch: 10 step: 158, loss is 0.05991646647453308\n",
      "epoch: 10 step: 159, loss is 0.038124166429042816\n",
      "epoch: 10 step: 160, loss is 0.01705068163573742\n",
      "epoch: 10 step: 161, loss is 0.050969406962394714\n",
      "epoch: 10 step: 162, loss is 0.054731324315071106\n",
      "epoch: 10 step: 163, loss is 0.031255315989255905\n",
      "epoch: 10 step: 164, loss is 0.13935492932796478\n",
      "epoch: 10 step: 165, loss is 0.02082410454750061\n",
      "epoch: 10 step: 166, loss is 0.04311415180563927\n",
      "epoch: 10 step: 167, loss is 0.07238893955945969\n",
      "epoch: 10 step: 168, loss is 0.0905999019742012\n",
      "epoch: 10 step: 169, loss is 0.06448264420032501\n",
      "epoch: 10 step: 170, loss is 0.05214344710111618\n",
      "epoch: 10 step: 171, loss is 0.016428612172603607\n",
      "epoch: 10 step: 172, loss is 0.07933687418699265\n",
      "epoch: 10 step: 173, loss is 0.07596294581890106\n",
      "epoch: 10 step: 174, loss is 0.05889458209276199\n",
      "epoch: 10 step: 175, loss is 0.10635170340538025\n",
      "epoch: 10 step: 176, loss is 0.06236735358834267\n",
      "epoch: 10 step: 177, loss is 0.07273347675800323\n",
      "epoch: 10 step: 178, loss is 0.08895310759544373\n",
      "epoch: 10 step: 179, loss is 0.048080574721097946\n",
      "epoch: 10 step: 180, loss is 0.05681956931948662\n",
      "epoch: 10 step: 181, loss is 0.020713526755571365\n",
      "epoch: 10 step: 182, loss is 0.036659955978393555\n",
      "epoch: 10 step: 183, loss is 0.08522836118936539\n",
      "epoch: 10 step: 184, loss is 0.040021080523729324\n",
      "epoch: 10 step: 185, loss is 0.04323916882276535\n",
      "epoch: 10 step: 186, loss is 0.03488987311720848\n",
      "epoch: 10 step: 187, loss is 0.07716365158557892\n",
      "epoch: 10 step: 188, loss is 0.0837421864271164\n",
      "epoch: 10 step: 189, loss is 0.02188795991241932\n",
      "epoch: 10 step: 190, loss is 0.029071561992168427\n",
      "epoch: 10 step: 191, loss is 0.1109403744339943\n",
      "epoch: 10 step: 192, loss is 0.15006226301193237\n",
      "epoch: 10 step: 193, loss is 0.04383166506886482\n",
      "epoch: 10 step: 194, loss is 0.025546038523316383\n",
      "epoch: 10 step: 195, loss is 0.016103556379675865\n",
      "epoch: 10 step: 196, loss is 0.022469477728009224\n",
      "epoch: 10 step: 197, loss is 0.06899890303611755\n",
      "epoch: 10 step: 198, loss is 0.07228857278823853\n",
      "epoch: 10 step: 199, loss is 0.030197812244296074\n",
      "epoch: 10 step: 200, loss is 0.10514239966869354\n",
      "epoch: 10 step: 201, loss is 0.10422766208648682\n",
      "epoch: 10 step: 202, loss is 0.12051592767238617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 203, loss is 0.11835820227861404\n",
      "epoch: 10 step: 204, loss is 0.047549743205308914\n",
      "epoch: 10 step: 205, loss is 0.06119261682033539\n",
      "epoch: 10 step: 206, loss is 0.0524771623313427\n",
      "epoch: 10 step: 207, loss is 0.02495899796485901\n",
      "epoch: 10 step: 208, loss is 0.06275603920221329\n",
      "epoch: 10 step: 209, loss is 0.015944551676511765\n",
      "epoch: 10 step: 210, loss is 0.1329827606678009\n",
      "epoch: 10 step: 211, loss is 0.007162204012274742\n",
      "epoch: 10 step: 212, loss is 0.10886703431606293\n",
      "epoch: 10 step: 213, loss is 0.14462336897850037\n",
      "epoch: 10 step: 214, loss is 0.027621811255812645\n",
      "epoch: 10 step: 215, loss is 0.09797696024179459\n",
      "epoch: 10 step: 216, loss is 0.12217123806476593\n",
      "epoch: 10 step: 217, loss is 0.021679135039448738\n",
      "epoch: 10 step: 218, loss is 0.016584737226366997\n",
      "epoch: 10 step: 219, loss is 0.05074133351445198\n",
      "epoch: 10 step: 220, loss is 0.06840372085571289\n",
      "epoch: 10 step: 221, loss is 0.11034786701202393\n",
      "epoch: 10 step: 222, loss is 0.08649047464132309\n",
      "epoch: 10 step: 223, loss is 0.026700424030423164\n",
      "epoch: 10 step: 224, loss is 0.10078065097332001\n",
      "epoch: 10 step: 225, loss is 0.08500462025403976\n",
      "epoch: 10 step: 226, loss is 0.16867075860500336\n",
      "epoch: 10 step: 227, loss is 0.05632970854640007\n",
      "epoch: 10 step: 228, loss is 0.06544715166091919\n",
      "epoch: 10 step: 229, loss is 0.139496847987175\n",
      "epoch: 10 step: 230, loss is 0.011167636141180992\n",
      "epoch: 10 step: 231, loss is 0.058375515043735504\n",
      "epoch: 10 step: 232, loss is 0.10898663848638535\n",
      "epoch: 10 step: 233, loss is 0.11155924946069717\n",
      "epoch: 10 step: 234, loss is 0.013557997532188892\n",
      "epoch: 10 step: 235, loss is 0.03707621619105339\n",
      "epoch: 10 step: 236, loss is 0.05857841297984123\n",
      "epoch: 10 step: 237, loss is 0.08776725083589554\n",
      "epoch: 10 step: 238, loss is 0.058415502309799194\n",
      "epoch: 10 step: 239, loss is 0.054967064410448074\n",
      "epoch: 10 step: 240, loss is 0.10548621416091919\n",
      "epoch: 10 step: 241, loss is 0.050069697201251984\n",
      "epoch: 10 step: 242, loss is 0.060611266642808914\n",
      "epoch: 10 step: 243, loss is 0.04182370379567146\n",
      "epoch: 10 step: 244, loss is 0.02395673654973507\n",
      "epoch: 10 step: 245, loss is 0.019677789881825447\n",
      "epoch: 10 step: 246, loss is 0.1384468823671341\n",
      "epoch: 10 step: 247, loss is 0.17907853424549103\n",
      "epoch: 10 step: 248, loss is 0.010601918213069439\n",
      "epoch: 10 step: 249, loss is 0.15669235587120056\n",
      "epoch: 10 step: 250, loss is 0.044590242207050323\n",
      "epoch: 10 step: 251, loss is 0.02134917490184307\n",
      "epoch: 10 step: 252, loss is 0.10242198407649994\n",
      "epoch: 10 step: 253, loss is 0.07293612509965897\n",
      "epoch: 10 step: 254, loss is 0.12162873148918152\n",
      "epoch: 10 step: 255, loss is 0.06555630266666412\n",
      "epoch: 10 step: 256, loss is 0.13259364664554596\n",
      "epoch: 10 step: 257, loss is 0.08043873310089111\n",
      "epoch: 10 step: 258, loss is 0.014787820167839527\n",
      "epoch: 10 step: 259, loss is 0.17796598374843597\n",
      "epoch: 10 step: 260, loss is 0.12001720815896988\n",
      "epoch: 10 step: 261, loss is 0.035610929131507874\n",
      "epoch: 10 step: 262, loss is 0.03407975286245346\n",
      "epoch: 10 step: 263, loss is 0.06203366816043854\n",
      "epoch: 10 step: 264, loss is 0.04268593713641167\n",
      "epoch: 10 step: 265, loss is 0.03875407576560974\n",
      "epoch: 10 step: 266, loss is 0.019616447389125824\n",
      "epoch: 10 step: 267, loss is 0.10900673270225525\n",
      "epoch: 10 step: 268, loss is 0.07107150554656982\n",
      "epoch: 10 step: 269, loss is 0.1376902163028717\n",
      "epoch: 10 step: 270, loss is 0.07115676999092102\n",
      "epoch: 10 step: 271, loss is 0.029854850843548775\n",
      "epoch: 10 step: 272, loss is 0.08654177188873291\n",
      "epoch: 10 step: 273, loss is 0.034689001739025116\n",
      "epoch: 10 step: 274, loss is 0.04291578009724617\n",
      "epoch: 10 step: 275, loss is 0.0592423677444458\n",
      "epoch: 10 step: 276, loss is 0.08237074315547943\n",
      "epoch: 10 step: 277, loss is 0.0443815141916275\n",
      "epoch: 10 step: 278, loss is 0.048452965915203094\n",
      "epoch: 10 step: 279, loss is 0.06679777801036835\n",
      "epoch: 10 step: 280, loss is 0.046979740262031555\n",
      "epoch: 10 step: 281, loss is 0.00877158809453249\n",
      "epoch: 10 step: 282, loss is 0.0318475104868412\n",
      "epoch: 10 step: 283, loss is 0.12134228646755219\n",
      "epoch: 10 step: 284, loss is 0.1349683403968811\n",
      "epoch: 10 step: 285, loss is 0.07128952443599701\n",
      "epoch: 10 step: 286, loss is 0.038285210728645325\n",
      "epoch: 10 step: 287, loss is 0.12716872990131378\n",
      "epoch: 10 step: 288, loss is 0.08292387425899506\n",
      "epoch: 10 step: 289, loss is 0.05257616937160492\n",
      "epoch: 10 step: 290, loss is 0.06418488174676895\n",
      "epoch: 10 step: 291, loss is 0.016304844990372658\n",
      "epoch: 10 step: 292, loss is 0.1593819558620453\n",
      "epoch: 10 step: 293, loss is 0.10087881982326508\n",
      "epoch: 10 step: 294, loss is 0.1936010867357254\n",
      "epoch: 10 step: 295, loss is 0.0654510110616684\n",
      "epoch: 10 step: 296, loss is 0.09365855902433395\n",
      "epoch: 10 step: 297, loss is 0.11989303678274155\n",
      "epoch: 10 step: 298, loss is 0.016084862872958183\n",
      "epoch: 10 step: 299, loss is 0.039898402988910675\n",
      "epoch: 10 step: 300, loss is 0.0771881714463234\n",
      "epoch: 10 step: 301, loss is 0.027278516441583633\n",
      "epoch: 10 step: 302, loss is 0.048425689339637756\n",
      "epoch: 10 step: 303, loss is 0.0445939302444458\n",
      "epoch: 10 step: 304, loss is 0.04400711879134178\n",
      "epoch: 10 step: 305, loss is 0.03700203448534012\n",
      "epoch: 10 step: 306, loss is 0.07243087142705917\n",
      "epoch: 10 step: 307, loss is 0.05992778390645981\n",
      "epoch: 10 step: 308, loss is 0.056106939911842346\n",
      "epoch: 10 step: 309, loss is 0.05544694513082504\n",
      "epoch: 10 step: 310, loss is 0.03951845318078995\n",
      "epoch: 10 step: 311, loss is 0.025476278737187386\n",
      "epoch: 10 step: 312, loss is 0.040102362632751465\n",
      "epoch: 10 step: 313, loss is 0.06487847119569778\n",
      "epoch: 10 step: 314, loss is 0.10755755752325058\n",
      "epoch: 10 step: 315, loss is 0.02135033719241619\n",
      "epoch: 10 step: 316, loss is 0.09972180426120758\n",
      "epoch: 10 step: 317, loss is 0.13960683345794678\n",
      "epoch: 10 step: 318, loss is 0.022590292617678642\n",
      "epoch: 10 step: 319, loss is 0.12552890181541443\n",
      "epoch: 10 step: 320, loss is 0.06112360209226608\n",
      "epoch: 10 step: 321, loss is 0.09199219942092896\n",
      "epoch: 10 step: 322, loss is 0.17335902154445648\n",
      "epoch: 10 step: 323, loss is 0.012101492844522\n",
      "epoch: 10 step: 324, loss is 0.0716722160577774\n",
      "epoch: 10 step: 325, loss is 0.0855056494474411\n",
      "epoch: 10 step: 326, loss is 0.05594758689403534\n",
      "epoch: 10 step: 327, loss is 0.0678262710571289\n",
      "epoch: 10 step: 328, loss is 0.02546844817698002\n",
      "epoch: 10 step: 329, loss is 0.07291979342699051\n",
      "epoch: 10 step: 330, loss is 0.03125270828604698\n",
      "epoch: 10 step: 331, loss is 0.07345061004161835\n",
      "epoch: 10 step: 332, loss is 0.04414290189743042\n",
      "epoch: 10 step: 333, loss is 0.020650872960686684\n",
      "epoch: 10 step: 334, loss is 0.026975423097610474\n",
      "epoch: 10 step: 335, loss is 0.04468892514705658\n",
      "epoch: 10 step: 336, loss is 0.02663639560341835\n",
      "epoch: 10 step: 337, loss is 0.09659036993980408\n",
      "epoch: 10 step: 338, loss is 0.01141276489943266\n",
      "epoch: 10 step: 339, loss is 0.06394980847835541\n",
      "epoch: 10 step: 340, loss is 0.05318897217512131\n",
      "epoch: 10 step: 341, loss is 0.006037332583218813\n",
      "epoch: 10 step: 342, loss is 0.017836015671491623\n",
      "epoch: 10 step: 343, loss is 0.1603413224220276\n",
      "epoch: 10 step: 344, loss is 0.026489535346627235\n",
      "epoch: 10 step: 345, loss is 0.037714917212724686\n",
      "epoch: 10 step: 346, loss is 0.02102266624569893\n",
      "epoch: 10 step: 347, loss is 0.043928924947977066\n",
      "epoch: 10 step: 348, loss is 0.05544465780258179\n",
      "epoch: 10 step: 349, loss is 0.015843171626329422\n",
      "epoch: 10 step: 350, loss is 0.034058719873428345\n",
      "epoch: 10 step: 351, loss is 0.08939214795827866\n",
      "epoch: 10 step: 352, loss is 0.023601356893777847\n",
      "epoch: 10 step: 353, loss is 0.008335535414516926\n",
      "epoch: 10 step: 354, loss is 0.05851825699210167\n",
      "epoch: 10 step: 355, loss is 0.0853518396615982\n",
      "epoch: 10 step: 356, loss is 0.04504197835922241\n",
      "epoch: 10 step: 357, loss is 0.030416149646043777\n",
      "epoch: 10 step: 358, loss is 0.049316443502902985\n",
      "epoch: 10 step: 359, loss is 0.02686278708279133\n",
      "epoch: 10 step: 360, loss is 0.042515113949775696\n",
      "epoch: 10 step: 361, loss is 0.08129386603832245\n",
      "epoch: 10 step: 362, loss is 0.009723106399178505\n",
      "epoch: 10 step: 363, loss is 0.05646473169326782\n",
      "epoch: 10 step: 364, loss is 0.12891602516174316\n",
      "epoch: 10 step: 365, loss is 0.04339855909347534\n",
      "epoch: 10 step: 366, loss is 0.08177565783262253\n",
      "epoch: 10 step: 367, loss is 0.04936571419239044\n",
      "epoch: 10 step: 368, loss is 0.06050412356853485\n",
      "epoch: 10 step: 369, loss is 0.18925441801548004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 370, loss is 0.04592113569378853\n",
      "epoch: 10 step: 371, loss is 0.04259975627064705\n",
      "epoch: 10 step: 372, loss is 0.061936795711517334\n",
      "epoch: 10 step: 373, loss is 0.024400636553764343\n",
      "epoch: 10 step: 374, loss is 0.07024331390857697\n",
      "epoch: 10 step: 375, loss is 0.0513797290623188\n",
      "epoch: 10 step: 376, loss is 0.018219467252492905\n",
      "epoch: 10 step: 377, loss is 0.07527747005224228\n",
      "epoch: 10 step: 378, loss is 0.028706442564725876\n",
      "epoch: 10 step: 379, loss is 0.012965991161763668\n",
      "epoch: 10 step: 380, loss is 0.02521096169948578\n",
      "epoch: 10 step: 381, loss is 0.14664122462272644\n",
      "epoch: 10 step: 382, loss is 0.13783937692642212\n",
      "epoch: 10 step: 383, loss is 0.03311675041913986\n",
      "epoch: 10 step: 384, loss is 0.024805326014757156\n",
      "epoch: 10 step: 385, loss is 0.06199715659022331\n",
      "epoch: 10 step: 386, loss is 0.017827047035098076\n",
      "epoch: 10 step: 387, loss is 0.04198535159230232\n",
      "epoch: 10 step: 388, loss is 0.03213854506611824\n",
      "epoch: 10 step: 389, loss is 0.035052940249443054\n",
      "epoch: 10 step: 390, loss is 0.05156631022691727\n",
      "epoch: 10 step: 391, loss is 0.1719086766242981\n",
      "epoch: 10 step: 392, loss is 0.038763321936130524\n",
      "epoch: 10 step: 393, loss is 0.11643026769161224\n",
      "epoch: 10 step: 394, loss is 0.02784455567598343\n",
      "epoch: 10 step: 395, loss is 0.033635709434747696\n",
      "epoch: 10 step: 396, loss is 0.08167531341314316\n",
      "epoch: 10 step: 397, loss is 0.03333297371864319\n",
      "epoch: 10 step: 398, loss is 0.026642922312021255\n",
      "epoch: 10 step: 399, loss is 0.04069596156477928\n",
      "epoch: 10 step: 400, loss is 0.10127218067646027\n",
      "epoch: 10 step: 401, loss is 0.18989267945289612\n",
      "epoch: 10 step: 402, loss is 0.08444732427597046\n",
      "epoch: 10 step: 403, loss is 0.05009813234210014\n",
      "epoch: 10 step: 404, loss is 0.04144968092441559\n",
      "epoch: 10 step: 405, loss is 0.07976823300123215\n",
      "epoch: 10 step: 406, loss is 0.049628451466560364\n",
      "epoch: 10 step: 407, loss is 0.1442805826663971\n",
      "epoch: 10 step: 408, loss is 0.07105396687984467\n",
      "epoch: 10 step: 409, loss is 0.03615092858672142\n",
      "epoch: 10 step: 410, loss is 0.01447365153580904\n",
      "epoch: 10 step: 411, loss is 0.0257547777146101\n",
      "epoch: 10 step: 412, loss is 0.1296619474887848\n",
      "epoch: 10 step: 413, loss is 0.06208161637187004\n",
      "epoch: 10 step: 414, loss is 0.017215343192219734\n",
      "epoch: 10 step: 415, loss is 0.05463944002985954\n",
      "epoch: 10 step: 416, loss is 0.029933232814073563\n",
      "epoch: 10 step: 417, loss is 0.05942102149128914\n",
      "epoch: 10 step: 418, loss is 0.028070319443941116\n",
      "epoch: 10 step: 419, loss is 0.01781832054257393\n",
      "epoch: 10 step: 420, loss is 0.05117904767394066\n",
      "epoch: 10 step: 421, loss is 0.01677071675658226\n",
      "epoch: 10 step: 422, loss is 0.13103936612606049\n",
      "epoch: 10 step: 423, loss is 0.05398455634713173\n",
      "epoch: 10 step: 424, loss is 0.03651745244860649\n",
      "epoch: 10 step: 425, loss is 0.034049637615680695\n",
      "epoch: 10 step: 426, loss is 0.04467755928635597\n",
      "epoch: 10 step: 427, loss is 0.018822556361556053\n",
      "epoch: 10 step: 428, loss is 0.021872779354453087\n",
      "epoch: 10 step: 429, loss is 0.0967547744512558\n",
      "epoch: 10 step: 430, loss is 0.07229095697402954\n",
      "epoch: 10 step: 431, loss is 0.01774843968451023\n",
      "epoch: 10 step: 432, loss is 0.07281256467103958\n",
      "epoch: 10 step: 433, loss is 0.0031777499243617058\n",
      "epoch: 10 step: 434, loss is 0.020966043695807457\n",
      "epoch: 10 step: 435, loss is 0.021366246044635773\n",
      "epoch: 10 step: 436, loss is 0.030880441889166832\n",
      "epoch: 10 step: 437, loss is 0.03577186539769173\n",
      "epoch: 10 step: 438, loss is 0.023006970062851906\n",
      "epoch: 10 step: 439, loss is 0.16814808547496796\n",
      "epoch: 10 step: 440, loss is 0.054375872015953064\n",
      "epoch: 10 step: 441, loss is 0.1081315204501152\n",
      "epoch: 10 step: 442, loss is 0.02136574126780033\n",
      "epoch: 10 step: 443, loss is 0.015344018116593361\n",
      "epoch: 10 step: 444, loss is 0.01001657173037529\n",
      "epoch: 10 step: 445, loss is 0.1144275963306427\n",
      "epoch: 10 step: 446, loss is 0.07828734070062637\n",
      "epoch: 10 step: 447, loss is 0.04965344816446304\n",
      "epoch: 10 step: 448, loss is 0.03600619360804558\n",
      "epoch: 10 step: 449, loss is 0.06148882582783699\n",
      "epoch: 10 step: 450, loss is 0.01890486106276512\n",
      "epoch: 10 step: 451, loss is 0.03374244645237923\n",
      "epoch: 10 step: 452, loss is 0.07787396758794785\n",
      "epoch: 10 step: 453, loss is 0.0673217847943306\n",
      "epoch: 10 step: 454, loss is 0.02896871045231819\n",
      "epoch: 10 step: 455, loss is 0.08610304445028305\n",
      "epoch: 10 step: 456, loss is 0.06509260088205338\n",
      "epoch: 10 step: 457, loss is 0.14570942521095276\n",
      "epoch: 10 step: 458, loss is 0.046712275594472885\n",
      "epoch: 10 step: 459, loss is 0.05116139352321625\n",
      "epoch: 10 step: 460, loss is 0.03677070513367653\n",
      "epoch: 10 step: 461, loss is 0.03887615725398064\n",
      "epoch: 10 step: 462, loss is 0.04839317500591278\n",
      "epoch: 10 step: 463, loss is 0.016106929630041122\n",
      "epoch: 10 step: 464, loss is 0.009991544298827648\n",
      "epoch: 10 step: 465, loss is 0.04863504320383072\n",
      "epoch: 10 step: 466, loss is 0.06047983840107918\n",
      "epoch: 10 step: 467, loss is 0.03434881940484047\n",
      "epoch: 10 step: 468, loss is 0.029938703402876854\n",
      "epoch: 10 step: 469, loss is 0.06331313401460648\n",
      "epoch: 10 step: 470, loss is 0.02242313139140606\n",
      "epoch: 10 step: 471, loss is 0.11344277113676071\n",
      "epoch: 10 step: 472, loss is 0.004141750279814005\n",
      "epoch: 10 step: 473, loss is 0.04256081581115723\n",
      "epoch: 10 step: 474, loss is 0.02627725899219513\n",
      "epoch: 10 step: 475, loss is 0.053888142108917236\n",
      "epoch: 10 step: 476, loss is 0.02631540782749653\n",
      "epoch: 10 step: 477, loss is 0.057414714246988297\n",
      "epoch: 10 step: 478, loss is 0.06280886381864548\n",
      "epoch: 10 step: 479, loss is 0.01854556053876877\n",
      "epoch: 10 step: 480, loss is 0.0729239284992218\n",
      "epoch: 10 step: 481, loss is 0.1093682050704956\n",
      "epoch: 10 step: 482, loss is 0.23171928524971008\n",
      "epoch: 10 step: 483, loss is 0.13489067554473877\n",
      "epoch: 10 step: 484, loss is 0.025530854240059853\n",
      "epoch: 10 step: 485, loss is 0.07721880078315735\n",
      "epoch: 10 step: 486, loss is 0.027495237067341805\n",
      "epoch: 10 step: 487, loss is 0.02320091240108013\n",
      "epoch: 10 step: 488, loss is 0.05840558186173439\n",
      "epoch: 10 step: 489, loss is 0.10406619310379028\n",
      "epoch: 10 step: 490, loss is 0.1075432226061821\n",
      "epoch: 10 step: 491, loss is 0.04022056236863136\n",
      "epoch: 10 step: 492, loss is 0.05745614692568779\n",
      "epoch: 10 step: 493, loss is 0.026307182386517525\n",
      "epoch: 10 step: 494, loss is 0.023651963099837303\n",
      "epoch: 10 step: 495, loss is 0.1690821647644043\n",
      "epoch: 10 step: 496, loss is 0.05722498148679733\n",
      "epoch: 10 step: 497, loss is 0.03364364802837372\n",
      "epoch: 10 step: 498, loss is 0.07977966964244843\n",
      "epoch: 10 step: 499, loss is 0.061205800622701645\n",
      "epoch: 10 step: 500, loss is 0.02355029433965683\n",
      "epoch: 10 step: 501, loss is 0.02763555198907852\n",
      "epoch: 10 step: 502, loss is 0.024735793471336365\n",
      "epoch: 10 step: 503, loss is 0.08042766898870468\n",
      "epoch: 10 step: 504, loss is 0.076671302318573\n",
      "epoch: 10 step: 505, loss is 0.04647984728217125\n",
      "epoch: 10 step: 506, loss is 0.03631539270281792\n",
      "epoch: 10 step: 507, loss is 0.006700417958199978\n",
      "epoch: 10 step: 508, loss is 0.08858147263526917\n",
      "epoch: 10 step: 509, loss is 0.007112520281225443\n",
      "epoch: 10 step: 510, loss is 0.004216951318085194\n",
      "epoch: 10 step: 511, loss is 0.04014051333069801\n",
      "epoch: 10 step: 512, loss is 0.0602385587990284\n",
      "epoch: 10 step: 513, loss is 0.07799620926380157\n",
      "epoch: 10 step: 514, loss is 0.05535838380455971\n",
      "epoch: 10 step: 515, loss is 0.09837528318166733\n",
      "epoch: 10 step: 516, loss is 0.0320298857986927\n",
      "epoch: 10 step: 517, loss is 0.025372503325343132\n",
      "epoch: 10 step: 518, loss is 0.06643287092447281\n",
      "epoch: 10 step: 519, loss is 0.020272137597203255\n",
      "epoch: 10 step: 520, loss is 0.03351105749607086\n",
      "epoch: 10 step: 521, loss is 0.20557157695293427\n",
      "epoch: 10 step: 522, loss is 0.03745567426085472\n",
      "epoch: 10 step: 523, loss is 0.06049659103155136\n",
      "epoch: 10 step: 524, loss is 0.09350046515464783\n",
      "epoch: 10 step: 525, loss is 0.03650388866662979\n",
      "epoch: 10 step: 526, loss is 0.05353045091032982\n",
      "epoch: 10 step: 527, loss is 0.031100377440452576\n",
      "epoch: 10 step: 528, loss is 0.09036456793546677\n",
      "epoch: 10 step: 529, loss is 0.017702437937259674\n",
      "epoch: 10 step: 530, loss is 0.05776756629347801\n",
      "epoch: 10 step: 531, loss is 0.048202551901340485\n",
      "epoch: 10 step: 532, loss is 0.0774158239364624\n",
      "epoch: 10 step: 533, loss is 0.06962981075048447\n",
      "epoch: 10 step: 534, loss is 0.030705220997333527\n",
      "epoch: 10 step: 535, loss is 0.084110789000988\n",
      "epoch: 10 step: 536, loss is 0.04580167308449745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 537, loss is 0.1884618103504181\n",
      "epoch: 10 step: 538, loss is 0.03854280337691307\n",
      "epoch: 10 step: 539, loss is 0.11129989475011826\n",
      "epoch: 10 step: 540, loss is 0.07050690054893494\n",
      "epoch: 10 step: 541, loss is 0.05226555094122887\n",
      "epoch: 10 step: 542, loss is 0.04187706485390663\n",
      "epoch: 10 step: 543, loss is 0.045643821358680725\n",
      "epoch: 10 step: 544, loss is 0.16323503851890564\n",
      "epoch: 10 step: 545, loss is 0.07499884814023972\n",
      "epoch: 10 step: 546, loss is 0.12590409815311432\n",
      "epoch: 10 step: 547, loss is 0.10262846946716309\n",
      "epoch: 10 step: 548, loss is 0.043533213436603546\n",
      "epoch: 10 step: 549, loss is 0.04364746809005737\n",
      "epoch: 10 step: 550, loss is 0.05116133764386177\n",
      "epoch: 10 step: 551, loss is 0.10725229233503342\n",
      "epoch: 10 step: 552, loss is 0.02825678326189518\n",
      "epoch: 10 step: 553, loss is 0.03895564004778862\n",
      "epoch: 10 step: 554, loss is 0.05921977013349533\n",
      "epoch: 10 step: 555, loss is 0.059587884694337845\n",
      "epoch: 10 step: 556, loss is 0.04088134318590164\n",
      "epoch: 10 step: 557, loss is 0.08083856850862503\n",
      "epoch: 10 step: 558, loss is 0.09289254248142242\n",
      "epoch: 10 step: 559, loss is 0.015996739268302917\n",
      "epoch: 10 step: 560, loss is 0.04860479012131691\n",
      "epoch: 10 step: 561, loss is 0.053311169147491455\n",
      "epoch: 10 step: 562, loss is 0.09086530655622482\n",
      "epoch: 10 step: 563, loss is 0.02767714485526085\n",
      "epoch: 10 step: 564, loss is 0.04219052195549011\n",
      "epoch: 10 step: 565, loss is 0.01851547509431839\n",
      "epoch: 10 step: 566, loss is 0.033408068120479584\n",
      "epoch: 10 step: 567, loss is 0.03362783044576645\n",
      "epoch: 10 step: 568, loss is 0.013002835214138031\n",
      "epoch: 10 step: 569, loss is 0.12141022831201553\n",
      "epoch: 10 step: 570, loss is 0.03785146400332451\n",
      "epoch: 10 step: 571, loss is 0.05551605671644211\n",
      "epoch: 10 step: 572, loss is 0.0947771966457367\n",
      "epoch: 10 step: 573, loss is 0.1747206747531891\n",
      "epoch: 10 step: 574, loss is 0.08157312124967575\n",
      "epoch: 10 step: 575, loss is 0.041727423667907715\n",
      "epoch: 10 step: 576, loss is 0.042080461978912354\n",
      "epoch: 10 step: 577, loss is 0.04521254822611809\n",
      "epoch: 10 step: 578, loss is 0.17213629186153412\n",
      "epoch: 10 step: 579, loss is 0.07395578175783157\n",
      "epoch: 10 step: 580, loss is 0.07730983197689056\n",
      "epoch: 10 step: 581, loss is 0.04698091000318527\n",
      "epoch: 10 step: 582, loss is 0.0811830535531044\n",
      "epoch: 10 step: 583, loss is 0.1051398441195488\n",
      "epoch: 10 step: 584, loss is 0.08890902251005173\n",
      "epoch: 10 step: 585, loss is 0.04863027483224869\n",
      "epoch: 10 step: 586, loss is 0.14871467649936676\n",
      "epoch: 10 step: 587, loss is 0.14191964268684387\n",
      "epoch: 10 step: 588, loss is 0.07307086139917374\n",
      "epoch: 10 step: 589, loss is 0.020691905170679092\n",
      "epoch: 10 step: 590, loss is 0.09087126702070236\n",
      "epoch: 10 step: 591, loss is 0.024516109377145767\n",
      "epoch: 10 step: 592, loss is 0.03232760354876518\n",
      "epoch: 10 step: 593, loss is 0.009864999912679195\n",
      "epoch: 10 step: 594, loss is 0.07158014923334122\n",
      "epoch: 10 step: 595, loss is 0.023715076968073845\n",
      "epoch: 10 step: 596, loss is 0.04478641226887703\n",
      "epoch: 10 step: 597, loss is 0.051035139709711075\n",
      "epoch: 10 step: 598, loss is 0.015869922935962677\n",
      "epoch: 10 step: 599, loss is 0.05489373207092285\n",
      "epoch: 10 step: 600, loss is 0.08388907462358475\n",
      "epoch: 10 step: 601, loss is 0.04245211184024811\n",
      "epoch: 10 step: 602, loss is 0.039694830775260925\n",
      "epoch: 10 step: 603, loss is 0.061146385967731476\n",
      "epoch: 10 step: 604, loss is 0.0732286274433136\n",
      "epoch: 10 step: 605, loss is 0.038478005677461624\n",
      "epoch: 10 step: 606, loss is 0.07174871861934662\n",
      "epoch: 10 step: 607, loss is 0.01624692790210247\n",
      "epoch: 10 step: 608, loss is 0.05448753759264946\n",
      "epoch: 10 step: 609, loss is 0.034008391201496124\n",
      "epoch: 10 step: 610, loss is 0.1565125286579132\n",
      "epoch: 10 step: 611, loss is 0.03832840174436569\n",
      "epoch: 10 step: 612, loss is 0.04950287193059921\n",
      "epoch: 10 step: 613, loss is 0.05415552854537964\n",
      "epoch: 10 step: 614, loss is 0.01270376332104206\n",
      "epoch: 10 step: 615, loss is 0.012349944561719894\n",
      "epoch: 10 step: 616, loss is 0.014598448760807514\n",
      "epoch: 10 step: 617, loss is 0.06501338630914688\n",
      "epoch: 10 step: 618, loss is 0.06164310500025749\n",
      "epoch: 10 step: 619, loss is 0.2059939056634903\n",
      "epoch: 10 step: 620, loss is 0.027936195954680443\n",
      "epoch: 10 step: 621, loss is 0.015886886045336723\n",
      "epoch: 10 step: 622, loss is 0.0517602302134037\n",
      "epoch: 10 step: 623, loss is 0.07444757968187332\n",
      "epoch: 10 step: 624, loss is 0.05625135451555252\n",
      "epoch: 10 step: 625, loss is 0.004305820446461439\n",
      "epoch: 10 step: 626, loss is 0.05020441859960556\n",
      "epoch: 10 step: 627, loss is 0.0572148896753788\n",
      "epoch: 10 step: 628, loss is 0.18358802795410156\n",
      "epoch: 10 step: 629, loss is 0.007661536335945129\n",
      "epoch: 10 step: 630, loss is 0.10129759460687637\n",
      "epoch: 10 step: 631, loss is 0.034696314483881\n",
      "epoch: 10 step: 632, loss is 0.09928401559591293\n",
      "epoch: 10 step: 633, loss is 0.05391668528318405\n",
      "epoch: 10 step: 634, loss is 0.1424759179353714\n",
      "epoch: 10 step: 635, loss is 0.12690681219100952\n",
      "epoch: 10 step: 636, loss is 0.13533943891525269\n",
      "epoch: 10 step: 637, loss is 0.04911399632692337\n",
      "epoch: 10 step: 638, loss is 0.015777191147208214\n",
      "epoch: 10 step: 639, loss is 0.22933697700500488\n",
      "epoch: 10 step: 640, loss is 0.03692423179745674\n",
      "epoch: 10 step: 641, loss is 0.04906187206506729\n",
      "epoch: 10 step: 642, loss is 0.01731533743441105\n",
      "epoch: 10 step: 643, loss is 0.011702784337103367\n",
      "epoch: 10 step: 644, loss is 0.03958756849169731\n",
      "epoch: 10 step: 645, loss is 0.017907289788126945\n",
      "epoch: 10 step: 646, loss is 0.04204638674855232\n",
      "epoch: 10 step: 647, loss is 0.08250223845243454\n",
      "epoch: 10 step: 648, loss is 0.08236262202262878\n",
      "epoch: 10 step: 649, loss is 0.10084154456853867\n",
      "epoch: 10 step: 650, loss is 0.01478962879627943\n",
      "epoch: 10 step: 651, loss is 0.07459980994462967\n",
      "epoch: 10 step: 652, loss is 0.07925217598676682\n",
      "epoch: 10 step: 653, loss is 0.1537550389766693\n",
      "epoch: 10 step: 654, loss is 0.03588208183646202\n",
      "epoch: 10 step: 655, loss is 0.25256800651550293\n",
      "epoch: 10 step: 656, loss is 0.039006490260362625\n",
      "epoch: 10 step: 657, loss is 0.07128634303808212\n",
      "epoch: 10 step: 658, loss is 0.05667664110660553\n",
      "epoch: 10 step: 659, loss is 0.053444184362888336\n",
      "epoch: 10 step: 660, loss is 0.0527820847928524\n",
      "epoch: 10 step: 661, loss is 0.06134442612528801\n",
      "epoch: 10 step: 662, loss is 0.004842834547162056\n",
      "epoch: 10 step: 663, loss is 0.07985250651836395\n",
      "epoch: 10 step: 664, loss is 0.11209282279014587\n",
      "epoch: 10 step: 665, loss is 0.1269288808107376\n",
      "epoch: 10 step: 666, loss is 0.07734130322933197\n",
      "epoch: 10 step: 667, loss is 0.1065138652920723\n",
      "epoch: 10 step: 668, loss is 0.07102379202842712\n",
      "epoch: 10 step: 669, loss is 0.10544271022081375\n",
      "epoch: 10 step: 670, loss is 0.11848454177379608\n",
      "epoch: 10 step: 671, loss is 0.09013403207063675\n",
      "epoch: 10 step: 672, loss is 0.1474764049053192\n",
      "epoch: 10 step: 673, loss is 0.11933505535125732\n",
      "epoch: 10 step: 674, loss is 0.032109927386045456\n",
      "epoch: 10 step: 675, loss is 0.09419278055429459\n",
      "epoch: 10 step: 676, loss is 0.04506290704011917\n",
      "epoch: 10 step: 677, loss is 0.01545287948101759\n",
      "epoch: 10 step: 678, loss is 0.02432381547987461\n",
      "epoch: 10 step: 679, loss is 0.02935328148305416\n",
      "epoch: 10 step: 680, loss is 0.03163020685315132\n",
      "epoch: 10 step: 681, loss is 0.06978730857372284\n",
      "epoch: 10 step: 682, loss is 0.07168585807085037\n",
      "epoch: 10 step: 683, loss is 0.06813237816095352\n",
      "epoch: 10 step: 684, loss is 0.03497602418065071\n",
      "epoch: 10 step: 685, loss is 0.07217114418745041\n",
      "epoch: 10 step: 686, loss is 0.13259214162826538\n",
      "epoch: 10 step: 687, loss is 0.13811485469341278\n",
      "epoch: 10 step: 688, loss is 0.02768092043697834\n",
      "epoch: 10 step: 689, loss is 0.06610247492790222\n",
      "epoch: 10 step: 690, loss is 0.09960128366947174\n",
      "epoch: 10 step: 691, loss is 0.04515545442700386\n",
      "epoch: 10 step: 692, loss is 0.15293514728546143\n",
      "epoch: 10 step: 693, loss is 0.006914070807397366\n",
      "epoch: 10 step: 694, loss is 0.09213922917842865\n",
      "epoch: 10 step: 695, loss is 0.1159549206495285\n",
      "epoch: 10 step: 696, loss is 0.06366328150033951\n",
      "epoch: 10 step: 697, loss is 0.06024365499615669\n",
      "epoch: 10 step: 698, loss is 0.07230734080076218\n",
      "epoch: 10 step: 699, loss is 0.0075138648971915245\n",
      "epoch: 10 step: 700, loss is 0.030268631875514984\n",
      "epoch: 10 step: 701, loss is 0.08538717031478882\n",
      "epoch: 10 step: 702, loss is 0.09489475190639496\n",
      "epoch: 10 step: 703, loss is 0.02649516798555851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 704, loss is 0.018043145537376404\n",
      "epoch: 10 step: 705, loss is 0.07716239988803864\n",
      "epoch: 10 step: 706, loss is 0.13541245460510254\n",
      "epoch: 10 step: 707, loss is 0.005513607524335384\n",
      "epoch: 10 step: 708, loss is 0.07108310610055923\n",
      "epoch: 10 step: 709, loss is 0.04702126979827881\n",
      "epoch: 10 step: 710, loss is 0.0665794089436531\n",
      "epoch: 10 step: 711, loss is 0.10661422461271286\n",
      "epoch: 10 step: 712, loss is 0.11478742957115173\n",
      "epoch: 10 step: 713, loss is 0.08028934895992279\n",
      "epoch: 10 step: 714, loss is 0.06646780669689178\n",
      "epoch: 10 step: 715, loss is 0.14550134539604187\n",
      "epoch: 10 step: 716, loss is 0.017161721363663673\n",
      "epoch: 10 step: 717, loss is 0.025642018765211105\n",
      "epoch: 10 step: 718, loss is 0.024711918085813522\n",
      "epoch: 10 step: 719, loss is 0.06980268657207489\n",
      "epoch: 10 step: 720, loss is 0.13103243708610535\n",
      "epoch: 10 step: 721, loss is 0.1519358605146408\n",
      "epoch: 10 step: 722, loss is 0.09597694873809814\n",
      "epoch: 10 step: 723, loss is 0.026838794350624084\n",
      "epoch: 10 step: 724, loss is 0.08841356635093689\n",
      "epoch: 10 step: 725, loss is 0.1038501039147377\n",
      "epoch: 10 step: 726, loss is 0.01422796305269003\n",
      "epoch: 10 step: 727, loss is 0.017809061333537102\n",
      "epoch: 10 step: 728, loss is 0.0655728206038475\n",
      "epoch: 10 step: 729, loss is 0.027917075902223587\n",
      "epoch: 10 step: 730, loss is 0.031139478087425232\n",
      "epoch: 10 step: 731, loss is 0.09373396635055542\n",
      "epoch: 10 step: 732, loss is 0.04759224131703377\n",
      "epoch: 10 step: 733, loss is 0.11221642047166824\n",
      "epoch: 10 step: 734, loss is 0.026610417291522026\n",
      "epoch: 10 step: 735, loss is 0.036765310913324356\n",
      "epoch: 10 step: 736, loss is 0.009929165244102478\n",
      "epoch: 10 step: 737, loss is 0.10527483373880386\n",
      "epoch: 10 step: 738, loss is 0.050711266696453094\n",
      "epoch: 10 step: 739, loss is 0.02642853558063507\n",
      "epoch: 10 step: 740, loss is 0.08133482933044434\n",
      "epoch: 10 step: 741, loss is 0.06928888708353043\n",
      "epoch: 10 step: 742, loss is 0.022835057228803635\n",
      "epoch: 10 step: 743, loss is 0.06650865823030472\n",
      "epoch: 10 step: 744, loss is 0.09105315059423447\n",
      "epoch: 10 step: 745, loss is 0.019860057160258293\n",
      "epoch: 10 step: 746, loss is 0.09634166210889816\n",
      "epoch: 10 step: 747, loss is 0.0527423731982708\n",
      "epoch: 10 step: 748, loss is 0.025611184537410736\n",
      "epoch: 10 step: 749, loss is 0.03434104844927788\n",
      "epoch: 10 step: 750, loss is 0.11315083503723145\n",
      "epoch: 10 step: 751, loss is 0.08328886330127716\n",
      "epoch: 10 step: 752, loss is 0.17647047340869904\n",
      "epoch: 10 step: 753, loss is 0.10195983946323395\n",
      "epoch: 10 step: 754, loss is 0.04346001520752907\n",
      "epoch: 10 step: 755, loss is 0.10587915778160095\n",
      "epoch: 10 step: 756, loss is 0.1047336757183075\n",
      "epoch: 10 step: 757, loss is 0.04539041221141815\n",
      "epoch: 10 step: 758, loss is 0.08507256209850311\n",
      "epoch: 10 step: 759, loss is 0.015844296663999557\n",
      "epoch: 10 step: 760, loss is 0.015472962521016598\n",
      "epoch: 10 step: 761, loss is 0.0786350890994072\n",
      "epoch: 10 step: 762, loss is 0.03491799160838127\n",
      "epoch: 10 step: 763, loss is 0.099528007209301\n",
      "epoch: 10 step: 764, loss is 0.04068957641720772\n",
      "epoch: 10 step: 765, loss is 0.033799078315496445\n",
      "epoch: 10 step: 766, loss is 0.018814431503415108\n",
      "epoch: 10 step: 767, loss is 0.04645250737667084\n",
      "epoch: 10 step: 768, loss is 0.06271517276763916\n",
      "epoch: 10 step: 769, loss is 0.09448961913585663\n",
      "epoch: 10 step: 770, loss is 0.08613675087690353\n",
      "epoch: 10 step: 771, loss is 0.09967032074928284\n",
      "epoch: 10 step: 772, loss is 0.02796056680381298\n",
      "epoch: 10 step: 773, loss is 0.0734279677271843\n",
      "epoch: 10 step: 774, loss is 0.044812124222517014\n",
      "epoch: 10 step: 775, loss is 0.019034994766116142\n",
      "epoch: 10 step: 776, loss is 0.029605427756905556\n",
      "epoch: 10 step: 777, loss is 0.09022123366594315\n",
      "epoch: 10 step: 778, loss is 0.160784050822258\n",
      "epoch: 10 step: 779, loss is 0.0968906432390213\n",
      "epoch: 10 step: 780, loss is 0.10350976139307022\n",
      "epoch: 10 step: 781, loss is 0.05249609798192978\n",
      "epoch: 10 step: 782, loss is 0.16788414120674133\n",
      "epoch: 10 step: 783, loss is 0.05476031452417374\n",
      "epoch: 10 step: 784, loss is 0.15669937431812286\n",
      "epoch: 10 step: 785, loss is 0.09403439611196518\n",
      "epoch: 10 step: 786, loss is 0.07448907196521759\n",
      "epoch: 10 step: 787, loss is 0.06826505064964294\n",
      "epoch: 10 step: 788, loss is 0.017352070659399033\n",
      "epoch: 10 step: 789, loss is 0.047144826501607895\n",
      "epoch: 10 step: 790, loss is 0.11272003501653671\n",
      "epoch: 10 step: 791, loss is 0.03651372715830803\n",
      "epoch: 10 step: 792, loss is 0.06872902065515518\n",
      "epoch: 10 step: 793, loss is 0.07643759250640869\n",
      "epoch: 10 step: 794, loss is 0.013882895931601524\n",
      "epoch: 10 step: 795, loss is 0.04166799411177635\n",
      "epoch: 10 step: 796, loss is 0.04806218668818474\n",
      "epoch: 10 step: 797, loss is 0.016527516767382622\n",
      "epoch: 10 step: 798, loss is 0.14393891394138336\n",
      "epoch: 10 step: 799, loss is 0.0680725947022438\n",
      "epoch: 10 step: 800, loss is 0.018654057756066322\n",
      "epoch: 10 step: 801, loss is 0.048217251896858215\n",
      "epoch: 10 step: 802, loss is 0.10575177520513535\n",
      "epoch: 10 step: 803, loss is 0.03510069474577904\n",
      "epoch: 10 step: 804, loss is 0.017541415989398956\n",
      "epoch: 10 step: 805, loss is 0.03075750730931759\n",
      "epoch: 10 step: 806, loss is 0.13526146113872528\n",
      "epoch: 10 step: 807, loss is 0.11229196190834045\n",
      "epoch: 10 step: 808, loss is 0.015429479070007801\n",
      "epoch: 10 step: 809, loss is 0.08642949163913727\n",
      "epoch: 10 step: 810, loss is 0.05408620089292526\n",
      "epoch: 10 step: 811, loss is 0.06387975066900253\n",
      "epoch: 10 step: 812, loss is 0.042558036744594574\n",
      "epoch: 10 step: 813, loss is 0.14965184032917023\n",
      "epoch: 10 step: 814, loss is 0.11247570812702179\n",
      "epoch: 10 step: 815, loss is 0.12105274945497513\n",
      "epoch: 10 step: 816, loss is 0.10808228701353073\n",
      "epoch: 10 step: 817, loss is 0.07278117537498474\n",
      "epoch: 10 step: 818, loss is 0.02504747547209263\n",
      "epoch: 10 step: 819, loss is 0.01715507172048092\n",
      "epoch: 10 step: 820, loss is 0.11778323352336884\n",
      "epoch: 10 step: 821, loss is 0.045321766287088394\n",
      "epoch: 10 step: 822, loss is 0.1397465318441391\n",
      "epoch: 10 step: 823, loss is 0.09187940508127213\n",
      "epoch: 10 step: 824, loss is 0.014222213998436928\n",
      "epoch: 10 step: 825, loss is 0.13840867578983307\n",
      "epoch: 10 step: 826, loss is 0.04363188147544861\n",
      "epoch: 10 step: 827, loss is 0.0599239207804203\n",
      "epoch: 10 step: 828, loss is 0.01678616739809513\n",
      "epoch: 10 step: 829, loss is 0.02272646874189377\n",
      "epoch: 10 step: 830, loss is 0.03056473098695278\n",
      "epoch: 10 step: 831, loss is 0.08229666203260422\n",
      "epoch: 10 step: 832, loss is 0.07641560584306717\n",
      "epoch: 10 step: 833, loss is 0.0499676950275898\n",
      "epoch: 10 step: 834, loss is 0.03983927145600319\n",
      "epoch: 10 step: 835, loss is 0.026001377031207085\n",
      "epoch: 10 step: 836, loss is 0.07766606658697128\n",
      "epoch: 10 step: 837, loss is 0.05688820406794548\n",
      "epoch: 10 step: 838, loss is 0.12999354302883148\n",
      "epoch: 10 step: 839, loss is 0.07754341512918472\n",
      "epoch: 10 step: 840, loss is 0.09762463718652725\n",
      "epoch: 10 step: 841, loss is 0.013042420148849487\n",
      "epoch: 10 step: 842, loss is 0.024910595268011093\n",
      "epoch: 10 step: 843, loss is 0.059605907648801804\n",
      "epoch: 10 step: 844, loss is 0.03786787390708923\n",
      "epoch: 10 step: 845, loss is 0.12024498730897903\n",
      "epoch: 10 step: 846, loss is 0.12564651668071747\n",
      "epoch: 10 step: 847, loss is 0.07212240248918533\n",
      "epoch: 10 step: 848, loss is 0.05472487956285477\n",
      "epoch: 10 step: 849, loss is 0.0771152675151825\n",
      "epoch: 10 step: 850, loss is 0.0639059767127037\n",
      "epoch: 10 step: 851, loss is 0.008859003894031048\n",
      "epoch: 10 step: 852, loss is 0.13689257204532623\n",
      "epoch: 10 step: 853, loss is 0.02194891683757305\n",
      "epoch: 10 step: 854, loss is 0.02281220257282257\n",
      "epoch: 10 step: 855, loss is 0.05741813778877258\n",
      "epoch: 10 step: 856, loss is 0.07341421395540237\n",
      "epoch: 10 step: 857, loss is 0.06213567033410072\n",
      "epoch: 10 step: 858, loss is 0.02435547299683094\n",
      "epoch: 10 step: 859, loss is 0.03413597121834755\n",
      "epoch: 10 step: 860, loss is 0.20120374858379364\n",
      "epoch: 10 step: 861, loss is 0.013092770241200924\n",
      "epoch: 10 step: 862, loss is 0.10334575176239014\n",
      "epoch: 10 step: 863, loss is 0.14172358810901642\n",
      "epoch: 10 step: 864, loss is 0.09424569457769394\n",
      "epoch: 10 step: 865, loss is 0.04602344334125519\n",
      "epoch: 10 step: 866, loss is 0.048881448805332184\n",
      "epoch: 10 step: 867, loss is 0.07918652147054672\n",
      "epoch: 10 step: 868, loss is 0.010544965974986553\n",
      "epoch: 10 step: 869, loss is 0.02975122258067131\n",
      "epoch: 10 step: 870, loss is 0.03897900879383087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 871, loss is 0.026710527017712593\n",
      "epoch: 10 step: 872, loss is 0.11849971860647202\n",
      "epoch: 10 step: 873, loss is 0.06615476310253143\n",
      "epoch: 10 step: 874, loss is 0.06149592995643616\n",
      "epoch: 10 step: 875, loss is 0.1742711216211319\n",
      "epoch: 10 step: 876, loss is 0.10532413423061371\n",
      "epoch: 10 step: 877, loss is 0.011833444237709045\n",
      "epoch: 10 step: 878, loss is 0.02735828422009945\n",
      "epoch: 10 step: 879, loss is 0.08021838963031769\n",
      "epoch: 10 step: 880, loss is 0.23131276667118073\n",
      "epoch: 10 step: 881, loss is 0.06587086617946625\n",
      "epoch: 10 step: 882, loss is 0.028077807277441025\n",
      "epoch: 10 step: 883, loss is 0.09000271558761597\n",
      "epoch: 10 step: 884, loss is 0.1252770870923996\n",
      "epoch: 10 step: 885, loss is 0.16599033772945404\n",
      "epoch: 10 step: 886, loss is 0.02231862209737301\n",
      "epoch: 10 step: 887, loss is 0.08702615648508072\n",
      "epoch: 10 step: 888, loss is 0.028010934591293335\n",
      "epoch: 10 step: 889, loss is 0.0504906103014946\n",
      "epoch: 10 step: 890, loss is 0.12089724093675613\n",
      "epoch: 10 step: 891, loss is 0.06165185943245888\n",
      "epoch: 10 step: 892, loss is 0.004079386126250029\n",
      "epoch: 10 step: 893, loss is 0.1117762103676796\n",
      "epoch: 10 step: 894, loss is 0.15817409753799438\n",
      "epoch: 10 step: 895, loss is 0.08139487355947495\n",
      "epoch: 10 step: 896, loss is 0.08964478224515915\n",
      "epoch: 10 step: 897, loss is 0.05835407227277756\n",
      "epoch: 10 step: 898, loss is 0.13146710395812988\n",
      "epoch: 10 step: 899, loss is 0.032092731446027756\n",
      "epoch: 10 step: 900, loss is 0.16613395512104034\n",
      "epoch: 10 step: 901, loss is 0.058280862867832184\n",
      "epoch: 10 step: 902, loss is 0.07975998520851135\n",
      "epoch: 10 step: 903, loss is 0.032315582036972046\n",
      "epoch: 10 step: 904, loss is 0.15128439664840698\n",
      "epoch: 10 step: 905, loss is 0.060999833047389984\n",
      "epoch: 10 step: 906, loss is 0.09078849107027054\n",
      "epoch: 10 step: 907, loss is 0.05624747648835182\n",
      "epoch: 10 step: 908, loss is 0.12628012895584106\n",
      "epoch: 10 step: 909, loss is 0.05677497014403343\n",
      "epoch: 10 step: 910, loss is 0.24188286066055298\n",
      "epoch: 10 step: 911, loss is 0.15670664608478546\n",
      "epoch: 10 step: 912, loss is 0.010682516731321812\n",
      "epoch: 10 step: 913, loss is 0.009556158445775509\n",
      "epoch: 10 step: 914, loss is 0.1044386476278305\n",
      "epoch: 10 step: 915, loss is 0.05927174165844917\n",
      "epoch: 10 step: 916, loss is 0.03264964371919632\n",
      "epoch: 10 step: 917, loss is 0.09841267764568329\n",
      "epoch: 10 step: 918, loss is 0.06849782913923264\n",
      "epoch: 10 step: 919, loss is 0.11256822198629379\n",
      "epoch: 10 step: 920, loss is 0.16861513257026672\n",
      "epoch: 10 step: 921, loss is 0.18746918439865112\n",
      "epoch: 10 step: 922, loss is 0.009148816578090191\n",
      "epoch: 10 step: 923, loss is 0.1634514033794403\n",
      "epoch: 10 step: 924, loss is 0.16776558756828308\n",
      "epoch: 10 step: 925, loss is 0.09493620693683624\n",
      "epoch: 10 step: 926, loss is 0.1638595461845398\n",
      "epoch: 10 step: 927, loss is 0.16765381395816803\n",
      "epoch: 10 step: 928, loss is 0.08177237957715988\n",
      "epoch: 10 step: 929, loss is 0.07365052402019501\n",
      "epoch: 10 step: 930, loss is 0.09046225249767303\n",
      "epoch: 10 step: 931, loss is 0.06477516144514084\n",
      "epoch: 10 step: 932, loss is 0.03360246866941452\n",
      "epoch: 10 step: 933, loss is 0.11530283093452454\n",
      "epoch: 10 step: 934, loss is 0.17261578142642975\n",
      "epoch: 10 step: 935, loss is 0.09203798323869705\n",
      "epoch: 10 step: 936, loss is 0.0346817821264267\n",
      "epoch: 10 step: 937, loss is 0.11084400862455368\n",
      "epoch: 11 step: 1, loss is 0.05252065882086754\n",
      "epoch: 11 step: 2, loss is 0.025295181199908257\n",
      "epoch: 11 step: 3, loss is 0.06389154493808746\n",
      "epoch: 11 step: 4, loss is 0.11647339910268784\n",
      "epoch: 11 step: 5, loss is 0.03519466519355774\n",
      "epoch: 11 step: 6, loss is 0.06747881323099136\n",
      "epoch: 11 step: 7, loss is 0.03694680333137512\n",
      "epoch: 11 step: 8, loss is 0.02593282237648964\n",
      "epoch: 11 step: 9, loss is 0.030447829514741898\n",
      "epoch: 11 step: 10, loss is 0.10690196603536606\n",
      "epoch: 11 step: 11, loss is 0.010704762302339077\n",
      "epoch: 11 step: 12, loss is 0.016851196065545082\n",
      "epoch: 11 step: 13, loss is 0.022934870794415474\n",
      "epoch: 11 step: 14, loss is 0.03088361583650112\n",
      "epoch: 11 step: 15, loss is 0.09339750558137894\n",
      "epoch: 11 step: 16, loss is 0.027478795498609543\n",
      "epoch: 11 step: 17, loss is 0.06494583934545517\n",
      "epoch: 11 step: 18, loss is 0.031482674181461334\n",
      "epoch: 11 step: 19, loss is 0.04456673562526703\n",
      "epoch: 11 step: 20, loss is 0.022016799077391624\n",
      "epoch: 11 step: 21, loss is 0.04683207347989082\n",
      "epoch: 11 step: 22, loss is 0.026516446843743324\n",
      "epoch: 11 step: 23, loss is 0.027924872934818268\n",
      "epoch: 11 step: 24, loss is 0.02494939975440502\n",
      "epoch: 11 step: 25, loss is 0.050762344151735306\n",
      "epoch: 11 step: 26, loss is 0.024101944640278816\n",
      "epoch: 11 step: 27, loss is 0.04602337256073952\n",
      "epoch: 11 step: 28, loss is 0.01308615691959858\n",
      "epoch: 11 step: 29, loss is 0.12787339091300964\n",
      "epoch: 11 step: 30, loss is 0.09354469925165176\n",
      "epoch: 11 step: 31, loss is 0.037932198494672775\n",
      "epoch: 11 step: 32, loss is 0.06606205552816391\n",
      "epoch: 11 step: 33, loss is 0.030037395656108856\n",
      "epoch: 11 step: 34, loss is 0.04905014485120773\n",
      "epoch: 11 step: 35, loss is 0.05187078192830086\n",
      "epoch: 11 step: 36, loss is 0.015087797306478024\n",
      "epoch: 11 step: 37, loss is 0.03300968557596207\n",
      "epoch: 11 step: 38, loss is 0.04373130202293396\n",
      "epoch: 11 step: 39, loss is 0.026520010083913803\n",
      "epoch: 11 step: 40, loss is 0.015960946679115295\n",
      "epoch: 11 step: 41, loss is 0.06467105448246002\n",
      "epoch: 11 step: 42, loss is 0.03455689176917076\n",
      "epoch: 11 step: 43, loss is 0.03782622888684273\n",
      "epoch: 11 step: 44, loss is 0.02892964519560337\n",
      "epoch: 11 step: 45, loss is 0.09740819782018661\n",
      "epoch: 11 step: 46, loss is 0.014251742511987686\n",
      "epoch: 11 step: 47, loss is 0.06263915449380875\n",
      "epoch: 11 step: 48, loss is 0.11332973837852478\n",
      "epoch: 11 step: 49, loss is 0.03514251485466957\n",
      "epoch: 11 step: 50, loss is 0.056272994726896286\n",
      "epoch: 11 step: 51, loss is 0.03152899444103241\n",
      "epoch: 11 step: 52, loss is 0.056083131581544876\n",
      "epoch: 11 step: 53, loss is 0.023794425651431084\n",
      "epoch: 11 step: 54, loss is 0.006260187830775976\n",
      "epoch: 11 step: 55, loss is 0.010608777403831482\n",
      "epoch: 11 step: 56, loss is 0.031811028718948364\n",
      "epoch: 11 step: 57, loss is 0.0685654729604721\n",
      "epoch: 11 step: 58, loss is 0.047439150512218475\n",
      "epoch: 11 step: 59, loss is 0.04315377399325371\n",
      "epoch: 11 step: 60, loss is 0.08011531829833984\n",
      "epoch: 11 step: 61, loss is 0.11200088262557983\n",
      "epoch: 11 step: 62, loss is 0.009072178974747658\n",
      "epoch: 11 step: 63, loss is 0.059913620352745056\n",
      "epoch: 11 step: 64, loss is 0.040317732840776443\n",
      "epoch: 11 step: 65, loss is 0.09303459525108337\n",
      "epoch: 11 step: 66, loss is 0.011222243309020996\n",
      "epoch: 11 step: 67, loss is 0.08013058453798294\n",
      "epoch: 11 step: 68, loss is 0.012261220254004002\n",
      "epoch: 11 step: 69, loss is 0.06294520944356918\n",
      "epoch: 11 step: 70, loss is 0.019612954929471016\n",
      "epoch: 11 step: 71, loss is 0.03961737081408501\n",
      "epoch: 11 step: 72, loss is 0.027154218405485153\n",
      "epoch: 11 step: 73, loss is 0.01798226870596409\n",
      "epoch: 11 step: 74, loss is 0.09139560908079147\n",
      "epoch: 11 step: 75, loss is 0.03425867110490799\n",
      "epoch: 11 step: 76, loss is 0.02316218614578247\n",
      "epoch: 11 step: 77, loss is 0.049787282943725586\n",
      "epoch: 11 step: 78, loss is 0.018387606367468834\n",
      "epoch: 11 step: 79, loss is 0.05649345740675926\n",
      "epoch: 11 step: 80, loss is 0.058633953332901\n",
      "epoch: 11 step: 81, loss is 0.0186799056828022\n",
      "epoch: 11 step: 82, loss is 0.04539535939693451\n",
      "epoch: 11 step: 83, loss is 0.05370848998427391\n",
      "epoch: 11 step: 84, loss is 0.017382778227329254\n",
      "epoch: 11 step: 85, loss is 0.0348077192902565\n",
      "epoch: 11 step: 86, loss is 0.10823323577642441\n",
      "epoch: 11 step: 87, loss is 0.020464560016989708\n",
      "epoch: 11 step: 88, loss is 0.013854777440428734\n",
      "epoch: 11 step: 89, loss is 0.04731624200940132\n",
      "epoch: 11 step: 90, loss is 0.006473415531218052\n",
      "epoch: 11 step: 91, loss is 0.04431594908237457\n",
      "epoch: 11 step: 92, loss is 0.025650840252637863\n",
      "epoch: 11 step: 93, loss is 0.011438067071139812\n",
      "epoch: 11 step: 94, loss is 0.020085182040929794\n",
      "epoch: 11 step: 95, loss is 0.034519944339990616\n",
      "epoch: 11 step: 96, loss is 0.03780335187911987\n",
      "epoch: 11 step: 97, loss is 0.014983284287154675\n",
      "epoch: 11 step: 98, loss is 0.05498458817601204\n",
      "epoch: 11 step: 99, loss is 0.027764001861214638\n",
      "epoch: 11 step: 100, loss is 0.05811895430088043\n",
      "epoch: 11 step: 101, loss is 0.07844183593988419\n",
      "epoch: 11 step: 102, loss is 0.03460954129695892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 103, loss is 0.009851833805441856\n",
      "epoch: 11 step: 104, loss is 0.10237888246774673\n",
      "epoch: 11 step: 105, loss is 0.0633721724152565\n",
      "epoch: 11 step: 106, loss is 0.04822337627410889\n",
      "epoch: 11 step: 107, loss is 0.038282815366983414\n",
      "epoch: 11 step: 108, loss is 0.021596718579530716\n",
      "epoch: 11 step: 109, loss is 0.008392537012696266\n",
      "epoch: 11 step: 110, loss is 0.0073684388771653175\n",
      "epoch: 11 step: 111, loss is 0.04050451144576073\n",
      "epoch: 11 step: 112, loss is 0.032822802662849426\n",
      "epoch: 11 step: 113, loss is 0.032781604677438736\n",
      "epoch: 11 step: 114, loss is 0.00782371312379837\n",
      "epoch: 11 step: 115, loss is 0.004452958237379789\n",
      "epoch: 11 step: 116, loss is 0.06679244339466095\n",
      "epoch: 11 step: 117, loss is 0.023405954241752625\n",
      "epoch: 11 step: 118, loss is 0.07003861665725708\n",
      "epoch: 11 step: 119, loss is 0.0458877868950367\n",
      "epoch: 11 step: 120, loss is 0.008376570418477058\n",
      "epoch: 11 step: 121, loss is 0.026028554886579514\n",
      "epoch: 11 step: 122, loss is 0.08334102481603622\n",
      "epoch: 11 step: 123, loss is 0.01788894645869732\n",
      "epoch: 11 step: 124, loss is 0.06100998446345329\n",
      "epoch: 11 step: 125, loss is 0.04755782335996628\n",
      "epoch: 11 step: 126, loss is 0.04895298555493355\n",
      "epoch: 11 step: 127, loss is 0.025810129940509796\n",
      "epoch: 11 step: 128, loss is 0.11432836949825287\n",
      "epoch: 11 step: 129, loss is 0.009171686135232449\n",
      "epoch: 11 step: 130, loss is 0.09150434285402298\n",
      "epoch: 11 step: 131, loss is 0.016846615821123123\n",
      "epoch: 11 step: 132, loss is 0.0026084776036441326\n",
      "epoch: 11 step: 133, loss is 0.049587465822696686\n",
      "epoch: 11 step: 134, loss is 0.02954835258424282\n",
      "epoch: 11 step: 135, loss is 0.007853686809539795\n",
      "epoch: 11 step: 136, loss is 0.10554859042167664\n",
      "epoch: 11 step: 137, loss is 0.028155576437711716\n",
      "epoch: 11 step: 138, loss is 0.050669554620981216\n",
      "epoch: 11 step: 139, loss is 0.02687600441277027\n",
      "epoch: 11 step: 140, loss is 0.020897947251796722\n",
      "epoch: 11 step: 141, loss is 0.014330142177641392\n",
      "epoch: 11 step: 142, loss is 0.0369768850505352\n",
      "epoch: 11 step: 143, loss is 0.014133748598396778\n",
      "epoch: 11 step: 144, loss is 0.017779024317860603\n",
      "epoch: 11 step: 145, loss is 0.053643062710762024\n",
      "epoch: 11 step: 146, loss is 0.05664508417248726\n",
      "epoch: 11 step: 147, loss is 0.005245761014521122\n",
      "epoch: 11 step: 148, loss is 0.05052623525261879\n",
      "epoch: 11 step: 149, loss is 0.035530153661966324\n",
      "epoch: 11 step: 150, loss is 0.06660865247249603\n",
      "epoch: 11 step: 151, loss is 0.018543846905231476\n",
      "epoch: 11 step: 152, loss is 0.023283112794160843\n",
      "epoch: 11 step: 153, loss is 0.027673520147800446\n",
      "epoch: 11 step: 154, loss is 0.05361420288681984\n",
      "epoch: 11 step: 155, loss is 0.07174629718065262\n",
      "epoch: 11 step: 156, loss is 0.010886608622968197\n",
      "epoch: 11 step: 157, loss is 0.07952573895454407\n",
      "epoch: 11 step: 158, loss is 0.020923161879181862\n",
      "epoch: 11 step: 159, loss is 0.01777205616235733\n",
      "epoch: 11 step: 160, loss is 0.04627715051174164\n",
      "epoch: 11 step: 161, loss is 0.06588100641965866\n",
      "epoch: 11 step: 162, loss is 0.028848392888903618\n",
      "epoch: 11 step: 163, loss is 0.03815944120287895\n",
      "epoch: 11 step: 164, loss is 0.008853345178067684\n",
      "epoch: 11 step: 165, loss is 0.08774324506521225\n",
      "epoch: 11 step: 166, loss is 0.01167753990739584\n",
      "epoch: 11 step: 167, loss is 0.015291926451027393\n",
      "epoch: 11 step: 168, loss is 0.10205463320016861\n",
      "epoch: 11 step: 169, loss is 0.018695121631026268\n",
      "epoch: 11 step: 170, loss is 0.033072177320718765\n",
      "epoch: 11 step: 171, loss is 0.045112401247024536\n",
      "epoch: 11 step: 172, loss is 0.023491227999329567\n",
      "epoch: 11 step: 173, loss is 0.006145507097244263\n",
      "epoch: 11 step: 174, loss is 0.04139380902051926\n",
      "epoch: 11 step: 175, loss is 0.005395006854087114\n",
      "epoch: 11 step: 176, loss is 0.14049986004829407\n",
      "epoch: 11 step: 177, loss is 0.050393082201480865\n",
      "epoch: 11 step: 178, loss is 0.006116936914622784\n",
      "epoch: 11 step: 179, loss is 0.15168212354183197\n",
      "epoch: 11 step: 180, loss is 0.10596555471420288\n",
      "epoch: 11 step: 181, loss is 0.0630687028169632\n",
      "epoch: 11 step: 182, loss is 0.03175948187708855\n",
      "epoch: 11 step: 183, loss is 0.02000414952635765\n",
      "epoch: 11 step: 184, loss is 0.10129877924919128\n",
      "epoch: 11 step: 185, loss is 0.05895569920539856\n",
      "epoch: 11 step: 186, loss is 0.06469742208719254\n",
      "epoch: 11 step: 187, loss is 0.04821763187646866\n",
      "epoch: 11 step: 188, loss is 0.033202163875103\n",
      "epoch: 11 step: 189, loss is 0.11156724393367767\n",
      "epoch: 11 step: 190, loss is 0.008530467748641968\n",
      "epoch: 11 step: 191, loss is 0.04493924602866173\n",
      "epoch: 11 step: 192, loss is 0.02997983805835247\n",
      "epoch: 11 step: 193, loss is 0.07434536516666412\n",
      "epoch: 11 step: 194, loss is 0.05433650687336922\n",
      "epoch: 11 step: 195, loss is 0.13227273523807526\n",
      "epoch: 11 step: 196, loss is 0.03863455727696419\n",
      "epoch: 11 step: 197, loss is 0.007173245772719383\n",
      "epoch: 11 step: 198, loss is 0.02476167306303978\n",
      "epoch: 11 step: 199, loss is 0.025560375303030014\n",
      "epoch: 11 step: 200, loss is 0.02844238467514515\n",
      "epoch: 11 step: 201, loss is 0.014583293348550797\n",
      "epoch: 11 step: 202, loss is 0.041101500391960144\n",
      "epoch: 11 step: 203, loss is 0.05972842499613762\n",
      "epoch: 11 step: 204, loss is 0.02958660200238228\n",
      "epoch: 11 step: 205, loss is 0.03699944540858269\n",
      "epoch: 11 step: 206, loss is 0.031248945742845535\n",
      "epoch: 11 step: 207, loss is 0.07680585235357285\n",
      "epoch: 11 step: 208, loss is 0.028258277103304863\n",
      "epoch: 11 step: 209, loss is 0.03490384668111801\n",
      "epoch: 11 step: 210, loss is 0.019726796075701714\n",
      "epoch: 11 step: 211, loss is 0.0033550867810845375\n",
      "epoch: 11 step: 212, loss is 0.013475020416080952\n",
      "epoch: 11 step: 213, loss is 0.0652499571442604\n",
      "epoch: 11 step: 214, loss is 0.06324565410614014\n",
      "epoch: 11 step: 215, loss is 0.11410918831825256\n",
      "epoch: 11 step: 216, loss is 0.03528689220547676\n",
      "epoch: 11 step: 217, loss is 0.01588987372815609\n",
      "epoch: 11 step: 218, loss is 0.00788680836558342\n",
      "epoch: 11 step: 219, loss is 0.0365426167845726\n",
      "epoch: 11 step: 220, loss is 0.03364456072449684\n",
      "epoch: 11 step: 221, loss is 0.02007403038442135\n",
      "epoch: 11 step: 222, loss is 0.07525860518217087\n",
      "epoch: 11 step: 223, loss is 0.02300594188272953\n",
      "epoch: 11 step: 224, loss is 0.028217924758791924\n",
      "epoch: 11 step: 225, loss is 0.03182213008403778\n",
      "epoch: 11 step: 226, loss is 0.03427598997950554\n",
      "epoch: 11 step: 227, loss is 0.035017963498830795\n",
      "epoch: 11 step: 228, loss is 0.010673951357603073\n",
      "epoch: 11 step: 229, loss is 0.02839842438697815\n",
      "epoch: 11 step: 230, loss is 0.013395978137850761\n",
      "epoch: 11 step: 231, loss is 0.05547565966844559\n",
      "epoch: 11 step: 232, loss is 0.07844151556491852\n",
      "epoch: 11 step: 233, loss is 0.007706121541559696\n",
      "epoch: 11 step: 234, loss is 0.030422553420066833\n",
      "epoch: 11 step: 235, loss is 0.007521132007241249\n",
      "epoch: 11 step: 236, loss is 0.017174996435642242\n",
      "epoch: 11 step: 237, loss is 0.06260687857866287\n",
      "epoch: 11 step: 238, loss is 0.040036801248788834\n",
      "epoch: 11 step: 239, loss is 0.06957463175058365\n",
      "epoch: 11 step: 240, loss is 0.03645533695816994\n",
      "epoch: 11 step: 241, loss is 0.040109746158123016\n",
      "epoch: 11 step: 242, loss is 0.007467026822268963\n",
      "epoch: 11 step: 243, loss is 0.03021620586514473\n",
      "epoch: 11 step: 244, loss is 0.010244540870189667\n",
      "epoch: 11 step: 245, loss is 0.07016055285930634\n",
      "epoch: 11 step: 246, loss is 0.011565206572413445\n",
      "epoch: 11 step: 247, loss is 0.03090618923306465\n",
      "epoch: 11 step: 248, loss is 0.049016322940588\n",
      "epoch: 11 step: 249, loss is 0.03836442157626152\n",
      "epoch: 11 step: 250, loss is 0.03700719028711319\n",
      "epoch: 11 step: 251, loss is 0.011211389675736427\n",
      "epoch: 11 step: 252, loss is 0.012949631549417973\n",
      "epoch: 11 step: 253, loss is 0.028802329674363136\n",
      "epoch: 11 step: 254, loss is 0.08076507598161697\n",
      "epoch: 11 step: 255, loss is 0.09470250457525253\n",
      "epoch: 11 step: 256, loss is 0.03814113140106201\n",
      "epoch: 11 step: 257, loss is 0.05162493512034416\n",
      "epoch: 11 step: 258, loss is 0.010730039328336716\n",
      "epoch: 11 step: 259, loss is 0.06086236611008644\n",
      "epoch: 11 step: 260, loss is 0.11308027058839798\n",
      "epoch: 11 step: 261, loss is 0.027632061392068863\n",
      "epoch: 11 step: 262, loss is 0.012246104888617992\n",
      "epoch: 11 step: 263, loss is 0.07880616933107376\n",
      "epoch: 11 step: 264, loss is 0.1445607841014862\n",
      "epoch: 11 step: 265, loss is 0.09476374834775925\n",
      "epoch: 11 step: 266, loss is 0.07379357516765594\n",
      "epoch: 11 step: 267, loss is 0.012397405691444874\n",
      "epoch: 11 step: 268, loss is 0.058170758187770844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 269, loss is 0.06120534986257553\n",
      "epoch: 11 step: 270, loss is 0.0031012610998004675\n",
      "epoch: 11 step: 271, loss is 0.0062502287328243256\n",
      "epoch: 11 step: 272, loss is 0.008768717758357525\n",
      "epoch: 11 step: 273, loss is 0.012033188715577126\n",
      "epoch: 11 step: 274, loss is 0.00527618546038866\n",
      "epoch: 11 step: 275, loss is 0.06512941420078278\n",
      "epoch: 11 step: 276, loss is 0.12083849310874939\n",
      "epoch: 11 step: 277, loss is 0.007957023568451405\n",
      "epoch: 11 step: 278, loss is 0.01312108151614666\n",
      "epoch: 11 step: 279, loss is 0.0312996432185173\n",
      "epoch: 11 step: 280, loss is 0.010448910295963287\n",
      "epoch: 11 step: 281, loss is 0.09186920523643494\n",
      "epoch: 11 step: 282, loss is 0.01594877429306507\n",
      "epoch: 11 step: 283, loss is 0.04995737969875336\n",
      "epoch: 11 step: 284, loss is 0.05723689869046211\n",
      "epoch: 11 step: 285, loss is 0.10694371908903122\n",
      "epoch: 11 step: 286, loss is 0.017785724252462387\n",
      "epoch: 11 step: 287, loss is 0.006605650298297405\n",
      "epoch: 11 step: 288, loss is 0.0626293271780014\n",
      "epoch: 11 step: 289, loss is 0.006752650719136\n",
      "epoch: 11 step: 290, loss is 0.054444581270217896\n",
      "epoch: 11 step: 291, loss is 0.2777666449546814\n",
      "epoch: 11 step: 292, loss is 0.04719242826104164\n",
      "epoch: 11 step: 293, loss is 0.06876756995916367\n",
      "epoch: 11 step: 294, loss is 0.07286238670349121\n",
      "epoch: 11 step: 295, loss is 0.009069508872926235\n",
      "epoch: 11 step: 296, loss is 0.022466342896223068\n",
      "epoch: 11 step: 297, loss is 0.027103042230010033\n",
      "epoch: 11 step: 298, loss is 0.004135976079851389\n",
      "epoch: 11 step: 299, loss is 0.005875971168279648\n",
      "epoch: 11 step: 300, loss is 0.018262039870023727\n",
      "epoch: 11 step: 301, loss is 0.07380414754152298\n",
      "epoch: 11 step: 302, loss is 0.00945446826517582\n",
      "epoch: 11 step: 303, loss is 0.02959522418677807\n",
      "epoch: 11 step: 304, loss is 0.0055240485817193985\n",
      "epoch: 11 step: 305, loss is 0.09337186813354492\n",
      "epoch: 11 step: 306, loss is 0.014381477609276772\n",
      "epoch: 11 step: 307, loss is 0.02095733769237995\n",
      "epoch: 11 step: 308, loss is 0.018997440114617348\n",
      "epoch: 11 step: 309, loss is 0.05340266972780228\n",
      "epoch: 11 step: 310, loss is 0.08395353704690933\n",
      "epoch: 11 step: 311, loss is 0.018703177571296692\n",
      "epoch: 11 step: 312, loss is 0.011968224309384823\n",
      "epoch: 11 step: 313, loss is 0.07557066529989243\n",
      "epoch: 11 step: 314, loss is 0.004926694091409445\n",
      "epoch: 11 step: 315, loss is 0.026706380769610405\n",
      "epoch: 11 step: 316, loss is 0.0952472984790802\n",
      "epoch: 11 step: 317, loss is 0.007655908353626728\n",
      "epoch: 11 step: 318, loss is 0.04108605906367302\n",
      "epoch: 11 step: 319, loss is 0.012304653413593769\n",
      "epoch: 11 step: 320, loss is 0.00459109665825963\n",
      "epoch: 11 step: 321, loss is 0.020462745800614357\n",
      "epoch: 11 step: 322, loss is 0.07279954850673676\n",
      "epoch: 11 step: 323, loss is 0.04465062543749809\n",
      "epoch: 11 step: 324, loss is 0.01570419780910015\n",
      "epoch: 11 step: 325, loss is 0.06801246851682663\n",
      "epoch: 11 step: 326, loss is 0.028940772637724876\n",
      "epoch: 11 step: 327, loss is 0.02656235359609127\n",
      "epoch: 11 step: 328, loss is 0.01713733933866024\n",
      "epoch: 11 step: 329, loss is 0.01294563990086317\n",
      "epoch: 11 step: 330, loss is 0.026860041543841362\n",
      "epoch: 11 step: 331, loss is 0.02453143708407879\n",
      "epoch: 11 step: 332, loss is 0.030072011053562164\n",
      "epoch: 11 step: 333, loss is 0.07623300701379776\n",
      "epoch: 11 step: 334, loss is 0.023379366844892502\n",
      "epoch: 11 step: 335, loss is 0.09331125766038895\n",
      "epoch: 11 step: 336, loss is 0.022903893142938614\n",
      "epoch: 11 step: 337, loss is 0.008621803484857082\n",
      "epoch: 11 step: 338, loss is 0.00892309844493866\n",
      "epoch: 11 step: 339, loss is 0.0077712698839604855\n",
      "epoch: 11 step: 340, loss is 0.027267728000879288\n",
      "epoch: 11 step: 341, loss is 0.02078501507639885\n",
      "epoch: 11 step: 342, loss is 0.05438408628106117\n",
      "epoch: 11 step: 343, loss is 0.02892698347568512\n",
      "epoch: 11 step: 344, loss is 0.10208268463611603\n",
      "epoch: 11 step: 345, loss is 0.021737806499004364\n",
      "epoch: 11 step: 346, loss is 0.010219566524028778\n",
      "epoch: 11 step: 347, loss is 0.025086047127842903\n",
      "epoch: 11 step: 348, loss is 0.00731993094086647\n",
      "epoch: 11 step: 349, loss is 0.031984440982341766\n",
      "epoch: 11 step: 350, loss is 0.04837968945503235\n",
      "epoch: 11 step: 351, loss is 0.02093227580189705\n",
      "epoch: 11 step: 352, loss is 0.08684217184782028\n",
      "epoch: 11 step: 353, loss is 0.09161336719989777\n",
      "epoch: 11 step: 354, loss is 0.008732875809073448\n",
      "epoch: 11 step: 355, loss is 0.06351420283317566\n",
      "epoch: 11 step: 356, loss is 0.04735240340232849\n",
      "epoch: 11 step: 357, loss is 0.009124774485826492\n",
      "epoch: 11 step: 358, loss is 0.044692762196063995\n",
      "epoch: 11 step: 359, loss is 0.024618174880743027\n",
      "epoch: 11 step: 360, loss is 0.019460109993815422\n",
      "epoch: 11 step: 361, loss is 0.008704019710421562\n",
      "epoch: 11 step: 362, loss is 0.07733506709337234\n",
      "epoch: 11 step: 363, loss is 0.04113311320543289\n",
      "epoch: 11 step: 364, loss is 0.19142843782901764\n",
      "epoch: 11 step: 365, loss is 0.052826520055532455\n",
      "epoch: 11 step: 366, loss is 0.06461655348539352\n",
      "epoch: 11 step: 367, loss is 0.15357829630374908\n",
      "epoch: 11 step: 368, loss is 0.018429432064294815\n",
      "epoch: 11 step: 369, loss is 0.0058609130792319775\n",
      "epoch: 11 step: 370, loss is 0.01233209203928709\n",
      "epoch: 11 step: 371, loss is 0.09585738927125931\n",
      "epoch: 11 step: 372, loss is 0.024276096373796463\n",
      "epoch: 11 step: 373, loss is 0.02266436628997326\n",
      "epoch: 11 step: 374, loss is 0.020612692460417747\n",
      "epoch: 11 step: 375, loss is 0.017441708594560623\n",
      "epoch: 11 step: 376, loss is 0.05937255918979645\n",
      "epoch: 11 step: 377, loss is 0.05003016069531441\n",
      "epoch: 11 step: 378, loss is 0.09326161444187164\n",
      "epoch: 11 step: 379, loss is 0.03618999943137169\n",
      "epoch: 11 step: 380, loss is 0.058986879885196686\n",
      "epoch: 11 step: 381, loss is 0.02396467514336109\n",
      "epoch: 11 step: 382, loss is 0.01617131568491459\n",
      "epoch: 11 step: 383, loss is 0.059282008558511734\n",
      "epoch: 11 step: 384, loss is 0.015043241903185844\n",
      "epoch: 11 step: 385, loss is 0.10420023649930954\n",
      "epoch: 11 step: 386, loss is 0.06847096979618073\n",
      "epoch: 11 step: 387, loss is 0.07158035039901733\n",
      "epoch: 11 step: 388, loss is 0.0400724858045578\n",
      "epoch: 11 step: 389, loss is 0.1289760321378708\n",
      "epoch: 11 step: 390, loss is 0.026740901172161102\n",
      "epoch: 11 step: 391, loss is 0.014490267261862755\n",
      "epoch: 11 step: 392, loss is 0.013576406985521317\n",
      "epoch: 11 step: 393, loss is 0.11019879579544067\n",
      "epoch: 11 step: 394, loss is 0.012128302827477455\n",
      "epoch: 11 step: 395, loss is 0.008095383644104004\n",
      "epoch: 11 step: 396, loss is 0.017150403931736946\n",
      "epoch: 11 step: 397, loss is 0.071494460105896\n",
      "epoch: 11 step: 398, loss is 0.007632371969521046\n",
      "epoch: 11 step: 399, loss is 0.032813746482133865\n",
      "epoch: 11 step: 400, loss is 0.09115061908960342\n",
      "epoch: 11 step: 401, loss is 0.09991412609815598\n",
      "epoch: 11 step: 402, loss is 0.09155004471540451\n",
      "epoch: 11 step: 403, loss is 0.0247235968708992\n",
      "epoch: 11 step: 404, loss is 0.04607083275914192\n",
      "epoch: 11 step: 405, loss is 0.12255006283521652\n",
      "epoch: 11 step: 406, loss is 0.091755710542202\n",
      "epoch: 11 step: 407, loss is 0.030694907531142235\n",
      "epoch: 11 step: 408, loss is 0.04377080127596855\n",
      "epoch: 11 step: 409, loss is 0.02094767615199089\n",
      "epoch: 11 step: 410, loss is 0.019011860713362694\n",
      "epoch: 11 step: 411, loss is 0.013194190338253975\n",
      "epoch: 11 step: 412, loss is 0.09163817018270493\n",
      "epoch: 11 step: 413, loss is 0.013107916340231895\n",
      "epoch: 11 step: 414, loss is 0.129493847489357\n",
      "epoch: 11 step: 415, loss is 0.0032484775874763727\n",
      "epoch: 11 step: 416, loss is 0.024147391319274902\n",
      "epoch: 11 step: 417, loss is 0.01813885197043419\n",
      "epoch: 11 step: 418, loss is 0.16128715872764587\n",
      "epoch: 11 step: 419, loss is 0.005880696699023247\n",
      "epoch: 11 step: 420, loss is 0.02999510057270527\n",
      "epoch: 11 step: 421, loss is 0.043550752103328705\n",
      "epoch: 11 step: 422, loss is 0.12760072946548462\n",
      "epoch: 11 step: 423, loss is 0.02367066591978073\n",
      "epoch: 11 step: 424, loss is 0.12077975273132324\n",
      "epoch: 11 step: 425, loss is 0.028146568685770035\n",
      "epoch: 11 step: 426, loss is 0.028951043263077736\n",
      "epoch: 11 step: 427, loss is 0.08155657351016998\n",
      "epoch: 11 step: 428, loss is 0.018294215202331543\n",
      "epoch: 11 step: 429, loss is 0.04155822470784187\n",
      "epoch: 11 step: 430, loss is 0.03263276442885399\n",
      "epoch: 11 step: 431, loss is 0.017360717058181763\n",
      "epoch: 11 step: 432, loss is 0.07800909876823425\n",
      "epoch: 11 step: 433, loss is 0.016881555318832397\n",
      "epoch: 11 step: 434, loss is 0.01238703727722168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 435, loss is 0.043465934693813324\n",
      "epoch: 11 step: 436, loss is 0.09315266460180283\n",
      "epoch: 11 step: 437, loss is 0.04070306196808815\n",
      "epoch: 11 step: 438, loss is 0.012313355691730976\n",
      "epoch: 11 step: 439, loss is 0.036679308861494064\n",
      "epoch: 11 step: 440, loss is 0.081175297498703\n",
      "epoch: 11 step: 441, loss is 0.023516619578003883\n",
      "epoch: 11 step: 442, loss is 0.04525841027498245\n",
      "epoch: 11 step: 443, loss is 0.05069076642394066\n",
      "epoch: 11 step: 444, loss is 0.05796680971980095\n",
      "epoch: 11 step: 445, loss is 0.07565786689519882\n",
      "epoch: 11 step: 446, loss is 0.02034161053597927\n",
      "epoch: 11 step: 447, loss is 0.054712437093257904\n",
      "epoch: 11 step: 448, loss is 0.0397171713411808\n",
      "epoch: 11 step: 449, loss is 0.03328464925289154\n",
      "epoch: 11 step: 450, loss is 0.029257431626319885\n",
      "epoch: 11 step: 451, loss is 0.1564822643995285\n",
      "epoch: 11 step: 452, loss is 0.034810230135917664\n",
      "epoch: 11 step: 453, loss is 0.046153724193573\n",
      "epoch: 11 step: 454, loss is 0.021510938182473183\n",
      "epoch: 11 step: 455, loss is 0.013233376666903496\n",
      "epoch: 11 step: 456, loss is 0.03931047394871712\n",
      "epoch: 11 step: 457, loss is 0.07971605658531189\n",
      "epoch: 11 step: 458, loss is 0.03098195046186447\n",
      "epoch: 11 step: 459, loss is 0.02601131610572338\n",
      "epoch: 11 step: 460, loss is 0.08304499089717865\n",
      "epoch: 11 step: 461, loss is 0.02426850236952305\n",
      "epoch: 11 step: 462, loss is 0.008435120806097984\n",
      "epoch: 11 step: 463, loss is 0.02399255894124508\n",
      "epoch: 11 step: 464, loss is 0.02998802810907364\n",
      "epoch: 11 step: 465, loss is 0.038734450936317444\n",
      "epoch: 11 step: 466, loss is 0.005210796371102333\n",
      "epoch: 11 step: 467, loss is 0.055607836693525314\n",
      "epoch: 11 step: 468, loss is 0.02334265410900116\n",
      "epoch: 11 step: 469, loss is 0.09054340422153473\n",
      "epoch: 11 step: 470, loss is 0.016981568187475204\n",
      "epoch: 11 step: 471, loss is 0.0204395093023777\n",
      "epoch: 11 step: 472, loss is 0.026856744661927223\n",
      "epoch: 11 step: 473, loss is 0.0828293189406395\n",
      "epoch: 11 step: 474, loss is 0.047825176268815994\n",
      "epoch: 11 step: 475, loss is 0.02174539305269718\n",
      "epoch: 11 step: 476, loss is 0.09689567983150482\n",
      "epoch: 11 step: 477, loss is 0.03314001113176346\n",
      "epoch: 11 step: 478, loss is 0.023289866745471954\n",
      "epoch: 11 step: 479, loss is 0.015248493291437626\n",
      "epoch: 11 step: 480, loss is 0.04098479822278023\n",
      "epoch: 11 step: 481, loss is 0.07729493081569672\n",
      "epoch: 11 step: 482, loss is 0.06053740158677101\n",
      "epoch: 11 step: 483, loss is 0.06933756917715073\n",
      "epoch: 11 step: 484, loss is 0.01819852739572525\n",
      "epoch: 11 step: 485, loss is 0.08088982850313187\n",
      "epoch: 11 step: 486, loss is 0.010886204428970814\n",
      "epoch: 11 step: 487, loss is 0.025566015392541885\n",
      "epoch: 11 step: 488, loss is 0.025680651888251305\n",
      "epoch: 11 step: 489, loss is 0.1059846356511116\n",
      "epoch: 11 step: 490, loss is 0.04192884638905525\n",
      "epoch: 11 step: 491, loss is 0.10626585781574249\n",
      "epoch: 11 step: 492, loss is 0.0755535215139389\n",
      "epoch: 11 step: 493, loss is 0.044000934809446335\n",
      "epoch: 11 step: 494, loss is 0.015796059742569923\n",
      "epoch: 11 step: 495, loss is 0.025858236476778984\n",
      "epoch: 11 step: 496, loss is 0.06411623954772949\n",
      "epoch: 11 step: 497, loss is 0.021701151505112648\n",
      "epoch: 11 step: 498, loss is 0.1542084515094757\n",
      "epoch: 11 step: 499, loss is 0.12468279898166656\n",
      "epoch: 11 step: 500, loss is 0.03482532501220703\n",
      "epoch: 11 step: 501, loss is 0.043147843331098557\n",
      "epoch: 11 step: 502, loss is 0.04391937330365181\n",
      "epoch: 11 step: 503, loss is 0.1101599633693695\n",
      "epoch: 11 step: 504, loss is 0.12830783426761627\n",
      "epoch: 11 step: 505, loss is 0.0830562487244606\n",
      "epoch: 11 step: 506, loss is 0.011198272928595543\n",
      "epoch: 11 step: 507, loss is 0.03445885330438614\n",
      "epoch: 11 step: 508, loss is 0.04804548621177673\n",
      "epoch: 11 step: 509, loss is 0.06673553586006165\n",
      "epoch: 11 step: 510, loss is 0.17333722114562988\n",
      "epoch: 11 step: 511, loss is 0.049732234328985214\n",
      "epoch: 11 step: 512, loss is 0.030585963279008865\n",
      "epoch: 11 step: 513, loss is 0.021091841161251068\n",
      "epoch: 11 step: 514, loss is 0.06231054291129112\n",
      "epoch: 11 step: 515, loss is 0.037607595324516296\n",
      "epoch: 11 step: 516, loss is 0.02560306526720524\n",
      "epoch: 11 step: 517, loss is 0.025140753015875816\n",
      "epoch: 11 step: 518, loss is 0.05404593050479889\n",
      "epoch: 11 step: 519, loss is 0.022935427725315094\n",
      "epoch: 11 step: 520, loss is 0.11794426292181015\n",
      "epoch: 11 step: 521, loss is 0.008311038836836815\n",
      "epoch: 11 step: 522, loss is 0.2123207449913025\n",
      "epoch: 11 step: 523, loss is 0.01735910214483738\n",
      "epoch: 11 step: 524, loss is 0.031296659260988235\n",
      "epoch: 11 step: 525, loss is 0.014342272654175758\n",
      "epoch: 11 step: 526, loss is 0.11388037353754044\n",
      "epoch: 11 step: 527, loss is 0.041762784123420715\n",
      "epoch: 11 step: 528, loss is 0.13572584092617035\n",
      "epoch: 11 step: 529, loss is 0.03154326230287552\n",
      "epoch: 11 step: 530, loss is 0.08608730137348175\n",
      "epoch: 11 step: 531, loss is 0.025517337024211884\n",
      "epoch: 11 step: 532, loss is 0.01028385292738676\n",
      "epoch: 11 step: 533, loss is 0.0634830892086029\n",
      "epoch: 11 step: 534, loss is 0.027722930535674095\n",
      "epoch: 11 step: 535, loss is 0.09504003077745438\n",
      "epoch: 11 step: 536, loss is 0.027424927800893784\n",
      "epoch: 11 step: 537, loss is 0.15406396985054016\n",
      "epoch: 11 step: 538, loss is 0.1200917661190033\n",
      "epoch: 11 step: 539, loss is 0.013190696947276592\n",
      "epoch: 11 step: 540, loss is 0.009361924603581429\n",
      "epoch: 11 step: 541, loss is 0.02102934941649437\n",
      "epoch: 11 step: 542, loss is 0.04776925966143608\n",
      "epoch: 11 step: 543, loss is 0.07371348887681961\n",
      "epoch: 11 step: 544, loss is 0.023615006357431412\n",
      "epoch: 11 step: 545, loss is 0.058386772871017456\n",
      "epoch: 11 step: 546, loss is 0.020678162574768066\n",
      "epoch: 11 step: 547, loss is 0.03780703991651535\n",
      "epoch: 11 step: 548, loss is 0.04577204957604408\n",
      "epoch: 11 step: 549, loss is 0.005158194340765476\n",
      "epoch: 11 step: 550, loss is 0.005114101804792881\n",
      "epoch: 11 step: 551, loss is 0.004904987756162882\n",
      "epoch: 11 step: 552, loss is 0.04448128864169121\n",
      "epoch: 11 step: 553, loss is 0.013418673537671566\n",
      "epoch: 11 step: 554, loss is 0.07439672946929932\n",
      "epoch: 11 step: 555, loss is 0.09269852936267853\n",
      "epoch: 11 step: 556, loss is 0.026490716263651848\n",
      "epoch: 11 step: 557, loss is 0.07816189527511597\n",
      "epoch: 11 step: 558, loss is 0.04061746224761009\n",
      "epoch: 11 step: 559, loss is 0.017329365015029907\n",
      "epoch: 11 step: 560, loss is 0.08033164590597153\n",
      "epoch: 11 step: 561, loss is 0.0460456907749176\n",
      "epoch: 11 step: 562, loss is 0.01955430768430233\n",
      "epoch: 11 step: 563, loss is 0.08652984350919724\n",
      "epoch: 11 step: 564, loss is 0.09714356064796448\n",
      "epoch: 11 step: 565, loss is 0.1459898054599762\n",
      "epoch: 11 step: 566, loss is 0.06593146920204163\n",
      "epoch: 11 step: 567, loss is 0.007017826195806265\n",
      "epoch: 11 step: 568, loss is 0.033587418496608734\n",
      "epoch: 11 step: 569, loss is 0.0063974629156291485\n",
      "epoch: 11 step: 570, loss is 0.05397322028875351\n",
      "epoch: 11 step: 571, loss is 0.07413054257631302\n",
      "epoch: 11 step: 572, loss is 0.11581852287054062\n",
      "epoch: 11 step: 573, loss is 0.11272233724594116\n",
      "epoch: 11 step: 574, loss is 0.03494572639465332\n",
      "epoch: 11 step: 575, loss is 0.060715533792972565\n",
      "epoch: 11 step: 576, loss is 0.06716569513082504\n",
      "epoch: 11 step: 577, loss is 0.02123664878308773\n",
      "epoch: 11 step: 578, loss is 0.040650006383657455\n",
      "epoch: 11 step: 579, loss is 0.018377695232629776\n",
      "epoch: 11 step: 580, loss is 0.049189675599336624\n",
      "epoch: 11 step: 581, loss is 0.03639158979058266\n",
      "epoch: 11 step: 582, loss is 0.034500688314437866\n",
      "epoch: 11 step: 583, loss is 0.07326418906450272\n",
      "epoch: 11 step: 584, loss is 0.01930910535156727\n",
      "epoch: 11 step: 585, loss is 0.015489231795072556\n",
      "epoch: 11 step: 586, loss is 0.04967686906456947\n",
      "epoch: 11 step: 587, loss is 0.051121413707733154\n",
      "epoch: 11 step: 588, loss is 0.020142730325460434\n",
      "epoch: 11 step: 589, loss is 0.06875413656234741\n",
      "epoch: 11 step: 590, loss is 0.006330534815788269\n",
      "epoch: 11 step: 591, loss is 0.025263039395213127\n",
      "epoch: 11 step: 592, loss is 0.03304368630051613\n",
      "epoch: 11 step: 593, loss is 0.05811048299074173\n",
      "epoch: 11 step: 594, loss is 0.03795221447944641\n",
      "epoch: 11 step: 595, loss is 0.04064762219786644\n",
      "epoch: 11 step: 596, loss is 0.04486352577805519\n",
      "epoch: 11 step: 597, loss is 0.008060826919972897\n",
      "epoch: 11 step: 598, loss is 0.020409831777215004\n",
      "epoch: 11 step: 599, loss is 0.012242485769093037\n",
      "epoch: 11 step: 600, loss is 0.08512694388628006\n",
      "epoch: 11 step: 601, loss is 0.02979765646159649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 602, loss is 0.0032723078038543463\n",
      "epoch: 11 step: 603, loss is 0.019945723935961723\n",
      "epoch: 11 step: 604, loss is 0.06757914274930954\n",
      "epoch: 11 step: 605, loss is 0.03841477632522583\n",
      "epoch: 11 step: 606, loss is 0.06790851801633835\n",
      "epoch: 11 step: 607, loss is 0.010411150753498077\n",
      "epoch: 11 step: 608, loss is 0.034185636788606644\n",
      "epoch: 11 step: 609, loss is 0.03415951877832413\n",
      "epoch: 11 step: 610, loss is 0.08791440725326538\n",
      "epoch: 11 step: 611, loss is 0.06149876117706299\n",
      "epoch: 11 step: 612, loss is 0.03573998808860779\n",
      "epoch: 11 step: 613, loss is 0.06127789616584778\n",
      "epoch: 11 step: 614, loss is 0.002011033706367016\n",
      "epoch: 11 step: 615, loss is 0.013301162049174309\n",
      "epoch: 11 step: 616, loss is 0.049948107451200485\n",
      "epoch: 11 step: 617, loss is 0.18138763308525085\n",
      "epoch: 11 step: 618, loss is 0.054534852504730225\n",
      "epoch: 11 step: 619, loss is 0.10361740738153458\n",
      "epoch: 11 step: 620, loss is 0.013439569622278214\n",
      "epoch: 11 step: 621, loss is 0.09777311980724335\n",
      "epoch: 11 step: 622, loss is 0.045508719980716705\n",
      "epoch: 11 step: 623, loss is 0.11469752341508865\n",
      "epoch: 11 step: 624, loss is 0.029296142980456352\n",
      "epoch: 11 step: 625, loss is 0.05454825982451439\n",
      "epoch: 11 step: 626, loss is 0.09097645431756973\n",
      "epoch: 11 step: 627, loss is 0.046033091843128204\n",
      "epoch: 11 step: 628, loss is 0.11330992728471756\n",
      "epoch: 11 step: 629, loss is 0.04248778149485588\n",
      "epoch: 11 step: 630, loss is 0.01873350702226162\n",
      "epoch: 11 step: 631, loss is 0.06366749852895737\n",
      "epoch: 11 step: 632, loss is 0.02008497342467308\n",
      "epoch: 11 step: 633, loss is 0.111278235912323\n",
      "epoch: 11 step: 634, loss is 0.0057100336998701096\n",
      "epoch: 11 step: 635, loss is 0.1293356716632843\n",
      "epoch: 11 step: 636, loss is 0.11700470000505447\n",
      "epoch: 11 step: 637, loss is 0.1063804179430008\n",
      "epoch: 11 step: 638, loss is 0.01735701970756054\n",
      "epoch: 11 step: 639, loss is 0.007617548573762178\n",
      "epoch: 11 step: 640, loss is 0.02497943490743637\n",
      "epoch: 11 step: 641, loss is 0.017513755708932877\n",
      "epoch: 11 step: 642, loss is 0.041252508759498596\n",
      "epoch: 11 step: 643, loss is 0.04133231192827225\n",
      "epoch: 11 step: 644, loss is 0.05258404463529587\n",
      "epoch: 11 step: 645, loss is 0.03710333630442619\n",
      "epoch: 11 step: 646, loss is 0.04436301440000534\n",
      "epoch: 11 step: 647, loss is 0.010235520079731941\n",
      "epoch: 11 step: 648, loss is 0.05242559686303139\n",
      "epoch: 11 step: 649, loss is 0.05672823637723923\n",
      "epoch: 11 step: 650, loss is 0.051053669303655624\n",
      "epoch: 11 step: 651, loss is 0.01475512608885765\n",
      "epoch: 11 step: 652, loss is 0.017699897289276123\n",
      "epoch: 11 step: 653, loss is 0.02616940066218376\n",
      "epoch: 11 step: 654, loss is 0.02569624036550522\n",
      "epoch: 11 step: 655, loss is 0.08714873343706131\n",
      "epoch: 11 step: 656, loss is 0.07472420483827591\n",
      "epoch: 11 step: 657, loss is 0.061520520597696304\n",
      "epoch: 11 step: 658, loss is 0.08088088035583496\n",
      "epoch: 11 step: 659, loss is 0.06657610833644867\n",
      "epoch: 11 step: 660, loss is 0.0035863956436514854\n",
      "epoch: 11 step: 661, loss is 0.02039419300854206\n",
      "epoch: 11 step: 662, loss is 0.024211011826992035\n",
      "epoch: 11 step: 663, loss is 0.011299077421426773\n",
      "epoch: 11 step: 664, loss is 0.05866313353180885\n",
      "epoch: 11 step: 665, loss is 0.1281513124704361\n",
      "epoch: 11 step: 666, loss is 0.091157466173172\n",
      "epoch: 11 step: 667, loss is 0.0317649282515049\n",
      "epoch: 11 step: 668, loss is 0.06601680815219879\n",
      "epoch: 11 step: 669, loss is 0.0166220311075449\n",
      "epoch: 11 step: 670, loss is 0.018840434029698372\n",
      "epoch: 11 step: 671, loss is 0.01762539893388748\n",
      "epoch: 11 step: 672, loss is 0.018225958570837975\n",
      "epoch: 11 step: 673, loss is 0.0455026738345623\n",
      "epoch: 11 step: 674, loss is 0.01754087768495083\n",
      "epoch: 11 step: 675, loss is 0.07155109196901321\n",
      "epoch: 11 step: 676, loss is 0.06766610592603683\n",
      "epoch: 11 step: 677, loss is 0.03764001652598381\n",
      "epoch: 11 step: 678, loss is 0.057236772030591965\n",
      "epoch: 11 step: 679, loss is 0.008113612420856953\n",
      "epoch: 11 step: 680, loss is 0.08997304737567902\n",
      "epoch: 11 step: 681, loss is 0.004715984221547842\n",
      "epoch: 11 step: 682, loss is 0.08004382252693176\n",
      "epoch: 11 step: 683, loss is 0.06718697398900986\n",
      "epoch: 11 step: 684, loss is 0.059331152588129044\n",
      "epoch: 11 step: 685, loss is 0.0054930890910327435\n",
      "epoch: 11 step: 686, loss is 0.016624800860881805\n",
      "epoch: 11 step: 687, loss is 0.09119759500026703\n",
      "epoch: 11 step: 688, loss is 0.08668298274278641\n",
      "epoch: 11 step: 689, loss is 0.10578333586454391\n",
      "epoch: 11 step: 690, loss is 0.009103820659220219\n",
      "epoch: 11 step: 691, loss is 0.10609997808933258\n",
      "epoch: 11 step: 692, loss is 0.02712521143257618\n",
      "epoch: 11 step: 693, loss is 0.06292799860239029\n",
      "epoch: 11 step: 694, loss is 0.15139436721801758\n",
      "epoch: 11 step: 695, loss is 0.027016321197152138\n",
      "epoch: 11 step: 696, loss is 0.1538182646036148\n",
      "epoch: 11 step: 697, loss is 0.051659341901540756\n",
      "epoch: 11 step: 698, loss is 0.0417262464761734\n",
      "epoch: 11 step: 699, loss is 0.07649099826812744\n",
      "epoch: 11 step: 700, loss is 0.026008468121290207\n",
      "epoch: 11 step: 701, loss is 0.009238815866410732\n",
      "epoch: 11 step: 702, loss is 0.04967067390680313\n",
      "epoch: 11 step: 703, loss is 0.050894416868686676\n",
      "epoch: 11 step: 704, loss is 0.015112536959350109\n",
      "epoch: 11 step: 705, loss is 0.04182567447423935\n",
      "epoch: 11 step: 706, loss is 0.005674772430211306\n",
      "epoch: 11 step: 707, loss is 0.03663618490099907\n",
      "epoch: 11 step: 708, loss is 0.02202809415757656\n",
      "epoch: 11 step: 709, loss is 0.08802108466625214\n",
      "epoch: 11 step: 710, loss is 0.07027500122785568\n",
      "epoch: 11 step: 711, loss is 0.03533225879073143\n",
      "epoch: 11 step: 712, loss is 0.07024446129798889\n",
      "epoch: 11 step: 713, loss is 0.035714294761419296\n",
      "epoch: 11 step: 714, loss is 0.028324533253908157\n",
      "epoch: 11 step: 715, loss is 0.04287180304527283\n",
      "epoch: 11 step: 716, loss is 0.033337466418743134\n",
      "epoch: 11 step: 717, loss is 0.00536843528971076\n",
      "epoch: 11 step: 718, loss is 0.092414490878582\n",
      "epoch: 11 step: 719, loss is 0.036931853741407394\n",
      "epoch: 11 step: 720, loss is 0.0810922309756279\n",
      "epoch: 11 step: 721, loss is 0.13128504157066345\n",
      "epoch: 11 step: 722, loss is 0.0856570303440094\n",
      "epoch: 11 step: 723, loss is 0.039384420961141586\n",
      "epoch: 11 step: 724, loss is 0.03312255069613457\n",
      "epoch: 11 step: 725, loss is 0.04512836039066315\n",
      "epoch: 11 step: 726, loss is 0.02914953976869583\n",
      "epoch: 11 step: 727, loss is 0.07449041306972504\n",
      "epoch: 11 step: 728, loss is 0.11464649438858032\n",
      "epoch: 11 step: 729, loss is 0.028572585433721542\n",
      "epoch: 11 step: 730, loss is 0.02182283252477646\n",
      "epoch: 11 step: 731, loss is 0.05188477039337158\n",
      "epoch: 11 step: 732, loss is 0.04627695679664612\n",
      "epoch: 11 step: 733, loss is 0.006640015635639429\n",
      "epoch: 11 step: 734, loss is 0.008360331878066063\n",
      "epoch: 11 step: 735, loss is 0.057597413659095764\n",
      "epoch: 11 step: 736, loss is 0.10207217931747437\n",
      "epoch: 11 step: 737, loss is 0.06255803257226944\n",
      "epoch: 11 step: 738, loss is 0.0195718165487051\n",
      "epoch: 11 step: 739, loss is 0.05362377315759659\n",
      "epoch: 11 step: 740, loss is 0.041372086852788925\n",
      "epoch: 11 step: 741, loss is 0.02749164216220379\n",
      "epoch: 11 step: 742, loss is 0.11932436376810074\n",
      "epoch: 11 step: 743, loss is 0.03546435385942459\n",
      "epoch: 11 step: 744, loss is 0.12853530049324036\n",
      "epoch: 11 step: 745, loss is 0.037707261741161346\n",
      "epoch: 11 step: 746, loss is 0.006722855847328901\n",
      "epoch: 11 step: 747, loss is 0.023025264963507652\n",
      "epoch: 11 step: 748, loss is 0.028861474245786667\n",
      "epoch: 11 step: 749, loss is 0.007577017415314913\n",
      "epoch: 11 step: 750, loss is 0.1190560907125473\n",
      "epoch: 11 step: 751, loss is 0.08204390853643417\n",
      "epoch: 11 step: 752, loss is 0.01562107540667057\n",
      "epoch: 11 step: 753, loss is 0.0030820032116025686\n",
      "epoch: 11 step: 754, loss is 0.04307251423597336\n",
      "epoch: 11 step: 755, loss is 0.018149377778172493\n",
      "epoch: 11 step: 756, loss is 0.016794413328170776\n",
      "epoch: 11 step: 757, loss is 0.040082160383462906\n",
      "epoch: 11 step: 758, loss is 0.05904848501086235\n",
      "epoch: 11 step: 759, loss is 0.01838134601712227\n",
      "epoch: 11 step: 760, loss is 0.015926040709018707\n",
      "epoch: 11 step: 761, loss is 0.01752380095422268\n",
      "epoch: 11 step: 762, loss is 0.05177159234881401\n",
      "epoch: 11 step: 763, loss is 0.016064966097474098\n",
      "epoch: 11 step: 764, loss is 0.047988638281822205\n",
      "epoch: 11 step: 765, loss is 0.0035425457172095776\n",
      "epoch: 11 step: 766, loss is 0.025787318125367165\n",
      "epoch: 11 step: 767, loss is 0.044046755880117416\n",
      "epoch: 11 step: 768, loss is 0.032252222299575806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 769, loss is 0.03660102188587189\n",
      "epoch: 11 step: 770, loss is 0.01437405962496996\n",
      "epoch: 11 step: 771, loss is 0.06054745614528656\n",
      "epoch: 11 step: 772, loss is 0.057139649987220764\n",
      "epoch: 11 step: 773, loss is 0.06444182246923447\n",
      "epoch: 11 step: 774, loss is 0.0291398037225008\n",
      "epoch: 11 step: 775, loss is 0.009613513946533203\n",
      "epoch: 11 step: 776, loss is 0.009579823352396488\n",
      "epoch: 11 step: 777, loss is 0.06757574528455734\n",
      "epoch: 11 step: 778, loss is 0.022337177768349648\n",
      "epoch: 11 step: 779, loss is 0.072077177464962\n",
      "epoch: 11 step: 780, loss is 0.009427516721189022\n",
      "epoch: 11 step: 781, loss is 0.0634179338812828\n",
      "epoch: 11 step: 782, loss is 0.13311955332756042\n",
      "epoch: 11 step: 783, loss is 0.008723764680325985\n",
      "epoch: 11 step: 784, loss is 0.07344495505094528\n",
      "epoch: 11 step: 785, loss is 0.04012809693813324\n",
      "epoch: 11 step: 786, loss is 0.017643611878156662\n",
      "epoch: 11 step: 787, loss is 0.06307917833328247\n",
      "epoch: 11 step: 788, loss is 0.09455204010009766\n",
      "epoch: 11 step: 789, loss is 0.01354701817035675\n",
      "epoch: 11 step: 790, loss is 0.06756971031427383\n",
      "epoch: 11 step: 791, loss is 0.047007039189338684\n",
      "epoch: 11 step: 792, loss is 0.05571218952536583\n",
      "epoch: 11 step: 793, loss is 0.10149473696947098\n",
      "epoch: 11 step: 794, loss is 0.1404844969511032\n",
      "epoch: 11 step: 795, loss is 0.009730239398777485\n",
      "epoch: 11 step: 796, loss is 0.06606226414442062\n",
      "epoch: 11 step: 797, loss is 0.04906236752867699\n",
      "epoch: 11 step: 798, loss is 0.045511551201343536\n",
      "epoch: 11 step: 799, loss is 0.0036817099899053574\n",
      "epoch: 11 step: 800, loss is 0.005765640642493963\n",
      "epoch: 11 step: 801, loss is 0.17462973296642303\n",
      "epoch: 11 step: 802, loss is 0.03298190236091614\n",
      "epoch: 11 step: 803, loss is 0.11743327230215073\n",
      "epoch: 11 step: 804, loss is 0.016244078055024147\n",
      "epoch: 11 step: 805, loss is 0.09421738237142563\n",
      "epoch: 11 step: 806, loss is 0.051161229610443115\n",
      "epoch: 11 step: 807, loss is 0.007899986580014229\n",
      "epoch: 11 step: 808, loss is 0.06208159402012825\n",
      "epoch: 11 step: 809, loss is 0.05052364990115166\n",
      "epoch: 11 step: 810, loss is 0.010126687586307526\n",
      "epoch: 11 step: 811, loss is 0.043482303619384766\n",
      "epoch: 11 step: 812, loss is 0.012915055267512798\n",
      "epoch: 11 step: 813, loss is 0.10395369678735733\n",
      "epoch: 11 step: 814, loss is 0.028355693444609642\n",
      "epoch: 11 step: 815, loss is 0.035685427486896515\n",
      "epoch: 11 step: 816, loss is 0.08143070340156555\n",
      "epoch: 11 step: 817, loss is 0.13554197549819946\n",
      "epoch: 11 step: 818, loss is 0.1776568442583084\n",
      "epoch: 11 step: 819, loss is 0.13573646545410156\n",
      "epoch: 11 step: 820, loss is 0.04485278204083443\n",
      "epoch: 11 step: 821, loss is 0.006991504225879908\n",
      "epoch: 11 step: 822, loss is 0.026809770613908768\n",
      "epoch: 11 step: 823, loss is 0.09141290187835693\n",
      "epoch: 11 step: 824, loss is 0.014069936238229275\n",
      "epoch: 11 step: 825, loss is 0.07603523880243301\n",
      "epoch: 11 step: 826, loss is 0.058838896453380585\n",
      "epoch: 11 step: 827, loss is 0.019132379442453384\n",
      "epoch: 11 step: 828, loss is 0.02564164251089096\n",
      "epoch: 11 step: 829, loss is 0.17387127876281738\n",
      "epoch: 11 step: 830, loss is 0.08147574216127396\n",
      "epoch: 11 step: 831, loss is 0.06961781531572342\n",
      "epoch: 11 step: 832, loss is 0.045987319201231\n",
      "epoch: 11 step: 833, loss is 0.008475546725094318\n",
      "epoch: 11 step: 834, loss is 0.02906951494514942\n",
      "epoch: 11 step: 835, loss is 0.039587728679180145\n",
      "epoch: 11 step: 836, loss is 0.03186696022748947\n",
      "epoch: 11 step: 837, loss is 0.016980674117803574\n",
      "epoch: 11 step: 838, loss is 0.06099434569478035\n",
      "epoch: 11 step: 839, loss is 0.02492850460112095\n",
      "epoch: 11 step: 840, loss is 0.02062915451824665\n",
      "epoch: 11 step: 841, loss is 0.035204604268074036\n",
      "epoch: 11 step: 842, loss is 0.006107188295572996\n",
      "epoch: 11 step: 843, loss is 0.007763729430735111\n",
      "epoch: 11 step: 844, loss is 0.05849578231573105\n",
      "epoch: 11 step: 845, loss is 0.058711376041173935\n",
      "epoch: 11 step: 846, loss is 0.10482413321733475\n",
      "epoch: 11 step: 847, loss is 0.1635381281375885\n",
      "epoch: 11 step: 848, loss is 0.14875797927379608\n",
      "epoch: 11 step: 849, loss is 0.005496981553733349\n",
      "epoch: 11 step: 850, loss is 0.062430478632450104\n",
      "epoch: 11 step: 851, loss is 0.029727747663855553\n",
      "epoch: 11 step: 852, loss is 0.17346780002117157\n",
      "epoch: 11 step: 853, loss is 0.027535567060112953\n",
      "epoch: 11 step: 854, loss is 0.026442574337124825\n",
      "epoch: 11 step: 855, loss is 0.024379098787903786\n",
      "epoch: 11 step: 856, loss is 0.08757555484771729\n",
      "epoch: 11 step: 857, loss is 0.05940806120634079\n",
      "epoch: 11 step: 858, loss is 0.07693477720022202\n",
      "epoch: 11 step: 859, loss is 0.14227406680583954\n",
      "epoch: 11 step: 860, loss is 0.030555138364434242\n",
      "epoch: 11 step: 861, loss is 0.04900844395160675\n",
      "epoch: 11 step: 862, loss is 0.05932082235813141\n",
      "epoch: 11 step: 863, loss is 0.26788637042045593\n",
      "epoch: 11 step: 864, loss is 0.089607834815979\n",
      "epoch: 11 step: 865, loss is 0.04638276994228363\n",
      "epoch: 11 step: 866, loss is 0.08233778178691864\n",
      "epoch: 11 step: 867, loss is 0.012234903872013092\n",
      "epoch: 11 step: 868, loss is 0.011502575129270554\n",
      "epoch: 11 step: 869, loss is 0.03036564774811268\n",
      "epoch: 11 step: 870, loss is 0.0090261772274971\n",
      "epoch: 11 step: 871, loss is 0.06511525809764862\n",
      "epoch: 11 step: 872, loss is 0.017630744725465775\n",
      "epoch: 11 step: 873, loss is 0.06709080934524536\n",
      "epoch: 11 step: 874, loss is 0.01515115238726139\n",
      "epoch: 11 step: 875, loss is 0.020755132660269737\n",
      "epoch: 11 step: 876, loss is 0.03837735205888748\n",
      "epoch: 11 step: 877, loss is 0.15551424026489258\n",
      "epoch: 11 step: 878, loss is 0.13258548080921173\n",
      "epoch: 11 step: 879, loss is 0.10434061288833618\n",
      "epoch: 11 step: 880, loss is 0.03981459513306618\n",
      "epoch: 11 step: 881, loss is 0.05968201905488968\n",
      "epoch: 11 step: 882, loss is 0.0524739995598793\n",
      "epoch: 11 step: 883, loss is 0.018529130145907402\n",
      "epoch: 11 step: 884, loss is 0.08018744736909866\n",
      "epoch: 11 step: 885, loss is 0.06557796150445938\n",
      "epoch: 11 step: 886, loss is 0.064535953104496\n",
      "epoch: 11 step: 887, loss is 0.012398656457662582\n",
      "epoch: 11 step: 888, loss is 0.1216314360499382\n",
      "epoch: 11 step: 889, loss is 0.006870521232485771\n",
      "epoch: 11 step: 890, loss is 0.1015482023358345\n",
      "epoch: 11 step: 891, loss is 0.16147048771381378\n",
      "epoch: 11 step: 892, loss is 0.1421893984079361\n",
      "epoch: 11 step: 893, loss is 0.0514969639480114\n",
      "epoch: 11 step: 894, loss is 0.00791092962026596\n",
      "epoch: 11 step: 895, loss is 0.03544270992279053\n",
      "epoch: 11 step: 896, loss is 0.09917528927326202\n",
      "epoch: 11 step: 897, loss is 0.07533878833055496\n",
      "epoch: 11 step: 898, loss is 0.06628533452749252\n",
      "epoch: 11 step: 899, loss is 0.07966453582048416\n",
      "epoch: 11 step: 900, loss is 0.03208836540579796\n",
      "epoch: 11 step: 901, loss is 0.040517162531614304\n",
      "epoch: 11 step: 902, loss is 0.0830795094370842\n",
      "epoch: 11 step: 903, loss is 0.009890423156321049\n",
      "epoch: 11 step: 904, loss is 0.048761580139398575\n",
      "epoch: 11 step: 905, loss is 0.011427083052694798\n",
      "epoch: 11 step: 906, loss is 0.02793416753411293\n",
      "epoch: 11 step: 907, loss is 0.030300956219434738\n",
      "epoch: 11 step: 908, loss is 0.015480881556868553\n",
      "epoch: 11 step: 909, loss is 0.13103386759757996\n",
      "epoch: 11 step: 910, loss is 0.041818682104349136\n",
      "epoch: 11 step: 911, loss is 0.045645810663700104\n",
      "epoch: 11 step: 912, loss is 0.052120283246040344\n",
      "epoch: 11 step: 913, loss is 0.058858759701251984\n",
      "epoch: 11 step: 914, loss is 0.021538082510232925\n",
      "epoch: 11 step: 915, loss is 0.010198745876550674\n",
      "epoch: 11 step: 916, loss is 0.011466648429632187\n",
      "epoch: 11 step: 917, loss is 0.05138356238603592\n",
      "epoch: 11 step: 918, loss is 0.15294979512691498\n",
      "epoch: 11 step: 919, loss is 0.01969996653497219\n",
      "epoch: 11 step: 920, loss is 0.06977593153715134\n",
      "epoch: 11 step: 921, loss is 0.027773616835474968\n",
      "epoch: 11 step: 922, loss is 0.05406201630830765\n",
      "epoch: 11 step: 923, loss is 0.020803282037377357\n",
      "epoch: 11 step: 924, loss is 0.020149780437350273\n",
      "epoch: 11 step: 925, loss is 0.07769223302602768\n",
      "epoch: 11 step: 926, loss is 0.14193940162658691\n",
      "epoch: 11 step: 927, loss is 0.12075351178646088\n",
      "epoch: 11 step: 928, loss is 0.009922817349433899\n",
      "epoch: 11 step: 929, loss is 0.20797841250896454\n",
      "epoch: 11 step: 930, loss is 0.13132396340370178\n",
      "epoch: 11 step: 931, loss is 0.08572209626436234\n",
      "epoch: 11 step: 932, loss is 0.07219994068145752\n",
      "epoch: 11 step: 933, loss is 0.19272173941135406\n",
      "epoch: 11 step: 934, loss is 0.048050880432128906\n",
      "epoch: 11 step: 935, loss is 0.01647699996829033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 936, loss is 0.09665458649396896\n",
      "epoch: 11 step: 937, loss is 0.017351193353533745\n",
      "epoch: 12 step: 1, loss is 0.014241882599890232\n",
      "epoch: 12 step: 2, loss is 0.002861477667465806\n",
      "epoch: 12 step: 3, loss is 0.04467049986124039\n",
      "epoch: 12 step: 4, loss is 0.016182200983166695\n",
      "epoch: 12 step: 5, loss is 0.042142096906900406\n",
      "epoch: 12 step: 6, loss is 0.033737219870090485\n",
      "epoch: 12 step: 7, loss is 0.07273935526609421\n",
      "epoch: 12 step: 8, loss is 0.011552057228982449\n",
      "epoch: 12 step: 9, loss is 0.043004944920539856\n",
      "epoch: 12 step: 10, loss is 0.007475879043340683\n",
      "epoch: 12 step: 11, loss is 0.053265977650880814\n",
      "epoch: 12 step: 12, loss is 0.019031206145882607\n",
      "epoch: 12 step: 13, loss is 0.009972970001399517\n",
      "epoch: 12 step: 14, loss is 0.033264268189668655\n",
      "epoch: 12 step: 15, loss is 0.04472483694553375\n",
      "epoch: 12 step: 16, loss is 0.03254834562540054\n",
      "epoch: 12 step: 17, loss is 0.023679818958044052\n",
      "epoch: 12 step: 18, loss is 0.005753900855779648\n",
      "epoch: 12 step: 19, loss is 0.06474192440509796\n",
      "epoch: 12 step: 20, loss is 0.043045368045568466\n",
      "epoch: 12 step: 21, loss is 0.16212202608585358\n",
      "epoch: 12 step: 22, loss is 0.03841715678572655\n",
      "epoch: 12 step: 23, loss is 0.06631872802972794\n",
      "epoch: 12 step: 24, loss is 0.06629082560539246\n",
      "epoch: 12 step: 25, loss is 0.05083458870649338\n",
      "epoch: 12 step: 26, loss is 0.008041009306907654\n",
      "epoch: 12 step: 27, loss is 0.023846544325351715\n",
      "epoch: 12 step: 28, loss is 0.006388056557625532\n",
      "epoch: 12 step: 29, loss is 0.01111939549446106\n",
      "epoch: 12 step: 30, loss is 0.04378994554281235\n",
      "epoch: 12 step: 31, loss is 0.07008866220712662\n",
      "epoch: 12 step: 32, loss is 0.03901875019073486\n",
      "epoch: 12 step: 33, loss is 0.061165135353803635\n",
      "epoch: 12 step: 34, loss is 0.017911525443196297\n",
      "epoch: 12 step: 35, loss is 0.024107083678245544\n",
      "epoch: 12 step: 36, loss is 0.00783701054751873\n",
      "epoch: 12 step: 37, loss is 0.05519283190369606\n",
      "epoch: 12 step: 38, loss is 0.010219920426607132\n",
      "epoch: 12 step: 39, loss is 0.002955810399726033\n",
      "epoch: 12 step: 40, loss is 0.028140544891357422\n",
      "epoch: 12 step: 41, loss is 0.012807858176529408\n",
      "epoch: 12 step: 42, loss is 0.013110417872667313\n",
      "epoch: 12 step: 43, loss is 0.044230833649635315\n",
      "epoch: 12 step: 44, loss is 0.018468383699655533\n",
      "epoch: 12 step: 45, loss is 0.040606483817100525\n",
      "epoch: 12 step: 46, loss is 0.020740054547786713\n",
      "epoch: 12 step: 47, loss is 0.018745051696896553\n",
      "epoch: 12 step: 48, loss is 0.02483554370701313\n",
      "epoch: 12 step: 49, loss is 0.05279603600502014\n",
      "epoch: 12 step: 50, loss is 0.05062305927276611\n",
      "epoch: 12 step: 51, loss is 0.011007992550730705\n",
      "epoch: 12 step: 52, loss is 0.015580817125737667\n",
      "epoch: 12 step: 53, loss is 0.020965196192264557\n",
      "epoch: 12 step: 54, loss is 0.012163480743765831\n",
      "epoch: 12 step: 55, loss is 0.006676245480775833\n",
      "epoch: 12 step: 56, loss is 0.03080412745475769\n",
      "epoch: 12 step: 57, loss is 0.03759073466062546\n",
      "epoch: 12 step: 58, loss is 0.04045475646853447\n",
      "epoch: 12 step: 59, loss is 0.013455395586788654\n",
      "epoch: 12 step: 60, loss is 0.007155926898121834\n",
      "epoch: 12 step: 61, loss is 0.005823152139782906\n",
      "epoch: 12 step: 62, loss is 0.010069703683257103\n",
      "epoch: 12 step: 63, loss is 0.021483702585101128\n",
      "epoch: 12 step: 64, loss is 0.03935404494404793\n",
      "epoch: 12 step: 65, loss is 0.01557637844234705\n",
      "epoch: 12 step: 66, loss is 0.04255148768424988\n",
      "epoch: 12 step: 67, loss is 0.07659649848937988\n",
      "epoch: 12 step: 68, loss is 0.038510970771312714\n",
      "epoch: 12 step: 69, loss is 0.03588980436325073\n",
      "epoch: 12 step: 70, loss is 0.08662968873977661\n",
      "epoch: 12 step: 71, loss is 0.0008370921132154763\n",
      "epoch: 12 step: 72, loss is 0.047262243926525116\n",
      "epoch: 12 step: 73, loss is 0.06015859544277191\n",
      "epoch: 12 step: 74, loss is 0.011650345288217068\n",
      "epoch: 12 step: 75, loss is 0.019357385113835335\n",
      "epoch: 12 step: 76, loss is 0.006733457092195749\n",
      "epoch: 12 step: 77, loss is 0.051897913217544556\n",
      "epoch: 12 step: 78, loss is 0.013220570050179958\n",
      "epoch: 12 step: 79, loss is 0.057985819876194\n",
      "epoch: 12 step: 80, loss is 0.18775822222232819\n",
      "epoch: 12 step: 81, loss is 0.01338538434356451\n",
      "epoch: 12 step: 82, loss is 0.01768520101904869\n",
      "epoch: 12 step: 83, loss is 0.07633529603481293\n",
      "epoch: 12 step: 84, loss is 0.007287504151463509\n",
      "epoch: 12 step: 85, loss is 0.011493104510009289\n",
      "epoch: 12 step: 86, loss is 0.026245543733239174\n",
      "epoch: 12 step: 87, loss is 0.03578872233629227\n",
      "epoch: 12 step: 88, loss is 0.03776191547513008\n",
      "epoch: 12 step: 89, loss is 0.026356812566518784\n",
      "epoch: 12 step: 90, loss is 0.04881603270769119\n",
      "epoch: 12 step: 91, loss is 0.02530805952847004\n",
      "epoch: 12 step: 92, loss is 0.027664219960570335\n",
      "epoch: 12 step: 93, loss is 0.021037640050053596\n",
      "epoch: 12 step: 94, loss is 0.0337466336786747\n",
      "epoch: 12 step: 95, loss is 0.09066842496395111\n",
      "epoch: 12 step: 96, loss is 0.035888463258743286\n",
      "epoch: 12 step: 97, loss is 0.00562576949596405\n",
      "epoch: 12 step: 98, loss is 0.01418891828507185\n",
      "epoch: 12 step: 99, loss is 0.0033918069675564766\n",
      "epoch: 12 step: 100, loss is 0.010730109177529812\n",
      "epoch: 12 step: 101, loss is 0.012606065720319748\n",
      "epoch: 12 step: 102, loss is 0.0580710731446743\n",
      "epoch: 12 step: 103, loss is 0.009081268683075905\n",
      "epoch: 12 step: 104, loss is 0.0018448339542374015\n",
      "epoch: 12 step: 105, loss is 0.02484584040939808\n",
      "epoch: 12 step: 106, loss is 0.05124308541417122\n",
      "epoch: 12 step: 107, loss is 0.009327846579253674\n",
      "epoch: 12 step: 108, loss is 0.008774197660386562\n",
      "epoch: 12 step: 109, loss is 0.018456509336829185\n",
      "epoch: 12 step: 110, loss is 0.0038988718297332525\n",
      "epoch: 12 step: 111, loss is 0.006026171613484621\n",
      "epoch: 12 step: 112, loss is 0.07346615195274353\n",
      "epoch: 12 step: 113, loss is 0.010652720928192139\n",
      "epoch: 12 step: 114, loss is 0.057706091552972794\n",
      "epoch: 12 step: 115, loss is 0.01781197264790535\n",
      "epoch: 12 step: 116, loss is 0.01636044681072235\n",
      "epoch: 12 step: 117, loss is 0.013394927605986595\n",
      "epoch: 12 step: 118, loss is 0.013350202701985836\n",
      "epoch: 12 step: 119, loss is 0.0583336241543293\n",
      "epoch: 12 step: 120, loss is 0.0012785859871655703\n",
      "epoch: 12 step: 121, loss is 0.1619023233652115\n",
      "epoch: 12 step: 122, loss is 0.003292099107056856\n",
      "epoch: 12 step: 123, loss is 0.0261828750371933\n",
      "epoch: 12 step: 124, loss is 0.014020204544067383\n",
      "epoch: 12 step: 125, loss is 0.0032679883297532797\n",
      "epoch: 12 step: 126, loss is 0.07001413404941559\n",
      "epoch: 12 step: 127, loss is 0.008478410542011261\n",
      "epoch: 12 step: 128, loss is 0.016316935420036316\n",
      "epoch: 12 step: 129, loss is 0.015615380369126797\n",
      "epoch: 12 step: 130, loss is 0.01501214038580656\n",
      "epoch: 12 step: 131, loss is 0.03874829411506653\n",
      "epoch: 12 step: 132, loss is 0.012389914132654667\n",
      "epoch: 12 step: 133, loss is 0.03375901281833649\n",
      "epoch: 12 step: 134, loss is 0.01599923148751259\n",
      "epoch: 12 step: 135, loss is 0.024793056771159172\n",
      "epoch: 12 step: 136, loss is 0.04353080317378044\n",
      "epoch: 12 step: 137, loss is 0.021046768873929977\n",
      "epoch: 12 step: 138, loss is 0.03591429442167282\n",
      "epoch: 12 step: 139, loss is 0.033730316907167435\n",
      "epoch: 12 step: 140, loss is 0.05732554569840431\n",
      "epoch: 12 step: 141, loss is 0.028328847140073776\n",
      "epoch: 12 step: 142, loss is 0.032238271087408066\n",
      "epoch: 12 step: 143, loss is 0.018107587471604347\n",
      "epoch: 12 step: 144, loss is 0.1443607211112976\n",
      "epoch: 12 step: 145, loss is 0.05868063494563103\n",
      "epoch: 12 step: 146, loss is 0.03913890942931175\n",
      "epoch: 12 step: 147, loss is 0.06265202164649963\n",
      "epoch: 12 step: 148, loss is 0.06086954101920128\n",
      "epoch: 12 step: 149, loss is 0.027041785418987274\n",
      "epoch: 12 step: 150, loss is 0.04734828323125839\n",
      "epoch: 12 step: 151, loss is 0.02384215034544468\n",
      "epoch: 12 step: 152, loss is 0.09839834272861481\n",
      "epoch: 12 step: 153, loss is 0.13472528755664825\n",
      "epoch: 12 step: 154, loss is 0.04880334436893463\n",
      "epoch: 12 step: 155, loss is 0.03640960529446602\n",
      "epoch: 12 step: 156, loss is 0.030670009553432465\n",
      "epoch: 12 step: 157, loss is 0.09369375556707382\n",
      "epoch: 12 step: 158, loss is 0.00757541274651885\n",
      "epoch: 12 step: 159, loss is 0.12017131596803665\n",
      "epoch: 12 step: 160, loss is 0.024640992283821106\n",
      "epoch: 12 step: 161, loss is 0.005167096853256226\n",
      "epoch: 12 step: 162, loss is 0.014227183535695076\n",
      "epoch: 12 step: 163, loss is 0.08982066065073013\n",
      "epoch: 12 step: 164, loss is 0.06310351938009262\n",
      "epoch: 12 step: 165, loss is 0.0043619838543236256\n",
      "epoch: 12 step: 166, loss is 0.009888316504657269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 167, loss is 0.026984024792909622\n",
      "epoch: 12 step: 168, loss is 0.02690732851624489\n",
      "epoch: 12 step: 169, loss is 0.03428696095943451\n",
      "epoch: 12 step: 170, loss is 0.03847401961684227\n",
      "epoch: 12 step: 171, loss is 0.014399776235222816\n",
      "epoch: 12 step: 172, loss is 0.07837914675474167\n",
      "epoch: 12 step: 173, loss is 0.044503312557935715\n",
      "epoch: 12 step: 174, loss is 0.012810668908059597\n",
      "epoch: 12 step: 175, loss is 0.006794213782995939\n",
      "epoch: 12 step: 176, loss is 0.014609009027481079\n",
      "epoch: 12 step: 177, loss is 0.004348548129200935\n",
      "epoch: 12 step: 178, loss is 0.01556325238198042\n",
      "epoch: 12 step: 179, loss is 0.016510002315044403\n",
      "epoch: 12 step: 180, loss is 0.022886939346790314\n",
      "epoch: 12 step: 181, loss is 0.052202921360731125\n",
      "epoch: 12 step: 182, loss is 0.013328210450708866\n",
      "epoch: 12 step: 183, loss is 0.004029522184282541\n",
      "epoch: 12 step: 184, loss is 0.014535645954310894\n",
      "epoch: 12 step: 185, loss is 0.044387321919202805\n",
      "epoch: 12 step: 186, loss is 0.04367229342460632\n",
      "epoch: 12 step: 187, loss is 0.08267134428024292\n",
      "epoch: 12 step: 188, loss is 0.00844681914895773\n",
      "epoch: 12 step: 189, loss is 0.014560818672180176\n",
      "epoch: 12 step: 190, loss is 0.0027584529016166925\n",
      "epoch: 12 step: 191, loss is 0.03074927069246769\n",
      "epoch: 12 step: 192, loss is 0.006082633044570684\n",
      "epoch: 12 step: 193, loss is 0.05800170078873634\n",
      "epoch: 12 step: 194, loss is 0.034291286021471024\n",
      "epoch: 12 step: 195, loss is 0.05272345244884491\n",
      "epoch: 12 step: 196, loss is 0.08263664692640305\n",
      "epoch: 12 step: 197, loss is 0.08004938811063766\n",
      "epoch: 12 step: 198, loss is 0.07815684378147125\n",
      "epoch: 12 step: 199, loss is 0.11728730797767639\n",
      "epoch: 12 step: 200, loss is 0.019419830292463303\n",
      "epoch: 12 step: 201, loss is 0.008893577381968498\n",
      "epoch: 12 step: 202, loss is 0.026827571913599968\n",
      "epoch: 12 step: 203, loss is 0.04407265782356262\n",
      "epoch: 12 step: 204, loss is 0.028830748051404953\n",
      "epoch: 12 step: 205, loss is 0.016991417855024338\n",
      "epoch: 12 step: 206, loss is 0.0064356387592852116\n",
      "epoch: 12 step: 207, loss is 0.004897734150290489\n",
      "epoch: 12 step: 208, loss is 0.018461747094988823\n",
      "epoch: 12 step: 209, loss is 0.01278398372232914\n",
      "epoch: 12 step: 210, loss is 0.04032642021775246\n",
      "epoch: 12 step: 211, loss is 0.006292551290243864\n",
      "epoch: 12 step: 212, loss is 0.04625473544001579\n",
      "epoch: 12 step: 213, loss is 0.03339061141014099\n",
      "epoch: 12 step: 214, loss is 0.020915750414133072\n",
      "epoch: 12 step: 215, loss is 0.033539067953825\n",
      "epoch: 12 step: 216, loss is 0.002276443410664797\n",
      "epoch: 12 step: 217, loss is 0.012885061092674732\n",
      "epoch: 12 step: 218, loss is 0.019170381128787994\n",
      "epoch: 12 step: 219, loss is 0.030042359605431557\n",
      "epoch: 12 step: 220, loss is 0.055729497224092484\n",
      "epoch: 12 step: 221, loss is 0.00859527662396431\n",
      "epoch: 12 step: 222, loss is 0.008662429638206959\n",
      "epoch: 12 step: 223, loss is 0.025256361812353134\n",
      "epoch: 12 step: 224, loss is 0.024894237518310547\n",
      "epoch: 12 step: 225, loss is 0.043723344802856445\n",
      "epoch: 12 step: 226, loss is 0.022531883791089058\n",
      "epoch: 12 step: 227, loss is 0.01470306795090437\n",
      "epoch: 12 step: 228, loss is 0.015600617974996567\n",
      "epoch: 12 step: 229, loss is 0.0054013412445783615\n",
      "epoch: 12 step: 230, loss is 0.0708453431725502\n",
      "epoch: 12 step: 231, loss is 0.06144332513213158\n",
      "epoch: 12 step: 232, loss is 0.11738792061805725\n",
      "epoch: 12 step: 233, loss is 0.012476904317736626\n",
      "epoch: 12 step: 234, loss is 0.018576694652438164\n",
      "epoch: 12 step: 235, loss is 0.03591420501470566\n",
      "epoch: 12 step: 236, loss is 0.009336598217487335\n",
      "epoch: 12 step: 237, loss is 0.08459749817848206\n",
      "epoch: 12 step: 238, loss is 0.018254727125167847\n",
      "epoch: 12 step: 239, loss is 0.011991772800683975\n",
      "epoch: 12 step: 240, loss is 0.030741311609745026\n",
      "epoch: 12 step: 241, loss is 0.01240769773721695\n",
      "epoch: 12 step: 242, loss is 0.046284936368465424\n",
      "epoch: 12 step: 243, loss is 0.049652330577373505\n",
      "epoch: 12 step: 244, loss is 0.0030429356265813112\n",
      "epoch: 12 step: 245, loss is 0.0266922228038311\n",
      "epoch: 12 step: 246, loss is 0.019778262823820114\n",
      "epoch: 12 step: 247, loss is 0.008574514649808407\n",
      "epoch: 12 step: 248, loss is 0.009770425036549568\n",
      "epoch: 12 step: 249, loss is 0.11060504615306854\n",
      "epoch: 12 step: 250, loss is 0.03493504598736763\n",
      "epoch: 12 step: 251, loss is 0.03892134130001068\n",
      "epoch: 12 step: 252, loss is 0.0801522433757782\n",
      "epoch: 12 step: 253, loss is 0.026846444234251976\n",
      "epoch: 12 step: 254, loss is 0.022587426006793976\n",
      "epoch: 12 step: 255, loss is 0.023313935846090317\n",
      "epoch: 12 step: 256, loss is 0.028217565268278122\n",
      "epoch: 12 step: 257, loss is 0.03556114807724953\n",
      "epoch: 12 step: 258, loss is 0.03578738495707512\n",
      "epoch: 12 step: 259, loss is 0.018623368814587593\n",
      "epoch: 12 step: 260, loss is 0.05519622564315796\n",
      "epoch: 12 step: 261, loss is 0.017428141087293625\n",
      "epoch: 12 step: 262, loss is 0.01700940914452076\n",
      "epoch: 12 step: 263, loss is 0.005822604987770319\n",
      "epoch: 12 step: 264, loss is 0.04277665540575981\n",
      "epoch: 12 step: 265, loss is 0.013032082468271255\n",
      "epoch: 12 step: 266, loss is 0.002654830925166607\n",
      "epoch: 12 step: 267, loss is 0.003522446844726801\n",
      "epoch: 12 step: 268, loss is 0.0032023987732827663\n",
      "epoch: 12 step: 269, loss is 0.027075156569480896\n",
      "epoch: 12 step: 270, loss is 0.003153036115691066\n",
      "epoch: 12 step: 271, loss is 0.05674295499920845\n",
      "epoch: 12 step: 272, loss is 0.05276020988821983\n",
      "epoch: 12 step: 273, loss is 0.015651417896151543\n",
      "epoch: 12 step: 274, loss is 0.001927521894685924\n",
      "epoch: 12 step: 275, loss is 0.08476484566926956\n",
      "epoch: 12 step: 276, loss is 0.02833634242415428\n",
      "epoch: 12 step: 277, loss is 0.032718855887651443\n",
      "epoch: 12 step: 278, loss is 0.09095268696546555\n",
      "epoch: 12 step: 279, loss is 0.04729713872075081\n",
      "epoch: 12 step: 280, loss is 0.015299353748559952\n",
      "epoch: 12 step: 281, loss is 0.02682088129222393\n",
      "epoch: 12 step: 282, loss is 0.021615048870444298\n",
      "epoch: 12 step: 283, loss is 0.022962985560297966\n",
      "epoch: 12 step: 284, loss is 0.01444449182599783\n",
      "epoch: 12 step: 285, loss is 0.04586494341492653\n",
      "epoch: 12 step: 286, loss is 0.016293341293931007\n",
      "epoch: 12 step: 287, loss is 0.020317763090133667\n",
      "epoch: 12 step: 288, loss is 0.011486993171274662\n",
      "epoch: 12 step: 289, loss is 0.01731414534151554\n",
      "epoch: 12 step: 290, loss is 0.04412030801177025\n",
      "epoch: 12 step: 291, loss is 0.003843950806185603\n",
      "epoch: 12 step: 292, loss is 0.024012409150600433\n",
      "epoch: 12 step: 293, loss is 0.05318653956055641\n",
      "epoch: 12 step: 294, loss is 0.062303327023983\n",
      "epoch: 12 step: 295, loss is 0.03458765521645546\n",
      "epoch: 12 step: 296, loss is 0.0658562183380127\n",
      "epoch: 12 step: 297, loss is 0.018228475004434586\n",
      "epoch: 12 step: 298, loss is 0.03583579510450363\n",
      "epoch: 12 step: 299, loss is 0.07663831859827042\n",
      "epoch: 12 step: 300, loss is 0.03486198931932449\n",
      "epoch: 12 step: 301, loss is 0.03486288711428642\n",
      "epoch: 12 step: 302, loss is 0.01050754263997078\n",
      "epoch: 12 step: 303, loss is 0.1451888531446457\n",
      "epoch: 12 step: 304, loss is 0.029354240745306015\n",
      "epoch: 12 step: 305, loss is 0.11791455745697021\n",
      "epoch: 12 step: 306, loss is 0.005627679638564587\n",
      "epoch: 12 step: 307, loss is 0.010831928811967373\n",
      "epoch: 12 step: 308, loss is 0.004945793654769659\n",
      "epoch: 12 step: 309, loss is 0.009276945143938065\n",
      "epoch: 12 step: 310, loss is 0.023897036910057068\n",
      "epoch: 12 step: 311, loss is 0.0275541041046381\n",
      "epoch: 12 step: 312, loss is 0.004613847937434912\n",
      "epoch: 12 step: 313, loss is 0.03498975187540054\n",
      "epoch: 12 step: 314, loss is 0.011720444075763226\n",
      "epoch: 12 step: 315, loss is 0.10000314563512802\n",
      "epoch: 12 step: 316, loss is 0.058700282126665115\n",
      "epoch: 12 step: 317, loss is 0.01874014362692833\n",
      "epoch: 12 step: 318, loss is 0.022567078471183777\n",
      "epoch: 12 step: 319, loss is 0.02011568285524845\n",
      "epoch: 12 step: 320, loss is 0.056868888437747955\n",
      "epoch: 12 step: 321, loss is 0.05799620598554611\n",
      "epoch: 12 step: 322, loss is 0.09576202183961868\n",
      "epoch: 12 step: 323, loss is 0.04382095858454704\n",
      "epoch: 12 step: 324, loss is 0.02674572356045246\n",
      "epoch: 12 step: 325, loss is 0.028606360778212547\n",
      "epoch: 12 step: 326, loss is 0.08448587357997894\n",
      "epoch: 12 step: 327, loss is 0.04609951749444008\n",
      "epoch: 12 step: 328, loss is 0.023549307137727737\n",
      "epoch: 12 step: 329, loss is 0.0045775156468153\n",
      "epoch: 12 step: 330, loss is 0.009990975260734558\n",
      "epoch: 12 step: 331, loss is 0.02228478714823723\n",
      "epoch: 12 step: 332, loss is 0.011307232081890106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 333, loss is 0.026354195550084114\n",
      "epoch: 12 step: 334, loss is 0.00593265937641263\n",
      "epoch: 12 step: 335, loss is 0.01121661439538002\n",
      "epoch: 12 step: 336, loss is 0.0754760354757309\n",
      "epoch: 12 step: 337, loss is 0.019133921712636948\n",
      "epoch: 12 step: 338, loss is 0.0038547904696315527\n",
      "epoch: 12 step: 339, loss is 0.04097813367843628\n",
      "epoch: 12 step: 340, loss is 0.08846548199653625\n",
      "epoch: 12 step: 341, loss is 0.017377041280269623\n",
      "epoch: 12 step: 342, loss is 0.07638775557279587\n",
      "epoch: 12 step: 343, loss is 0.06868499517440796\n",
      "epoch: 12 step: 344, loss is 0.01620953343808651\n",
      "epoch: 12 step: 345, loss is 0.008128033950924873\n",
      "epoch: 12 step: 346, loss is 0.056020669639110565\n",
      "epoch: 12 step: 347, loss is 0.07743561267852783\n",
      "epoch: 12 step: 348, loss is 0.050775472074747086\n",
      "epoch: 12 step: 349, loss is 0.011466041207313538\n",
      "epoch: 12 step: 350, loss is 0.09394431114196777\n",
      "epoch: 12 step: 351, loss is 0.08137636631727219\n",
      "epoch: 12 step: 352, loss is 0.07702334970235825\n",
      "epoch: 12 step: 353, loss is 0.0023500160314142704\n",
      "epoch: 12 step: 354, loss is 0.010569763369858265\n",
      "epoch: 12 step: 355, loss is 0.030137745663523674\n",
      "epoch: 12 step: 356, loss is 0.015583124943077564\n",
      "epoch: 12 step: 357, loss is 0.05435006320476532\n",
      "epoch: 12 step: 358, loss is 0.004445044323801994\n",
      "epoch: 12 step: 359, loss is 0.01386498473584652\n",
      "epoch: 12 step: 360, loss is 0.0763988196849823\n",
      "epoch: 12 step: 361, loss is 0.01359498780220747\n",
      "epoch: 12 step: 362, loss is 0.05471309646964073\n",
      "epoch: 12 step: 363, loss is 0.009011754766106606\n",
      "epoch: 12 step: 364, loss is 0.03621215000748634\n",
      "epoch: 12 step: 365, loss is 0.03479209542274475\n",
      "epoch: 12 step: 366, loss is 0.010055403225123882\n",
      "epoch: 12 step: 367, loss is 0.047369085252285004\n",
      "epoch: 12 step: 368, loss is 0.059247348457574844\n",
      "epoch: 12 step: 369, loss is 0.05296481028199196\n",
      "epoch: 12 step: 370, loss is 0.0056011569686234\n",
      "epoch: 12 step: 371, loss is 0.07945234328508377\n",
      "epoch: 12 step: 372, loss is 0.00967957079410553\n",
      "epoch: 12 step: 373, loss is 0.04275008663535118\n",
      "epoch: 12 step: 374, loss is 0.10909395664930344\n",
      "epoch: 12 step: 375, loss is 0.02332139015197754\n",
      "epoch: 12 step: 376, loss is 0.007165469694882631\n",
      "epoch: 12 step: 377, loss is 0.002082149963825941\n",
      "epoch: 12 step: 378, loss is 0.04052475094795227\n",
      "epoch: 12 step: 379, loss is 0.022513823583722115\n",
      "epoch: 12 step: 380, loss is 0.008039196021854877\n",
      "epoch: 12 step: 381, loss is 0.0984397679567337\n",
      "epoch: 12 step: 382, loss is 0.035973355174064636\n",
      "epoch: 12 step: 383, loss is 0.07777522504329681\n",
      "epoch: 12 step: 384, loss is 0.03407665714621544\n",
      "epoch: 12 step: 385, loss is 0.1447780579328537\n",
      "epoch: 12 step: 386, loss is 0.0958966389298439\n",
      "epoch: 12 step: 387, loss is 0.1511957049369812\n",
      "epoch: 12 step: 388, loss is 0.028616564348340034\n",
      "epoch: 12 step: 389, loss is 0.14563557505607605\n",
      "epoch: 12 step: 390, loss is 0.01026890892535448\n",
      "epoch: 12 step: 391, loss is 0.02613999880850315\n",
      "epoch: 12 step: 392, loss is 0.16593675315380096\n",
      "epoch: 12 step: 393, loss is 0.027815787121653557\n",
      "epoch: 12 step: 394, loss is 0.14165207743644714\n",
      "epoch: 12 step: 395, loss is 0.0031552317086607218\n",
      "epoch: 12 step: 396, loss is 0.014071824960410595\n",
      "epoch: 12 step: 397, loss is 0.061952143907547\n",
      "epoch: 12 step: 398, loss is 0.01598653756082058\n",
      "epoch: 12 step: 399, loss is 0.052659209817647934\n",
      "epoch: 12 step: 400, loss is 0.03655274212360382\n",
      "epoch: 12 step: 401, loss is 0.003050062106922269\n",
      "epoch: 12 step: 402, loss is 0.026052717119455338\n",
      "epoch: 12 step: 403, loss is 0.027438867837190628\n",
      "epoch: 12 step: 404, loss is 0.08594446629285812\n",
      "epoch: 12 step: 405, loss is 0.013143769465386868\n",
      "epoch: 12 step: 406, loss is 0.066269651055336\n",
      "epoch: 12 step: 407, loss is 0.017413778230547905\n",
      "epoch: 12 step: 408, loss is 0.013402068987488747\n",
      "epoch: 12 step: 409, loss is 0.026871632784605026\n",
      "epoch: 12 step: 410, loss is 0.08816086500883102\n",
      "epoch: 12 step: 411, loss is 0.021932274103164673\n",
      "epoch: 12 step: 412, loss is 0.007803532760590315\n",
      "epoch: 12 step: 413, loss is 0.05461911857128143\n",
      "epoch: 12 step: 414, loss is 0.018730876967310905\n",
      "epoch: 12 step: 415, loss is 0.039727844297885895\n",
      "epoch: 12 step: 416, loss is 0.001515644951723516\n",
      "epoch: 12 step: 417, loss is 0.03892248868942261\n",
      "epoch: 12 step: 418, loss is 0.027472930029034615\n",
      "epoch: 12 step: 419, loss is 0.036415647715330124\n",
      "epoch: 12 step: 420, loss is 0.05020078644156456\n",
      "epoch: 12 step: 421, loss is 0.043104659765958786\n",
      "epoch: 12 step: 422, loss is 0.03461296111345291\n",
      "epoch: 12 step: 423, loss is 0.023158786818385124\n",
      "epoch: 12 step: 424, loss is 0.07308488339185715\n",
      "epoch: 12 step: 425, loss is 0.013308237306773663\n",
      "epoch: 12 step: 426, loss is 0.028428398072719574\n",
      "epoch: 12 step: 427, loss is 0.04658452048897743\n",
      "epoch: 12 step: 428, loss is 0.0047339084558188915\n",
      "epoch: 12 step: 429, loss is 0.013068785890936852\n",
      "epoch: 12 step: 430, loss is 0.08383654803037643\n",
      "epoch: 12 step: 431, loss is 0.022133713588118553\n",
      "epoch: 12 step: 432, loss is 0.016498954966664314\n",
      "epoch: 12 step: 433, loss is 0.029901821166276932\n",
      "epoch: 12 step: 434, loss is 0.026558857411146164\n",
      "epoch: 12 step: 435, loss is 0.0042625973001122475\n",
      "epoch: 12 step: 436, loss is 0.01026358362287283\n",
      "epoch: 12 step: 437, loss is 0.0209446270018816\n",
      "epoch: 12 step: 438, loss is 0.017952527850866318\n",
      "epoch: 12 step: 439, loss is 0.006236686371266842\n",
      "epoch: 12 step: 440, loss is 0.06657980382442474\n",
      "epoch: 12 step: 441, loss is 0.045219626277685165\n",
      "epoch: 12 step: 442, loss is 0.0324288085103035\n",
      "epoch: 12 step: 443, loss is 0.051007069647312164\n",
      "epoch: 12 step: 444, loss is 0.001550941844470799\n",
      "epoch: 12 step: 445, loss is 0.08369176834821701\n",
      "epoch: 12 step: 446, loss is 0.05970989912748337\n",
      "epoch: 12 step: 447, loss is 0.0023920293897390366\n",
      "epoch: 12 step: 448, loss is 0.07111681997776031\n",
      "epoch: 12 step: 449, loss is 0.03922038897871971\n",
      "epoch: 12 step: 450, loss is 0.012694080360233784\n",
      "epoch: 12 step: 451, loss is 0.05771391838788986\n",
      "epoch: 12 step: 452, loss is 0.03938283398747444\n",
      "epoch: 12 step: 453, loss is 0.004366538021713495\n",
      "epoch: 12 step: 454, loss is 0.02305745519697666\n",
      "epoch: 12 step: 455, loss is 0.013443012721836567\n",
      "epoch: 12 step: 456, loss is 0.09463795274496078\n",
      "epoch: 12 step: 457, loss is 0.031914737075567245\n",
      "epoch: 12 step: 458, loss is 0.0633365660905838\n",
      "epoch: 12 step: 459, loss is 0.06960450112819672\n",
      "epoch: 12 step: 460, loss is 0.013033422641456127\n",
      "epoch: 12 step: 461, loss is 0.023725325241684914\n",
      "epoch: 12 step: 462, loss is 0.012069843709468842\n",
      "epoch: 12 step: 463, loss is 0.004859726410359144\n",
      "epoch: 12 step: 464, loss is 0.02920490875840187\n",
      "epoch: 12 step: 465, loss is 0.013050422072410583\n",
      "epoch: 12 step: 466, loss is 0.11399105936288834\n",
      "epoch: 12 step: 467, loss is 0.0018251899164170027\n",
      "epoch: 12 step: 468, loss is 0.012173883616924286\n",
      "epoch: 12 step: 469, loss is 0.04712817817926407\n",
      "epoch: 12 step: 470, loss is 0.040467455983161926\n",
      "epoch: 12 step: 471, loss is 0.02775501273572445\n",
      "epoch: 12 step: 472, loss is 0.02455770969390869\n",
      "epoch: 12 step: 473, loss is 0.056199949234724045\n",
      "epoch: 12 step: 474, loss is 0.04804563894867897\n",
      "epoch: 12 step: 475, loss is 0.010466361418366432\n",
      "epoch: 12 step: 476, loss is 0.033499762415885925\n",
      "epoch: 12 step: 477, loss is 0.015397313982248306\n",
      "epoch: 12 step: 478, loss is 0.0584586039185524\n",
      "epoch: 12 step: 479, loss is 0.020278802141547203\n",
      "epoch: 12 step: 480, loss is 0.02144310437142849\n",
      "epoch: 12 step: 481, loss is 0.0990978479385376\n",
      "epoch: 12 step: 482, loss is 0.008468080312013626\n",
      "epoch: 12 step: 483, loss is 0.052950259298086166\n",
      "epoch: 12 step: 484, loss is 0.07320086658000946\n",
      "epoch: 12 step: 485, loss is 0.023956432938575745\n",
      "epoch: 12 step: 486, loss is 0.017174620181322098\n",
      "epoch: 12 step: 487, loss is 0.036529093980789185\n",
      "epoch: 12 step: 488, loss is 0.009890437126159668\n",
      "epoch: 12 step: 489, loss is 0.013955529779195786\n",
      "epoch: 12 step: 490, loss is 0.007760308217257261\n",
      "epoch: 12 step: 491, loss is 0.0387747623026371\n",
      "epoch: 12 step: 492, loss is 0.01642812043428421\n",
      "epoch: 12 step: 493, loss is 0.03463952615857124\n",
      "epoch: 12 step: 494, loss is 0.05241420492529869\n",
      "epoch: 12 step: 495, loss is 0.09025828540325165\n",
      "epoch: 12 step: 496, loss is 0.044162899255752563\n",
      "epoch: 12 step: 497, loss is 0.00636304123327136\n",
      "epoch: 12 step: 498, loss is 0.12197056412696838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 499, loss is 0.015733079984784126\n",
      "epoch: 12 step: 500, loss is 0.03702281042933464\n",
      "epoch: 12 step: 501, loss is 0.040522653609514236\n",
      "epoch: 12 step: 502, loss is 0.09611134231090546\n",
      "epoch: 12 step: 503, loss is 0.03716970980167389\n",
      "epoch: 12 step: 504, loss is 0.00802213791757822\n",
      "epoch: 12 step: 505, loss is 0.04834451526403427\n",
      "epoch: 12 step: 506, loss is 0.049307890236377716\n",
      "epoch: 12 step: 507, loss is 0.011411156505346298\n",
      "epoch: 12 step: 508, loss is 0.032875414937734604\n",
      "epoch: 12 step: 509, loss is 0.009966113604605198\n",
      "epoch: 12 step: 510, loss is 0.01780744083225727\n",
      "epoch: 12 step: 511, loss is 0.012479026801884174\n",
      "epoch: 12 step: 512, loss is 0.005224813707172871\n",
      "epoch: 12 step: 513, loss is 0.010620891116559505\n",
      "epoch: 12 step: 514, loss is 0.07642896473407745\n",
      "epoch: 12 step: 515, loss is 0.007952803745865822\n",
      "epoch: 12 step: 516, loss is 0.05231183394789696\n",
      "epoch: 12 step: 517, loss is 0.04440838843584061\n",
      "epoch: 12 step: 518, loss is 0.08197330683469772\n",
      "epoch: 12 step: 519, loss is 0.028465144336223602\n",
      "epoch: 12 step: 520, loss is 0.020559262484312057\n",
      "epoch: 12 step: 521, loss is 0.0195928905159235\n",
      "epoch: 12 step: 522, loss is 0.09304624795913696\n",
      "epoch: 12 step: 523, loss is 0.0067108431831002235\n",
      "epoch: 12 step: 524, loss is 0.10718188434839249\n",
      "epoch: 12 step: 525, loss is 0.004807407036423683\n",
      "epoch: 12 step: 526, loss is 0.03305191546678543\n",
      "epoch: 12 step: 527, loss is 0.09603211283683777\n",
      "epoch: 12 step: 528, loss is 0.006197252310812473\n",
      "epoch: 12 step: 529, loss is 0.013046753592789173\n",
      "epoch: 12 step: 530, loss is 0.03015441633760929\n",
      "epoch: 12 step: 531, loss is 0.11767150461673737\n",
      "epoch: 12 step: 532, loss is 0.038680385798215866\n",
      "epoch: 12 step: 533, loss is 0.006005492061376572\n",
      "epoch: 12 step: 534, loss is 0.06782365590333939\n",
      "epoch: 12 step: 535, loss is 0.00833352841436863\n",
      "epoch: 12 step: 536, loss is 0.002665791194885969\n",
      "epoch: 12 step: 537, loss is 0.15573205053806305\n",
      "epoch: 12 step: 538, loss is 0.05332291126251221\n",
      "epoch: 12 step: 539, loss is 0.03947112336754799\n",
      "epoch: 12 step: 540, loss is 0.04009592905640602\n",
      "epoch: 12 step: 541, loss is 0.02524484507739544\n",
      "epoch: 12 step: 542, loss is 0.026368819177150726\n",
      "epoch: 12 step: 543, loss is 0.0978628545999527\n",
      "epoch: 12 step: 544, loss is 0.07056108117103577\n",
      "epoch: 12 step: 545, loss is 0.11454467475414276\n",
      "epoch: 12 step: 546, loss is 0.007113364990800619\n",
      "epoch: 12 step: 547, loss is 0.022865943610668182\n",
      "epoch: 12 step: 548, loss is 0.05422955006361008\n",
      "epoch: 12 step: 549, loss is 0.009854665026068687\n",
      "epoch: 12 step: 550, loss is 0.1346815973520279\n",
      "epoch: 12 step: 551, loss is 0.01944594644010067\n",
      "epoch: 12 step: 552, loss is 0.05019135773181915\n",
      "epoch: 12 step: 553, loss is 0.07058163732290268\n",
      "epoch: 12 step: 554, loss is 0.02679416537284851\n",
      "epoch: 12 step: 555, loss is 0.025746939703822136\n",
      "epoch: 12 step: 556, loss is 0.02076704241335392\n",
      "epoch: 12 step: 557, loss is 0.06394047290086746\n",
      "epoch: 12 step: 558, loss is 0.009502063505351543\n",
      "epoch: 12 step: 559, loss is 0.0055719711817801\n",
      "epoch: 12 step: 560, loss is 0.03732072189450264\n",
      "epoch: 12 step: 561, loss is 0.05168096721172333\n",
      "epoch: 12 step: 562, loss is 0.014921020716428757\n",
      "epoch: 12 step: 563, loss is 0.06347738951444626\n",
      "epoch: 12 step: 564, loss is 0.01038417313247919\n",
      "epoch: 12 step: 565, loss is 0.03087487630546093\n",
      "epoch: 12 step: 566, loss is 0.02906435914337635\n",
      "epoch: 12 step: 567, loss is 0.056544750928878784\n",
      "epoch: 12 step: 568, loss is 0.026122992858290672\n",
      "epoch: 12 step: 569, loss is 0.031168458983302116\n",
      "epoch: 12 step: 570, loss is 0.03795758634805679\n",
      "epoch: 12 step: 571, loss is 0.012126179412007332\n",
      "epoch: 12 step: 572, loss is 0.009601329453289509\n",
      "epoch: 12 step: 573, loss is 0.0072471825405955315\n",
      "epoch: 12 step: 574, loss is 0.006303018890321255\n",
      "epoch: 12 step: 575, loss is 0.05879359692335129\n",
      "epoch: 12 step: 576, loss is 0.04875706508755684\n",
      "epoch: 12 step: 577, loss is 0.04294895380735397\n",
      "epoch: 12 step: 578, loss is 0.013938002288341522\n",
      "epoch: 12 step: 579, loss is 0.042770981788635254\n",
      "epoch: 12 step: 580, loss is 0.027885939925909042\n",
      "epoch: 12 step: 581, loss is 0.004433684982359409\n",
      "epoch: 12 step: 582, loss is 0.023071622475981712\n",
      "epoch: 12 step: 583, loss is 0.0664949119091034\n",
      "epoch: 12 step: 584, loss is 0.09117691218852997\n",
      "epoch: 12 step: 585, loss is 0.05678553506731987\n",
      "epoch: 12 step: 586, loss is 0.0034451079554855824\n",
      "epoch: 12 step: 587, loss is 0.04083956032991409\n",
      "epoch: 12 step: 588, loss is 0.006793844047933817\n",
      "epoch: 12 step: 589, loss is 0.002326758811250329\n",
      "epoch: 12 step: 590, loss is 0.03164926916360855\n",
      "epoch: 12 step: 591, loss is 0.019433187320828438\n",
      "epoch: 12 step: 592, loss is 0.0313376784324646\n",
      "epoch: 12 step: 593, loss is 0.021112101152539253\n",
      "epoch: 12 step: 594, loss is 0.07541478425264359\n",
      "epoch: 12 step: 595, loss is 0.036931876093149185\n",
      "epoch: 12 step: 596, loss is 0.07724840193986893\n",
      "epoch: 12 step: 597, loss is 0.07105465233325958\n",
      "epoch: 12 step: 598, loss is 0.17969797551631927\n",
      "epoch: 12 step: 599, loss is 0.08854074031114578\n",
      "epoch: 12 step: 600, loss is 0.129671111702919\n",
      "epoch: 12 step: 601, loss is 0.10385703295469284\n",
      "epoch: 12 step: 602, loss is 0.05224296823143959\n",
      "epoch: 12 step: 603, loss is 0.09164972603321075\n",
      "epoch: 12 step: 604, loss is 0.017624687403440475\n",
      "epoch: 12 step: 605, loss is 0.01931629329919815\n",
      "epoch: 12 step: 606, loss is 0.01893913745880127\n",
      "epoch: 12 step: 607, loss is 0.03339995816349983\n",
      "epoch: 12 step: 608, loss is 0.11604643613100052\n",
      "epoch: 12 step: 609, loss is 0.07513605058193207\n",
      "epoch: 12 step: 610, loss is 0.054513875395059586\n",
      "epoch: 12 step: 611, loss is 0.07619792968034744\n",
      "epoch: 12 step: 612, loss is 0.015384534373879433\n",
      "epoch: 12 step: 613, loss is 0.025848757475614548\n",
      "epoch: 12 step: 614, loss is 0.028026236221194267\n",
      "epoch: 12 step: 615, loss is 0.0479525662958622\n",
      "epoch: 12 step: 616, loss is 0.009704364463686943\n",
      "epoch: 12 step: 617, loss is 0.0176888108253479\n",
      "epoch: 12 step: 618, loss is 0.029581081122159958\n",
      "epoch: 12 step: 619, loss is 0.03514443710446358\n",
      "epoch: 12 step: 620, loss is 0.009067820385098457\n",
      "epoch: 12 step: 621, loss is 0.004296244587749243\n",
      "epoch: 12 step: 622, loss is 0.003045146819204092\n",
      "epoch: 12 step: 623, loss is 0.03799993172287941\n",
      "epoch: 12 step: 624, loss is 0.09503982961177826\n",
      "epoch: 12 step: 625, loss is 0.01892961375415325\n",
      "epoch: 12 step: 626, loss is 0.07222770154476166\n",
      "epoch: 12 step: 627, loss is 0.08768106997013092\n",
      "epoch: 12 step: 628, loss is 0.04076128453016281\n",
      "epoch: 12 step: 629, loss is 0.005121905822306871\n",
      "epoch: 12 step: 630, loss is 0.03411821275949478\n",
      "epoch: 12 step: 631, loss is 0.009171769954264164\n",
      "epoch: 12 step: 632, loss is 0.07942727208137512\n",
      "epoch: 12 step: 633, loss is 0.05147343873977661\n",
      "epoch: 12 step: 634, loss is 0.0492619164288044\n",
      "epoch: 12 step: 635, loss is 0.057179730385541916\n",
      "epoch: 12 step: 636, loss is 0.01896328665316105\n",
      "epoch: 12 step: 637, loss is 0.01966281421482563\n",
      "epoch: 12 step: 638, loss is 0.026413708925247192\n",
      "epoch: 12 step: 639, loss is 0.12312290072441101\n",
      "epoch: 12 step: 640, loss is 0.07751388102769852\n",
      "epoch: 12 step: 641, loss is 0.011171595193445683\n",
      "epoch: 12 step: 642, loss is 0.023528041318058968\n",
      "epoch: 12 step: 643, loss is 0.007471343502402306\n",
      "epoch: 12 step: 644, loss is 0.004662398714572191\n",
      "epoch: 12 step: 645, loss is 0.023468120023608208\n",
      "epoch: 12 step: 646, loss is 0.011836175806820393\n",
      "epoch: 12 step: 647, loss is 0.03669869154691696\n",
      "epoch: 12 step: 648, loss is 0.03716955706477165\n",
      "epoch: 12 step: 649, loss is 0.01783255860209465\n",
      "epoch: 12 step: 650, loss is 0.038394778966903687\n",
      "epoch: 12 step: 651, loss is 0.005791470408439636\n",
      "epoch: 12 step: 652, loss is 0.05876845493912697\n",
      "epoch: 12 step: 653, loss is 0.04808402061462402\n",
      "epoch: 12 step: 654, loss is 0.06483013182878494\n",
      "epoch: 12 step: 655, loss is 0.01722336746752262\n",
      "epoch: 12 step: 656, loss is 0.0555662140250206\n",
      "epoch: 12 step: 657, loss is 0.03377886861562729\n",
      "epoch: 12 step: 658, loss is 0.002223438350483775\n",
      "epoch: 12 step: 659, loss is 0.03873090073466301\n",
      "epoch: 12 step: 660, loss is 0.07940651476383209\n",
      "epoch: 12 step: 661, loss is 0.025489646941423416\n",
      "epoch: 12 step: 662, loss is 0.031112665310502052\n",
      "epoch: 12 step: 663, loss is 0.029345909133553505\n",
      "epoch: 12 step: 664, loss is 0.04197274148464203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 665, loss is 0.06820236146450043\n",
      "epoch: 12 step: 666, loss is 0.04101467877626419\n",
      "epoch: 12 step: 667, loss is 0.044298868626356125\n",
      "epoch: 12 step: 668, loss is 0.02538994699716568\n",
      "epoch: 12 step: 669, loss is 0.018035711720585823\n",
      "epoch: 12 step: 670, loss is 0.04759462550282478\n",
      "epoch: 12 step: 671, loss is 0.02808820828795433\n",
      "epoch: 12 step: 672, loss is 0.025103405117988586\n",
      "epoch: 12 step: 673, loss is 0.01655043289065361\n",
      "epoch: 12 step: 674, loss is 0.03484821692109108\n",
      "epoch: 12 step: 675, loss is 0.03560091182589531\n",
      "epoch: 12 step: 676, loss is 0.05433936044573784\n",
      "epoch: 12 step: 677, loss is 0.009826092049479485\n",
      "epoch: 12 step: 678, loss is 0.029904574155807495\n",
      "epoch: 12 step: 679, loss is 0.05170177295804024\n",
      "epoch: 12 step: 680, loss is 0.03251131996512413\n",
      "epoch: 12 step: 681, loss is 0.004717318341135979\n",
      "epoch: 12 step: 682, loss is 0.006119309924542904\n",
      "epoch: 12 step: 683, loss is 0.05689135193824768\n",
      "epoch: 12 step: 684, loss is 0.01370654534548521\n",
      "epoch: 12 step: 685, loss is 0.03161868080496788\n",
      "epoch: 12 step: 686, loss is 0.03555507957935333\n",
      "epoch: 12 step: 687, loss is 0.030412476509809494\n",
      "epoch: 12 step: 688, loss is 0.008117836900055408\n",
      "epoch: 12 step: 689, loss is 0.029697934165596962\n",
      "epoch: 12 step: 690, loss is 0.07990004867315292\n",
      "epoch: 12 step: 691, loss is 0.12480639666318893\n",
      "epoch: 12 step: 692, loss is 0.10178979486227036\n",
      "epoch: 12 step: 693, loss is 0.051139868795871735\n",
      "epoch: 12 step: 694, loss is 0.045547742396593094\n",
      "epoch: 12 step: 695, loss is 0.04926462844014168\n",
      "epoch: 12 step: 696, loss is 0.1114160344004631\n",
      "epoch: 12 step: 697, loss is 0.10252895951271057\n",
      "epoch: 12 step: 698, loss is 0.024461157619953156\n",
      "epoch: 12 step: 699, loss is 0.043867625296115875\n",
      "epoch: 12 step: 700, loss is 0.024361006915569305\n",
      "epoch: 12 step: 701, loss is 0.012600927613675594\n",
      "epoch: 12 step: 702, loss is 0.14967475831508636\n",
      "epoch: 12 step: 703, loss is 0.03454213961958885\n",
      "epoch: 12 step: 704, loss is 0.054994456470012665\n",
      "epoch: 12 step: 705, loss is 0.03824230656027794\n",
      "epoch: 12 step: 706, loss is 0.015730440616607666\n",
      "epoch: 12 step: 707, loss is 0.04733794927597046\n",
      "epoch: 12 step: 708, loss is 0.02168618142604828\n",
      "epoch: 12 step: 709, loss is 0.0439901165664196\n",
      "epoch: 12 step: 710, loss is 0.020118720829486847\n",
      "epoch: 12 step: 711, loss is 0.006653208751231432\n",
      "epoch: 12 step: 712, loss is 0.08274087309837341\n",
      "epoch: 12 step: 713, loss is 0.029240524396300316\n",
      "epoch: 12 step: 714, loss is 0.04305895417928696\n",
      "epoch: 12 step: 715, loss is 0.03330196440219879\n",
      "epoch: 12 step: 716, loss is 0.058299481868743896\n",
      "epoch: 12 step: 717, loss is 0.03842304274439812\n",
      "epoch: 12 step: 718, loss is 0.03978719189763069\n",
      "epoch: 12 step: 719, loss is 0.06890762597322464\n",
      "epoch: 12 step: 720, loss is 0.030146513134241104\n",
      "epoch: 12 step: 721, loss is 0.012213028967380524\n",
      "epoch: 12 step: 722, loss is 0.05976373329758644\n",
      "epoch: 12 step: 723, loss is 0.05663587152957916\n",
      "epoch: 12 step: 724, loss is 0.07917696237564087\n",
      "epoch: 12 step: 725, loss is 0.013059984892606735\n",
      "epoch: 12 step: 726, loss is 0.06414519995450974\n",
      "epoch: 12 step: 727, loss is 0.015021804720163345\n",
      "epoch: 12 step: 728, loss is 0.03468324616551399\n",
      "epoch: 12 step: 729, loss is 0.010905038565397263\n",
      "epoch: 12 step: 730, loss is 0.06779050081968307\n",
      "epoch: 12 step: 731, loss is 0.004682664293795824\n",
      "epoch: 12 step: 732, loss is 0.018551889806985855\n",
      "epoch: 12 step: 733, loss is 0.12657731771469116\n",
      "epoch: 12 step: 734, loss is 0.06667689979076385\n",
      "epoch: 12 step: 735, loss is 0.032256849110126495\n",
      "epoch: 12 step: 736, loss is 0.021612057462334633\n",
      "epoch: 12 step: 737, loss is 0.08928976953029633\n",
      "epoch: 12 step: 738, loss is 0.021209927275776863\n",
      "epoch: 12 step: 739, loss is 0.028136257082223892\n",
      "epoch: 12 step: 740, loss is 0.0853770300745964\n",
      "epoch: 12 step: 741, loss is 0.01675518788397312\n",
      "epoch: 12 step: 742, loss is 0.032362762838602066\n",
      "epoch: 12 step: 743, loss is 0.015473324805498123\n",
      "epoch: 12 step: 744, loss is 0.015138444490730762\n",
      "epoch: 12 step: 745, loss is 0.00225826189853251\n",
      "epoch: 12 step: 746, loss is 0.04259282350540161\n",
      "epoch: 12 step: 747, loss is 0.01828068494796753\n",
      "epoch: 12 step: 748, loss is 0.07060481607913971\n",
      "epoch: 12 step: 749, loss is 0.04395519196987152\n",
      "epoch: 12 step: 750, loss is 0.017618730664253235\n",
      "epoch: 12 step: 751, loss is 0.12061949074268341\n",
      "epoch: 12 step: 752, loss is 0.10948716104030609\n",
      "epoch: 12 step: 753, loss is 0.056745417416095734\n",
      "epoch: 12 step: 754, loss is 0.0035993466153740883\n",
      "epoch: 12 step: 755, loss is 0.011851741932332516\n",
      "epoch: 12 step: 756, loss is 0.011775699444115162\n",
      "epoch: 12 step: 757, loss is 0.043409314006567\n",
      "epoch: 12 step: 758, loss is 0.058121226727962494\n",
      "epoch: 12 step: 759, loss is 0.09598730504512787\n",
      "epoch: 12 step: 760, loss is 0.0239529088139534\n",
      "epoch: 12 step: 761, loss is 0.005261467769742012\n",
      "epoch: 12 step: 762, loss is 0.022588633000850677\n",
      "epoch: 12 step: 763, loss is 0.06127528101205826\n",
      "epoch: 12 step: 764, loss is 0.0729169100522995\n",
      "epoch: 12 step: 765, loss is 0.07763484120368958\n",
      "epoch: 12 step: 766, loss is 0.09984339028596878\n",
      "epoch: 12 step: 767, loss is 0.05776543915271759\n",
      "epoch: 12 step: 768, loss is 0.1427326798439026\n",
      "epoch: 12 step: 769, loss is 0.030872255563735962\n",
      "epoch: 12 step: 770, loss is 0.08704886585474014\n",
      "epoch: 12 step: 771, loss is 0.028016887605190277\n",
      "epoch: 12 step: 772, loss is 0.007427811622619629\n",
      "epoch: 12 step: 773, loss is 0.007991907186806202\n",
      "epoch: 12 step: 774, loss is 0.016250725835561752\n",
      "epoch: 12 step: 775, loss is 0.0349753201007843\n",
      "epoch: 12 step: 776, loss is 0.06719100475311279\n",
      "epoch: 12 step: 777, loss is 0.06913604587316513\n",
      "epoch: 12 step: 778, loss is 0.11475571244955063\n",
      "epoch: 12 step: 779, loss is 0.09869077056646347\n",
      "epoch: 12 step: 780, loss is 0.01638091541826725\n",
      "epoch: 12 step: 781, loss is 0.007497782353311777\n",
      "epoch: 12 step: 782, loss is 0.008701949380338192\n",
      "epoch: 12 step: 783, loss is 0.090071901679039\n",
      "epoch: 12 step: 784, loss is 0.058585379272699356\n",
      "epoch: 12 step: 785, loss is 0.036173015832901\n",
      "epoch: 12 step: 786, loss is 0.03060368448495865\n",
      "epoch: 12 step: 787, loss is 0.04679620638489723\n",
      "epoch: 12 step: 788, loss is 0.06107643246650696\n",
      "epoch: 12 step: 789, loss is 0.012793662957847118\n",
      "epoch: 12 step: 790, loss is 0.12937135994434357\n",
      "epoch: 12 step: 791, loss is 0.023324988782405853\n",
      "epoch: 12 step: 792, loss is 0.014854364097118378\n",
      "epoch: 12 step: 793, loss is 0.0252398569136858\n",
      "epoch: 12 step: 794, loss is 0.021403511986136436\n",
      "epoch: 12 step: 795, loss is 0.12999090552330017\n",
      "epoch: 12 step: 796, loss is 0.009019110351800919\n",
      "epoch: 12 step: 797, loss is 0.04451368749141693\n",
      "epoch: 12 step: 798, loss is 0.026783565059304237\n",
      "epoch: 12 step: 799, loss is 0.048285823315382004\n",
      "epoch: 12 step: 800, loss is 0.012527198530733585\n",
      "epoch: 12 step: 801, loss is 0.028411267325282097\n",
      "epoch: 12 step: 802, loss is 0.0386783666908741\n",
      "epoch: 12 step: 803, loss is 0.042862653732299805\n",
      "epoch: 12 step: 804, loss is 0.06437825411558151\n",
      "epoch: 12 step: 805, loss is 0.04055633395910263\n",
      "epoch: 12 step: 806, loss is 0.024966273456811905\n",
      "epoch: 12 step: 807, loss is 0.033321626484394073\n",
      "epoch: 12 step: 808, loss is 0.022451553493738174\n",
      "epoch: 12 step: 809, loss is 0.03013639710843563\n",
      "epoch: 12 step: 810, loss is 0.008339181542396545\n",
      "epoch: 12 step: 811, loss is 0.07169333100318909\n",
      "epoch: 12 step: 812, loss is 0.19445817172527313\n",
      "epoch: 12 step: 813, loss is 0.02036862075328827\n",
      "epoch: 12 step: 814, loss is 0.03221065551042557\n",
      "epoch: 12 step: 815, loss is 0.03593086078763008\n",
      "epoch: 12 step: 816, loss is 0.028691602870821953\n",
      "epoch: 12 step: 817, loss is 0.040645115077495575\n",
      "epoch: 12 step: 818, loss is 0.03869420289993286\n",
      "epoch: 12 step: 819, loss is 0.053257692605257034\n",
      "epoch: 12 step: 820, loss is 0.010909520089626312\n",
      "epoch: 12 step: 821, loss is 0.06633692979812622\n",
      "epoch: 12 step: 822, loss is 0.027434026822447777\n",
      "epoch: 12 step: 823, loss is 0.07558207213878632\n",
      "epoch: 12 step: 824, loss is 0.014500176534056664\n",
      "epoch: 12 step: 825, loss is 0.044342976063489914\n",
      "epoch: 12 step: 826, loss is 0.13077694177627563\n",
      "epoch: 12 step: 827, loss is 0.059870000928640366\n",
      "epoch: 12 step: 828, loss is 0.004930940922349691\n",
      "epoch: 12 step: 829, loss is 0.07113765925168991\n",
      "epoch: 12 step: 830, loss is 0.0017765284283086658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 831, loss is 0.018494725227355957\n",
      "epoch: 12 step: 832, loss is 0.005911446176469326\n",
      "epoch: 12 step: 833, loss is 0.00416473438963294\n",
      "epoch: 12 step: 834, loss is 0.0674159973859787\n",
      "epoch: 12 step: 835, loss is 0.042509764432907104\n",
      "epoch: 12 step: 836, loss is 0.0011646200437098742\n",
      "epoch: 12 step: 837, loss is 0.17282696068286896\n",
      "epoch: 12 step: 838, loss is 0.03951950743794441\n",
      "epoch: 12 step: 839, loss is 0.08287732303142548\n",
      "epoch: 12 step: 840, loss is 0.07413727045059204\n",
      "epoch: 12 step: 841, loss is 0.02494755946099758\n",
      "epoch: 12 step: 842, loss is 0.05243874713778496\n",
      "epoch: 12 step: 843, loss is 0.009334749542176723\n",
      "epoch: 12 step: 844, loss is 0.0655343234539032\n",
      "epoch: 12 step: 845, loss is 0.05146922543644905\n",
      "epoch: 12 step: 846, loss is 0.015064730308949947\n",
      "epoch: 12 step: 847, loss is 0.06078270450234413\n",
      "epoch: 12 step: 848, loss is 0.19938631355762482\n",
      "epoch: 12 step: 849, loss is 0.010164749808609486\n",
      "epoch: 12 step: 850, loss is 0.027104629203677177\n",
      "epoch: 12 step: 851, loss is 0.020806988701224327\n",
      "epoch: 12 step: 852, loss is 0.013762547634541988\n",
      "epoch: 12 step: 853, loss is 0.05824752151966095\n",
      "epoch: 12 step: 854, loss is 0.03376230224967003\n",
      "epoch: 12 step: 855, loss is 0.0021278830245137215\n",
      "epoch: 12 step: 856, loss is 0.07573218643665314\n",
      "epoch: 12 step: 857, loss is 0.05046292394399643\n",
      "epoch: 12 step: 858, loss is 0.10752250254154205\n",
      "epoch: 12 step: 859, loss is 0.011795458383858204\n",
      "epoch: 12 step: 860, loss is 0.016549358144402504\n",
      "epoch: 12 step: 861, loss is 0.03985016793012619\n",
      "epoch: 12 step: 862, loss is 0.016453715041279793\n",
      "epoch: 12 step: 863, loss is 0.0065665533766150475\n",
      "epoch: 12 step: 864, loss is 0.06070459634065628\n",
      "epoch: 12 step: 865, loss is 0.019627925008535385\n",
      "epoch: 12 step: 866, loss is 0.025905894115567207\n",
      "epoch: 12 step: 867, loss is 0.05271529406309128\n",
      "epoch: 12 step: 868, loss is 0.0183354914188385\n",
      "epoch: 12 step: 869, loss is 0.0824592113494873\n",
      "epoch: 12 step: 870, loss is 0.02146589383482933\n",
      "epoch: 12 step: 871, loss is 0.00945676676928997\n",
      "epoch: 12 step: 872, loss is 0.10398315638303757\n",
      "epoch: 12 step: 873, loss is 0.047777168452739716\n",
      "epoch: 12 step: 874, loss is 0.009777529165148735\n",
      "epoch: 12 step: 875, loss is 0.030046630650758743\n",
      "epoch: 12 step: 876, loss is 0.1072549968957901\n",
      "epoch: 12 step: 877, loss is 0.03983820974826813\n",
      "epoch: 12 step: 878, loss is 0.06059793382883072\n",
      "epoch: 12 step: 879, loss is 0.1538168489933014\n",
      "epoch: 12 step: 880, loss is 0.017388885840773582\n",
      "epoch: 12 step: 881, loss is 0.01020953431725502\n",
      "epoch: 12 step: 882, loss is 0.041100651025772095\n",
      "epoch: 12 step: 883, loss is 0.013553992845118046\n",
      "epoch: 12 step: 884, loss is 0.02989705465734005\n",
      "epoch: 12 step: 885, loss is 0.009632036089897156\n",
      "epoch: 12 step: 886, loss is 0.007871147245168686\n",
      "epoch: 12 step: 887, loss is 0.0431671217083931\n",
      "epoch: 12 step: 888, loss is 0.0931553915143013\n",
      "epoch: 12 step: 889, loss is 0.07173314690589905\n",
      "epoch: 12 step: 890, loss is 0.041193533688783646\n",
      "epoch: 12 step: 891, loss is 0.11607342213392258\n",
      "epoch: 12 step: 892, loss is 0.0699472576379776\n",
      "epoch: 12 step: 893, loss is 0.03204376995563507\n",
      "epoch: 12 step: 894, loss is 0.07101967185735703\n",
      "epoch: 12 step: 895, loss is 0.07164221256971359\n",
      "epoch: 12 step: 896, loss is 0.01727832481265068\n",
      "epoch: 12 step: 897, loss is 0.1949429214000702\n",
      "epoch: 12 step: 898, loss is 0.05891227722167969\n",
      "epoch: 12 step: 899, loss is 0.05415131896734238\n",
      "epoch: 12 step: 900, loss is 0.04923019930720329\n",
      "epoch: 12 step: 901, loss is 0.008137830533087254\n",
      "epoch: 12 step: 902, loss is 0.09774209558963776\n",
      "epoch: 12 step: 903, loss is 0.0036916094832122326\n",
      "epoch: 12 step: 904, loss is 0.016768336296081543\n",
      "epoch: 12 step: 905, loss is 0.01814289763569832\n",
      "epoch: 12 step: 906, loss is 0.04306972026824951\n",
      "epoch: 12 step: 907, loss is 0.11740577965974808\n",
      "epoch: 12 step: 908, loss is 0.03471429646015167\n",
      "epoch: 12 step: 909, loss is 0.09868114441633224\n",
      "epoch: 12 step: 910, loss is 0.03620358183979988\n",
      "epoch: 12 step: 911, loss is 0.035925257951021194\n",
      "epoch: 12 step: 912, loss is 0.08425220847129822\n",
      "epoch: 12 step: 913, loss is 0.19811506569385529\n",
      "epoch: 12 step: 914, loss is 0.018215510994195938\n",
      "epoch: 12 step: 915, loss is 0.05237346142530441\n",
      "epoch: 12 step: 916, loss is 0.20374804735183716\n",
      "epoch: 12 step: 917, loss is 0.08901352435350418\n",
      "epoch: 12 step: 918, loss is 0.053899914026260376\n",
      "epoch: 12 step: 919, loss is 0.039735015481710434\n",
      "epoch: 12 step: 920, loss is 0.026694785803556442\n",
      "epoch: 12 step: 921, loss is 0.02353466860949993\n",
      "epoch: 12 step: 922, loss is 0.05409849435091019\n",
      "epoch: 12 step: 923, loss is 0.03135867044329643\n",
      "epoch: 12 step: 924, loss is 0.09444905072450638\n",
      "epoch: 12 step: 925, loss is 0.1219928041100502\n",
      "epoch: 12 step: 926, loss is 0.10586845129728317\n",
      "epoch: 12 step: 927, loss is 0.009166790172457695\n",
      "epoch: 12 step: 928, loss is 0.07383877784013748\n",
      "epoch: 12 step: 929, loss is 0.0915181115269661\n",
      "epoch: 12 step: 930, loss is 0.019350076094269753\n",
      "epoch: 12 step: 931, loss is 0.02392568811774254\n",
      "epoch: 12 step: 932, loss is 0.04742942005395889\n",
      "epoch: 12 step: 933, loss is 0.0580078586935997\n",
      "epoch: 12 step: 934, loss is 0.020197458565235138\n",
      "epoch: 12 step: 935, loss is 0.008368338458240032\n",
      "epoch: 12 step: 936, loss is 0.09902194887399673\n",
      "epoch: 12 step: 937, loss is 0.02395430952310562\n",
      "epoch: 13 step: 1, loss is 0.027166811749339104\n",
      "epoch: 13 step: 2, loss is 0.01678282767534256\n",
      "epoch: 13 step: 3, loss is 0.17704951763153076\n",
      "epoch: 13 step: 4, loss is 0.004208260215818882\n",
      "epoch: 13 step: 5, loss is 0.0141702089458704\n",
      "epoch: 13 step: 6, loss is 0.049481239169836044\n",
      "epoch: 13 step: 7, loss is 0.09576235711574554\n",
      "epoch: 13 step: 8, loss is 0.003603982971981168\n",
      "epoch: 13 step: 9, loss is 0.035968538373708725\n",
      "epoch: 13 step: 10, loss is 0.020927371457219124\n",
      "epoch: 13 step: 11, loss is 0.02768876776099205\n",
      "epoch: 13 step: 12, loss is 0.006299722008407116\n",
      "epoch: 13 step: 13, loss is 0.0036555519327521324\n",
      "epoch: 13 step: 14, loss is 0.022701075300574303\n",
      "epoch: 13 step: 15, loss is 0.015122830867767334\n",
      "epoch: 13 step: 16, loss is 0.00882102269679308\n",
      "epoch: 13 step: 17, loss is 0.06220104545354843\n",
      "epoch: 13 step: 18, loss is 0.03809862583875656\n",
      "epoch: 13 step: 19, loss is 0.030069604516029358\n",
      "epoch: 13 step: 20, loss is 0.007520922459661961\n",
      "epoch: 13 step: 21, loss is 0.002650605980306864\n",
      "epoch: 13 step: 22, loss is 0.006216710899025202\n",
      "epoch: 13 step: 23, loss is 0.06299792230129242\n",
      "epoch: 13 step: 24, loss is 0.02462991699576378\n",
      "epoch: 13 step: 25, loss is 0.023886702954769135\n",
      "epoch: 13 step: 26, loss is 0.0069433883763849735\n",
      "epoch: 13 step: 27, loss is 0.03785325214266777\n",
      "epoch: 13 step: 28, loss is 0.0033861633855849504\n",
      "epoch: 13 step: 29, loss is 0.003534160554409027\n",
      "epoch: 13 step: 30, loss is 0.026754116639494896\n",
      "epoch: 13 step: 31, loss is 0.011669717729091644\n",
      "epoch: 13 step: 32, loss is 0.10027015954256058\n",
      "epoch: 13 step: 33, loss is 0.017768528312444687\n",
      "epoch: 13 step: 34, loss is 0.01897699572145939\n",
      "epoch: 13 step: 35, loss is 0.024556349962949753\n",
      "epoch: 13 step: 36, loss is 0.007339861243963242\n",
      "epoch: 13 step: 37, loss is 0.03604275360703468\n",
      "epoch: 13 step: 38, loss is 0.03332630544900894\n",
      "epoch: 13 step: 39, loss is 0.025804175063967705\n",
      "epoch: 13 step: 40, loss is 0.024660952389240265\n",
      "epoch: 13 step: 41, loss is 0.020367255434393883\n",
      "epoch: 13 step: 42, loss is 0.01994825154542923\n",
      "epoch: 13 step: 43, loss is 0.003798794699832797\n",
      "epoch: 13 step: 44, loss is 0.03217754140496254\n",
      "epoch: 13 step: 45, loss is 0.01707814820110798\n",
      "epoch: 13 step: 46, loss is 0.05047103017568588\n",
      "epoch: 13 step: 47, loss is 0.0033824616111814976\n",
      "epoch: 13 step: 48, loss is 0.006996531039476395\n",
      "epoch: 13 step: 49, loss is 0.014381377026438713\n",
      "epoch: 13 step: 50, loss is 0.03651440888643265\n",
      "epoch: 13 step: 51, loss is 0.0013793635880574584\n",
      "epoch: 13 step: 52, loss is 0.017364762723445892\n",
      "epoch: 13 step: 53, loss is 0.026923688128590584\n",
      "epoch: 13 step: 54, loss is 0.029445786029100418\n",
      "epoch: 13 step: 55, loss is 0.010877045802772045\n",
      "epoch: 13 step: 56, loss is 0.015083415433764458\n",
      "epoch: 13 step: 57, loss is 0.013786165043711662\n",
      "epoch: 13 step: 58, loss is 0.004345516674220562\n",
      "epoch: 13 step: 59, loss is 0.02442634105682373\n",
      "epoch: 13 step: 60, loss is 0.008473468944430351\n",
      "epoch: 13 step: 61, loss is 0.018740609288215637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 62, loss is 0.038765452802181244\n",
      "epoch: 13 step: 63, loss is 0.0036759015638381243\n",
      "epoch: 13 step: 64, loss is 0.009483406320214272\n",
      "epoch: 13 step: 65, loss is 0.017887704074382782\n",
      "epoch: 13 step: 66, loss is 0.007097711320966482\n",
      "epoch: 13 step: 67, loss is 0.009388498030602932\n",
      "epoch: 13 step: 68, loss is 0.008982070721685886\n",
      "epoch: 13 step: 69, loss is 0.01559100765734911\n",
      "epoch: 13 step: 70, loss is 0.007271256297826767\n",
      "epoch: 13 step: 71, loss is 0.003703052643686533\n",
      "epoch: 13 step: 72, loss is 0.012666601687669754\n",
      "epoch: 13 step: 73, loss is 0.013833767734467983\n",
      "epoch: 13 step: 74, loss is 0.02472095564007759\n",
      "epoch: 13 step: 75, loss is 0.0012001049472019076\n",
      "epoch: 13 step: 76, loss is 0.003305324586108327\n",
      "epoch: 13 step: 77, loss is 0.0010997944045811892\n",
      "epoch: 13 step: 78, loss is 0.005606815218925476\n",
      "epoch: 13 step: 79, loss is 0.04021713510155678\n",
      "epoch: 13 step: 80, loss is 0.14025641977787018\n",
      "epoch: 13 step: 81, loss is 0.02451474964618683\n",
      "epoch: 13 step: 82, loss is 0.04101414233446121\n",
      "epoch: 13 step: 83, loss is 0.012471434660255909\n",
      "epoch: 13 step: 84, loss is 0.011450543999671936\n",
      "epoch: 13 step: 85, loss is 0.01852654479444027\n",
      "epoch: 13 step: 86, loss is 0.014087788760662079\n",
      "epoch: 13 step: 87, loss is 0.006735327187925577\n",
      "epoch: 13 step: 88, loss is 0.00772099569439888\n",
      "epoch: 13 step: 89, loss is 0.01668953336775303\n",
      "epoch: 13 step: 90, loss is 0.025085194036364555\n",
      "epoch: 13 step: 91, loss is 0.002757080364972353\n",
      "epoch: 13 step: 92, loss is 0.010184177197515965\n",
      "epoch: 13 step: 93, loss is 0.09061882644891739\n",
      "epoch: 13 step: 94, loss is 0.01930185966193676\n",
      "epoch: 13 step: 95, loss is 0.007562051527202129\n",
      "epoch: 13 step: 96, loss is 0.021406270563602448\n",
      "epoch: 13 step: 97, loss is 0.08435903489589691\n",
      "epoch: 13 step: 98, loss is 0.04914689436554909\n",
      "epoch: 13 step: 99, loss is 0.005031134933233261\n",
      "epoch: 13 step: 100, loss is 0.004786808509379625\n",
      "epoch: 13 step: 101, loss is 0.0017830594442784786\n",
      "epoch: 13 step: 102, loss is 0.002760510891675949\n",
      "epoch: 13 step: 103, loss is 0.015214328654110432\n",
      "epoch: 13 step: 104, loss is 0.006205067504197359\n",
      "epoch: 13 step: 105, loss is 0.020314596593379974\n",
      "epoch: 13 step: 106, loss is 0.006654740776866674\n",
      "epoch: 13 step: 107, loss is 0.02858264558017254\n",
      "epoch: 13 step: 108, loss is 0.007514996454119682\n",
      "epoch: 13 step: 109, loss is 0.04974617436528206\n",
      "epoch: 13 step: 110, loss is 0.02512049488723278\n",
      "epoch: 13 step: 111, loss is 0.0088884262368083\n",
      "epoch: 13 step: 112, loss is 0.08401255309581757\n",
      "epoch: 13 step: 113, loss is 0.005144595634192228\n",
      "epoch: 13 step: 114, loss is 0.017516467720270157\n",
      "epoch: 13 step: 115, loss is 0.006608463358134031\n",
      "epoch: 13 step: 116, loss is 0.016846051439642906\n",
      "epoch: 13 step: 117, loss is 0.014438892714679241\n",
      "epoch: 13 step: 118, loss is 0.028614245355129242\n",
      "epoch: 13 step: 119, loss is 0.0033452664501965046\n",
      "epoch: 13 step: 120, loss is 0.08495402336120605\n",
      "epoch: 13 step: 121, loss is 0.01518908143043518\n",
      "epoch: 13 step: 122, loss is 0.021330034360289574\n",
      "epoch: 13 step: 123, loss is 0.01623886078596115\n",
      "epoch: 13 step: 124, loss is 0.006376770790666342\n",
      "epoch: 13 step: 125, loss is 0.05413909628987312\n",
      "epoch: 13 step: 126, loss is 0.02336372807621956\n",
      "epoch: 13 step: 127, loss is 0.011691436171531677\n",
      "epoch: 13 step: 128, loss is 0.0025187102146446705\n",
      "epoch: 13 step: 129, loss is 0.010470030829310417\n",
      "epoch: 13 step: 130, loss is 0.003336852416396141\n",
      "epoch: 13 step: 131, loss is 0.0317988246679306\n",
      "epoch: 13 step: 132, loss is 0.005441772285848856\n",
      "epoch: 13 step: 133, loss is 0.0209315437823534\n",
      "epoch: 13 step: 134, loss is 0.008643430657684803\n",
      "epoch: 13 step: 135, loss is 0.02820047177374363\n",
      "epoch: 13 step: 136, loss is 0.005144535098224878\n",
      "epoch: 13 step: 137, loss is 0.0059118554927408695\n",
      "epoch: 13 step: 138, loss is 0.005121534690260887\n",
      "epoch: 13 step: 139, loss is 0.004105863627046347\n",
      "epoch: 13 step: 140, loss is 0.03901573270559311\n",
      "epoch: 13 step: 141, loss is 0.018137291073799133\n",
      "epoch: 13 step: 142, loss is 0.011756980791687965\n",
      "epoch: 13 step: 143, loss is 0.033730242401361465\n",
      "epoch: 13 step: 144, loss is 0.004844368435442448\n",
      "epoch: 13 step: 145, loss is 0.028401194140315056\n",
      "epoch: 13 step: 146, loss is 0.026736276224255562\n",
      "epoch: 13 step: 147, loss is 0.007784740533679724\n",
      "epoch: 13 step: 148, loss is 0.002034110715612769\n",
      "epoch: 13 step: 149, loss is 0.005662248004227877\n",
      "epoch: 13 step: 150, loss is 0.043200504034757614\n",
      "epoch: 13 step: 151, loss is 0.01719891093671322\n",
      "epoch: 13 step: 152, loss is 0.009096633642911911\n",
      "epoch: 13 step: 153, loss is 0.015741081908345222\n",
      "epoch: 13 step: 154, loss is 0.020712686702609062\n",
      "epoch: 13 step: 155, loss is 0.03761383518576622\n",
      "epoch: 13 step: 156, loss is 0.009487730450928211\n",
      "epoch: 13 step: 157, loss is 0.054987065494060516\n",
      "epoch: 13 step: 158, loss is 0.0032143560238182545\n",
      "epoch: 13 step: 159, loss is 0.003002573037520051\n",
      "epoch: 13 step: 160, loss is 0.005863926373422146\n",
      "epoch: 13 step: 161, loss is 0.047099437564611435\n",
      "epoch: 13 step: 162, loss is 0.010154279880225658\n",
      "epoch: 13 step: 163, loss is 0.021863045170903206\n",
      "epoch: 13 step: 164, loss is 0.005033846013247967\n",
      "epoch: 13 step: 165, loss is 0.015242681838572025\n",
      "epoch: 13 step: 166, loss is 0.005596972536295652\n",
      "epoch: 13 step: 167, loss is 0.003989040851593018\n",
      "epoch: 13 step: 168, loss is 0.004884534981101751\n",
      "epoch: 13 step: 169, loss is 0.002366776345297694\n",
      "epoch: 13 step: 170, loss is 0.003552646143361926\n",
      "epoch: 13 step: 171, loss is 0.010047288611531258\n",
      "epoch: 13 step: 172, loss is 0.0064246621914207935\n",
      "epoch: 13 step: 173, loss is 0.0051615359261631966\n",
      "epoch: 13 step: 174, loss is 0.05670167878270149\n",
      "epoch: 13 step: 175, loss is 0.003692228812724352\n",
      "epoch: 13 step: 176, loss is 0.0014760583871975541\n",
      "epoch: 13 step: 177, loss is 0.003328369464725256\n",
      "epoch: 13 step: 178, loss is 0.002995159709826112\n",
      "epoch: 13 step: 179, loss is 0.006390330381691456\n",
      "epoch: 13 step: 180, loss is 0.07275990396738052\n",
      "epoch: 13 step: 181, loss is 0.004937899764627218\n",
      "epoch: 13 step: 182, loss is 0.019959041848778725\n",
      "epoch: 13 step: 183, loss is 0.01760425604879856\n",
      "epoch: 13 step: 184, loss is 0.004253411199897528\n",
      "epoch: 13 step: 185, loss is 0.014694263227283955\n",
      "epoch: 13 step: 186, loss is 0.017678922042250633\n",
      "epoch: 13 step: 187, loss is 0.0011361428769305348\n",
      "epoch: 13 step: 188, loss is 0.03153209015727043\n",
      "epoch: 13 step: 189, loss is 0.05013143643736839\n",
      "epoch: 13 step: 190, loss is 0.03125768527388573\n",
      "epoch: 13 step: 191, loss is 0.0048259408213198185\n",
      "epoch: 13 step: 192, loss is 0.026852155104279518\n",
      "epoch: 13 step: 193, loss is 0.0032121327240020037\n",
      "epoch: 13 step: 194, loss is 0.004918867256492376\n",
      "epoch: 13 step: 195, loss is 0.023357253521680832\n",
      "epoch: 13 step: 196, loss is 0.0032859521452337503\n",
      "epoch: 13 step: 197, loss is 0.006326830945909023\n",
      "epoch: 13 step: 198, loss is 0.018240826204419136\n",
      "epoch: 13 step: 199, loss is 0.024233831092715263\n",
      "epoch: 13 step: 200, loss is 0.0013445764780044556\n",
      "epoch: 13 step: 201, loss is 0.018884552642703056\n",
      "epoch: 13 step: 202, loss is 0.11057626456022263\n",
      "epoch: 13 step: 203, loss is 0.0020383752416819334\n",
      "epoch: 13 step: 204, loss is 0.015430411323904991\n",
      "epoch: 13 step: 205, loss is 0.007071571424603462\n",
      "epoch: 13 step: 206, loss is 0.015030999667942524\n",
      "epoch: 13 step: 207, loss is 0.006761621683835983\n",
      "epoch: 13 step: 208, loss is 0.002895292127504945\n",
      "epoch: 13 step: 209, loss is 0.0038383600767701864\n",
      "epoch: 13 step: 210, loss is 0.00895440299063921\n",
      "epoch: 13 step: 211, loss is 0.01637672819197178\n",
      "epoch: 13 step: 212, loss is 0.007611085195094347\n",
      "epoch: 13 step: 213, loss is 0.004238082095980644\n",
      "epoch: 13 step: 214, loss is 0.010749464854598045\n",
      "epoch: 13 step: 215, loss is 0.0013742580777034163\n",
      "epoch: 13 step: 216, loss is 0.025720523670315742\n",
      "epoch: 13 step: 217, loss is 0.029107995331287384\n",
      "epoch: 13 step: 218, loss is 0.03707616403698921\n",
      "epoch: 13 step: 219, loss is 0.018132958561182022\n",
      "epoch: 13 step: 220, loss is 0.021475834771990776\n",
      "epoch: 13 step: 221, loss is 0.010754192247986794\n",
      "epoch: 13 step: 222, loss is 0.026321319863200188\n",
      "epoch: 13 step: 223, loss is 0.00029894313775002956\n",
      "epoch: 13 step: 224, loss is 0.026832064613699913\n",
      "epoch: 13 step: 225, loss is 0.0350995771586895\n",
      "epoch: 13 step: 226, loss is 0.023399554193019867\n",
      "epoch: 13 step: 227, loss is 0.005626488942652941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 228, loss is 0.024573083966970444\n",
      "epoch: 13 step: 229, loss is 0.04761473834514618\n",
      "epoch: 13 step: 230, loss is 0.06288223713636398\n",
      "epoch: 13 step: 231, loss is 0.015726126730442047\n",
      "epoch: 13 step: 232, loss is 0.014422371052205563\n",
      "epoch: 13 step: 233, loss is 0.021984463557600975\n",
      "epoch: 13 step: 234, loss is 0.02526351995766163\n",
      "epoch: 13 step: 235, loss is 0.010038502514362335\n",
      "epoch: 13 step: 236, loss is 0.001818732125684619\n",
      "epoch: 13 step: 237, loss is 0.005721263121813536\n",
      "epoch: 13 step: 238, loss is 0.025870973244309425\n",
      "epoch: 13 step: 239, loss is 0.0037684328854084015\n",
      "epoch: 13 step: 240, loss is 0.005042916629463434\n",
      "epoch: 13 step: 241, loss is 0.09366052597761154\n",
      "epoch: 13 step: 242, loss is 0.01308892946690321\n",
      "epoch: 13 step: 243, loss is 0.010822589509189129\n",
      "epoch: 13 step: 244, loss is 0.0816740021109581\n",
      "epoch: 13 step: 245, loss is 0.051051199436187744\n",
      "epoch: 13 step: 246, loss is 0.031395137310028076\n",
      "epoch: 13 step: 247, loss is 0.04493686556816101\n",
      "epoch: 13 step: 248, loss is 0.02143489196896553\n",
      "epoch: 13 step: 249, loss is 0.008591163903474808\n",
      "epoch: 13 step: 250, loss is 0.008566399104893208\n",
      "epoch: 13 step: 251, loss is 0.05825990438461304\n",
      "epoch: 13 step: 252, loss is 0.00363927218131721\n",
      "epoch: 13 step: 253, loss is 0.031193487346172333\n",
      "epoch: 13 step: 254, loss is 0.041755564510822296\n",
      "epoch: 13 step: 255, loss is 0.07081782817840576\n",
      "epoch: 13 step: 256, loss is 0.02869301289319992\n",
      "epoch: 13 step: 257, loss is 0.0014719616156071424\n",
      "epoch: 13 step: 258, loss is 0.040265779942274094\n",
      "epoch: 13 step: 259, loss is 0.07902716845273972\n",
      "epoch: 13 step: 260, loss is 0.038537830114364624\n",
      "epoch: 13 step: 261, loss is 0.005566521547734737\n",
      "epoch: 13 step: 262, loss is 0.023275936022400856\n",
      "epoch: 13 step: 263, loss is 0.042553480714559555\n",
      "epoch: 13 step: 264, loss is 0.0546945258975029\n",
      "epoch: 13 step: 265, loss is 0.03405136615037918\n",
      "epoch: 13 step: 266, loss is 0.03978639468550682\n",
      "epoch: 13 step: 267, loss is 0.06880789995193481\n",
      "epoch: 13 step: 268, loss is 0.005025589372962713\n",
      "epoch: 13 step: 269, loss is 0.015904167667031288\n",
      "epoch: 13 step: 270, loss is 0.0226544551551342\n",
      "epoch: 13 step: 271, loss is 0.017442723736166954\n",
      "epoch: 13 step: 272, loss is 0.03447460010647774\n",
      "epoch: 13 step: 273, loss is 0.00813035573810339\n",
      "epoch: 13 step: 274, loss is 0.07783883064985275\n",
      "epoch: 13 step: 275, loss is 0.005624852143228054\n",
      "epoch: 13 step: 276, loss is 0.011443059891462326\n",
      "epoch: 13 step: 277, loss is 0.05066029354929924\n",
      "epoch: 13 step: 278, loss is 0.08516370505094528\n",
      "epoch: 13 step: 279, loss is 0.005955124273896217\n",
      "epoch: 13 step: 280, loss is 0.01846989244222641\n",
      "epoch: 13 step: 281, loss is 0.0370393842458725\n",
      "epoch: 13 step: 282, loss is 0.007339672185480595\n",
      "epoch: 13 step: 283, loss is 0.05118901655077934\n",
      "epoch: 13 step: 284, loss is 0.017495181411504745\n",
      "epoch: 13 step: 285, loss is 0.02313201315701008\n",
      "epoch: 13 step: 286, loss is 0.08789900690317154\n",
      "epoch: 13 step: 287, loss is 0.05006852373480797\n",
      "epoch: 13 step: 288, loss is 0.007863132283091545\n",
      "epoch: 13 step: 289, loss is 0.03601571172475815\n",
      "epoch: 13 step: 290, loss is 0.03386687487363815\n",
      "epoch: 13 step: 291, loss is 0.03152405843138695\n",
      "epoch: 13 step: 292, loss is 0.008806407451629639\n",
      "epoch: 13 step: 293, loss is 0.024571619927883148\n",
      "epoch: 13 step: 294, loss is 0.03309627249836922\n",
      "epoch: 13 step: 295, loss is 0.016339678317308426\n",
      "epoch: 13 step: 296, loss is 0.022498713806271553\n",
      "epoch: 13 step: 297, loss is 0.03893652558326721\n",
      "epoch: 13 step: 298, loss is 0.010425824671983719\n",
      "epoch: 13 step: 299, loss is 0.0034231068566441536\n",
      "epoch: 13 step: 300, loss is 0.0335538424551487\n",
      "epoch: 13 step: 301, loss is 0.010961626656353474\n",
      "epoch: 13 step: 302, loss is 0.057280752807855606\n",
      "epoch: 13 step: 303, loss is 0.005131210666149855\n",
      "epoch: 13 step: 304, loss is 0.02076772414147854\n",
      "epoch: 13 step: 305, loss is 0.0004923258093185723\n",
      "epoch: 13 step: 306, loss is 0.016273749992251396\n",
      "epoch: 13 step: 307, loss is 0.015131128951907158\n",
      "epoch: 13 step: 308, loss is 0.041769322007894516\n",
      "epoch: 13 step: 309, loss is 0.0047324541956186295\n",
      "epoch: 13 step: 310, loss is 0.13290157914161682\n",
      "epoch: 13 step: 311, loss is 0.05022105574607849\n",
      "epoch: 13 step: 312, loss is 0.012086957693099976\n",
      "epoch: 13 step: 313, loss is 0.09145282208919525\n",
      "epoch: 13 step: 314, loss is 0.05756702646613121\n",
      "epoch: 13 step: 315, loss is 0.015151305124163628\n",
      "epoch: 13 step: 316, loss is 0.0003712979087140411\n",
      "epoch: 13 step: 317, loss is 0.006069991737604141\n",
      "epoch: 13 step: 318, loss is 0.03549855947494507\n",
      "epoch: 13 step: 319, loss is 0.008470864966511726\n",
      "epoch: 13 step: 320, loss is 0.10603860765695572\n",
      "epoch: 13 step: 321, loss is 0.04208189994096756\n",
      "epoch: 13 step: 322, loss is 0.003262302605435252\n",
      "epoch: 13 step: 323, loss is 0.016778521239757538\n",
      "epoch: 13 step: 324, loss is 0.014793197624385357\n",
      "epoch: 13 step: 325, loss is 0.027877487242221832\n",
      "epoch: 13 step: 326, loss is 0.01135354395955801\n",
      "epoch: 13 step: 327, loss is 0.005229049362242222\n",
      "epoch: 13 step: 328, loss is 0.016475452110171318\n",
      "epoch: 13 step: 329, loss is 0.031614311039447784\n",
      "epoch: 13 step: 330, loss is 0.08463460952043533\n",
      "epoch: 13 step: 331, loss is 0.028551854193210602\n",
      "epoch: 13 step: 332, loss is 0.05175717920064926\n",
      "epoch: 13 step: 333, loss is 0.0025578448548913\n",
      "epoch: 13 step: 334, loss is 0.011202855966985226\n",
      "epoch: 13 step: 335, loss is 0.005313700530678034\n",
      "epoch: 13 step: 336, loss is 0.04972103238105774\n",
      "epoch: 13 step: 337, loss is 0.01615038327872753\n",
      "epoch: 13 step: 338, loss is 0.01122032105922699\n",
      "epoch: 13 step: 339, loss is 0.014643922448158264\n",
      "epoch: 13 step: 340, loss is 0.04046832025051117\n",
      "epoch: 13 step: 341, loss is 0.03055577352643013\n",
      "epoch: 13 step: 342, loss is 0.0033339671790599823\n",
      "epoch: 13 step: 343, loss is 0.015413555316627026\n",
      "epoch: 13 step: 344, loss is 0.009523699060082436\n",
      "epoch: 13 step: 345, loss is 0.01076928898692131\n",
      "epoch: 13 step: 346, loss is 0.005164382513612509\n",
      "epoch: 13 step: 347, loss is 0.01495054829865694\n",
      "epoch: 13 step: 348, loss is 0.028678162023425102\n",
      "epoch: 13 step: 349, loss is 0.06172608956694603\n",
      "epoch: 13 step: 350, loss is 0.01565607823431492\n",
      "epoch: 13 step: 351, loss is 0.046177931129932404\n",
      "epoch: 13 step: 352, loss is 0.015402446500957012\n",
      "epoch: 13 step: 353, loss is 0.044032927602529526\n",
      "epoch: 13 step: 354, loss is 0.03366647660732269\n",
      "epoch: 13 step: 355, loss is 0.06811749935150146\n",
      "epoch: 13 step: 356, loss is 0.02994517982006073\n",
      "epoch: 13 step: 357, loss is 0.022404277697205544\n",
      "epoch: 13 step: 358, loss is 0.011793876066803932\n",
      "epoch: 13 step: 359, loss is 0.004317841958254576\n",
      "epoch: 13 step: 360, loss is 0.04914266988635063\n",
      "epoch: 13 step: 361, loss is 0.021415771916508675\n",
      "epoch: 13 step: 362, loss is 0.03587190434336662\n",
      "epoch: 13 step: 363, loss is 0.0054288338869810104\n",
      "epoch: 13 step: 364, loss is 0.01822601445019245\n",
      "epoch: 13 step: 365, loss is 0.009742947295308113\n",
      "epoch: 13 step: 366, loss is 0.08269420266151428\n",
      "epoch: 13 step: 367, loss is 0.011210347525775433\n",
      "epoch: 13 step: 368, loss is 0.1083906888961792\n",
      "epoch: 13 step: 369, loss is 0.023048801347613335\n",
      "epoch: 13 step: 370, loss is 0.006181497126817703\n",
      "epoch: 13 step: 371, loss is 0.006686364766210318\n",
      "epoch: 13 step: 372, loss is 0.027159249410033226\n",
      "epoch: 13 step: 373, loss is 0.007758195511996746\n",
      "epoch: 13 step: 374, loss is 0.0210840106010437\n",
      "epoch: 13 step: 375, loss is 0.052706070244312286\n",
      "epoch: 13 step: 376, loss is 0.11886372417211533\n",
      "epoch: 13 step: 377, loss is 0.01967046596109867\n",
      "epoch: 13 step: 378, loss is 0.0026760066393762827\n",
      "epoch: 13 step: 379, loss is 0.1259133219718933\n",
      "epoch: 13 step: 380, loss is 0.005316045135259628\n",
      "epoch: 13 step: 381, loss is 0.007109316531568766\n",
      "epoch: 13 step: 382, loss is 0.0430513471364975\n",
      "epoch: 13 step: 383, loss is 0.042764484882354736\n",
      "epoch: 13 step: 384, loss is 0.05342109128832817\n",
      "epoch: 13 step: 385, loss is 0.03816871717572212\n",
      "epoch: 13 step: 386, loss is 0.0050950124859809875\n",
      "epoch: 13 step: 387, loss is 0.033314742147922516\n",
      "epoch: 13 step: 388, loss is 0.022687215358018875\n",
      "epoch: 13 step: 389, loss is 0.05032022297382355\n",
      "epoch: 13 step: 390, loss is 0.041675418615341187\n",
      "epoch: 13 step: 391, loss is 0.0026139209512621164\n",
      "epoch: 13 step: 392, loss is 0.042426589876413345\n",
      "epoch: 13 step: 393, loss is 0.034107644110918045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 394, loss is 0.010346967726945877\n",
      "epoch: 13 step: 395, loss is 0.009048222564160824\n",
      "epoch: 13 step: 396, loss is 0.06378016620874405\n",
      "epoch: 13 step: 397, loss is 0.0819440633058548\n",
      "epoch: 13 step: 398, loss is 0.03060855343937874\n",
      "epoch: 13 step: 399, loss is 0.0050109270960092545\n",
      "epoch: 13 step: 400, loss is 0.029156440868973732\n",
      "epoch: 13 step: 401, loss is 0.009153252467513084\n",
      "epoch: 13 step: 402, loss is 0.012715904973447323\n",
      "epoch: 13 step: 403, loss is 0.1307602971792221\n",
      "epoch: 13 step: 404, loss is 0.10844694077968597\n",
      "epoch: 13 step: 405, loss is 0.07546839118003845\n",
      "epoch: 13 step: 406, loss is 0.008769681677222252\n",
      "epoch: 13 step: 407, loss is 0.048525940626859665\n",
      "epoch: 13 step: 408, loss is 0.011870218440890312\n",
      "epoch: 13 step: 409, loss is 0.0175805501639843\n",
      "epoch: 13 step: 410, loss is 0.06148875504732132\n",
      "epoch: 13 step: 411, loss is 0.03290069103240967\n",
      "epoch: 13 step: 412, loss is 0.024029074236750603\n",
      "epoch: 13 step: 413, loss is 0.019779082387685776\n",
      "epoch: 13 step: 414, loss is 0.015778798609972\n",
      "epoch: 13 step: 415, loss is 0.005704013630747795\n",
      "epoch: 13 step: 416, loss is 0.05465848371386528\n",
      "epoch: 13 step: 417, loss is 0.03169071301817894\n",
      "epoch: 13 step: 418, loss is 0.02774628810584545\n",
      "epoch: 13 step: 419, loss is 0.02566956914961338\n",
      "epoch: 13 step: 420, loss is 0.003065287135541439\n",
      "epoch: 13 step: 421, loss is 0.004744524136185646\n",
      "epoch: 13 step: 422, loss is 0.08941099792718887\n",
      "epoch: 13 step: 423, loss is 0.028467638418078423\n",
      "epoch: 13 step: 424, loss is 0.030456261709332466\n",
      "epoch: 13 step: 425, loss is 0.03685867041349411\n",
      "epoch: 13 step: 426, loss is 0.03850303217768669\n",
      "epoch: 13 step: 427, loss is 0.13463781774044037\n",
      "epoch: 13 step: 428, loss is 0.0286505538970232\n",
      "epoch: 13 step: 429, loss is 0.043845921754837036\n",
      "epoch: 13 step: 430, loss is 0.047572799026966095\n",
      "epoch: 13 step: 431, loss is 0.012845522724092007\n",
      "epoch: 13 step: 432, loss is 0.09403738379478455\n",
      "epoch: 13 step: 433, loss is 0.03738684207201004\n",
      "epoch: 13 step: 434, loss is 0.06045570597052574\n",
      "epoch: 13 step: 435, loss is 0.049000825732946396\n",
      "epoch: 13 step: 436, loss is 0.09763146191835403\n",
      "epoch: 13 step: 437, loss is 0.0585799515247345\n",
      "epoch: 13 step: 438, loss is 0.005360790528357029\n",
      "epoch: 13 step: 439, loss is 0.030362596735358238\n",
      "epoch: 13 step: 440, loss is 0.0036877363454550505\n",
      "epoch: 13 step: 441, loss is 0.14307910203933716\n",
      "epoch: 13 step: 442, loss is 0.06964239478111267\n",
      "epoch: 13 step: 443, loss is 0.010582841001451015\n",
      "epoch: 13 step: 444, loss is 0.03836064413189888\n",
      "epoch: 13 step: 445, loss is 0.04796948656439781\n",
      "epoch: 13 step: 446, loss is 0.01471705548465252\n",
      "epoch: 13 step: 447, loss is 0.009717490524053574\n",
      "epoch: 13 step: 448, loss is 0.018676716834306717\n",
      "epoch: 13 step: 449, loss is 0.037907421588897705\n",
      "epoch: 13 step: 450, loss is 0.054922569543123245\n",
      "epoch: 13 step: 451, loss is 0.0780368223786354\n",
      "epoch: 13 step: 452, loss is 0.033506494015455246\n",
      "epoch: 13 step: 453, loss is 0.0013717280235141516\n",
      "epoch: 13 step: 454, loss is 0.02171524614095688\n",
      "epoch: 13 step: 455, loss is 0.011101434007287025\n",
      "epoch: 13 step: 456, loss is 0.007621583063155413\n",
      "epoch: 13 step: 457, loss is 0.054100822657346725\n",
      "epoch: 13 step: 458, loss is 0.04671289026737213\n",
      "epoch: 13 step: 459, loss is 0.07632551342248917\n",
      "epoch: 13 step: 460, loss is 0.02924438752233982\n",
      "epoch: 13 step: 461, loss is 0.0172123946249485\n",
      "epoch: 13 step: 462, loss is 0.017808688804507256\n",
      "epoch: 13 step: 463, loss is 0.05743827670812607\n",
      "epoch: 13 step: 464, loss is 0.007935969159007072\n",
      "epoch: 13 step: 465, loss is 0.02518177404999733\n",
      "epoch: 13 step: 466, loss is 0.02808755822479725\n",
      "epoch: 13 step: 467, loss is 0.0598936565220356\n",
      "epoch: 13 step: 468, loss is 0.016043387353420258\n",
      "epoch: 13 step: 469, loss is 0.023879213258624077\n",
      "epoch: 13 step: 470, loss is 0.05274219438433647\n",
      "epoch: 13 step: 471, loss is 0.0617336742579937\n",
      "epoch: 13 step: 472, loss is 0.12716028094291687\n",
      "epoch: 13 step: 473, loss is 0.014322788454592228\n",
      "epoch: 13 step: 474, loss is 0.004162936005741358\n",
      "epoch: 13 step: 475, loss is 0.2860642969608307\n",
      "epoch: 13 step: 476, loss is 0.030925529077649117\n",
      "epoch: 13 step: 477, loss is 0.00795820727944374\n",
      "epoch: 13 step: 478, loss is 0.02099645882844925\n",
      "epoch: 13 step: 479, loss is 0.016407275572419167\n",
      "epoch: 13 step: 480, loss is 0.07342030107975006\n",
      "epoch: 13 step: 481, loss is 0.016154715791344643\n",
      "epoch: 13 step: 482, loss is 0.014324543066322803\n",
      "epoch: 13 step: 483, loss is 0.0047653610818088055\n",
      "epoch: 13 step: 484, loss is 0.02070385031402111\n",
      "epoch: 13 step: 485, loss is 0.040802888572216034\n",
      "epoch: 13 step: 486, loss is 0.006425617728382349\n",
      "epoch: 13 step: 487, loss is 0.01747215911746025\n",
      "epoch: 13 step: 488, loss is 0.05766203626990318\n",
      "epoch: 13 step: 489, loss is 0.0196088720113039\n",
      "epoch: 13 step: 490, loss is 0.029696639627218246\n",
      "epoch: 13 step: 491, loss is 0.10033780336380005\n",
      "epoch: 13 step: 492, loss is 0.00476965494453907\n",
      "epoch: 13 step: 493, loss is 0.018795253708958626\n",
      "epoch: 13 step: 494, loss is 0.08907435834407806\n",
      "epoch: 13 step: 495, loss is 0.04828597977757454\n",
      "epoch: 13 step: 496, loss is 0.022324422374367714\n",
      "epoch: 13 step: 497, loss is 0.155203178524971\n",
      "epoch: 13 step: 498, loss is 0.00915804598480463\n",
      "epoch: 13 step: 499, loss is 0.15553681552410126\n",
      "epoch: 13 step: 500, loss is 0.08331780880689621\n",
      "epoch: 13 step: 501, loss is 0.058367740362882614\n",
      "epoch: 13 step: 502, loss is 0.022301511839032173\n",
      "epoch: 13 step: 503, loss is 0.026980282738804817\n",
      "epoch: 13 step: 504, loss is 0.0014432608149945736\n",
      "epoch: 13 step: 505, loss is 0.026415454223752022\n",
      "epoch: 13 step: 506, loss is 0.010548547841608524\n",
      "epoch: 13 step: 507, loss is 0.10619638115167618\n",
      "epoch: 13 step: 508, loss is 0.025323059409856796\n",
      "epoch: 13 step: 509, loss is 0.033797867596149445\n",
      "epoch: 13 step: 510, loss is 0.014400620944797993\n",
      "epoch: 13 step: 511, loss is 0.006206869147717953\n",
      "epoch: 13 step: 512, loss is 0.02338610589504242\n",
      "epoch: 13 step: 513, loss is 0.11141665279865265\n",
      "epoch: 13 step: 514, loss is 0.16275392472743988\n",
      "epoch: 13 step: 515, loss is 0.020686384290456772\n",
      "epoch: 13 step: 516, loss is 0.04769927263259888\n",
      "epoch: 13 step: 517, loss is 0.02700941637158394\n",
      "epoch: 13 step: 518, loss is 0.018373198807239532\n",
      "epoch: 13 step: 519, loss is 0.013318548910319805\n",
      "epoch: 13 step: 520, loss is 0.09078016132116318\n",
      "epoch: 13 step: 521, loss is 0.040076740086078644\n",
      "epoch: 13 step: 522, loss is 0.009398872032761574\n",
      "epoch: 13 step: 523, loss is 0.010929068550467491\n",
      "epoch: 13 step: 524, loss is 0.009683163836598396\n",
      "epoch: 13 step: 525, loss is 0.10764147341251373\n",
      "epoch: 13 step: 526, loss is 0.03862997516989708\n",
      "epoch: 13 step: 527, loss is 0.014400403015315533\n",
      "epoch: 13 step: 528, loss is 0.004818229004740715\n",
      "epoch: 13 step: 529, loss is 0.02411954291164875\n",
      "epoch: 13 step: 530, loss is 0.03454289212822914\n",
      "epoch: 13 step: 531, loss is 0.006326453760266304\n",
      "epoch: 13 step: 532, loss is 0.00432461267337203\n",
      "epoch: 13 step: 533, loss is 0.041966408491134644\n",
      "epoch: 13 step: 534, loss is 0.07788702845573425\n",
      "epoch: 13 step: 535, loss is 0.06903979182243347\n",
      "epoch: 13 step: 536, loss is 0.011484695598483086\n",
      "epoch: 13 step: 537, loss is 0.0010814040433615446\n",
      "epoch: 13 step: 538, loss is 0.06959154456853867\n",
      "epoch: 13 step: 539, loss is 0.0041147745214402676\n",
      "epoch: 13 step: 540, loss is 0.004044545814394951\n",
      "epoch: 13 step: 541, loss is 0.028760865330696106\n",
      "epoch: 13 step: 542, loss is 0.07353001832962036\n",
      "epoch: 13 step: 543, loss is 0.006890301126986742\n",
      "epoch: 13 step: 544, loss is 0.00942830927670002\n",
      "epoch: 13 step: 545, loss is 0.044881779700517654\n",
      "epoch: 13 step: 546, loss is 0.07246977090835571\n",
      "epoch: 13 step: 547, loss is 0.008245832286775112\n",
      "epoch: 13 step: 548, loss is 0.050507523119449615\n",
      "epoch: 13 step: 549, loss is 0.05574478954076767\n",
      "epoch: 13 step: 550, loss is 0.028786204755306244\n",
      "epoch: 13 step: 551, loss is 0.005788853857666254\n",
      "epoch: 13 step: 552, loss is 0.04425416141748428\n",
      "epoch: 13 step: 553, loss is 0.015114274807274342\n",
      "epoch: 13 step: 554, loss is 0.00817816611379385\n",
      "epoch: 13 step: 555, loss is 0.07791650295257568\n",
      "epoch: 13 step: 556, loss is 0.038252007216215134\n",
      "epoch: 13 step: 557, loss is 0.01952703297138214\n",
      "epoch: 13 step: 558, loss is 0.03679788485169411\n",
      "epoch: 13 step: 559, loss is 0.05300044268369675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 560, loss is 0.015550535172224045\n",
      "epoch: 13 step: 561, loss is 0.020413700491189957\n",
      "epoch: 13 step: 562, loss is 0.051348477602005005\n",
      "epoch: 13 step: 563, loss is 0.03225710615515709\n",
      "epoch: 13 step: 564, loss is 0.03910819813609123\n",
      "epoch: 13 step: 565, loss is 0.03653745353221893\n",
      "epoch: 13 step: 566, loss is 0.03910824656486511\n",
      "epoch: 13 step: 567, loss is 0.03909677639603615\n",
      "epoch: 13 step: 568, loss is 0.011349025182425976\n",
      "epoch: 13 step: 569, loss is 0.1276405155658722\n",
      "epoch: 13 step: 570, loss is 0.08713085949420929\n",
      "epoch: 13 step: 571, loss is 0.017623744904994965\n",
      "epoch: 13 step: 572, loss is 0.014765900559723377\n",
      "epoch: 13 step: 573, loss is 0.03470547869801521\n",
      "epoch: 13 step: 574, loss is 0.010786162689328194\n",
      "epoch: 13 step: 575, loss is 0.007402653805911541\n",
      "epoch: 13 step: 576, loss is 0.02977602370083332\n",
      "epoch: 13 step: 577, loss is 0.018148789182305336\n",
      "epoch: 13 step: 578, loss is 0.062315985560417175\n",
      "epoch: 13 step: 579, loss is 0.015101506374776363\n",
      "epoch: 13 step: 580, loss is 0.027684150263667107\n",
      "epoch: 13 step: 581, loss is 0.025171490386128426\n",
      "epoch: 13 step: 582, loss is 0.005734638310968876\n",
      "epoch: 13 step: 583, loss is 0.04248605668544769\n",
      "epoch: 13 step: 584, loss is 0.0194325540214777\n",
      "epoch: 13 step: 585, loss is 0.027710402384400368\n",
      "epoch: 13 step: 586, loss is 0.011978893540799618\n",
      "epoch: 13 step: 587, loss is 0.04597550630569458\n",
      "epoch: 13 step: 588, loss is 0.017984971404075623\n",
      "epoch: 13 step: 589, loss is 0.03173226863145828\n",
      "epoch: 13 step: 590, loss is 0.06649324297904968\n",
      "epoch: 13 step: 591, loss is 0.011914669536054134\n",
      "epoch: 13 step: 592, loss is 0.05426144599914551\n",
      "epoch: 13 step: 593, loss is 0.02525888942182064\n",
      "epoch: 13 step: 594, loss is 0.06569967418909073\n",
      "epoch: 13 step: 595, loss is 0.0025808592326939106\n",
      "epoch: 13 step: 596, loss is 0.005966284777969122\n",
      "epoch: 13 step: 597, loss is 0.007071776315569878\n",
      "epoch: 13 step: 598, loss is 0.024037299677729607\n",
      "epoch: 13 step: 599, loss is 0.002564162015914917\n",
      "epoch: 13 step: 600, loss is 0.02152206003665924\n",
      "epoch: 13 step: 601, loss is 0.010244475677609444\n",
      "epoch: 13 step: 602, loss is 0.10779906064271927\n",
      "epoch: 13 step: 603, loss is 0.02743213064968586\n",
      "epoch: 13 step: 604, loss is 0.014515207149088383\n",
      "epoch: 13 step: 605, loss is 0.06875113397836685\n",
      "epoch: 13 step: 606, loss is 0.07255814969539642\n",
      "epoch: 13 step: 607, loss is 0.042434848845005035\n",
      "epoch: 13 step: 608, loss is 0.013294326141476631\n",
      "epoch: 13 step: 609, loss is 0.10374598205089569\n",
      "epoch: 13 step: 610, loss is 0.011640752665698528\n",
      "epoch: 13 step: 611, loss is 0.004756778944283724\n",
      "epoch: 13 step: 612, loss is 0.14231695234775543\n",
      "epoch: 13 step: 613, loss is 0.027234289795160294\n",
      "epoch: 13 step: 614, loss is 0.04226250946521759\n",
      "epoch: 13 step: 615, loss is 0.009294142946600914\n",
      "epoch: 13 step: 616, loss is 0.07434725761413574\n",
      "epoch: 13 step: 617, loss is 0.018635809421539307\n",
      "epoch: 13 step: 618, loss is 0.018563896417617798\n",
      "epoch: 13 step: 619, loss is 0.034684039652347565\n",
      "epoch: 13 step: 620, loss is 0.07475996762514114\n",
      "epoch: 13 step: 621, loss is 0.05338793247938156\n",
      "epoch: 13 step: 622, loss is 0.018579138442873955\n",
      "epoch: 13 step: 623, loss is 0.07003245502710342\n",
      "epoch: 13 step: 624, loss is 0.00900872703641653\n",
      "epoch: 13 step: 625, loss is 0.014462459832429886\n",
      "epoch: 13 step: 626, loss is 0.014206425286829472\n",
      "epoch: 13 step: 627, loss is 0.02143780142068863\n",
      "epoch: 13 step: 628, loss is 0.026049302890896797\n",
      "epoch: 13 step: 629, loss is 0.12600697576999664\n",
      "epoch: 13 step: 630, loss is 0.026403624564409256\n",
      "epoch: 13 step: 631, loss is 0.025014463812112808\n",
      "epoch: 13 step: 632, loss is 0.03132537379860878\n",
      "epoch: 13 step: 633, loss is 0.07243095338344574\n",
      "epoch: 13 step: 634, loss is 0.015071287751197815\n",
      "epoch: 13 step: 635, loss is 0.06544902175664902\n",
      "epoch: 13 step: 636, loss is 0.09904032945632935\n",
      "epoch: 13 step: 637, loss is 0.04280047118663788\n",
      "epoch: 13 step: 638, loss is 0.002816019346937537\n",
      "epoch: 13 step: 639, loss is 0.10158148407936096\n",
      "epoch: 13 step: 640, loss is 0.08266215771436691\n",
      "epoch: 13 step: 641, loss is 0.07993666082620621\n",
      "epoch: 13 step: 642, loss is 0.02672717347741127\n",
      "epoch: 13 step: 643, loss is 0.050034914165735245\n",
      "epoch: 13 step: 644, loss is 0.03790747746825218\n",
      "epoch: 13 step: 645, loss is 0.027458207681775093\n",
      "epoch: 13 step: 646, loss is 0.0044670444913208485\n",
      "epoch: 13 step: 647, loss is 0.009639582596719265\n",
      "epoch: 13 step: 648, loss is 0.03655407577753067\n",
      "epoch: 13 step: 649, loss is 0.040339358150959015\n",
      "epoch: 13 step: 650, loss is 0.0029668821953237057\n",
      "epoch: 13 step: 651, loss is 0.022922905161976814\n",
      "epoch: 13 step: 652, loss is 0.0583789087831974\n",
      "epoch: 13 step: 653, loss is 0.004022456239908934\n",
      "epoch: 13 step: 654, loss is 0.021555259823799133\n",
      "epoch: 13 step: 655, loss is 0.011938202194869518\n",
      "epoch: 13 step: 656, loss is 0.008402162231504917\n",
      "epoch: 13 step: 657, loss is 0.033816542476415634\n",
      "epoch: 13 step: 658, loss is 0.06164025142788887\n",
      "epoch: 13 step: 659, loss is 0.08400030434131622\n",
      "epoch: 13 step: 660, loss is 0.02166733704507351\n",
      "epoch: 13 step: 661, loss is 0.007020107936114073\n",
      "epoch: 13 step: 662, loss is 0.022118207067251205\n",
      "epoch: 13 step: 663, loss is 0.0032935349736362696\n",
      "epoch: 13 step: 664, loss is 0.011242435313761234\n",
      "epoch: 13 step: 665, loss is 0.0361652672290802\n",
      "epoch: 13 step: 666, loss is 0.10838255286216736\n",
      "epoch: 13 step: 667, loss is 0.020382242277264595\n",
      "epoch: 13 step: 668, loss is 0.03745617717504501\n",
      "epoch: 13 step: 669, loss is 0.038316287100315094\n",
      "epoch: 13 step: 670, loss is 0.007559612393379211\n",
      "epoch: 13 step: 671, loss is 0.03789333999156952\n",
      "epoch: 13 step: 672, loss is 0.04007294028997421\n",
      "epoch: 13 step: 673, loss is 0.00912854541093111\n",
      "epoch: 13 step: 674, loss is 0.07313892990350723\n",
      "epoch: 13 step: 675, loss is 0.045993052423000336\n",
      "epoch: 13 step: 676, loss is 0.060630641877651215\n",
      "epoch: 13 step: 677, loss is 0.005521752405911684\n",
      "epoch: 13 step: 678, loss is 0.05019504949450493\n",
      "epoch: 13 step: 679, loss is 0.07095348834991455\n",
      "epoch: 13 step: 680, loss is 0.07758847624063492\n",
      "epoch: 13 step: 681, loss is 0.005290677305310965\n",
      "epoch: 13 step: 682, loss is 0.0792880430817604\n",
      "epoch: 13 step: 683, loss is 0.025316474959254265\n",
      "epoch: 13 step: 684, loss is 0.03770209848880768\n",
      "epoch: 13 step: 685, loss is 0.02174236625432968\n",
      "epoch: 13 step: 686, loss is 0.0712011381983757\n",
      "epoch: 13 step: 687, loss is 0.025287361815571785\n",
      "epoch: 13 step: 688, loss is 0.02560650184750557\n",
      "epoch: 13 step: 689, loss is 0.035514947026968\n",
      "epoch: 13 step: 690, loss is 0.0322941392660141\n",
      "epoch: 13 step: 691, loss is 0.04508543387055397\n",
      "epoch: 13 step: 692, loss is 0.04019627347588539\n",
      "epoch: 13 step: 693, loss is 0.0025730435736477375\n",
      "epoch: 13 step: 694, loss is 0.009211341850459576\n",
      "epoch: 13 step: 695, loss is 0.11765874177217484\n",
      "epoch: 13 step: 696, loss is 0.030705366283655167\n",
      "epoch: 13 step: 697, loss is 0.0052113463170826435\n",
      "epoch: 13 step: 698, loss is 0.0034500276669859886\n",
      "epoch: 13 step: 699, loss is 0.04440411180257797\n",
      "epoch: 13 step: 700, loss is 0.020391924306750298\n",
      "epoch: 13 step: 701, loss is 0.024294594302773476\n",
      "epoch: 13 step: 702, loss is 0.0835283100605011\n",
      "epoch: 13 step: 703, loss is 0.018692541867494583\n",
      "epoch: 13 step: 704, loss is 0.09243914484977722\n",
      "epoch: 13 step: 705, loss is 0.040272779762744904\n",
      "epoch: 13 step: 706, loss is 0.11125870048999786\n",
      "epoch: 13 step: 707, loss is 0.01533548440784216\n",
      "epoch: 13 step: 708, loss is 0.0034993968438357115\n",
      "epoch: 13 step: 709, loss is 0.025375420227646828\n",
      "epoch: 13 step: 710, loss is 0.018044812604784966\n",
      "epoch: 13 step: 711, loss is 0.02616773545742035\n",
      "epoch: 13 step: 712, loss is 0.23562149703502655\n",
      "epoch: 13 step: 713, loss is 0.05701552703976631\n",
      "epoch: 13 step: 714, loss is 0.030995475128293037\n",
      "epoch: 13 step: 715, loss is 0.02765801176428795\n",
      "epoch: 13 step: 716, loss is 0.003993417602032423\n",
      "epoch: 13 step: 717, loss is 0.1353653222322464\n",
      "epoch: 13 step: 718, loss is 0.05771946534514427\n",
      "epoch: 13 step: 719, loss is 0.04744018241763115\n",
      "epoch: 13 step: 720, loss is 0.011520972475409508\n",
      "epoch: 13 step: 721, loss is 0.012362202629446983\n",
      "epoch: 13 step: 722, loss is 0.05156150832772255\n",
      "epoch: 13 step: 723, loss is 0.0838601291179657\n",
      "epoch: 13 step: 724, loss is 0.030571334064006805\n",
      "epoch: 13 step: 725, loss is 0.011362085118889809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 726, loss is 0.0071167536079883575\n",
      "epoch: 13 step: 727, loss is 0.019522836431860924\n",
      "epoch: 13 step: 728, loss is 0.1022396981716156\n",
      "epoch: 13 step: 729, loss is 0.011301621794700623\n",
      "epoch: 13 step: 730, loss is 0.21371543407440186\n",
      "epoch: 13 step: 731, loss is 0.11180666089057922\n",
      "epoch: 13 step: 732, loss is 0.06192933768033981\n",
      "epoch: 13 step: 733, loss is 0.008120396174490452\n",
      "epoch: 13 step: 734, loss is 0.028674863278865814\n",
      "epoch: 13 step: 735, loss is 0.029368439689278603\n",
      "epoch: 13 step: 736, loss is 0.05640653520822525\n",
      "epoch: 13 step: 737, loss is 0.0038881327491253614\n",
      "epoch: 13 step: 738, loss is 0.03155279904603958\n",
      "epoch: 13 step: 739, loss is 0.0056711044162511826\n",
      "epoch: 13 step: 740, loss is 0.004044777248054743\n",
      "epoch: 13 step: 741, loss is 0.019276613369584084\n",
      "epoch: 13 step: 742, loss is 0.08845154196023941\n",
      "epoch: 13 step: 743, loss is 0.003657745197415352\n",
      "epoch: 13 step: 744, loss is 0.044760674238204956\n",
      "epoch: 13 step: 745, loss is 0.0413765013217926\n",
      "epoch: 13 step: 746, loss is 0.03485392406582832\n",
      "epoch: 13 step: 747, loss is 0.02717214822769165\n",
      "epoch: 13 step: 748, loss is 0.061974577605724335\n",
      "epoch: 13 step: 749, loss is 0.033744461834430695\n",
      "epoch: 13 step: 750, loss is 0.027535201981663704\n",
      "epoch: 13 step: 751, loss is 0.036946795880794525\n",
      "epoch: 13 step: 752, loss is 0.012134847231209278\n",
      "epoch: 13 step: 753, loss is 0.029027335345745087\n",
      "epoch: 13 step: 754, loss is 0.021340925246477127\n",
      "epoch: 13 step: 755, loss is 0.027619263157248497\n",
      "epoch: 13 step: 756, loss is 0.08165308088064194\n",
      "epoch: 13 step: 757, loss is 0.02420833520591259\n",
      "epoch: 13 step: 758, loss is 0.06685937196016312\n",
      "epoch: 13 step: 759, loss is 0.01435434352606535\n",
      "epoch: 13 step: 760, loss is 0.013398334383964539\n",
      "epoch: 13 step: 761, loss is 0.030151885002851486\n",
      "epoch: 13 step: 762, loss is 0.009227386675775051\n",
      "epoch: 13 step: 763, loss is 0.003494726261124015\n",
      "epoch: 13 step: 764, loss is 0.05234520882368088\n",
      "epoch: 13 step: 765, loss is 0.004748249892145395\n",
      "epoch: 13 step: 766, loss is 0.049639999866485596\n",
      "epoch: 13 step: 767, loss is 0.047661781311035156\n",
      "epoch: 13 step: 768, loss is 0.0028133096639066935\n",
      "epoch: 13 step: 769, loss is 0.00463354866951704\n",
      "epoch: 13 step: 770, loss is 0.07738469541072845\n",
      "epoch: 13 step: 771, loss is 0.0025086253881454468\n",
      "epoch: 13 step: 772, loss is 0.015454525128006935\n",
      "epoch: 13 step: 773, loss is 0.12499001622200012\n",
      "epoch: 13 step: 774, loss is 0.019098199903964996\n",
      "epoch: 13 step: 775, loss is 0.0025709441397339106\n",
      "epoch: 13 step: 776, loss is 0.010188327170908451\n",
      "epoch: 13 step: 777, loss is 0.011273832060396671\n",
      "epoch: 13 step: 778, loss is 0.011826835572719574\n",
      "epoch: 13 step: 779, loss is 0.04222626984119415\n",
      "epoch: 13 step: 780, loss is 0.06220810487866402\n",
      "epoch: 13 step: 781, loss is 0.08016981929540634\n",
      "epoch: 13 step: 782, loss is 0.02590327151119709\n",
      "epoch: 13 step: 783, loss is 0.01970810815691948\n",
      "epoch: 13 step: 784, loss is 0.024054594337940216\n",
      "epoch: 13 step: 785, loss is 0.07382205128669739\n",
      "epoch: 13 step: 786, loss is 0.02490082010626793\n",
      "epoch: 13 step: 787, loss is 0.007535930257290602\n",
      "epoch: 13 step: 788, loss is 0.03287402540445328\n",
      "epoch: 13 step: 789, loss is 0.0037179572973400354\n",
      "epoch: 13 step: 790, loss is 0.07637570053339005\n",
      "epoch: 13 step: 791, loss is 0.001284716883674264\n",
      "epoch: 13 step: 792, loss is 0.0419747456908226\n",
      "epoch: 13 step: 793, loss is 0.06277249753475189\n",
      "epoch: 13 step: 794, loss is 0.008700050413608551\n",
      "epoch: 13 step: 795, loss is 0.053367409855127335\n",
      "epoch: 13 step: 796, loss is 0.015896549448370934\n",
      "epoch: 13 step: 797, loss is 0.02494456060230732\n",
      "epoch: 13 step: 798, loss is 0.02499292604625225\n",
      "epoch: 13 step: 799, loss is 0.0073673962615430355\n",
      "epoch: 13 step: 800, loss is 0.015222698450088501\n",
      "epoch: 13 step: 801, loss is 0.05851900577545166\n",
      "epoch: 13 step: 802, loss is 0.09012174606323242\n",
      "epoch: 13 step: 803, loss is 0.016705527901649475\n",
      "epoch: 13 step: 804, loss is 0.006190615706145763\n",
      "epoch: 13 step: 805, loss is 0.02312505431473255\n",
      "epoch: 13 step: 806, loss is 0.012187276035547256\n",
      "epoch: 13 step: 807, loss is 0.037764254957437515\n",
      "epoch: 13 step: 808, loss is 0.01889384165406227\n",
      "epoch: 13 step: 809, loss is 0.019532131031155586\n",
      "epoch: 13 step: 810, loss is 0.04538939148187637\n",
      "epoch: 13 step: 811, loss is 0.02131567895412445\n",
      "epoch: 13 step: 812, loss is 0.036301176995038986\n",
      "epoch: 13 step: 813, loss is 0.0032897619530558586\n",
      "epoch: 13 step: 814, loss is 0.009358632378280163\n",
      "epoch: 13 step: 815, loss is 0.030672160908579826\n",
      "epoch: 13 step: 816, loss is 0.0043281689286231995\n",
      "epoch: 13 step: 817, loss is 0.015453055500984192\n",
      "epoch: 13 step: 818, loss is 0.008717422373592854\n",
      "epoch: 13 step: 819, loss is 0.006230201572179794\n",
      "epoch: 13 step: 820, loss is 0.03893023729324341\n",
      "epoch: 13 step: 821, loss is 0.027554266154766083\n",
      "epoch: 13 step: 822, loss is 0.06917144358158112\n",
      "epoch: 13 step: 823, loss is 0.0319681279361248\n",
      "epoch: 13 step: 824, loss is 0.044687360525131226\n",
      "epoch: 13 step: 825, loss is 0.011722943745553493\n",
      "epoch: 13 step: 826, loss is 0.03639165684580803\n",
      "epoch: 13 step: 827, loss is 0.05592932924628258\n",
      "epoch: 13 step: 828, loss is 0.08350714296102524\n",
      "epoch: 13 step: 829, loss is 0.009782045148313046\n",
      "epoch: 13 step: 830, loss is 0.03383341431617737\n",
      "epoch: 13 step: 831, loss is 0.009840273298323154\n",
      "epoch: 13 step: 832, loss is 0.012753921560943127\n",
      "epoch: 13 step: 833, loss is 0.03203970193862915\n",
      "epoch: 13 step: 834, loss is 0.004937308374792337\n",
      "epoch: 13 step: 835, loss is 0.024652911350131035\n",
      "epoch: 13 step: 836, loss is 0.0479327030479908\n",
      "epoch: 13 step: 837, loss is 0.02219141647219658\n",
      "epoch: 13 step: 838, loss is 0.008888784795999527\n",
      "epoch: 13 step: 839, loss is 0.0017289731185883284\n",
      "epoch: 13 step: 840, loss is 0.009479419328272343\n",
      "epoch: 13 step: 841, loss is 0.033223558217287064\n",
      "epoch: 13 step: 842, loss is 0.004416889511048794\n",
      "epoch: 13 step: 843, loss is 0.040267426520586014\n",
      "epoch: 13 step: 844, loss is 0.017458966001868248\n",
      "epoch: 13 step: 845, loss is 0.08249903470277786\n",
      "epoch: 13 step: 846, loss is 0.005256804171949625\n",
      "epoch: 13 step: 847, loss is 0.030594710260629654\n",
      "epoch: 13 step: 848, loss is 0.011067531071603298\n",
      "epoch: 13 step: 849, loss is 0.007792962249368429\n",
      "epoch: 13 step: 850, loss is 0.0057458411902189255\n",
      "epoch: 13 step: 851, loss is 0.01522514782845974\n",
      "epoch: 13 step: 852, loss is 0.019870109856128693\n",
      "epoch: 13 step: 853, loss is 0.026532979682087898\n",
      "epoch: 13 step: 854, loss is 0.0211188904941082\n",
      "epoch: 13 step: 855, loss is 0.0570695735514164\n",
      "epoch: 13 step: 856, loss is 0.026440678164362907\n",
      "epoch: 13 step: 857, loss is 0.025581946596503258\n",
      "epoch: 13 step: 858, loss is 0.005525305867195129\n",
      "epoch: 13 step: 859, loss is 0.08857489377260208\n",
      "epoch: 13 step: 860, loss is 0.022166693583130836\n",
      "epoch: 13 step: 861, loss is 0.052625615149736404\n",
      "epoch: 13 step: 862, loss is 0.0030064787715673447\n",
      "epoch: 13 step: 863, loss is 0.01345513854175806\n",
      "epoch: 13 step: 864, loss is 0.022610140964388847\n",
      "epoch: 13 step: 865, loss is 0.039712462574243546\n",
      "epoch: 13 step: 866, loss is 0.003159100655466318\n",
      "epoch: 13 step: 867, loss is 0.1214153841137886\n",
      "epoch: 13 step: 868, loss is 0.012016993016004562\n",
      "epoch: 13 step: 869, loss is 0.10787586867809296\n",
      "epoch: 13 step: 870, loss is 0.046828534454107285\n",
      "epoch: 13 step: 871, loss is 0.03157755732536316\n",
      "epoch: 13 step: 872, loss is 0.021203145384788513\n",
      "epoch: 13 step: 873, loss is 0.08299770206212997\n",
      "epoch: 13 step: 874, loss is 0.052771806716918945\n",
      "epoch: 13 step: 875, loss is 0.008688532747328281\n",
      "epoch: 13 step: 876, loss is 0.0020407463889569044\n",
      "epoch: 13 step: 877, loss is 0.015495319850742817\n",
      "epoch: 13 step: 878, loss is 0.028865816071629524\n",
      "epoch: 13 step: 879, loss is 0.05684858188033104\n",
      "epoch: 13 step: 880, loss is 0.031228546053171158\n",
      "epoch: 13 step: 881, loss is 0.03876388072967529\n",
      "epoch: 13 step: 882, loss is 0.015042539685964584\n",
      "epoch: 13 step: 883, loss is 0.014148904010653496\n",
      "epoch: 13 step: 884, loss is 0.010657919570803642\n",
      "epoch: 13 step: 885, loss is 0.027332408353686333\n",
      "epoch: 13 step: 886, loss is 0.020582592114806175\n",
      "epoch: 13 step: 887, loss is 0.019625984132289886\n",
      "epoch: 13 step: 888, loss is 0.046805474907159805\n",
      "epoch: 13 step: 889, loss is 0.024891138076782227\n",
      "epoch: 13 step: 890, loss is 0.018014632165431976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 891, loss is 0.030202435329556465\n",
      "epoch: 13 step: 892, loss is 0.033044084906578064\n",
      "epoch: 13 step: 893, loss is 0.016780788078904152\n",
      "epoch: 13 step: 894, loss is 0.02050936035811901\n",
      "epoch: 13 step: 895, loss is 0.08168921619653702\n",
      "epoch: 13 step: 896, loss is 0.051542118191719055\n",
      "epoch: 13 step: 897, loss is 0.0010011312551796436\n",
      "epoch: 13 step: 898, loss is 0.001963990507647395\n",
      "epoch: 13 step: 899, loss is 0.017867548391222954\n",
      "epoch: 13 step: 900, loss is 0.002930672839283943\n",
      "epoch: 13 step: 901, loss is 0.01743185520172119\n",
      "epoch: 13 step: 902, loss is 0.1193113848567009\n",
      "epoch: 13 step: 903, loss is 0.020820951089262962\n",
      "epoch: 13 step: 904, loss is 0.034698694944381714\n",
      "epoch: 13 step: 905, loss is 0.14884702861309052\n",
      "epoch: 13 step: 906, loss is 0.04317427799105644\n",
      "epoch: 13 step: 907, loss is 0.020224962383508682\n",
      "epoch: 13 step: 908, loss is 0.005803780630230904\n",
      "epoch: 13 step: 909, loss is 0.01009820681065321\n",
      "epoch: 13 step: 910, loss is 0.0015029702335596085\n",
      "epoch: 13 step: 911, loss is 0.003575034672394395\n",
      "epoch: 13 step: 912, loss is 0.05936233699321747\n",
      "epoch: 13 step: 913, loss is 0.002006931696087122\n",
      "epoch: 13 step: 914, loss is 0.035134457051754\n",
      "epoch: 13 step: 915, loss is 0.015595095232129097\n",
      "epoch: 13 step: 916, loss is 0.021441346034407616\n",
      "epoch: 13 step: 917, loss is 0.024212947115302086\n",
      "epoch: 13 step: 918, loss is 0.045352913439273834\n",
      "epoch: 13 step: 919, loss is 0.1111438050866127\n",
      "epoch: 13 step: 920, loss is 0.008233590051531792\n",
      "epoch: 13 step: 921, loss is 0.036029648035764694\n",
      "epoch: 13 step: 922, loss is 0.08033876121044159\n",
      "epoch: 13 step: 923, loss is 0.0434783473610878\n",
      "epoch: 13 step: 924, loss is 0.005224552005529404\n",
      "epoch: 13 step: 925, loss is 0.0013856221921741962\n",
      "epoch: 13 step: 926, loss is 0.0015263946261256933\n",
      "epoch: 13 step: 927, loss is 0.024680783972144127\n",
      "epoch: 13 step: 928, loss is 0.004365295171737671\n",
      "epoch: 13 step: 929, loss is 0.01285881083458662\n",
      "epoch: 13 step: 930, loss is 0.0329129695892334\n",
      "epoch: 13 step: 931, loss is 0.013229639269411564\n",
      "epoch: 13 step: 932, loss is 0.012246942147612572\n",
      "epoch: 13 step: 933, loss is 0.01516017410904169\n",
      "epoch: 13 step: 934, loss is 0.009457007981836796\n",
      "epoch: 13 step: 935, loss is 0.006616233382374048\n",
      "epoch: 13 step: 936, loss is 0.008583315648138523\n",
      "epoch: 13 step: 937, loss is 0.08619708567857742\n",
      "epoch: 14 step: 1, loss is 0.07037891447544098\n",
      "epoch: 14 step: 2, loss is 0.0008639966836199164\n",
      "epoch: 14 step: 3, loss is 0.010329840704798698\n",
      "epoch: 14 step: 4, loss is 0.018588270992040634\n",
      "epoch: 14 step: 5, loss is 0.005928346887230873\n",
      "epoch: 14 step: 6, loss is 0.018484609201550484\n",
      "epoch: 14 step: 7, loss is 0.0010689065093174577\n",
      "epoch: 14 step: 8, loss is 0.020735980942845345\n",
      "epoch: 14 step: 9, loss is 0.012466509826481342\n",
      "epoch: 14 step: 10, loss is 0.027506094425916672\n",
      "epoch: 14 step: 11, loss is 0.004114360548555851\n",
      "epoch: 14 step: 12, loss is 0.02468162402510643\n",
      "epoch: 14 step: 13, loss is 0.02409929223358631\n",
      "epoch: 14 step: 14, loss is 0.0389368012547493\n",
      "epoch: 14 step: 15, loss is 0.016929199919104576\n",
      "epoch: 14 step: 16, loss is 0.014861319214105606\n",
      "epoch: 14 step: 17, loss is 0.007050370331853628\n",
      "epoch: 14 step: 18, loss is 0.0201441440731287\n",
      "epoch: 14 step: 19, loss is 0.018491026014089584\n",
      "epoch: 14 step: 20, loss is 0.00922366976737976\n",
      "epoch: 14 step: 21, loss is 0.002794325351715088\n",
      "epoch: 14 step: 22, loss is 0.00273289461620152\n",
      "epoch: 14 step: 23, loss is 0.009679491631686687\n",
      "epoch: 14 step: 24, loss is 0.010541410185396671\n",
      "epoch: 14 step: 25, loss is 0.009613126516342163\n",
      "epoch: 14 step: 26, loss is 0.051557496190071106\n",
      "epoch: 14 step: 27, loss is 0.0156198525801301\n",
      "epoch: 14 step: 28, loss is 0.015442710369825363\n",
      "epoch: 14 step: 29, loss is 0.01506899669766426\n",
      "epoch: 14 step: 30, loss is 0.0028129983693361282\n",
      "epoch: 14 step: 31, loss is 0.02136833406984806\n",
      "epoch: 14 step: 32, loss is 0.006575984414666891\n",
      "epoch: 14 step: 33, loss is 0.0027156430296599865\n",
      "epoch: 14 step: 34, loss is 0.007149192970246077\n",
      "epoch: 14 step: 35, loss is 0.006528825964778662\n",
      "epoch: 14 step: 36, loss is 0.0054384577088057995\n",
      "epoch: 14 step: 37, loss is 0.00798366591334343\n",
      "epoch: 14 step: 38, loss is 0.057470835745334625\n",
      "epoch: 14 step: 39, loss is 0.0767129436135292\n",
      "epoch: 14 step: 40, loss is 0.03360264375805855\n",
      "epoch: 14 step: 41, loss is 0.0025513041764497757\n",
      "epoch: 14 step: 42, loss is 0.004670972470194101\n",
      "epoch: 14 step: 43, loss is 0.02606821060180664\n",
      "epoch: 14 step: 44, loss is 0.023411542177200317\n",
      "epoch: 14 step: 45, loss is 0.11119143664836884\n",
      "epoch: 14 step: 46, loss is 0.02271169237792492\n",
      "epoch: 14 step: 47, loss is 0.007681136019527912\n",
      "epoch: 14 step: 48, loss is 0.002174139255657792\n",
      "epoch: 14 step: 49, loss is 0.04458536580204964\n",
      "epoch: 14 step: 50, loss is 0.0005452514160424471\n",
      "epoch: 14 step: 51, loss is 0.029397280886769295\n",
      "epoch: 14 step: 52, loss is 0.008930094540119171\n",
      "epoch: 14 step: 53, loss is 0.07378371059894562\n",
      "epoch: 14 step: 54, loss is 0.024752678349614143\n",
      "epoch: 14 step: 55, loss is 0.02558360993862152\n",
      "epoch: 14 step: 56, loss is 0.0071134259924292564\n",
      "epoch: 14 step: 57, loss is 0.015388861298561096\n",
      "epoch: 14 step: 58, loss is 0.02455669827759266\n",
      "epoch: 14 step: 59, loss is 0.006714329123497009\n",
      "epoch: 14 step: 60, loss is 0.010664254426956177\n",
      "epoch: 14 step: 61, loss is 0.010056579485535622\n",
      "epoch: 14 step: 62, loss is 0.005138259381055832\n",
      "epoch: 14 step: 63, loss is 0.0019550619181245565\n",
      "epoch: 14 step: 64, loss is 0.017647823318839073\n",
      "epoch: 14 step: 65, loss is 0.015256037935614586\n",
      "epoch: 14 step: 66, loss is 0.012605092488229275\n",
      "epoch: 14 step: 67, loss is 0.02551928535103798\n",
      "epoch: 14 step: 68, loss is 0.0014794274466112256\n",
      "epoch: 14 step: 69, loss is 0.008227894082665443\n",
      "epoch: 14 step: 70, loss is 0.016576062887907028\n",
      "epoch: 14 step: 71, loss is 0.0038666753098368645\n",
      "epoch: 14 step: 72, loss is 0.010244148783385754\n",
      "epoch: 14 step: 73, loss is 0.019657276570796967\n",
      "epoch: 14 step: 74, loss is 0.010451402515172958\n",
      "epoch: 14 step: 75, loss is 0.02987808734178543\n",
      "epoch: 14 step: 76, loss is 0.0691361129283905\n",
      "epoch: 14 step: 77, loss is 0.012225024402141571\n",
      "epoch: 14 step: 78, loss is 0.01173495128750801\n",
      "epoch: 14 step: 79, loss is 0.03054140694439411\n",
      "epoch: 14 step: 80, loss is 0.00647134892642498\n",
      "epoch: 14 step: 81, loss is 0.003408757969737053\n",
      "epoch: 14 step: 82, loss is 0.0024355431087315083\n",
      "epoch: 14 step: 83, loss is 0.024780306965112686\n",
      "epoch: 14 step: 84, loss is 0.0023002924863249063\n",
      "epoch: 14 step: 85, loss is 0.010750485584139824\n",
      "epoch: 14 step: 86, loss is 0.001687253825366497\n",
      "epoch: 14 step: 87, loss is 0.017536545172333717\n",
      "epoch: 14 step: 88, loss is 0.017866209149360657\n",
      "epoch: 14 step: 89, loss is 0.004821984563022852\n",
      "epoch: 14 step: 90, loss is 0.09068110585212708\n",
      "epoch: 14 step: 91, loss is 0.023384075611829758\n",
      "epoch: 14 step: 92, loss is 0.004493039101362228\n",
      "epoch: 14 step: 93, loss is 0.04101186245679855\n",
      "epoch: 14 step: 94, loss is 0.013826554641127586\n",
      "epoch: 14 step: 95, loss is 0.008776756003499031\n",
      "epoch: 14 step: 96, loss is 0.04747309535741806\n",
      "epoch: 14 step: 97, loss is 0.02874593809247017\n",
      "epoch: 14 step: 98, loss is 0.026598019525408745\n",
      "epoch: 14 step: 99, loss is 0.005576509051024914\n",
      "epoch: 14 step: 100, loss is 0.003938823007047176\n",
      "epoch: 14 step: 101, loss is 0.08057593554258347\n",
      "epoch: 14 step: 102, loss is 0.01354384608566761\n",
      "epoch: 14 step: 103, loss is 0.022188257426023483\n",
      "epoch: 14 step: 104, loss is 0.004863987676799297\n",
      "epoch: 14 step: 105, loss is 0.0031495527364313602\n",
      "epoch: 14 step: 106, loss is 0.0049433354288339615\n",
      "epoch: 14 step: 107, loss is 0.011212272569537163\n",
      "epoch: 14 step: 108, loss is 0.1410013735294342\n",
      "epoch: 14 step: 109, loss is 0.01442764326930046\n",
      "epoch: 14 step: 110, loss is 0.04182517156004906\n",
      "epoch: 14 step: 111, loss is 0.004701462108641863\n",
      "epoch: 14 step: 112, loss is 0.010967190377414227\n",
      "epoch: 14 step: 113, loss is 0.011182159185409546\n",
      "epoch: 14 step: 114, loss is 0.00045238222810439765\n",
      "epoch: 14 step: 115, loss is 0.011822356842458248\n",
      "epoch: 14 step: 116, loss is 0.007045503705739975\n",
      "epoch: 14 step: 117, loss is 0.007238399237394333\n",
      "epoch: 14 step: 118, loss is 0.0032719727605581284\n",
      "epoch: 14 step: 119, loss is 0.0029625981114804745\n",
      "epoch: 14 step: 120, loss is 0.03259267285466194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 121, loss is 0.04441742226481438\n",
      "epoch: 14 step: 122, loss is 0.003765776287764311\n",
      "epoch: 14 step: 123, loss is 0.010569789446890354\n",
      "epoch: 14 step: 124, loss is 0.04400050640106201\n",
      "epoch: 14 step: 125, loss is 0.003140278160572052\n",
      "epoch: 14 step: 126, loss is 0.0020747934468090534\n",
      "epoch: 14 step: 127, loss is 0.0016772901872172952\n",
      "epoch: 14 step: 128, loss is 0.0028686579316854477\n",
      "epoch: 14 step: 129, loss is 0.024736635386943817\n",
      "epoch: 14 step: 130, loss is 0.011557255871593952\n",
      "epoch: 14 step: 131, loss is 0.03529525548219681\n",
      "epoch: 14 step: 132, loss is 0.013042120262980461\n",
      "epoch: 14 step: 133, loss is 0.0015560719184577465\n",
      "epoch: 14 step: 134, loss is 0.06727278977632523\n",
      "epoch: 14 step: 135, loss is 0.017580950632691383\n",
      "epoch: 14 step: 136, loss is 0.006142911501228809\n",
      "epoch: 14 step: 137, loss is 0.016996216028928757\n",
      "epoch: 14 step: 138, loss is 0.010031122714281082\n",
      "epoch: 14 step: 139, loss is 0.027926508337259293\n",
      "epoch: 14 step: 140, loss is 0.03478960320353508\n",
      "epoch: 14 step: 141, loss is 0.06380413472652435\n",
      "epoch: 14 step: 142, loss is 0.004770753905177116\n",
      "epoch: 14 step: 143, loss is 0.02597186341881752\n",
      "epoch: 14 step: 144, loss is 0.0013079368509352207\n",
      "epoch: 14 step: 145, loss is 0.00690939137712121\n",
      "epoch: 14 step: 146, loss is 0.0014824054669588804\n",
      "epoch: 14 step: 147, loss is 0.014669070020318031\n",
      "epoch: 14 step: 148, loss is 0.01843014359474182\n",
      "epoch: 14 step: 149, loss is 0.010032426565885544\n",
      "epoch: 14 step: 150, loss is 0.021652065217494965\n",
      "epoch: 14 step: 151, loss is 0.0019447539234533906\n",
      "epoch: 14 step: 152, loss is 0.0022072219289839268\n",
      "epoch: 14 step: 153, loss is 0.023615049198269844\n",
      "epoch: 14 step: 154, loss is 0.05096670240163803\n",
      "epoch: 14 step: 155, loss is 0.0022320880088955164\n",
      "epoch: 14 step: 156, loss is 0.011711819097399712\n",
      "epoch: 14 step: 157, loss is 0.010877746157348156\n",
      "epoch: 14 step: 158, loss is 0.008806579746305943\n",
      "epoch: 14 step: 159, loss is 0.001753694610670209\n",
      "epoch: 14 step: 160, loss is 0.04465505853295326\n",
      "epoch: 14 step: 161, loss is 0.011024370789527893\n",
      "epoch: 14 step: 162, loss is 0.018194187432527542\n",
      "epoch: 14 step: 163, loss is 0.08528860658407211\n",
      "epoch: 14 step: 164, loss is 0.022703973576426506\n",
      "epoch: 14 step: 165, loss is 0.019758563488721848\n",
      "epoch: 14 step: 166, loss is 0.0386781208217144\n",
      "epoch: 14 step: 167, loss is 0.004833335522562265\n",
      "epoch: 14 step: 168, loss is 0.007288064807653427\n",
      "epoch: 14 step: 169, loss is 0.007769135292619467\n",
      "epoch: 14 step: 170, loss is 0.006089095026254654\n",
      "epoch: 14 step: 171, loss is 0.09388217329978943\n",
      "epoch: 14 step: 172, loss is 0.02372385375201702\n",
      "epoch: 14 step: 173, loss is 0.0064092096872627735\n",
      "epoch: 14 step: 174, loss is 0.0009917129063978791\n",
      "epoch: 14 step: 175, loss is 0.039335474371910095\n",
      "epoch: 14 step: 176, loss is 0.007864492014050484\n",
      "epoch: 14 step: 177, loss is 0.002592499367892742\n",
      "epoch: 14 step: 178, loss is 0.06975997239351273\n",
      "epoch: 14 step: 179, loss is 0.011472438462078571\n",
      "epoch: 14 step: 180, loss is 0.004167189821600914\n",
      "epoch: 14 step: 181, loss is 0.011998853646218777\n",
      "epoch: 14 step: 182, loss is 0.013508660718798637\n",
      "epoch: 14 step: 183, loss is 0.00036502329749055207\n",
      "epoch: 14 step: 184, loss is 0.016023973003029823\n",
      "epoch: 14 step: 185, loss is 0.08010679483413696\n",
      "epoch: 14 step: 186, loss is 0.03377188369631767\n",
      "epoch: 14 step: 187, loss is 0.013441890478134155\n",
      "epoch: 14 step: 188, loss is 0.05293095111846924\n",
      "epoch: 14 step: 189, loss is 0.003887907601892948\n",
      "epoch: 14 step: 190, loss is 0.02416200563311577\n",
      "epoch: 14 step: 191, loss is 0.0171500276774168\n",
      "epoch: 14 step: 192, loss is 0.02985340543091297\n",
      "epoch: 14 step: 193, loss is 0.052696678787469864\n",
      "epoch: 14 step: 194, loss is 0.004448283929377794\n",
      "epoch: 14 step: 195, loss is 0.005317083094269037\n",
      "epoch: 14 step: 196, loss is 0.004806057084351778\n",
      "epoch: 14 step: 197, loss is 0.0003445471520535648\n",
      "epoch: 14 step: 198, loss is 0.03553314879536629\n",
      "epoch: 14 step: 199, loss is 0.013268139213323593\n",
      "epoch: 14 step: 200, loss is 0.007337982300668955\n",
      "epoch: 14 step: 201, loss is 0.005695752799510956\n",
      "epoch: 14 step: 202, loss is 0.0021197388414293528\n",
      "epoch: 14 step: 203, loss is 0.004068171605467796\n",
      "epoch: 14 step: 204, loss is 0.0012991457479074597\n",
      "epoch: 14 step: 205, loss is 0.01779233105480671\n",
      "epoch: 14 step: 206, loss is 0.0033561247400939465\n",
      "epoch: 14 step: 207, loss is 0.005193628836423159\n",
      "epoch: 14 step: 208, loss is 0.003956048283725977\n",
      "epoch: 14 step: 209, loss is 0.016670947894454002\n",
      "epoch: 14 step: 210, loss is 0.018416645005345345\n",
      "epoch: 14 step: 211, loss is 0.004018136765807867\n",
      "epoch: 14 step: 212, loss is 0.03512011468410492\n",
      "epoch: 14 step: 213, loss is 0.06985952705144882\n",
      "epoch: 14 step: 214, loss is 0.014935829676687717\n",
      "epoch: 14 step: 215, loss is 0.002829781034961343\n",
      "epoch: 14 step: 216, loss is 0.02771751768887043\n",
      "epoch: 14 step: 217, loss is 0.0027835688088089228\n",
      "epoch: 14 step: 218, loss is 0.05138541758060455\n",
      "epoch: 14 step: 219, loss is 0.0007855491130612791\n",
      "epoch: 14 step: 220, loss is 0.010471399873495102\n",
      "epoch: 14 step: 221, loss is 0.003325532888993621\n",
      "epoch: 14 step: 222, loss is 0.005681512877345085\n",
      "epoch: 14 step: 223, loss is 0.0342743881046772\n",
      "epoch: 14 step: 224, loss is 0.014623411931097507\n",
      "epoch: 14 step: 225, loss is 0.010295662097632885\n",
      "epoch: 14 step: 226, loss is 0.009112936444580555\n",
      "epoch: 14 step: 227, loss is 0.05730518326163292\n",
      "epoch: 14 step: 228, loss is 0.00587313249707222\n",
      "epoch: 14 step: 229, loss is 0.014557582326233387\n",
      "epoch: 14 step: 230, loss is 0.030584312975406647\n",
      "epoch: 14 step: 231, loss is 0.003170840907841921\n",
      "epoch: 14 step: 232, loss is 0.02189094014465809\n",
      "epoch: 14 step: 233, loss is 0.006413011811673641\n",
      "epoch: 14 step: 234, loss is 0.004713011439889669\n",
      "epoch: 14 step: 235, loss is 0.0005508696776814759\n",
      "epoch: 14 step: 236, loss is 0.0346074178814888\n",
      "epoch: 14 step: 237, loss is 0.07189036160707474\n",
      "epoch: 14 step: 238, loss is 0.011003716848790646\n",
      "epoch: 14 step: 239, loss is 0.033590637147426605\n",
      "epoch: 14 step: 240, loss is 0.0235306303948164\n",
      "epoch: 14 step: 241, loss is 0.02262893319129944\n",
      "epoch: 14 step: 242, loss is 0.026418982073664665\n",
      "epoch: 14 step: 243, loss is 0.04714185371994972\n",
      "epoch: 14 step: 244, loss is 0.0076083955354988575\n",
      "epoch: 14 step: 245, loss is 0.03251186013221741\n",
      "epoch: 14 step: 246, loss is 0.007005906663835049\n",
      "epoch: 14 step: 247, loss is 0.009236474521458149\n",
      "epoch: 14 step: 248, loss is 0.0037094319704920053\n",
      "epoch: 14 step: 249, loss is 0.00917497556656599\n",
      "epoch: 14 step: 250, loss is 0.02469133771955967\n",
      "epoch: 14 step: 251, loss is 0.05810686945915222\n",
      "epoch: 14 step: 252, loss is 0.03062553144991398\n",
      "epoch: 14 step: 253, loss is 0.03233873471617699\n",
      "epoch: 14 step: 254, loss is 0.03166522830724716\n",
      "epoch: 14 step: 255, loss is 0.037124816328287125\n",
      "epoch: 14 step: 256, loss is 0.022218599915504456\n",
      "epoch: 14 step: 257, loss is 0.022184234112501144\n",
      "epoch: 14 step: 258, loss is 0.01312094647437334\n",
      "epoch: 14 step: 259, loss is 0.01665283553302288\n",
      "epoch: 14 step: 260, loss is 0.006175140850245953\n",
      "epoch: 14 step: 261, loss is 0.06255821138620377\n",
      "epoch: 14 step: 262, loss is 0.014631819911301136\n",
      "epoch: 14 step: 263, loss is 0.043625980615615845\n",
      "epoch: 14 step: 264, loss is 0.009532853960990906\n",
      "epoch: 14 step: 265, loss is 0.0013703564181923866\n",
      "epoch: 14 step: 266, loss is 0.022057408466935158\n",
      "epoch: 14 step: 267, loss is 0.015164322219789028\n",
      "epoch: 14 step: 268, loss is 0.006007912568747997\n",
      "epoch: 14 step: 269, loss is 0.04317677021026611\n",
      "epoch: 14 step: 270, loss is 0.07417663931846619\n",
      "epoch: 14 step: 271, loss is 0.027103103697299957\n",
      "epoch: 14 step: 272, loss is 0.03164844587445259\n",
      "epoch: 14 step: 273, loss is 0.03156373277306557\n",
      "epoch: 14 step: 274, loss is 0.028847411274909973\n",
      "epoch: 14 step: 275, loss is 0.010144843719899654\n",
      "epoch: 14 step: 276, loss is 0.02703739143908024\n",
      "epoch: 14 step: 277, loss is 0.010694260708987713\n",
      "epoch: 14 step: 278, loss is 0.10252533107995987\n",
      "epoch: 14 step: 279, loss is 0.003835200099274516\n",
      "epoch: 14 step: 280, loss is 0.043502457439899445\n",
      "epoch: 14 step: 281, loss is 0.0018602230120450258\n",
      "epoch: 14 step: 282, loss is 0.007600396405905485\n",
      "epoch: 14 step: 283, loss is 0.022068578749895096\n",
      "epoch: 14 step: 284, loss is 0.03491906821727753\n",
      "epoch: 14 step: 285, loss is 0.018129471689462662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 286, loss is 0.00538893835619092\n",
      "epoch: 14 step: 287, loss is 0.028000367805361748\n",
      "epoch: 14 step: 288, loss is 0.012818876653909683\n",
      "epoch: 14 step: 289, loss is 0.0032339333556592464\n",
      "epoch: 14 step: 290, loss is 0.022427672520279884\n",
      "epoch: 14 step: 291, loss is 0.0385720357298851\n",
      "epoch: 14 step: 292, loss is 0.002858729101717472\n",
      "epoch: 14 step: 293, loss is 0.0025080079212784767\n",
      "epoch: 14 step: 294, loss is 0.0015889318892732263\n",
      "epoch: 14 step: 295, loss is 0.005688648670911789\n",
      "epoch: 14 step: 296, loss is 0.007078083232045174\n",
      "epoch: 14 step: 297, loss is 0.017257876694202423\n",
      "epoch: 14 step: 298, loss is 0.08392208069562912\n",
      "epoch: 14 step: 299, loss is 0.0021505856420844793\n",
      "epoch: 14 step: 300, loss is 0.0031864852644503117\n",
      "epoch: 14 step: 301, loss is 0.0017796525498852134\n",
      "epoch: 14 step: 302, loss is 0.006177127826958895\n",
      "epoch: 14 step: 303, loss is 0.019294673576951027\n",
      "epoch: 14 step: 304, loss is 0.009563978761434555\n",
      "epoch: 14 step: 305, loss is 0.014870094135403633\n",
      "epoch: 14 step: 306, loss is 0.009556703269481659\n",
      "epoch: 14 step: 307, loss is 0.003853629343211651\n",
      "epoch: 14 step: 308, loss is 0.01410598587244749\n",
      "epoch: 14 step: 309, loss is 0.037110891193151474\n",
      "epoch: 14 step: 310, loss is 0.008053412660956383\n",
      "epoch: 14 step: 311, loss is 0.026915591210126877\n",
      "epoch: 14 step: 312, loss is 0.031017251312732697\n",
      "epoch: 14 step: 313, loss is 0.0015759922098368406\n",
      "epoch: 14 step: 314, loss is 0.010997751727700233\n",
      "epoch: 14 step: 315, loss is 0.02116001769900322\n",
      "epoch: 14 step: 316, loss is 0.038205552846193314\n",
      "epoch: 14 step: 317, loss is 0.010313790291547775\n",
      "epoch: 14 step: 318, loss is 0.009768669493496418\n",
      "epoch: 14 step: 319, loss is 0.08491764217615128\n",
      "epoch: 14 step: 320, loss is 0.014640814624726772\n",
      "epoch: 14 step: 321, loss is 0.0014183834427967668\n",
      "epoch: 14 step: 322, loss is 0.004275835584849119\n",
      "epoch: 14 step: 323, loss is 0.028496144339442253\n",
      "epoch: 14 step: 324, loss is 0.03842652961611748\n",
      "epoch: 14 step: 325, loss is 0.054171837866306305\n",
      "epoch: 14 step: 326, loss is 0.013617409393191338\n",
      "epoch: 14 step: 327, loss is 0.03285924345254898\n",
      "epoch: 14 step: 328, loss is 0.01661735214293003\n",
      "epoch: 14 step: 329, loss is 0.036998629570007324\n",
      "epoch: 14 step: 330, loss is 0.04030565917491913\n",
      "epoch: 14 step: 331, loss is 0.006435784045606852\n",
      "epoch: 14 step: 332, loss is 0.005792170763015747\n",
      "epoch: 14 step: 333, loss is 0.012412476353347301\n",
      "epoch: 14 step: 334, loss is 0.0012769371969625354\n",
      "epoch: 14 step: 335, loss is 0.004270085599273443\n",
      "epoch: 14 step: 336, loss is 0.0037261180114001036\n",
      "epoch: 14 step: 337, loss is 0.004646402318030596\n",
      "epoch: 14 step: 338, loss is 0.006064426153898239\n",
      "epoch: 14 step: 339, loss is 0.10086023807525635\n",
      "epoch: 14 step: 340, loss is 0.0055042109452188015\n",
      "epoch: 14 step: 341, loss is 0.04774941876530647\n",
      "epoch: 14 step: 342, loss is 0.012149190530180931\n",
      "epoch: 14 step: 343, loss is 0.02954702451825142\n",
      "epoch: 14 step: 344, loss is 0.00229244283400476\n",
      "epoch: 14 step: 345, loss is 0.007394730579108\n",
      "epoch: 14 step: 346, loss is 0.09146485477685928\n",
      "epoch: 14 step: 347, loss is 0.016872946172952652\n",
      "epoch: 14 step: 348, loss is 0.013429940678179264\n",
      "epoch: 14 step: 349, loss is 0.008906492032110691\n",
      "epoch: 14 step: 350, loss is 0.005869546439498663\n",
      "epoch: 14 step: 351, loss is 0.005701315123587847\n",
      "epoch: 14 step: 352, loss is 0.0021754757035523653\n",
      "epoch: 14 step: 353, loss is 0.023248201236128807\n",
      "epoch: 14 step: 354, loss is 0.028650403022766113\n",
      "epoch: 14 step: 355, loss is 0.0019037696765735745\n",
      "epoch: 14 step: 356, loss is 0.004564831964671612\n",
      "epoch: 14 step: 357, loss is 0.016364583745598793\n",
      "epoch: 14 step: 358, loss is 0.0049221087247133255\n",
      "epoch: 14 step: 359, loss is 0.01267043873667717\n",
      "epoch: 14 step: 360, loss is 0.0131040308624506\n",
      "epoch: 14 step: 361, loss is 0.07828047126531601\n",
      "epoch: 14 step: 362, loss is 0.007886188104748726\n",
      "epoch: 14 step: 363, loss is 0.10513144731521606\n",
      "epoch: 14 step: 364, loss is 0.012967896647751331\n",
      "epoch: 14 step: 365, loss is 0.0009230646537616849\n",
      "epoch: 14 step: 366, loss is 0.08563369512557983\n",
      "epoch: 14 step: 367, loss is 0.0066420165821909904\n",
      "epoch: 14 step: 368, loss is 0.033099569380283356\n",
      "epoch: 14 step: 369, loss is 0.003731413511559367\n",
      "epoch: 14 step: 370, loss is 0.003460891777649522\n",
      "epoch: 14 step: 371, loss is 0.059310272336006165\n",
      "epoch: 14 step: 372, loss is 0.02944253757596016\n",
      "epoch: 14 step: 373, loss is 0.07251585274934769\n",
      "epoch: 14 step: 374, loss is 0.01999739557504654\n",
      "epoch: 14 step: 375, loss is 0.006227191537618637\n",
      "epoch: 14 step: 376, loss is 0.042583007365465164\n",
      "epoch: 14 step: 377, loss is 0.01598019152879715\n",
      "epoch: 14 step: 378, loss is 0.023614613339304924\n",
      "epoch: 14 step: 379, loss is 0.023235976696014404\n",
      "epoch: 14 step: 380, loss is 0.019031768664717674\n",
      "epoch: 14 step: 381, loss is 0.04591594636440277\n",
      "epoch: 14 step: 382, loss is 0.08293916285037994\n",
      "epoch: 14 step: 383, loss is 0.013235628604888916\n",
      "epoch: 14 step: 384, loss is 0.08957850933074951\n",
      "epoch: 14 step: 385, loss is 0.05261634290218353\n",
      "epoch: 14 step: 386, loss is 0.010148337110877037\n",
      "epoch: 14 step: 387, loss is 0.058842409402132034\n",
      "epoch: 14 step: 388, loss is 0.006404269021004438\n",
      "epoch: 14 step: 389, loss is 0.011588584631681442\n",
      "epoch: 14 step: 390, loss is 0.004707391373813152\n",
      "epoch: 14 step: 391, loss is 0.11731460690498352\n",
      "epoch: 14 step: 392, loss is 0.062055639922618866\n",
      "epoch: 14 step: 393, loss is 0.09587337076663971\n",
      "epoch: 14 step: 394, loss is 0.02042737603187561\n",
      "epoch: 14 step: 395, loss is 0.020946701988577843\n",
      "epoch: 14 step: 396, loss is 0.008746061474084854\n",
      "epoch: 14 step: 397, loss is 0.0056345802731812\n",
      "epoch: 14 step: 398, loss is 0.028552180156111717\n",
      "epoch: 14 step: 399, loss is 0.06879277527332306\n",
      "epoch: 14 step: 400, loss is 0.017218906432390213\n",
      "epoch: 14 step: 401, loss is 0.09603637456893921\n",
      "epoch: 14 step: 402, loss is 0.00837612897157669\n",
      "epoch: 14 step: 403, loss is 0.07542824000120163\n",
      "epoch: 14 step: 404, loss is 0.060124583542346954\n",
      "epoch: 14 step: 405, loss is 0.03413690999150276\n",
      "epoch: 14 step: 406, loss is 0.013259206898510456\n",
      "epoch: 14 step: 407, loss is 0.09538508206605911\n",
      "epoch: 14 step: 408, loss is 0.11766335368156433\n",
      "epoch: 14 step: 409, loss is 0.015448792837560177\n",
      "epoch: 14 step: 410, loss is 0.09152907878160477\n",
      "epoch: 14 step: 411, loss is 0.0025094992015510798\n",
      "epoch: 14 step: 412, loss is 0.044761400669813156\n",
      "epoch: 14 step: 413, loss is 0.0077808331698179245\n",
      "epoch: 14 step: 414, loss is 0.007520750630646944\n",
      "epoch: 14 step: 415, loss is 0.021967466920614243\n",
      "epoch: 14 step: 416, loss is 0.0572848878800869\n",
      "epoch: 14 step: 417, loss is 0.05379930138587952\n",
      "epoch: 14 step: 418, loss is 0.0620897002518177\n",
      "epoch: 14 step: 419, loss is 0.002886898349970579\n",
      "epoch: 14 step: 420, loss is 0.0246823001652956\n",
      "epoch: 14 step: 421, loss is 0.08451829850673676\n",
      "epoch: 14 step: 422, loss is 0.04074250906705856\n",
      "epoch: 14 step: 423, loss is 0.01588001288473606\n",
      "epoch: 14 step: 424, loss is 0.07486114650964737\n",
      "epoch: 14 step: 425, loss is 0.03423214703798294\n",
      "epoch: 14 step: 426, loss is 0.02978472411632538\n",
      "epoch: 14 step: 427, loss is 0.04567809775471687\n",
      "epoch: 14 step: 428, loss is 0.09675227850675583\n",
      "epoch: 14 step: 429, loss is 0.13130098581314087\n",
      "epoch: 14 step: 430, loss is 0.036257755011320114\n",
      "epoch: 14 step: 431, loss is 0.037833549082279205\n",
      "epoch: 14 step: 432, loss is 0.001027816440910101\n",
      "epoch: 14 step: 433, loss is 0.017586683854460716\n",
      "epoch: 14 step: 434, loss is 0.05104871466755867\n",
      "epoch: 14 step: 435, loss is 0.053209301084280014\n",
      "epoch: 14 step: 436, loss is 0.015595749951899052\n",
      "epoch: 14 step: 437, loss is 0.0019547934643924236\n",
      "epoch: 14 step: 438, loss is 0.01261449046432972\n",
      "epoch: 14 step: 439, loss is 0.001241780468262732\n",
      "epoch: 14 step: 440, loss is 0.028312865644693375\n",
      "epoch: 14 step: 441, loss is 0.01862899214029312\n",
      "epoch: 14 step: 442, loss is 0.04284733906388283\n",
      "epoch: 14 step: 443, loss is 0.014573716558516026\n",
      "epoch: 14 step: 444, loss is 0.016039784997701645\n",
      "epoch: 14 step: 445, loss is 0.002750474726781249\n",
      "epoch: 14 step: 446, loss is 0.0169609934091568\n",
      "epoch: 14 step: 447, loss is 0.006213797722011805\n",
      "epoch: 14 step: 448, loss is 0.002370252273976803\n",
      "epoch: 14 step: 449, loss is 0.01047892589122057\n",
      "epoch: 14 step: 450, loss is 0.011053362861275673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 451, loss is 0.03870950639247894\n",
      "epoch: 14 step: 452, loss is 0.005493958480656147\n",
      "epoch: 14 step: 453, loss is 0.0665895938873291\n",
      "epoch: 14 step: 454, loss is 0.07765838503837585\n",
      "epoch: 14 step: 455, loss is 0.02848861552774906\n",
      "epoch: 14 step: 456, loss is 0.033820804208517075\n",
      "epoch: 14 step: 457, loss is 0.0035569239407777786\n",
      "epoch: 14 step: 458, loss is 0.042562924325466156\n",
      "epoch: 14 step: 459, loss is 0.006813351530581713\n",
      "epoch: 14 step: 460, loss is 0.00043512872071005404\n",
      "epoch: 14 step: 461, loss is 0.01269383542239666\n",
      "epoch: 14 step: 462, loss is 0.005182673688977957\n",
      "epoch: 14 step: 463, loss is 0.009790305979549885\n",
      "epoch: 14 step: 464, loss is 0.00979562010616064\n",
      "epoch: 14 step: 465, loss is 0.013277971185743809\n",
      "epoch: 14 step: 466, loss is 0.02904827520251274\n",
      "epoch: 14 step: 467, loss is 0.00532484520226717\n",
      "epoch: 14 step: 468, loss is 0.019405009225010872\n",
      "epoch: 14 step: 469, loss is 0.03248388692736626\n",
      "epoch: 14 step: 470, loss is 0.01799936592578888\n",
      "epoch: 14 step: 471, loss is 0.00418282113969326\n",
      "epoch: 14 step: 472, loss is 0.03126298263669014\n",
      "epoch: 14 step: 473, loss is 0.07316994667053223\n",
      "epoch: 14 step: 474, loss is 0.13149139285087585\n",
      "epoch: 14 step: 475, loss is 0.02226800099015236\n",
      "epoch: 14 step: 476, loss is 0.016053158789873123\n",
      "epoch: 14 step: 477, loss is 0.046594664454460144\n",
      "epoch: 14 step: 478, loss is 0.007084797136485577\n",
      "epoch: 14 step: 479, loss is 0.046624936163425446\n",
      "epoch: 14 step: 480, loss is 0.021076688542962074\n",
      "epoch: 14 step: 481, loss is 0.05443204566836357\n",
      "epoch: 14 step: 482, loss is 0.053053632378578186\n",
      "epoch: 14 step: 483, loss is 0.0480920746922493\n",
      "epoch: 14 step: 484, loss is 0.06255261600017548\n",
      "epoch: 14 step: 485, loss is 0.0034405800979584455\n",
      "epoch: 14 step: 486, loss is 0.04073989391326904\n",
      "epoch: 14 step: 487, loss is 0.06891658157110214\n",
      "epoch: 14 step: 488, loss is 0.03020481765270233\n",
      "epoch: 14 step: 489, loss is 0.0213306937366724\n",
      "epoch: 14 step: 490, loss is 0.002054298995062709\n",
      "epoch: 14 step: 491, loss is 0.06559237837791443\n",
      "epoch: 14 step: 492, loss is 0.008858627639710903\n",
      "epoch: 14 step: 493, loss is 0.05653946101665497\n",
      "epoch: 14 step: 494, loss is 0.0047262683510780334\n",
      "epoch: 14 step: 495, loss is 0.06541997194290161\n",
      "epoch: 14 step: 496, loss is 0.011395357549190521\n",
      "epoch: 14 step: 497, loss is 0.04651770740747452\n",
      "epoch: 14 step: 498, loss is 0.010032887570559978\n",
      "epoch: 14 step: 499, loss is 0.0014533286448568106\n",
      "epoch: 14 step: 500, loss is 0.05692770332098007\n",
      "epoch: 14 step: 501, loss is 0.012512077577412128\n",
      "epoch: 14 step: 502, loss is 0.12445364892482758\n",
      "epoch: 14 step: 503, loss is 0.035062287002801895\n",
      "epoch: 14 step: 504, loss is 0.003479349659755826\n",
      "epoch: 14 step: 505, loss is 0.0041298698633909225\n",
      "epoch: 14 step: 506, loss is 0.010049793869256973\n",
      "epoch: 14 step: 507, loss is 0.016289662569761276\n",
      "epoch: 14 step: 508, loss is 0.05498507618904114\n",
      "epoch: 14 step: 509, loss is 0.0025861039757728577\n",
      "epoch: 14 step: 510, loss is 0.0238459762185812\n",
      "epoch: 14 step: 511, loss is 0.15701398253440857\n",
      "epoch: 14 step: 512, loss is 0.19610339403152466\n",
      "epoch: 14 step: 513, loss is 0.07036852836608887\n",
      "epoch: 14 step: 514, loss is 0.0231949295848608\n",
      "epoch: 14 step: 515, loss is 0.004804563242942095\n",
      "epoch: 14 step: 516, loss is 0.004336423706263304\n",
      "epoch: 14 step: 517, loss is 0.007807343266904354\n",
      "epoch: 14 step: 518, loss is 0.022886209189891815\n",
      "epoch: 14 step: 519, loss is 0.025881148874759674\n",
      "epoch: 14 step: 520, loss is 0.04292760044336319\n",
      "epoch: 14 step: 521, loss is 0.014296707697212696\n",
      "epoch: 14 step: 522, loss is 0.01045124139636755\n",
      "epoch: 14 step: 523, loss is 0.0803803950548172\n",
      "epoch: 14 step: 524, loss is 0.05883472412824631\n",
      "epoch: 14 step: 525, loss is 0.08377541601657867\n",
      "epoch: 14 step: 526, loss is 0.11966056376695633\n",
      "epoch: 14 step: 527, loss is 0.009962193667888641\n",
      "epoch: 14 step: 528, loss is 0.006926721427589655\n",
      "epoch: 14 step: 529, loss is 0.03292888402938843\n",
      "epoch: 14 step: 530, loss is 0.04072653874754906\n",
      "epoch: 14 step: 531, loss is 0.05394785478711128\n",
      "epoch: 14 step: 532, loss is 0.053769566118717194\n",
      "epoch: 14 step: 533, loss is 0.05425894260406494\n",
      "epoch: 14 step: 534, loss is 0.04342924803495407\n",
      "epoch: 14 step: 535, loss is 0.0018167232628911734\n",
      "epoch: 14 step: 536, loss is 0.02348541095852852\n",
      "epoch: 14 step: 537, loss is 0.027989208698272705\n",
      "epoch: 14 step: 538, loss is 0.05211750417947769\n",
      "epoch: 14 step: 539, loss is 0.016051048412919044\n",
      "epoch: 14 step: 540, loss is 0.00252429093234241\n",
      "epoch: 14 step: 541, loss is 0.00880671851336956\n",
      "epoch: 14 step: 542, loss is 0.0004934921162202954\n",
      "epoch: 14 step: 543, loss is 0.0012347435113042593\n",
      "epoch: 14 step: 544, loss is 0.016946593299508095\n",
      "epoch: 14 step: 545, loss is 0.01653999276459217\n",
      "epoch: 14 step: 546, loss is 0.014229081571102142\n",
      "epoch: 14 step: 547, loss is 0.13267314434051514\n",
      "epoch: 14 step: 548, loss is 0.017668690532445908\n",
      "epoch: 14 step: 549, loss is 0.03573112189769745\n",
      "epoch: 14 step: 550, loss is 0.11143792420625687\n",
      "epoch: 14 step: 551, loss is 0.0049898335710167885\n",
      "epoch: 14 step: 552, loss is 0.01629074290394783\n",
      "epoch: 14 step: 553, loss is 0.02223903499543667\n",
      "epoch: 14 step: 554, loss is 0.0208167415112257\n",
      "epoch: 14 step: 555, loss is 0.03900584951043129\n",
      "epoch: 14 step: 556, loss is 0.053000565618276596\n",
      "epoch: 14 step: 557, loss is 0.006126622669398785\n",
      "epoch: 14 step: 558, loss is 0.022424718365073204\n",
      "epoch: 14 step: 559, loss is 0.0014408500865101814\n",
      "epoch: 14 step: 560, loss is 0.026959912851452827\n",
      "epoch: 14 step: 561, loss is 0.033388856798410416\n",
      "epoch: 14 step: 562, loss is 0.013499297201633453\n",
      "epoch: 14 step: 563, loss is 0.03644707426428795\n",
      "epoch: 14 step: 564, loss is 0.10477177053689957\n",
      "epoch: 14 step: 565, loss is 0.09357922524213791\n",
      "epoch: 14 step: 566, loss is 0.04185763746500015\n",
      "epoch: 14 step: 567, loss is 0.023516105487942696\n",
      "epoch: 14 step: 568, loss is 0.00459761219099164\n",
      "epoch: 14 step: 569, loss is 0.0349125862121582\n",
      "epoch: 14 step: 570, loss is 0.007245536427944899\n",
      "epoch: 14 step: 571, loss is 0.0334756039083004\n",
      "epoch: 14 step: 572, loss is 0.006097486242651939\n",
      "epoch: 14 step: 573, loss is 0.025622578337788582\n",
      "epoch: 14 step: 574, loss is 0.04950960725545883\n",
      "epoch: 14 step: 575, loss is 0.1257820427417755\n",
      "epoch: 14 step: 576, loss is 0.11724286526441574\n",
      "epoch: 14 step: 577, loss is 0.12466241419315338\n",
      "epoch: 14 step: 578, loss is 0.05082979053258896\n",
      "epoch: 14 step: 579, loss is 0.13121941685676575\n",
      "epoch: 14 step: 580, loss is 0.06564314663410187\n",
      "epoch: 14 step: 581, loss is 0.09636373072862625\n",
      "epoch: 14 step: 582, loss is 0.07329313457012177\n",
      "epoch: 14 step: 583, loss is 0.007927819155156612\n",
      "epoch: 14 step: 584, loss is 0.055724818259477615\n",
      "epoch: 14 step: 585, loss is 0.007521104998886585\n",
      "epoch: 14 step: 586, loss is 0.01764252409338951\n",
      "epoch: 14 step: 587, loss is 0.06982417404651642\n",
      "epoch: 14 step: 588, loss is 0.018629787489771843\n",
      "epoch: 14 step: 589, loss is 0.01522101555019617\n",
      "epoch: 14 step: 590, loss is 0.07882919907569885\n",
      "epoch: 14 step: 591, loss is 0.10171565413475037\n",
      "epoch: 14 step: 592, loss is 0.027734331786632538\n",
      "epoch: 14 step: 593, loss is 0.00984590221196413\n",
      "epoch: 14 step: 594, loss is 0.06640614569187164\n",
      "epoch: 14 step: 595, loss is 0.05716884136199951\n",
      "epoch: 14 step: 596, loss is 0.04412047937512398\n",
      "epoch: 14 step: 597, loss is 0.04676520824432373\n",
      "epoch: 14 step: 598, loss is 0.08048366010189056\n",
      "epoch: 14 step: 599, loss is 0.0073194983415305614\n",
      "epoch: 14 step: 600, loss is 0.0035626571625471115\n",
      "epoch: 14 step: 601, loss is 0.04292292147874832\n",
      "epoch: 14 step: 602, loss is 0.06019800901412964\n",
      "epoch: 14 step: 603, loss is 0.07483258098363876\n",
      "epoch: 14 step: 604, loss is 0.040683384984731674\n",
      "epoch: 14 step: 605, loss is 0.019098998978734016\n",
      "epoch: 14 step: 606, loss is 0.04067918285727501\n",
      "epoch: 14 step: 607, loss is 0.025612618774175644\n",
      "epoch: 14 step: 608, loss is 0.007052083499729633\n",
      "epoch: 14 step: 609, loss is 0.08131568133831024\n",
      "epoch: 14 step: 610, loss is 0.0205900389701128\n",
      "epoch: 14 step: 611, loss is 0.012048684991896152\n",
      "epoch: 14 step: 612, loss is 0.11797656863927841\n",
      "epoch: 14 step: 613, loss is 0.02539183385670185\n",
      "epoch: 14 step: 614, loss is 0.003997286316007376\n",
      "epoch: 14 step: 615, loss is 0.016001250594854355\n",
      "epoch: 14 step: 616, loss is 0.08222652226686478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 617, loss is 0.015124128200113773\n",
      "epoch: 14 step: 618, loss is 0.004589704796671867\n",
      "epoch: 14 step: 619, loss is 0.005874514579772949\n",
      "epoch: 14 step: 620, loss is 0.006970492657274008\n",
      "epoch: 14 step: 621, loss is 0.013799102045595646\n",
      "epoch: 14 step: 622, loss is 0.06616538763046265\n",
      "epoch: 14 step: 623, loss is 0.024540433660149574\n",
      "epoch: 14 step: 624, loss is 0.011108716949820518\n",
      "epoch: 14 step: 625, loss is 0.01560280378907919\n",
      "epoch: 14 step: 626, loss is 0.007295362651348114\n",
      "epoch: 14 step: 627, loss is 0.009681257419288158\n",
      "epoch: 14 step: 628, loss is 0.05207933485507965\n",
      "epoch: 14 step: 629, loss is 0.014346487820148468\n",
      "epoch: 14 step: 630, loss is 0.031605497002601624\n",
      "epoch: 14 step: 631, loss is 0.056105393916368484\n",
      "epoch: 14 step: 632, loss is 0.05316672474145889\n",
      "epoch: 14 step: 633, loss is 0.006694462615996599\n",
      "epoch: 14 step: 634, loss is 0.02262141741812229\n",
      "epoch: 14 step: 635, loss is 0.02161780185997486\n",
      "epoch: 14 step: 636, loss is 0.004444461315870285\n",
      "epoch: 14 step: 637, loss is 0.0017182359006255865\n",
      "epoch: 14 step: 638, loss is 0.0010734971147030592\n",
      "epoch: 14 step: 639, loss is 0.00575055368244648\n",
      "epoch: 14 step: 640, loss is 0.024483205750584602\n",
      "epoch: 14 step: 641, loss is 0.02230653539299965\n",
      "epoch: 14 step: 642, loss is 0.044064465910196304\n",
      "epoch: 14 step: 643, loss is 0.005260942503809929\n",
      "epoch: 14 step: 644, loss is 0.002026166534051299\n",
      "epoch: 14 step: 645, loss is 0.0033140908926725388\n",
      "epoch: 14 step: 646, loss is 0.07478068768978119\n",
      "epoch: 14 step: 647, loss is 0.08489768952131271\n",
      "epoch: 14 step: 648, loss is 0.039028920233249664\n",
      "epoch: 14 step: 649, loss is 0.09170334786176682\n",
      "epoch: 14 step: 650, loss is 0.02179054357111454\n",
      "epoch: 14 step: 651, loss is 0.012351988814771175\n",
      "epoch: 14 step: 652, loss is 0.05268413946032524\n",
      "epoch: 14 step: 653, loss is 0.03474300354719162\n",
      "epoch: 14 step: 654, loss is 0.006017143372446299\n",
      "epoch: 14 step: 655, loss is 0.0061371298506855965\n",
      "epoch: 14 step: 656, loss is 0.08081740140914917\n",
      "epoch: 14 step: 657, loss is 0.04915668070316315\n",
      "epoch: 14 step: 658, loss is 0.038240522146224976\n",
      "epoch: 14 step: 659, loss is 0.041198696941137314\n",
      "epoch: 14 step: 660, loss is 0.002034431556239724\n",
      "epoch: 14 step: 661, loss is 0.09719022363424301\n",
      "epoch: 14 step: 662, loss is 0.12310001254081726\n",
      "epoch: 14 step: 663, loss is 0.004534951411187649\n",
      "epoch: 14 step: 664, loss is 0.021083060652017593\n",
      "epoch: 14 step: 665, loss is 0.0023429214488714933\n",
      "epoch: 14 step: 666, loss is 0.031721748411655426\n",
      "epoch: 14 step: 667, loss is 0.007416817359626293\n",
      "epoch: 14 step: 668, loss is 0.006500616203993559\n",
      "epoch: 14 step: 669, loss is 0.04633605480194092\n",
      "epoch: 14 step: 670, loss is 0.014462646096944809\n",
      "epoch: 14 step: 671, loss is 0.0028549032285809517\n",
      "epoch: 14 step: 672, loss is 0.028006237000226974\n",
      "epoch: 14 step: 673, loss is 0.06798294186592102\n",
      "epoch: 14 step: 674, loss is 0.07435044646263123\n",
      "epoch: 14 step: 675, loss is 0.05882967263460159\n",
      "epoch: 14 step: 676, loss is 0.06973746418952942\n",
      "epoch: 14 step: 677, loss is 0.04091690480709076\n",
      "epoch: 14 step: 678, loss is 0.0013689385959878564\n",
      "epoch: 14 step: 679, loss is 0.03143720701336861\n",
      "epoch: 14 step: 680, loss is 0.029087165370583534\n",
      "epoch: 14 step: 681, loss is 0.054666440933942795\n",
      "epoch: 14 step: 682, loss is 0.012259908020496368\n",
      "epoch: 14 step: 683, loss is 0.040265023708343506\n",
      "epoch: 14 step: 684, loss is 0.03408687561750412\n",
      "epoch: 14 step: 685, loss is 0.014754421077668667\n",
      "epoch: 14 step: 686, loss is 0.025683719664812088\n",
      "epoch: 14 step: 687, loss is 0.0010347866918891668\n",
      "epoch: 14 step: 688, loss is 0.1497533619403839\n",
      "epoch: 14 step: 689, loss is 0.09084483981132507\n",
      "epoch: 14 step: 690, loss is 0.015354670584201813\n",
      "epoch: 14 step: 691, loss is 0.050759539008140564\n",
      "epoch: 14 step: 692, loss is 0.019973216578364372\n",
      "epoch: 14 step: 693, loss is 0.02422638051211834\n",
      "epoch: 14 step: 694, loss is 0.06285256147384644\n",
      "epoch: 14 step: 695, loss is 0.07263088971376419\n",
      "epoch: 14 step: 696, loss is 0.025434069335460663\n",
      "epoch: 14 step: 697, loss is 0.02008419670164585\n",
      "epoch: 14 step: 698, loss is 0.03794785588979721\n",
      "epoch: 14 step: 699, loss is 0.012697036378085613\n",
      "epoch: 14 step: 700, loss is 0.07340868562459946\n",
      "epoch: 14 step: 701, loss is 0.04356255382299423\n",
      "epoch: 14 step: 702, loss is 0.1107301265001297\n",
      "epoch: 14 step: 703, loss is 0.06738860160112381\n",
      "epoch: 14 step: 704, loss is 0.02588794194161892\n",
      "epoch: 14 step: 705, loss is 0.03316286951303482\n",
      "epoch: 14 step: 706, loss is 0.11167892068624496\n",
      "epoch: 14 step: 707, loss is 0.04584703594446182\n",
      "epoch: 14 step: 708, loss is 0.07341737300157547\n",
      "epoch: 14 step: 709, loss is 0.010391725227236748\n",
      "epoch: 14 step: 710, loss is 0.008513608947396278\n",
      "epoch: 14 step: 711, loss is 0.04688379541039467\n",
      "epoch: 14 step: 712, loss is 0.08144986629486084\n",
      "epoch: 14 step: 713, loss is 0.0642407163977623\n",
      "epoch: 14 step: 714, loss is 0.05104804039001465\n",
      "epoch: 14 step: 715, loss is 0.09903358668088913\n",
      "epoch: 14 step: 716, loss is 0.028406912460923195\n",
      "epoch: 14 step: 717, loss is 0.0669429749250412\n",
      "epoch: 14 step: 718, loss is 0.12352536618709564\n",
      "epoch: 14 step: 719, loss is 0.02479439787566662\n",
      "epoch: 14 step: 720, loss is 0.10700720548629761\n",
      "epoch: 14 step: 721, loss is 0.07397677004337311\n",
      "epoch: 14 step: 722, loss is 0.016890371218323708\n",
      "epoch: 14 step: 723, loss is 0.005594504531472921\n",
      "epoch: 14 step: 724, loss is 0.018406910821795464\n",
      "epoch: 14 step: 725, loss is 0.03220338374376297\n",
      "epoch: 14 step: 726, loss is 0.056723255664110184\n",
      "epoch: 14 step: 727, loss is 0.04605570062994957\n",
      "epoch: 14 step: 728, loss is 0.019696827977895737\n",
      "epoch: 14 step: 729, loss is 0.051734063774347305\n",
      "epoch: 14 step: 730, loss is 0.011656141839921474\n",
      "epoch: 14 step: 731, loss is 0.05793341249227524\n",
      "epoch: 14 step: 732, loss is 0.010615319944918156\n",
      "epoch: 14 step: 733, loss is 0.010848646983504295\n",
      "epoch: 14 step: 734, loss is 0.03466403856873512\n",
      "epoch: 14 step: 735, loss is 0.012188003398478031\n",
      "epoch: 14 step: 736, loss is 0.025548504665493965\n",
      "epoch: 14 step: 737, loss is 0.05438769981265068\n",
      "epoch: 14 step: 738, loss is 0.022870775312185287\n",
      "epoch: 14 step: 739, loss is 0.16314344108104706\n",
      "epoch: 14 step: 740, loss is 0.028143122792243958\n",
      "epoch: 14 step: 741, loss is 0.10235442221164703\n",
      "epoch: 14 step: 742, loss is 0.0008501045522280037\n",
      "epoch: 14 step: 743, loss is 0.16837839782238007\n",
      "epoch: 14 step: 744, loss is 0.01660621352493763\n",
      "epoch: 14 step: 745, loss is 0.03361788019537926\n",
      "epoch: 14 step: 746, loss is 0.033669985830783844\n",
      "epoch: 14 step: 747, loss is 0.060497138649225235\n",
      "epoch: 14 step: 748, loss is 0.18100813031196594\n",
      "epoch: 14 step: 749, loss is 0.003347995225340128\n",
      "epoch: 14 step: 750, loss is 0.008513563312590122\n",
      "epoch: 14 step: 751, loss is 0.0545651949942112\n",
      "epoch: 14 step: 752, loss is 0.018551820889115334\n",
      "epoch: 14 step: 753, loss is 0.0019243541173636913\n",
      "epoch: 14 step: 754, loss is 0.03147558122873306\n",
      "epoch: 14 step: 755, loss is 0.006530194077640772\n",
      "epoch: 14 step: 756, loss is 0.02355414256453514\n",
      "epoch: 14 step: 757, loss is 0.07487834990024567\n",
      "epoch: 14 step: 758, loss is 0.05431189760565758\n",
      "epoch: 14 step: 759, loss is 0.007847246713936329\n",
      "epoch: 14 step: 760, loss is 0.07445273548364639\n",
      "epoch: 14 step: 761, loss is 0.044884830713272095\n",
      "epoch: 14 step: 762, loss is 0.0056100101210176945\n",
      "epoch: 14 step: 763, loss is 0.0044784219935536385\n",
      "epoch: 14 step: 764, loss is 0.015677466988563538\n",
      "epoch: 14 step: 765, loss is 0.01734068989753723\n",
      "epoch: 14 step: 766, loss is 0.010841552168130875\n",
      "epoch: 14 step: 767, loss is 0.023112354800105095\n",
      "epoch: 14 step: 768, loss is 0.03845256194472313\n",
      "epoch: 14 step: 769, loss is 0.02385263331234455\n",
      "epoch: 14 step: 770, loss is 0.03606759011745453\n",
      "epoch: 14 step: 771, loss is 0.021332036703824997\n",
      "epoch: 14 step: 772, loss is 0.02469736710190773\n",
      "epoch: 14 step: 773, loss is 0.05948273092508316\n",
      "epoch: 14 step: 774, loss is 0.025662686675786972\n",
      "epoch: 14 step: 775, loss is 0.08568630367517471\n",
      "epoch: 14 step: 776, loss is 0.023318950086832047\n",
      "epoch: 14 step: 777, loss is 0.007859925739467144\n",
      "epoch: 14 step: 778, loss is 0.00866058748215437\n",
      "epoch: 14 step: 779, loss is 0.047722503542900085\n",
      "epoch: 14 step: 780, loss is 0.01939753256738186\n",
      "epoch: 14 step: 781, loss is 0.05653487890958786\n",
      "epoch: 14 step: 782, loss is 0.010699652135372162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 783, loss is 0.08927477151155472\n",
      "epoch: 14 step: 784, loss is 0.011702480725944042\n",
      "epoch: 14 step: 785, loss is 0.06581911444664001\n",
      "epoch: 14 step: 786, loss is 0.05426060035824776\n",
      "epoch: 14 step: 787, loss is 0.07039283215999603\n",
      "epoch: 14 step: 788, loss is 0.026366721838712692\n",
      "epoch: 14 step: 789, loss is 0.10527186840772629\n",
      "epoch: 14 step: 790, loss is 0.0636928603053093\n",
      "epoch: 14 step: 791, loss is 0.0140761099755764\n",
      "epoch: 14 step: 792, loss is 0.01035219244658947\n",
      "epoch: 14 step: 793, loss is 0.11228486895561218\n",
      "epoch: 14 step: 794, loss is 0.032548729330301285\n",
      "epoch: 14 step: 795, loss is 0.026370109990239143\n",
      "epoch: 14 step: 796, loss is 0.03356197476387024\n",
      "epoch: 14 step: 797, loss is 0.021324703469872475\n",
      "epoch: 14 step: 798, loss is 0.02939523570239544\n",
      "epoch: 14 step: 799, loss is 0.020569151267409325\n",
      "epoch: 14 step: 800, loss is 0.016849184408783913\n",
      "epoch: 14 step: 801, loss is 0.07489148527383804\n",
      "epoch: 14 step: 802, loss is 0.011922626756131649\n",
      "epoch: 14 step: 803, loss is 0.01367359422147274\n",
      "epoch: 14 step: 804, loss is 0.02181178145110607\n",
      "epoch: 14 step: 805, loss is 0.011833815835416317\n",
      "epoch: 14 step: 806, loss is 0.08903763443231583\n",
      "epoch: 14 step: 807, loss is 0.06291405111551285\n",
      "epoch: 14 step: 808, loss is 0.05285593122243881\n",
      "epoch: 14 step: 809, loss is 0.012133612297475338\n",
      "epoch: 14 step: 810, loss is 0.019329551607370377\n",
      "epoch: 14 step: 811, loss is 0.15025044977664948\n",
      "epoch: 14 step: 812, loss is 0.018029989674687386\n",
      "epoch: 14 step: 813, loss is 0.04686941206455231\n",
      "epoch: 14 step: 814, loss is 0.104744553565979\n",
      "epoch: 14 step: 815, loss is 0.09156735241413116\n",
      "epoch: 14 step: 816, loss is 0.006952638737857342\n",
      "epoch: 14 step: 817, loss is 0.008565632626414299\n",
      "epoch: 14 step: 818, loss is 0.009095053188502789\n",
      "epoch: 14 step: 819, loss is 0.06125309318304062\n",
      "epoch: 14 step: 820, loss is 0.02310497872531414\n",
      "epoch: 14 step: 821, loss is 0.011551016010344028\n",
      "epoch: 14 step: 822, loss is 0.02001282013952732\n",
      "epoch: 14 step: 823, loss is 0.006084522232413292\n",
      "epoch: 14 step: 824, loss is 0.1586935967206955\n",
      "epoch: 14 step: 825, loss is 0.04158991202712059\n",
      "epoch: 14 step: 826, loss is 0.009227722883224487\n",
      "epoch: 14 step: 827, loss is 0.0006575040752068162\n",
      "epoch: 14 step: 828, loss is 0.008801378309726715\n",
      "epoch: 14 step: 829, loss is 0.013971827924251556\n",
      "epoch: 14 step: 830, loss is 0.007647107820957899\n",
      "epoch: 14 step: 831, loss is 0.08304855972528458\n",
      "epoch: 14 step: 832, loss is 0.019006215035915375\n",
      "epoch: 14 step: 833, loss is 0.010816734284162521\n",
      "epoch: 14 step: 834, loss is 0.026451362296938896\n",
      "epoch: 14 step: 835, loss is 0.013161376118659973\n",
      "epoch: 14 step: 836, loss is 0.01279451884329319\n",
      "epoch: 14 step: 837, loss is 0.03237607702612877\n",
      "epoch: 14 step: 838, loss is 0.010767223313450813\n",
      "epoch: 14 step: 839, loss is 0.008375562727451324\n",
      "epoch: 14 step: 840, loss is 0.050542861223220825\n",
      "epoch: 14 step: 841, loss is 0.10874833166599274\n",
      "epoch: 14 step: 842, loss is 0.0038926296401768923\n",
      "epoch: 14 step: 843, loss is 0.01852114498615265\n",
      "epoch: 14 step: 844, loss is 0.013704041950404644\n",
      "epoch: 14 step: 845, loss is 0.022527864202857018\n",
      "epoch: 14 step: 846, loss is 0.20389752089977264\n",
      "epoch: 14 step: 847, loss is 0.07369772344827652\n",
      "epoch: 14 step: 848, loss is 0.017269589006900787\n",
      "epoch: 14 step: 849, loss is 0.04028277471661568\n",
      "epoch: 14 step: 850, loss is 0.02602430246770382\n",
      "epoch: 14 step: 851, loss is 0.013810791075229645\n",
      "epoch: 14 step: 852, loss is 0.030064096674323082\n",
      "epoch: 14 step: 853, loss is 0.05973370373249054\n",
      "epoch: 14 step: 854, loss is 0.006951681338250637\n",
      "epoch: 14 step: 855, loss is 0.035722292959690094\n",
      "epoch: 14 step: 856, loss is 0.02147550694644451\n",
      "epoch: 14 step: 857, loss is 0.10886771231889725\n",
      "epoch: 14 step: 858, loss is 0.03860265389084816\n",
      "epoch: 14 step: 859, loss is 0.004649918992072344\n",
      "epoch: 14 step: 860, loss is 0.031178124248981476\n",
      "epoch: 14 step: 861, loss is 0.04944407939910889\n",
      "epoch: 14 step: 862, loss is 0.054846905171871185\n",
      "epoch: 14 step: 863, loss is 0.015175295062363148\n",
      "epoch: 14 step: 864, loss is 0.06417134404182434\n",
      "epoch: 14 step: 865, loss is 0.02560088038444519\n",
      "epoch: 14 step: 866, loss is 0.0025859419256448746\n",
      "epoch: 14 step: 867, loss is 0.004904038738459349\n",
      "epoch: 14 step: 868, loss is 0.0019455525325611234\n",
      "epoch: 14 step: 869, loss is 0.008936768397688866\n",
      "epoch: 14 step: 870, loss is 0.03561285883188248\n",
      "epoch: 14 step: 871, loss is 0.005776422098278999\n",
      "epoch: 14 step: 872, loss is 0.05439608171582222\n",
      "epoch: 14 step: 873, loss is 0.02855292521417141\n",
      "epoch: 14 step: 874, loss is 0.011514128185808659\n",
      "epoch: 14 step: 875, loss is 0.07965821027755737\n",
      "epoch: 14 step: 876, loss is 0.10178171843290329\n",
      "epoch: 14 step: 877, loss is 0.035919055342674255\n",
      "epoch: 14 step: 878, loss is 0.022712072357535362\n",
      "epoch: 14 step: 879, loss is 0.01302342675626278\n",
      "epoch: 14 step: 880, loss is 0.006634393706917763\n",
      "epoch: 14 step: 881, loss is 0.014106394723057747\n",
      "epoch: 14 step: 882, loss is 0.010397861711680889\n",
      "epoch: 14 step: 883, loss is 0.008136652410030365\n",
      "epoch: 14 step: 884, loss is 0.004022741224616766\n",
      "epoch: 14 step: 885, loss is 0.062222957611083984\n",
      "epoch: 14 step: 886, loss is 0.0022979136556386948\n",
      "epoch: 14 step: 887, loss is 0.006788124796003103\n",
      "epoch: 14 step: 888, loss is 0.14294208586215973\n",
      "epoch: 14 step: 889, loss is 0.12412942945957184\n",
      "epoch: 14 step: 890, loss is 0.02475847117602825\n",
      "epoch: 14 step: 891, loss is 0.013943711295723915\n",
      "epoch: 14 step: 892, loss is 0.020837191492319107\n",
      "epoch: 14 step: 893, loss is 0.010312271304428577\n",
      "epoch: 14 step: 894, loss is 0.057224541902542114\n",
      "epoch: 14 step: 895, loss is 0.02990710362792015\n",
      "epoch: 14 step: 896, loss is 0.024866627529263496\n",
      "epoch: 14 step: 897, loss is 0.038510553538799286\n",
      "epoch: 14 step: 898, loss is 0.16074326634407043\n",
      "epoch: 14 step: 899, loss is 0.0023444152902811766\n",
      "epoch: 14 step: 900, loss is 0.007126650772988796\n",
      "epoch: 14 step: 901, loss is 0.020256318151950836\n",
      "epoch: 14 step: 902, loss is 0.033032458275556564\n",
      "epoch: 14 step: 903, loss is 0.004020661115646362\n",
      "epoch: 14 step: 904, loss is 0.002441096818074584\n",
      "epoch: 14 step: 905, loss is 0.009734580293297768\n",
      "epoch: 14 step: 906, loss is 0.01740388199687004\n",
      "epoch: 14 step: 907, loss is 0.014386596158146858\n",
      "epoch: 14 step: 908, loss is 0.06777969747781754\n",
      "epoch: 14 step: 909, loss is 0.03553028032183647\n",
      "epoch: 14 step: 910, loss is 0.025710145011544228\n",
      "epoch: 14 step: 911, loss is 0.023822875693440437\n",
      "epoch: 14 step: 912, loss is 0.011005319654941559\n",
      "epoch: 14 step: 913, loss is 0.04490795359015465\n",
      "epoch: 14 step: 914, loss is 0.12290577590465546\n",
      "epoch: 14 step: 915, loss is 0.019361071288585663\n",
      "epoch: 14 step: 916, loss is 0.012541496194899082\n",
      "epoch: 14 step: 917, loss is 0.005823895335197449\n",
      "epoch: 14 step: 918, loss is 0.00504974415525794\n",
      "epoch: 14 step: 919, loss is 0.011391074396669865\n",
      "epoch: 14 step: 920, loss is 0.02765943855047226\n",
      "epoch: 14 step: 921, loss is 0.06515346467494965\n",
      "epoch: 14 step: 922, loss is 0.004049929790198803\n",
      "epoch: 14 step: 923, loss is 0.0025464706122875214\n",
      "epoch: 14 step: 924, loss is 0.05007106065750122\n",
      "epoch: 14 step: 925, loss is 0.10504782199859619\n",
      "epoch: 14 step: 926, loss is 0.012824729084968567\n",
      "epoch: 14 step: 927, loss is 0.015567583963274956\n",
      "epoch: 14 step: 928, loss is 0.020466947928071022\n",
      "epoch: 14 step: 929, loss is 0.06228061392903328\n",
      "epoch: 14 step: 930, loss is 0.09439484775066376\n",
      "epoch: 14 step: 931, loss is 0.026877522468566895\n",
      "epoch: 14 step: 932, loss is 0.018564216792583466\n",
      "epoch: 14 step: 933, loss is 0.06582733988761902\n",
      "epoch: 14 step: 934, loss is 0.040898632258176804\n",
      "epoch: 14 step: 935, loss is 0.020698877051472664\n",
      "epoch: 14 step: 936, loss is 0.004587459843605757\n",
      "epoch: 14 step: 937, loss is 0.04323684424161911\n",
      "epoch: 15 step: 1, loss is 0.002581412438303232\n",
      "epoch: 15 step: 2, loss is 0.03851926326751709\n",
      "epoch: 15 step: 3, loss is 0.0021240117494016886\n",
      "epoch: 15 step: 4, loss is 0.005713185761123896\n",
      "epoch: 15 step: 5, loss is 0.004381612874567509\n",
      "epoch: 15 step: 6, loss is 0.0013298795092850924\n",
      "epoch: 15 step: 7, loss is 0.02237800322473049\n",
      "epoch: 15 step: 8, loss is 0.021807502955198288\n",
      "epoch: 15 step: 9, loss is 0.0073103103786706924\n",
      "epoch: 15 step: 10, loss is 0.020327065140008926\n",
      "epoch: 15 step: 11, loss is 0.024023883044719696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 12, loss is 0.004393836949020624\n",
      "epoch: 15 step: 13, loss is 0.030960427597165108\n",
      "epoch: 15 step: 14, loss is 0.007937868125736713\n",
      "epoch: 15 step: 15, loss is 0.0022091681603342295\n",
      "epoch: 15 step: 16, loss is 0.018862277269363403\n",
      "epoch: 15 step: 17, loss is 0.004071480128914118\n",
      "epoch: 15 step: 18, loss is 0.016216063871979713\n",
      "epoch: 15 step: 19, loss is 0.007119679823517799\n",
      "epoch: 15 step: 20, loss is 0.002584242494776845\n",
      "epoch: 15 step: 21, loss is 0.0027337539941072464\n",
      "epoch: 15 step: 22, loss is 0.031594570726156235\n",
      "epoch: 15 step: 23, loss is 0.024205802008509636\n",
      "epoch: 15 step: 24, loss is 0.039558038115501404\n",
      "epoch: 15 step: 25, loss is 0.0003462047898210585\n",
      "epoch: 15 step: 26, loss is 0.03210848569869995\n",
      "epoch: 15 step: 27, loss is 0.04031853750348091\n",
      "epoch: 15 step: 28, loss is 0.03286060690879822\n",
      "epoch: 15 step: 29, loss is 0.0020476263016462326\n",
      "epoch: 15 step: 30, loss is 0.027957215905189514\n",
      "epoch: 15 step: 31, loss is 0.004809356760233641\n",
      "epoch: 15 step: 32, loss is 0.01542922668159008\n",
      "epoch: 15 step: 33, loss is 0.005879531614482403\n",
      "epoch: 15 step: 34, loss is 0.0034110916312783957\n",
      "epoch: 15 step: 35, loss is 0.027351651340723038\n",
      "epoch: 15 step: 36, loss is 0.04457300528883934\n",
      "epoch: 15 step: 37, loss is 0.02821100875735283\n",
      "epoch: 15 step: 38, loss is 0.029357893392443657\n",
      "epoch: 15 step: 39, loss is 0.013681769371032715\n",
      "epoch: 15 step: 40, loss is 0.01498674787580967\n",
      "epoch: 15 step: 41, loss is 0.0022021608892828226\n",
      "epoch: 15 step: 42, loss is 0.03401513397693634\n",
      "epoch: 15 step: 43, loss is 0.027899272739887238\n",
      "epoch: 15 step: 44, loss is 0.033820945769548416\n",
      "epoch: 15 step: 45, loss is 0.008261485025286674\n",
      "epoch: 15 step: 46, loss is 0.00431986665353179\n",
      "epoch: 15 step: 47, loss is 0.009174223989248276\n",
      "epoch: 15 step: 48, loss is 0.005657852161675692\n",
      "epoch: 15 step: 49, loss is 0.05101999267935753\n",
      "epoch: 15 step: 50, loss is 0.03843684121966362\n",
      "epoch: 15 step: 51, loss is 0.002964933170005679\n",
      "epoch: 15 step: 52, loss is 0.013717583380639553\n",
      "epoch: 15 step: 53, loss is 0.001480707200244069\n",
      "epoch: 15 step: 54, loss is 0.027670888230204582\n",
      "epoch: 15 step: 55, loss is 0.007554585579782724\n",
      "epoch: 15 step: 56, loss is 0.12203538417816162\n",
      "epoch: 15 step: 57, loss is 0.006472870707511902\n",
      "epoch: 15 step: 58, loss is 0.0013566531706601381\n",
      "epoch: 15 step: 59, loss is 0.076560378074646\n",
      "epoch: 15 step: 60, loss is 0.0051970817148685455\n",
      "epoch: 15 step: 61, loss is 0.015025016851723194\n",
      "epoch: 15 step: 62, loss is 0.015980897471308708\n",
      "epoch: 15 step: 63, loss is 0.018805909901857376\n",
      "epoch: 15 step: 64, loss is 0.0065092300064861774\n",
      "epoch: 15 step: 65, loss is 0.0161367766559124\n",
      "epoch: 15 step: 66, loss is 0.0007985685369931161\n",
      "epoch: 15 step: 67, loss is 0.011969057843089104\n",
      "epoch: 15 step: 68, loss is 0.1184486374258995\n",
      "epoch: 15 step: 69, loss is 0.011851456947624683\n",
      "epoch: 15 step: 70, loss is 0.007443581707775593\n",
      "epoch: 15 step: 71, loss is 0.006017139181494713\n",
      "epoch: 15 step: 72, loss is 0.015498298220336437\n",
      "epoch: 15 step: 73, loss is 0.0078111509792506695\n",
      "epoch: 15 step: 74, loss is 0.026229023933410645\n",
      "epoch: 15 step: 75, loss is 0.02877672389149666\n",
      "epoch: 15 step: 76, loss is 0.00561487814411521\n",
      "epoch: 15 step: 77, loss is 0.006090257316827774\n",
      "epoch: 15 step: 78, loss is 0.00294133136048913\n",
      "epoch: 15 step: 79, loss is 0.001651938073337078\n",
      "epoch: 15 step: 80, loss is 0.005780617706477642\n",
      "epoch: 15 step: 81, loss is 0.006217154674232006\n",
      "epoch: 15 step: 82, loss is 0.025806475430727005\n",
      "epoch: 15 step: 83, loss is 0.031338952481746674\n",
      "epoch: 15 step: 84, loss is 0.0036653694696724415\n",
      "epoch: 15 step: 85, loss is 0.013158627785742283\n",
      "epoch: 15 step: 86, loss is 0.007176845334470272\n",
      "epoch: 15 step: 87, loss is 0.06155470758676529\n",
      "epoch: 15 step: 88, loss is 0.022145630791783333\n",
      "epoch: 15 step: 89, loss is 0.011142351664602757\n",
      "epoch: 15 step: 90, loss is 0.0014330545673146844\n",
      "epoch: 15 step: 91, loss is 0.027182351797819138\n",
      "epoch: 15 step: 92, loss is 0.015177492052316666\n",
      "epoch: 15 step: 93, loss is 0.012968365103006363\n",
      "epoch: 15 step: 94, loss is 0.00587807223200798\n",
      "epoch: 15 step: 95, loss is 0.0034400017466396093\n",
      "epoch: 15 step: 96, loss is 0.00304265390150249\n",
      "epoch: 15 step: 97, loss is 0.0439111702144146\n",
      "epoch: 15 step: 98, loss is 0.00822412222623825\n",
      "epoch: 15 step: 99, loss is 0.008061873726546764\n",
      "epoch: 15 step: 100, loss is 0.006151688750833273\n",
      "epoch: 15 step: 101, loss is 0.02107926458120346\n",
      "epoch: 15 step: 102, loss is 0.0030550507362931967\n",
      "epoch: 15 step: 103, loss is 0.007784715387970209\n",
      "epoch: 15 step: 104, loss is 0.07248209416866302\n",
      "epoch: 15 step: 105, loss is 0.02903607115149498\n",
      "epoch: 15 step: 106, loss is 0.029204605147242546\n",
      "epoch: 15 step: 107, loss is 0.004437956027686596\n",
      "epoch: 15 step: 108, loss is 0.017802366986870766\n",
      "epoch: 15 step: 109, loss is 0.005636547692120075\n",
      "epoch: 15 step: 110, loss is 0.001137619954533875\n",
      "epoch: 15 step: 111, loss is 0.0024500463623553514\n",
      "epoch: 15 step: 112, loss is 0.013602416031062603\n",
      "epoch: 15 step: 113, loss is 0.008090047165751457\n",
      "epoch: 15 step: 114, loss is 0.027146456763148308\n",
      "epoch: 15 step: 115, loss is 0.03326484188437462\n",
      "epoch: 15 step: 116, loss is 0.0020123806316405535\n",
      "epoch: 15 step: 117, loss is 0.005296316463500261\n",
      "epoch: 15 step: 118, loss is 0.02200889028608799\n",
      "epoch: 15 step: 119, loss is 0.04934053122997284\n",
      "epoch: 15 step: 120, loss is 0.053592387586832047\n",
      "epoch: 15 step: 121, loss is 0.022789742797613144\n",
      "epoch: 15 step: 122, loss is 0.0030786858405917883\n",
      "epoch: 15 step: 123, loss is 0.06219594180583954\n",
      "epoch: 15 step: 124, loss is 0.01956585794687271\n",
      "epoch: 15 step: 125, loss is 0.015141435898840427\n",
      "epoch: 15 step: 126, loss is 0.0022940116468816996\n",
      "epoch: 15 step: 127, loss is 0.006786724086850882\n",
      "epoch: 15 step: 128, loss is 0.00298382923938334\n",
      "epoch: 15 step: 129, loss is 0.015828438103199005\n",
      "epoch: 15 step: 130, loss is 0.008905263617634773\n",
      "epoch: 15 step: 131, loss is 0.009654160588979721\n",
      "epoch: 15 step: 132, loss is 0.013697685673832893\n",
      "epoch: 15 step: 133, loss is 0.021700887009501457\n",
      "epoch: 15 step: 134, loss is 0.003352588042616844\n",
      "epoch: 15 step: 135, loss is 0.006963352207094431\n",
      "epoch: 15 step: 136, loss is 0.04752076044678688\n",
      "epoch: 15 step: 137, loss is 0.013334390707314014\n",
      "epoch: 15 step: 138, loss is 0.007739291060715914\n",
      "epoch: 15 step: 139, loss is 0.0029100393876433372\n",
      "epoch: 15 step: 140, loss is 0.002644297666847706\n",
      "epoch: 15 step: 141, loss is 0.002860089298337698\n",
      "epoch: 15 step: 142, loss is 0.005378113128244877\n",
      "epoch: 15 step: 143, loss is 0.0023070424795150757\n",
      "epoch: 15 step: 144, loss is 0.006465318612754345\n",
      "epoch: 15 step: 145, loss is 0.0018383171409368515\n",
      "epoch: 15 step: 146, loss is 0.021469028666615486\n",
      "epoch: 15 step: 147, loss is 0.007622427772730589\n",
      "epoch: 15 step: 148, loss is 0.023030655458569527\n",
      "epoch: 15 step: 149, loss is 0.006196335889399052\n",
      "epoch: 15 step: 150, loss is 0.044232770800590515\n",
      "epoch: 15 step: 151, loss is 0.038878168910741806\n",
      "epoch: 15 step: 152, loss is 0.0013678227551281452\n",
      "epoch: 15 step: 153, loss is 0.016136247664690018\n",
      "epoch: 15 step: 154, loss is 0.010680561885237694\n",
      "epoch: 15 step: 155, loss is 0.010522820055484772\n",
      "epoch: 15 step: 156, loss is 0.0005935687222518027\n",
      "epoch: 15 step: 157, loss is 0.007639524061232805\n",
      "epoch: 15 step: 158, loss is 0.16316957771778107\n",
      "epoch: 15 step: 159, loss is 0.004500965587794781\n",
      "epoch: 15 step: 160, loss is 0.012624402530491352\n",
      "epoch: 15 step: 161, loss is 0.0129462955519557\n",
      "epoch: 15 step: 162, loss is 0.08726860582828522\n",
      "epoch: 15 step: 163, loss is 0.006316544488072395\n",
      "epoch: 15 step: 164, loss is 0.015015768818557262\n",
      "epoch: 15 step: 165, loss is 0.015085550025105476\n",
      "epoch: 15 step: 166, loss is 0.008765039034187794\n",
      "epoch: 15 step: 167, loss is 0.017001498490571976\n",
      "epoch: 15 step: 168, loss is 0.005766631569713354\n",
      "epoch: 15 step: 169, loss is 0.03188313916325569\n",
      "epoch: 15 step: 170, loss is 0.06138044223189354\n",
      "epoch: 15 step: 171, loss is 0.012245501391589642\n",
      "epoch: 15 step: 172, loss is 0.0020071258768439293\n",
      "epoch: 15 step: 173, loss is 0.012582508847117424\n",
      "epoch: 15 step: 174, loss is 0.010555295273661613\n",
      "epoch: 15 step: 175, loss is 0.009140445850789547\n",
      "epoch: 15 step: 176, loss is 0.05385712534189224\n",
      "epoch: 15 step: 177, loss is 0.023197071626782417\n",
      "epoch: 15 step: 178, loss is 0.057424187660217285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 179, loss is 0.008431547321379185\n",
      "epoch: 15 step: 180, loss is 0.0023895236663520336\n",
      "epoch: 15 step: 181, loss is 0.005353334825485945\n",
      "epoch: 15 step: 182, loss is 0.006444226484745741\n",
      "epoch: 15 step: 183, loss is 0.004168333951383829\n",
      "epoch: 15 step: 184, loss is 0.002081711310893297\n",
      "epoch: 15 step: 185, loss is 0.002454394241794944\n",
      "epoch: 15 step: 186, loss is 0.08714472502470016\n",
      "epoch: 15 step: 187, loss is 0.016249872744083405\n",
      "epoch: 15 step: 188, loss is 0.016979385167360306\n",
      "epoch: 15 step: 189, loss is 0.0028720719274133444\n",
      "epoch: 15 step: 190, loss is 0.0011775633320212364\n",
      "epoch: 15 step: 191, loss is 0.0009113287669606507\n",
      "epoch: 15 step: 192, loss is 0.002191011793911457\n",
      "epoch: 15 step: 193, loss is 0.01253088004887104\n",
      "epoch: 15 step: 194, loss is 0.010153168812394142\n",
      "epoch: 15 step: 195, loss is 0.015217674896121025\n",
      "epoch: 15 step: 196, loss is 0.001621269853785634\n",
      "epoch: 15 step: 197, loss is 0.08439576625823975\n",
      "epoch: 15 step: 198, loss is 0.009553547948598862\n",
      "epoch: 15 step: 199, loss is 0.008235631510615349\n",
      "epoch: 15 step: 200, loss is 0.008604663424193859\n",
      "epoch: 15 step: 201, loss is 0.014438482001423836\n",
      "epoch: 15 step: 202, loss is 0.04577988013625145\n",
      "epoch: 15 step: 203, loss is 0.011149751022458076\n",
      "epoch: 15 step: 204, loss is 0.0052993870340287685\n",
      "epoch: 15 step: 205, loss is 0.0024702127557247877\n",
      "epoch: 15 step: 206, loss is 0.02849767357110977\n",
      "epoch: 15 step: 207, loss is 0.009696759283542633\n",
      "epoch: 15 step: 208, loss is 0.013553707860410213\n",
      "epoch: 15 step: 209, loss is 0.022783871740102768\n",
      "epoch: 15 step: 210, loss is 0.0040927426889538765\n",
      "epoch: 15 step: 211, loss is 0.02439805120229721\n",
      "epoch: 15 step: 212, loss is 0.08686630427837372\n",
      "epoch: 15 step: 213, loss is 0.01541085634380579\n",
      "epoch: 15 step: 214, loss is 0.03397709131240845\n",
      "epoch: 15 step: 215, loss is 0.0014155433746054769\n",
      "epoch: 15 step: 216, loss is 0.005188560113310814\n",
      "epoch: 15 step: 217, loss is 0.020194537937641144\n",
      "epoch: 15 step: 218, loss is 0.002660993719473481\n",
      "epoch: 15 step: 219, loss is 0.0031536398455500603\n",
      "epoch: 15 step: 220, loss is 0.04508969560265541\n",
      "epoch: 15 step: 221, loss is 0.009714371524751186\n",
      "epoch: 15 step: 222, loss is 0.0846622884273529\n",
      "epoch: 15 step: 223, loss is 0.016228901222348213\n",
      "epoch: 15 step: 224, loss is 0.0020059053786098957\n",
      "epoch: 15 step: 225, loss is 0.07644502818584442\n",
      "epoch: 15 step: 226, loss is 0.07141062617301941\n",
      "epoch: 15 step: 227, loss is 0.0019521113717928529\n",
      "epoch: 15 step: 228, loss is 0.01584557071328163\n",
      "epoch: 15 step: 229, loss is 0.003796143690124154\n",
      "epoch: 15 step: 230, loss is 0.03143272548913956\n",
      "epoch: 15 step: 231, loss is 0.01894659362733364\n",
      "epoch: 15 step: 232, loss is 0.04058113694190979\n",
      "epoch: 15 step: 233, loss is 0.01827521249651909\n",
      "epoch: 15 step: 234, loss is 0.06781776994466782\n",
      "epoch: 15 step: 235, loss is 0.006428254768252373\n",
      "epoch: 15 step: 236, loss is 0.008092005737125874\n",
      "epoch: 15 step: 237, loss is 0.0023965141735970974\n",
      "epoch: 15 step: 238, loss is 0.0022050957195460796\n",
      "epoch: 15 step: 239, loss is 0.06726589053869247\n",
      "epoch: 15 step: 240, loss is 0.0036542390007525682\n",
      "epoch: 15 step: 241, loss is 0.006953380536288023\n",
      "epoch: 15 step: 242, loss is 0.0017005025874823332\n",
      "epoch: 15 step: 243, loss is 0.02975713089108467\n",
      "epoch: 15 step: 244, loss is 0.017539961263537407\n",
      "epoch: 15 step: 245, loss is 0.02743346244096756\n",
      "epoch: 15 step: 246, loss is 0.0005970150232315063\n",
      "epoch: 15 step: 247, loss is 0.08351560682058334\n",
      "epoch: 15 step: 248, loss is 0.0053080362267792225\n",
      "epoch: 15 step: 249, loss is 0.012531338259577751\n",
      "epoch: 15 step: 250, loss is 0.08981677144765854\n",
      "epoch: 15 step: 251, loss is 0.026239901781082153\n",
      "epoch: 15 step: 252, loss is 0.003438779152929783\n",
      "epoch: 15 step: 253, loss is 0.04417448490858078\n",
      "epoch: 15 step: 254, loss is 0.06064387410879135\n",
      "epoch: 15 step: 255, loss is 0.007396041881293058\n",
      "epoch: 15 step: 256, loss is 0.0030408762395381927\n",
      "epoch: 15 step: 257, loss is 0.059225257486104965\n",
      "epoch: 15 step: 258, loss is 0.046518705785274506\n",
      "epoch: 15 step: 259, loss is 0.031455300748348236\n",
      "epoch: 15 step: 260, loss is 0.00532880611717701\n",
      "epoch: 15 step: 261, loss is 0.07574140280485153\n",
      "epoch: 15 step: 262, loss is 0.09978234022855759\n",
      "epoch: 15 step: 263, loss is 0.02603950724005699\n",
      "epoch: 15 step: 264, loss is 0.0029690638184547424\n",
      "epoch: 15 step: 265, loss is 0.027701672166585922\n",
      "epoch: 15 step: 266, loss is 0.0013028462417423725\n",
      "epoch: 15 step: 267, loss is 0.007513951510190964\n",
      "epoch: 15 step: 268, loss is 0.018133802339434624\n",
      "epoch: 15 step: 269, loss is 0.00607737060636282\n",
      "epoch: 15 step: 270, loss is 0.020000943914055824\n",
      "epoch: 15 step: 271, loss is 0.10232442617416382\n",
      "epoch: 15 step: 272, loss is 0.01945522427558899\n",
      "epoch: 15 step: 273, loss is 0.024280261248350143\n",
      "epoch: 15 step: 274, loss is 0.008762132376432419\n",
      "epoch: 15 step: 275, loss is 0.05351125821471214\n",
      "epoch: 15 step: 276, loss is 0.034492552280426025\n",
      "epoch: 15 step: 277, loss is 0.02989029325544834\n",
      "epoch: 15 step: 278, loss is 0.0004369280650280416\n",
      "epoch: 15 step: 279, loss is 0.02699900232255459\n",
      "epoch: 15 step: 280, loss is 0.01191400084644556\n",
      "epoch: 15 step: 281, loss is 0.020001577213406563\n",
      "epoch: 15 step: 282, loss is 0.0011576407123357058\n",
      "epoch: 15 step: 283, loss is 0.009475428611040115\n",
      "epoch: 15 step: 284, loss is 0.023489048704504967\n",
      "epoch: 15 step: 285, loss is 0.040259867906570435\n",
      "epoch: 15 step: 286, loss is 0.052943289279937744\n",
      "epoch: 15 step: 287, loss is 0.11192013323307037\n",
      "epoch: 15 step: 288, loss is 0.00587476184591651\n",
      "epoch: 15 step: 289, loss is 0.01648387685418129\n",
      "epoch: 15 step: 290, loss is 0.007891194894909859\n",
      "epoch: 15 step: 291, loss is 0.04456916078925133\n",
      "epoch: 15 step: 292, loss is 0.025918487459421158\n",
      "epoch: 15 step: 293, loss is 0.0034273178316652775\n",
      "epoch: 15 step: 294, loss is 0.026626106351614\n",
      "epoch: 15 step: 295, loss is 0.02514597959816456\n",
      "epoch: 15 step: 296, loss is 0.0033081285655498505\n",
      "epoch: 15 step: 297, loss is 0.029434822499752045\n",
      "epoch: 15 step: 298, loss is 0.018008338287472725\n",
      "epoch: 15 step: 299, loss is 0.059749163687229156\n",
      "epoch: 15 step: 300, loss is 0.014910395257174969\n",
      "epoch: 15 step: 301, loss is 0.0013643187703564763\n",
      "epoch: 15 step: 302, loss is 0.02005118690431118\n",
      "epoch: 15 step: 303, loss is 0.004901641979813576\n",
      "epoch: 15 step: 304, loss is 0.10400696098804474\n",
      "epoch: 15 step: 305, loss is 0.031162990257143974\n",
      "epoch: 15 step: 306, loss is 0.02674846351146698\n",
      "epoch: 15 step: 307, loss is 0.03572271019220352\n",
      "epoch: 15 step: 308, loss is 0.008576801978051662\n",
      "epoch: 15 step: 309, loss is 0.04334757477045059\n",
      "epoch: 15 step: 310, loss is 0.01307142898440361\n",
      "epoch: 15 step: 311, loss is 0.025192657485604286\n",
      "epoch: 15 step: 312, loss is 0.08485060930252075\n",
      "epoch: 15 step: 313, loss is 0.011800266802310944\n",
      "epoch: 15 step: 314, loss is 0.0075200945138931274\n",
      "epoch: 15 step: 315, loss is 0.0974530354142189\n",
      "epoch: 15 step: 316, loss is 0.006725160405039787\n",
      "epoch: 15 step: 317, loss is 0.0287119559943676\n",
      "epoch: 15 step: 318, loss is 0.019862467423081398\n",
      "epoch: 15 step: 319, loss is 0.0038345458451658487\n",
      "epoch: 15 step: 320, loss is 0.022001154720783234\n",
      "epoch: 15 step: 321, loss is 0.008014613762497902\n",
      "epoch: 15 step: 322, loss is 0.002034921897575259\n",
      "epoch: 15 step: 323, loss is 0.00743075180798769\n",
      "epoch: 15 step: 324, loss is 0.0042089251801371574\n",
      "epoch: 15 step: 325, loss is 0.0017430763691663742\n",
      "epoch: 15 step: 326, loss is 0.0027487194165587425\n",
      "epoch: 15 step: 327, loss is 0.015896352007985115\n",
      "epoch: 15 step: 328, loss is 0.016399798914790154\n",
      "epoch: 15 step: 329, loss is 0.05720499902963638\n",
      "epoch: 15 step: 330, loss is 0.013346098363399506\n",
      "epoch: 15 step: 331, loss is 0.007886107079684734\n",
      "epoch: 15 step: 332, loss is 0.003656434128060937\n",
      "epoch: 15 step: 333, loss is 0.011708893813192844\n",
      "epoch: 15 step: 334, loss is 0.014236798509955406\n",
      "epoch: 15 step: 335, loss is 0.010364712215960026\n",
      "epoch: 15 step: 336, loss is 0.03355742618441582\n",
      "epoch: 15 step: 337, loss is 0.0015128743834793568\n",
      "epoch: 15 step: 338, loss is 0.13439667224884033\n",
      "epoch: 15 step: 339, loss is 0.006521246396005154\n",
      "epoch: 15 step: 340, loss is 0.008877566084265709\n",
      "epoch: 15 step: 341, loss is 0.002847685944288969\n",
      "epoch: 15 step: 342, loss is 0.013021860271692276\n",
      "epoch: 15 step: 343, loss is 0.03676963970065117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 344, loss is 0.07382375746965408\n",
      "epoch: 15 step: 345, loss is 0.009443736635148525\n",
      "epoch: 15 step: 346, loss is 0.007968246936798096\n",
      "epoch: 15 step: 347, loss is 0.006017575040459633\n",
      "epoch: 15 step: 348, loss is 0.00930585153400898\n",
      "epoch: 15 step: 349, loss is 0.02719515562057495\n",
      "epoch: 15 step: 350, loss is 0.0026192094665020704\n",
      "epoch: 15 step: 351, loss is 0.014272093772888184\n",
      "epoch: 15 step: 352, loss is 0.010834777727723122\n",
      "epoch: 15 step: 353, loss is 0.045886050909757614\n",
      "epoch: 15 step: 354, loss is 0.0021424086298793554\n",
      "epoch: 15 step: 355, loss is 0.020151367411017418\n",
      "epoch: 15 step: 356, loss is 0.004462048411369324\n",
      "epoch: 15 step: 357, loss is 0.007904445752501488\n",
      "epoch: 15 step: 358, loss is 0.010435929521918297\n",
      "epoch: 15 step: 359, loss is 0.02773493342101574\n",
      "epoch: 15 step: 360, loss is 0.005237256176769733\n",
      "epoch: 15 step: 361, loss is 0.0022032642737030983\n",
      "epoch: 15 step: 362, loss is 0.02529006078839302\n",
      "epoch: 15 step: 363, loss is 0.0062678805552423\n",
      "epoch: 15 step: 364, loss is 0.004050516989082098\n",
      "epoch: 15 step: 365, loss is 0.03865249454975128\n",
      "epoch: 15 step: 366, loss is 0.006926161237061024\n",
      "epoch: 15 step: 367, loss is 0.019572103396058083\n",
      "epoch: 15 step: 368, loss is 0.010589202865958214\n",
      "epoch: 15 step: 369, loss is 0.008870656602084637\n",
      "epoch: 15 step: 370, loss is 0.0010729474015533924\n",
      "epoch: 15 step: 371, loss is 0.03242768719792366\n",
      "epoch: 15 step: 372, loss is 0.028323126956820488\n",
      "epoch: 15 step: 373, loss is 0.03932840749621391\n",
      "epoch: 15 step: 374, loss is 0.10112452507019043\n",
      "epoch: 15 step: 375, loss is 0.039088502526283264\n",
      "epoch: 15 step: 376, loss is 0.1775229424238205\n",
      "epoch: 15 step: 377, loss is 0.015248415991663933\n",
      "epoch: 15 step: 378, loss is 0.009851493872702122\n",
      "epoch: 15 step: 379, loss is 0.013829456642270088\n",
      "epoch: 15 step: 380, loss is 0.05486452579498291\n",
      "epoch: 15 step: 381, loss is 0.008170939050614834\n",
      "epoch: 15 step: 382, loss is 0.012387311086058617\n",
      "epoch: 15 step: 383, loss is 0.004427320323884487\n",
      "epoch: 15 step: 384, loss is 0.06595194339752197\n",
      "epoch: 15 step: 385, loss is 0.007766218855977058\n",
      "epoch: 15 step: 386, loss is 0.05218523368239403\n",
      "epoch: 15 step: 387, loss is 0.009141500107944012\n",
      "epoch: 15 step: 388, loss is 0.05966673418879509\n",
      "epoch: 15 step: 389, loss is 0.08994696289300919\n",
      "epoch: 15 step: 390, loss is 0.01739308424293995\n",
      "epoch: 15 step: 391, loss is 0.02169085666537285\n",
      "epoch: 15 step: 392, loss is 0.007122707553207874\n",
      "epoch: 15 step: 393, loss is 0.004266669973731041\n",
      "epoch: 15 step: 394, loss is 0.05639355629682541\n",
      "epoch: 15 step: 395, loss is 0.02368038520216942\n",
      "epoch: 15 step: 396, loss is 0.006270488258451223\n",
      "epoch: 15 step: 397, loss is 0.012778299860656261\n",
      "epoch: 15 step: 398, loss is 0.005163461901247501\n",
      "epoch: 15 step: 399, loss is 0.013739274814724922\n",
      "epoch: 15 step: 400, loss is 0.0036786978598684072\n",
      "epoch: 15 step: 401, loss is 0.04302410036325455\n",
      "epoch: 15 step: 402, loss is 0.018583107739686966\n",
      "epoch: 15 step: 403, loss is 0.008386443369090557\n",
      "epoch: 15 step: 404, loss is 0.0033575575798749924\n",
      "epoch: 15 step: 405, loss is 0.003551529021933675\n",
      "epoch: 15 step: 406, loss is 0.005501758772879839\n",
      "epoch: 15 step: 407, loss is 0.0021743960678577423\n",
      "epoch: 15 step: 408, loss is 0.025147195905447006\n",
      "epoch: 15 step: 409, loss is 0.013867724686861038\n",
      "epoch: 15 step: 410, loss is 0.04040168225765228\n",
      "epoch: 15 step: 411, loss is 0.004241566639393568\n",
      "epoch: 15 step: 412, loss is 0.04447216913104057\n",
      "epoch: 15 step: 413, loss is 0.040617313235998154\n",
      "epoch: 15 step: 414, loss is 0.024380117654800415\n",
      "epoch: 15 step: 415, loss is 0.026771117001771927\n",
      "epoch: 15 step: 416, loss is 0.0025290190242230892\n",
      "epoch: 15 step: 417, loss is 0.011979403905570507\n",
      "epoch: 15 step: 418, loss is 0.062106162309646606\n",
      "epoch: 15 step: 419, loss is 0.009752575308084488\n",
      "epoch: 15 step: 420, loss is 0.017537668347358704\n",
      "epoch: 15 step: 421, loss is 0.050506673753261566\n",
      "epoch: 15 step: 422, loss is 0.02243472822010517\n",
      "epoch: 15 step: 423, loss is 0.09371516853570938\n",
      "epoch: 15 step: 424, loss is 0.0332554429769516\n",
      "epoch: 15 step: 425, loss is 0.03439865633845329\n",
      "epoch: 15 step: 426, loss is 0.00889741163700819\n",
      "epoch: 15 step: 427, loss is 0.010667303577065468\n",
      "epoch: 15 step: 428, loss is 0.01296287216246128\n",
      "epoch: 15 step: 429, loss is 0.0287907887250185\n",
      "epoch: 15 step: 430, loss is 0.02545672468841076\n",
      "epoch: 15 step: 431, loss is 0.019399847835302353\n",
      "epoch: 15 step: 432, loss is 0.008452534675598145\n",
      "epoch: 15 step: 433, loss is 0.07945876568555832\n",
      "epoch: 15 step: 434, loss is 0.0019420117605477571\n",
      "epoch: 15 step: 435, loss is 0.010928359813988209\n",
      "epoch: 15 step: 436, loss is 0.019868088886141777\n",
      "epoch: 15 step: 437, loss is 0.08554074913263321\n",
      "epoch: 15 step: 438, loss is 0.0072441017255187035\n",
      "epoch: 15 step: 439, loss is 0.004273596219718456\n",
      "epoch: 15 step: 440, loss is 0.011751512065529823\n",
      "epoch: 15 step: 441, loss is 0.06794293224811554\n",
      "epoch: 15 step: 442, loss is 0.0025885405484586954\n",
      "epoch: 15 step: 443, loss is 0.007513117045164108\n",
      "epoch: 15 step: 444, loss is 0.010310124605894089\n",
      "epoch: 15 step: 445, loss is 0.008486505597829819\n",
      "epoch: 15 step: 446, loss is 0.004826241638511419\n",
      "epoch: 15 step: 447, loss is 0.0012426807079464197\n",
      "epoch: 15 step: 448, loss is 0.0014272258849814534\n",
      "epoch: 15 step: 449, loss is 0.0027607260271906853\n",
      "epoch: 15 step: 450, loss is 0.0059709688648581505\n",
      "epoch: 15 step: 451, loss is 0.027679111808538437\n",
      "epoch: 15 step: 452, loss is 0.09373430162668228\n",
      "epoch: 15 step: 453, loss is 0.1485467404127121\n",
      "epoch: 15 step: 454, loss is 0.013503300957381725\n",
      "epoch: 15 step: 455, loss is 0.018928252160549164\n",
      "epoch: 15 step: 456, loss is 0.0029152934439480305\n",
      "epoch: 15 step: 457, loss is 0.037654973566532135\n",
      "epoch: 15 step: 458, loss is 0.021007437258958817\n",
      "epoch: 15 step: 459, loss is 0.036065474152565\n",
      "epoch: 15 step: 460, loss is 0.004035572055727243\n",
      "epoch: 15 step: 461, loss is 0.016538210213184357\n",
      "epoch: 15 step: 462, loss is 0.11742129176855087\n",
      "epoch: 15 step: 463, loss is 0.012891024351119995\n",
      "epoch: 15 step: 464, loss is 0.025644095614552498\n",
      "epoch: 15 step: 465, loss is 0.0022310728672891855\n",
      "epoch: 15 step: 466, loss is 0.006655780132859945\n",
      "epoch: 15 step: 467, loss is 0.02471277490258217\n",
      "epoch: 15 step: 468, loss is 0.04092959314584732\n",
      "epoch: 15 step: 469, loss is 0.012958989478647709\n",
      "epoch: 15 step: 470, loss is 0.004887149669229984\n",
      "epoch: 15 step: 471, loss is 0.05855643004179001\n",
      "epoch: 15 step: 472, loss is 0.05323256552219391\n",
      "epoch: 15 step: 473, loss is 0.0023015921469777822\n",
      "epoch: 15 step: 474, loss is 0.015072529204189777\n",
      "epoch: 15 step: 475, loss is 0.003174381796270609\n",
      "epoch: 15 step: 476, loss is 0.029886214062571526\n",
      "epoch: 15 step: 477, loss is 0.03603581711649895\n",
      "epoch: 15 step: 478, loss is 0.011043591424822807\n",
      "epoch: 15 step: 479, loss is 0.015084893442690372\n",
      "epoch: 15 step: 480, loss is 0.0007236911333166063\n",
      "epoch: 15 step: 481, loss is 0.0010610203025862575\n",
      "epoch: 15 step: 482, loss is 0.09718889743089676\n",
      "epoch: 15 step: 483, loss is 0.0018365284195169806\n",
      "epoch: 15 step: 484, loss is 0.008677680976688862\n",
      "epoch: 15 step: 485, loss is 0.012533897534012794\n",
      "epoch: 15 step: 486, loss is 0.013894591480493546\n",
      "epoch: 15 step: 487, loss is 0.030299175530672073\n",
      "epoch: 15 step: 488, loss is 8.008488657651469e-05\n",
      "epoch: 15 step: 489, loss is 0.001837041461840272\n",
      "epoch: 15 step: 490, loss is 0.0008172076195478439\n",
      "epoch: 15 step: 491, loss is 0.00552037637680769\n",
      "epoch: 15 step: 492, loss is 0.007137342821806669\n",
      "epoch: 15 step: 493, loss is 0.001082917908206582\n",
      "epoch: 15 step: 494, loss is 0.03597961738705635\n",
      "epoch: 15 step: 495, loss is 0.01878361590206623\n",
      "epoch: 15 step: 496, loss is 0.013318972662091255\n",
      "epoch: 15 step: 497, loss is 0.02656039223074913\n",
      "epoch: 15 step: 498, loss is 0.03871522098779678\n",
      "epoch: 15 step: 499, loss is 0.005166248418390751\n",
      "epoch: 15 step: 500, loss is 0.05857962369918823\n",
      "epoch: 15 step: 501, loss is 0.021346135064959526\n",
      "epoch: 15 step: 502, loss is 0.00931453611701727\n",
      "epoch: 15 step: 503, loss is 0.030975747853517532\n",
      "epoch: 15 step: 504, loss is 0.003804703475907445\n",
      "epoch: 15 step: 505, loss is 0.006378518883138895\n",
      "epoch: 15 step: 506, loss is 0.06665591150522232\n",
      "epoch: 15 step: 507, loss is 0.05372843146324158\n",
      "epoch: 15 step: 508, loss is 0.001842142315581441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 509, loss is 0.017170140519738197\n",
      "epoch: 15 step: 510, loss is 0.012000318616628647\n",
      "epoch: 15 step: 511, loss is 0.0010187759762629867\n",
      "epoch: 15 step: 512, loss is 0.03051042929291725\n",
      "epoch: 15 step: 513, loss is 0.03954917937517166\n",
      "epoch: 15 step: 514, loss is 0.048410847783088684\n",
      "epoch: 15 step: 515, loss is 0.035249143838882446\n",
      "epoch: 15 step: 516, loss is 0.0417947918176651\n",
      "epoch: 15 step: 517, loss is 0.03976893797516823\n",
      "epoch: 15 step: 518, loss is 0.007307439111173153\n",
      "epoch: 15 step: 519, loss is 0.0030925970058888197\n",
      "epoch: 15 step: 520, loss is 0.03747449070215225\n",
      "epoch: 15 step: 521, loss is 0.0007425681105814874\n",
      "epoch: 15 step: 522, loss is 0.0018461612053215504\n",
      "epoch: 15 step: 523, loss is 0.006180657539516687\n",
      "epoch: 15 step: 524, loss is 0.0002738966140896082\n",
      "epoch: 15 step: 525, loss is 0.03121127001941204\n",
      "epoch: 15 step: 526, loss is 0.003607863327488303\n",
      "epoch: 15 step: 527, loss is 0.025851640850305557\n",
      "epoch: 15 step: 528, loss is 0.03123418614268303\n",
      "epoch: 15 step: 529, loss is 0.004798287525773048\n",
      "epoch: 15 step: 530, loss is 0.023544583469629288\n",
      "epoch: 15 step: 531, loss is 0.009279401041567326\n",
      "epoch: 15 step: 532, loss is 0.00685470923781395\n",
      "epoch: 15 step: 533, loss is 0.017724238336086273\n",
      "epoch: 15 step: 534, loss is 0.009680045768618584\n",
      "epoch: 15 step: 535, loss is 0.035063061863183975\n",
      "epoch: 15 step: 536, loss is 0.04653496295213699\n",
      "epoch: 15 step: 537, loss is 0.005031341686844826\n",
      "epoch: 15 step: 538, loss is 0.07034878432750702\n",
      "epoch: 15 step: 539, loss is 0.041407544165849686\n",
      "epoch: 15 step: 540, loss is 0.0008315626764670014\n",
      "epoch: 15 step: 541, loss is 0.015065486542880535\n",
      "epoch: 15 step: 542, loss is 0.03646276891231537\n",
      "epoch: 15 step: 543, loss is 0.0002150399232050404\n",
      "epoch: 15 step: 544, loss is 0.005144839640706778\n",
      "epoch: 15 step: 545, loss is 0.06884143501520157\n",
      "epoch: 15 step: 546, loss is 0.054182469844818115\n",
      "epoch: 15 step: 547, loss is 0.058253299444913864\n",
      "epoch: 15 step: 548, loss is 0.021788233891129494\n",
      "epoch: 15 step: 549, loss is 0.014439743012189865\n",
      "epoch: 15 step: 550, loss is 0.03427736461162567\n",
      "epoch: 15 step: 551, loss is 0.013395079411566257\n",
      "epoch: 15 step: 552, loss is 0.01110571064054966\n",
      "epoch: 15 step: 553, loss is 0.06298775225877762\n",
      "epoch: 15 step: 554, loss is 0.08386535942554474\n",
      "epoch: 15 step: 555, loss is 0.009380152449011803\n",
      "epoch: 15 step: 556, loss is 0.00936790369451046\n",
      "epoch: 15 step: 557, loss is 0.05763714015483856\n",
      "epoch: 15 step: 558, loss is 0.02015821635723114\n",
      "epoch: 15 step: 559, loss is 0.007655357476323843\n",
      "epoch: 15 step: 560, loss is 0.005389472469687462\n",
      "epoch: 15 step: 561, loss is 0.05535618215799332\n",
      "epoch: 15 step: 562, loss is 0.0024002883583307266\n",
      "epoch: 15 step: 563, loss is 0.02196895144879818\n",
      "epoch: 15 step: 564, loss is 0.03052392229437828\n",
      "epoch: 15 step: 565, loss is 0.06817042827606201\n",
      "epoch: 15 step: 566, loss is 0.06546390801668167\n",
      "epoch: 15 step: 567, loss is 0.030754365026950836\n",
      "epoch: 15 step: 568, loss is 0.151962548494339\n",
      "epoch: 15 step: 569, loss is 0.0699397549033165\n",
      "epoch: 15 step: 570, loss is 0.06164701282978058\n",
      "epoch: 15 step: 571, loss is 0.008621933870017529\n",
      "epoch: 15 step: 572, loss is 0.07739760726690292\n",
      "epoch: 15 step: 573, loss is 0.0074171023443341255\n",
      "epoch: 15 step: 574, loss is 0.011739253997802734\n",
      "epoch: 15 step: 575, loss is 0.013159340247511864\n",
      "epoch: 15 step: 576, loss is 0.004923181142657995\n",
      "epoch: 15 step: 577, loss is 0.004392482340335846\n",
      "epoch: 15 step: 578, loss is 0.01203394215553999\n",
      "epoch: 15 step: 579, loss is 0.10300208628177643\n",
      "epoch: 15 step: 580, loss is 0.03273971378803253\n",
      "epoch: 15 step: 581, loss is 0.017987636849284172\n",
      "epoch: 15 step: 582, loss is 0.04134117066860199\n",
      "epoch: 15 step: 583, loss is 0.0023235208354890347\n",
      "epoch: 15 step: 584, loss is 0.01363356877118349\n",
      "epoch: 15 step: 585, loss is 0.043373655527830124\n",
      "epoch: 15 step: 586, loss is 0.0022260183468461037\n",
      "epoch: 15 step: 587, loss is 0.0050872922874987125\n",
      "epoch: 15 step: 588, loss is 0.05402660369873047\n",
      "epoch: 15 step: 589, loss is 0.0010710518108680844\n",
      "epoch: 15 step: 590, loss is 0.01766493357717991\n",
      "epoch: 15 step: 591, loss is 0.004419681616127491\n",
      "epoch: 15 step: 592, loss is 0.07827897369861603\n",
      "epoch: 15 step: 593, loss is 0.004287029150873423\n",
      "epoch: 15 step: 594, loss is 0.020883401855826378\n",
      "epoch: 15 step: 595, loss is 0.0249836053699255\n",
      "epoch: 15 step: 596, loss is 0.10247085243463516\n",
      "epoch: 15 step: 597, loss is 0.021183306351304054\n",
      "epoch: 15 step: 598, loss is 0.0007365352939814329\n",
      "epoch: 15 step: 599, loss is 0.02039119228720665\n",
      "epoch: 15 step: 600, loss is 0.07163240760564804\n",
      "epoch: 15 step: 601, loss is 0.06968357414007187\n",
      "epoch: 15 step: 602, loss is 0.028968295082449913\n",
      "epoch: 15 step: 603, loss is 0.00812661275267601\n",
      "epoch: 15 step: 604, loss is 0.05734819173812866\n",
      "epoch: 15 step: 605, loss is 0.06599549949169159\n",
      "epoch: 15 step: 606, loss is 0.010894677601754665\n",
      "epoch: 15 step: 607, loss is 0.007116766180843115\n",
      "epoch: 15 step: 608, loss is 0.005811768118292093\n",
      "epoch: 15 step: 609, loss is 0.02618020586669445\n",
      "epoch: 15 step: 610, loss is 0.005585167557001114\n",
      "epoch: 15 step: 611, loss is 0.0006065545603632927\n",
      "epoch: 15 step: 612, loss is 0.029271384701132774\n",
      "epoch: 15 step: 613, loss is 0.05052056163549423\n",
      "epoch: 15 step: 614, loss is 0.020473245531320572\n",
      "epoch: 15 step: 615, loss is 0.18755125999450684\n",
      "epoch: 15 step: 616, loss is 0.042997345328330994\n",
      "epoch: 15 step: 617, loss is 0.021897848695516586\n",
      "epoch: 15 step: 618, loss is 0.021284323185682297\n",
      "epoch: 15 step: 619, loss is 0.082940973341465\n",
      "epoch: 15 step: 620, loss is 0.05918480455875397\n",
      "epoch: 15 step: 621, loss is 0.025887107476592064\n",
      "epoch: 15 step: 622, loss is 0.006696759723126888\n",
      "epoch: 15 step: 623, loss is 0.051790691912174225\n",
      "epoch: 15 step: 624, loss is 0.052355676889419556\n",
      "epoch: 15 step: 625, loss is 0.027130454778671265\n",
      "epoch: 15 step: 626, loss is 0.018067536875605583\n",
      "epoch: 15 step: 627, loss is 0.060365229845047\n",
      "epoch: 15 step: 628, loss is 0.0009039947763085365\n",
      "epoch: 15 step: 629, loss is 0.008005719631910324\n",
      "epoch: 15 step: 630, loss is 0.020000547170639038\n",
      "epoch: 15 step: 631, loss is 0.001260753720998764\n",
      "epoch: 15 step: 632, loss is 0.006974481511861086\n",
      "epoch: 15 step: 633, loss is 0.022170420736074448\n",
      "epoch: 15 step: 634, loss is 0.011000863276422024\n",
      "epoch: 15 step: 635, loss is 0.02076023444533348\n",
      "epoch: 15 step: 636, loss is 0.01888718269765377\n",
      "epoch: 15 step: 637, loss is 0.0012607935350388288\n",
      "epoch: 15 step: 638, loss is 0.018422849476337433\n",
      "epoch: 15 step: 639, loss is 0.0019205429125577211\n",
      "epoch: 15 step: 640, loss is 0.047699663788080215\n",
      "epoch: 15 step: 641, loss is 0.003798385849222541\n",
      "epoch: 15 step: 642, loss is 0.08999013155698776\n",
      "epoch: 15 step: 643, loss is 0.10320816189050674\n",
      "epoch: 15 step: 644, loss is 0.04983735829591751\n",
      "epoch: 15 step: 645, loss is 0.017429469153285027\n",
      "epoch: 15 step: 646, loss is 0.004108906723558903\n",
      "epoch: 15 step: 647, loss is 0.004503924399614334\n",
      "epoch: 15 step: 648, loss is 0.03512682765722275\n",
      "epoch: 15 step: 649, loss is 0.05132351815700531\n",
      "epoch: 15 step: 650, loss is 0.01807009056210518\n",
      "epoch: 15 step: 651, loss is 0.006211199332028627\n",
      "epoch: 15 step: 652, loss is 0.018527422100305557\n",
      "epoch: 15 step: 653, loss is 0.04775306582450867\n",
      "epoch: 15 step: 654, loss is 0.007665233686566353\n",
      "epoch: 15 step: 655, loss is 0.00603121891617775\n",
      "epoch: 15 step: 656, loss is 0.0099525460973382\n",
      "epoch: 15 step: 657, loss is 0.056268539279699326\n",
      "epoch: 15 step: 658, loss is 0.05218943580985069\n",
      "epoch: 15 step: 659, loss is 0.0051229591481387615\n",
      "epoch: 15 step: 660, loss is 0.009231910109519958\n",
      "epoch: 15 step: 661, loss is 0.0066274311393499374\n",
      "epoch: 15 step: 662, loss is 0.016466455534100533\n",
      "epoch: 15 step: 663, loss is 0.020286191254854202\n",
      "epoch: 15 step: 664, loss is 0.04035002738237381\n",
      "epoch: 15 step: 665, loss is 0.008608329109847546\n",
      "epoch: 15 step: 666, loss is 0.011591599322855473\n",
      "epoch: 15 step: 667, loss is 0.027299756184220314\n",
      "epoch: 15 step: 668, loss is 0.015276038087904453\n",
      "epoch: 15 step: 669, loss is 0.0458187535405159\n",
      "epoch: 15 step: 670, loss is 0.008563065901398659\n",
      "epoch: 15 step: 671, loss is 0.055955782532691956\n",
      "epoch: 15 step: 672, loss is 0.005661413073539734\n",
      "epoch: 15 step: 673, loss is 0.0010073469020426273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 674, loss is 0.05937083810567856\n",
      "epoch: 15 step: 675, loss is 0.004186232108622789\n",
      "epoch: 15 step: 676, loss is 0.05060529336333275\n",
      "epoch: 15 step: 677, loss is 0.0024646741803735495\n",
      "epoch: 15 step: 678, loss is 0.001218315795995295\n",
      "epoch: 15 step: 679, loss is 0.1024809181690216\n",
      "epoch: 15 step: 680, loss is 0.03106306493282318\n",
      "epoch: 15 step: 681, loss is 0.018125060945749283\n",
      "epoch: 15 step: 682, loss is 0.03712257370352745\n",
      "epoch: 15 step: 683, loss is 0.10391745716333389\n",
      "epoch: 15 step: 684, loss is 0.002331158146262169\n",
      "epoch: 15 step: 685, loss is 0.013053584843873978\n",
      "epoch: 15 step: 686, loss is 0.030501428991556168\n",
      "epoch: 15 step: 687, loss is 0.02148493193089962\n",
      "epoch: 15 step: 688, loss is 0.052504561841487885\n",
      "epoch: 15 step: 689, loss is 0.011747447773814201\n",
      "epoch: 15 step: 690, loss is 0.012059110216796398\n",
      "epoch: 15 step: 691, loss is 0.007154041901230812\n",
      "epoch: 15 step: 692, loss is 0.07547249644994736\n",
      "epoch: 15 step: 693, loss is 0.018270105123519897\n",
      "epoch: 15 step: 694, loss is 0.08420505374670029\n",
      "epoch: 15 step: 695, loss is 0.0019839927554130554\n",
      "epoch: 15 step: 696, loss is 0.129001185297966\n",
      "epoch: 15 step: 697, loss is 0.017012199386954308\n",
      "epoch: 15 step: 698, loss is 0.07631556689739227\n",
      "epoch: 15 step: 699, loss is 0.031571678817272186\n",
      "epoch: 15 step: 700, loss is 0.02873905934393406\n",
      "epoch: 15 step: 701, loss is 0.026090744882822037\n",
      "epoch: 15 step: 702, loss is 0.01786329783499241\n",
      "epoch: 15 step: 703, loss is 0.04635831341147423\n",
      "epoch: 15 step: 704, loss is 0.0020600049756467342\n",
      "epoch: 15 step: 705, loss is 0.011832399293780327\n",
      "epoch: 15 step: 706, loss is 0.005583602469414473\n",
      "epoch: 15 step: 707, loss is 0.00333594623953104\n",
      "epoch: 15 step: 708, loss is 0.013699304312467575\n",
      "epoch: 15 step: 709, loss is 0.01029292307794094\n",
      "epoch: 15 step: 710, loss is 0.00949516799300909\n",
      "epoch: 15 step: 711, loss is 0.0024328718427568674\n",
      "epoch: 15 step: 712, loss is 0.0609186589717865\n",
      "epoch: 15 step: 713, loss is 0.03797420859336853\n",
      "epoch: 15 step: 714, loss is 0.014660640619695187\n",
      "epoch: 15 step: 715, loss is 0.14084234833717346\n",
      "epoch: 15 step: 716, loss is 0.0030382052063941956\n",
      "epoch: 15 step: 717, loss is 0.050391364842653275\n",
      "epoch: 15 step: 718, loss is 0.0265051256865263\n",
      "epoch: 15 step: 719, loss is 0.01804736629128456\n",
      "epoch: 15 step: 720, loss is 0.03569323197007179\n",
      "epoch: 15 step: 721, loss is 0.06194295361638069\n",
      "epoch: 15 step: 722, loss is 0.043657854199409485\n",
      "epoch: 15 step: 723, loss is 0.11757804453372955\n",
      "epoch: 15 step: 724, loss is 0.011969364248216152\n",
      "epoch: 15 step: 725, loss is 0.025779379531741142\n",
      "epoch: 15 step: 726, loss is 0.05705695599317551\n",
      "epoch: 15 step: 727, loss is 0.07089784741401672\n",
      "epoch: 15 step: 728, loss is 0.0029974887147545815\n",
      "epoch: 15 step: 729, loss is 0.009836321696639061\n",
      "epoch: 15 step: 730, loss is 0.010596297681331635\n",
      "epoch: 15 step: 731, loss is 0.015213407576084137\n",
      "epoch: 15 step: 732, loss is 0.019402720034122467\n",
      "epoch: 15 step: 733, loss is 0.022778697311878204\n",
      "epoch: 15 step: 734, loss is 0.07451537996530533\n",
      "epoch: 15 step: 735, loss is 0.0021311428863555193\n",
      "epoch: 15 step: 736, loss is 0.015368626452982426\n",
      "epoch: 15 step: 737, loss is 0.0031583344098180532\n",
      "epoch: 15 step: 738, loss is 0.05100110545754433\n",
      "epoch: 15 step: 739, loss is 0.03420282155275345\n",
      "epoch: 15 step: 740, loss is 0.038300685584545135\n",
      "epoch: 15 step: 741, loss is 0.07374755293130875\n",
      "epoch: 15 step: 742, loss is 0.027698388323187828\n",
      "epoch: 15 step: 743, loss is 0.07207754999399185\n",
      "epoch: 15 step: 744, loss is 0.002371782436966896\n",
      "epoch: 15 step: 745, loss is 0.025255605578422546\n",
      "epoch: 15 step: 746, loss is 0.0017222831957042217\n",
      "epoch: 15 step: 747, loss is 0.08345995843410492\n",
      "epoch: 15 step: 748, loss is 0.010250012390315533\n",
      "epoch: 15 step: 749, loss is 0.004739506170153618\n",
      "epoch: 15 step: 750, loss is 0.009175263345241547\n",
      "epoch: 15 step: 751, loss is 0.19898313283920288\n",
      "epoch: 15 step: 752, loss is 0.02017804980278015\n",
      "epoch: 15 step: 753, loss is 0.022852221503853798\n",
      "epoch: 15 step: 754, loss is 0.00027022085851058364\n",
      "epoch: 15 step: 755, loss is 0.039697159081697464\n",
      "epoch: 15 step: 756, loss is 0.022173786535859108\n",
      "epoch: 15 step: 757, loss is 0.14617224037647247\n",
      "epoch: 15 step: 758, loss is 0.1427011489868164\n",
      "epoch: 15 step: 759, loss is 0.0034075200092047453\n",
      "epoch: 15 step: 760, loss is 0.06285355240106583\n",
      "epoch: 15 step: 761, loss is 0.006183847784996033\n",
      "epoch: 15 step: 762, loss is 0.00459622498601675\n",
      "epoch: 15 step: 763, loss is 0.051113929599523544\n",
      "epoch: 15 step: 764, loss is 0.03012516349554062\n",
      "epoch: 15 step: 765, loss is 0.021232357248663902\n",
      "epoch: 15 step: 766, loss is 0.024916278198361397\n",
      "epoch: 15 step: 767, loss is 0.13099370896816254\n",
      "epoch: 15 step: 768, loss is 0.023087957873940468\n",
      "epoch: 15 step: 769, loss is 0.006956599187105894\n",
      "epoch: 15 step: 770, loss is 0.04095260798931122\n",
      "epoch: 15 step: 771, loss is 0.019007006660103798\n",
      "epoch: 15 step: 772, loss is 0.10017174482345581\n",
      "epoch: 15 step: 773, loss is 0.04531639441847801\n",
      "epoch: 15 step: 774, loss is 0.0278709065169096\n",
      "epoch: 15 step: 775, loss is 0.0169745571911335\n",
      "epoch: 15 step: 776, loss is 0.00610599759966135\n",
      "epoch: 15 step: 777, loss is 0.04749090224504471\n",
      "epoch: 15 step: 778, loss is 0.04897496476769447\n",
      "epoch: 15 step: 779, loss is 0.01110116858035326\n",
      "epoch: 15 step: 780, loss is 0.013701731339097023\n",
      "epoch: 15 step: 781, loss is 0.0019100859062746167\n",
      "epoch: 15 step: 782, loss is 0.010039743036031723\n",
      "epoch: 15 step: 783, loss is 0.016862576827406883\n",
      "epoch: 15 step: 784, loss is 0.04452591761946678\n",
      "epoch: 15 step: 785, loss is 0.009324426762759686\n",
      "epoch: 15 step: 786, loss is 0.003923716023564339\n",
      "epoch: 15 step: 787, loss is 0.022357048466801643\n",
      "epoch: 15 step: 788, loss is 0.037156712263822556\n",
      "epoch: 15 step: 789, loss is 0.0298256054520607\n",
      "epoch: 15 step: 790, loss is 0.0018615337321534753\n",
      "epoch: 15 step: 791, loss is 0.15011872351169586\n",
      "epoch: 15 step: 792, loss is 0.004989343695342541\n",
      "epoch: 15 step: 793, loss is 0.018039613962173462\n",
      "epoch: 15 step: 794, loss is 0.0032628870103508234\n",
      "epoch: 15 step: 795, loss is 0.03779752179980278\n",
      "epoch: 15 step: 796, loss is 0.019660623744130135\n",
      "epoch: 15 step: 797, loss is 0.0031043440103530884\n",
      "epoch: 15 step: 798, loss is 0.0013323130551725626\n",
      "epoch: 15 step: 799, loss is 0.0022324637975543737\n",
      "epoch: 15 step: 800, loss is 0.012389293871819973\n",
      "epoch: 15 step: 801, loss is 0.028380027040839195\n",
      "epoch: 15 step: 802, loss is 0.0207179207354784\n",
      "epoch: 15 step: 803, loss is 0.004688806366175413\n",
      "epoch: 15 step: 804, loss is 0.005058815237134695\n",
      "epoch: 15 step: 805, loss is 0.00991913303732872\n",
      "epoch: 15 step: 806, loss is 0.07861810177564621\n",
      "epoch: 15 step: 807, loss is 0.10430027544498444\n",
      "epoch: 15 step: 808, loss is 0.17866338789463043\n",
      "epoch: 15 step: 809, loss is 0.05033532530069351\n",
      "epoch: 15 step: 810, loss is 0.018877560272812843\n",
      "epoch: 15 step: 811, loss is 0.020441239699721336\n",
      "epoch: 15 step: 812, loss is 0.10271597653627396\n",
      "epoch: 15 step: 813, loss is 0.030042942613363266\n",
      "epoch: 15 step: 814, loss is 0.06267789751291275\n",
      "epoch: 15 step: 815, loss is 0.0017514160135760903\n",
      "epoch: 15 step: 816, loss is 0.056671470403671265\n",
      "epoch: 15 step: 817, loss is 0.01663529872894287\n",
      "epoch: 15 step: 818, loss is 0.04648592695593834\n",
      "epoch: 15 step: 819, loss is 0.03534473106265068\n",
      "epoch: 15 step: 820, loss is 0.06254665553569794\n",
      "epoch: 15 step: 821, loss is 0.008592470549046993\n",
      "epoch: 15 step: 822, loss is 0.00627301586791873\n",
      "epoch: 15 step: 823, loss is 0.012964673340320587\n",
      "epoch: 15 step: 824, loss is 0.0390009731054306\n",
      "epoch: 15 step: 825, loss is 0.12560723721981049\n",
      "epoch: 15 step: 826, loss is 0.07952840626239777\n",
      "epoch: 15 step: 827, loss is 0.009997096844017506\n",
      "epoch: 15 step: 828, loss is 0.0076076555997133255\n",
      "epoch: 15 step: 829, loss is 0.03280235826969147\n",
      "epoch: 15 step: 830, loss is 0.008242007344961166\n",
      "epoch: 15 step: 831, loss is 0.025533754378557205\n",
      "epoch: 15 step: 832, loss is 0.02842617593705654\n",
      "epoch: 15 step: 833, loss is 0.004600706044584513\n",
      "epoch: 15 step: 834, loss is 0.05750199407339096\n",
      "epoch: 15 step: 835, loss is 0.09029808640480042\n",
      "epoch: 15 step: 836, loss is 0.001968465046957135\n",
      "epoch: 15 step: 837, loss is 0.005746673792600632\n",
      "epoch: 15 step: 838, loss is 0.002906197914853692\n",
      "epoch: 15 step: 839, loss is 0.0028803576715290546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 840, loss is 0.03523993492126465\n",
      "epoch: 15 step: 841, loss is 0.04471005126833916\n",
      "epoch: 15 step: 842, loss is 0.005836895201355219\n",
      "epoch: 15 step: 843, loss is 0.004271925892680883\n",
      "epoch: 15 step: 844, loss is 0.05856804549694061\n",
      "epoch: 15 step: 845, loss is 0.016817830502986908\n",
      "epoch: 15 step: 846, loss is 0.04532286524772644\n",
      "epoch: 15 step: 847, loss is 0.026169124990701675\n",
      "epoch: 15 step: 848, loss is 0.04617060720920563\n",
      "epoch: 15 step: 849, loss is 0.03642580658197403\n",
      "epoch: 15 step: 850, loss is 0.008745121769607067\n",
      "epoch: 15 step: 851, loss is 0.008516408503055573\n",
      "epoch: 15 step: 852, loss is 0.0139166209846735\n",
      "epoch: 15 step: 853, loss is 0.0060952636413276196\n",
      "epoch: 15 step: 854, loss is 0.015333008952438831\n",
      "epoch: 15 step: 855, loss is 0.039969149976968765\n",
      "epoch: 15 step: 856, loss is 0.06408775597810745\n",
      "epoch: 15 step: 857, loss is 0.09109840542078018\n",
      "epoch: 15 step: 858, loss is 0.006461767479777336\n",
      "epoch: 15 step: 859, loss is 0.020252810791134834\n",
      "epoch: 15 step: 860, loss is 0.03963597118854523\n",
      "epoch: 15 step: 861, loss is 0.01148317288607359\n",
      "epoch: 15 step: 862, loss is 0.00445370189845562\n",
      "epoch: 15 step: 863, loss is 0.0025662106927484274\n",
      "epoch: 15 step: 864, loss is 0.011789822950959206\n",
      "epoch: 15 step: 865, loss is 0.012003516778349876\n",
      "epoch: 15 step: 866, loss is 0.0022623338736593723\n",
      "epoch: 15 step: 867, loss is 0.043168842792510986\n",
      "epoch: 15 step: 868, loss is 0.03984616696834564\n",
      "epoch: 15 step: 869, loss is 0.0011400737566873431\n",
      "epoch: 15 step: 870, loss is 0.05412045121192932\n",
      "epoch: 15 step: 871, loss is 0.0050455546006560326\n",
      "epoch: 15 step: 872, loss is 0.040549419820308685\n",
      "epoch: 15 step: 873, loss is 0.1450500190258026\n",
      "epoch: 15 step: 874, loss is 0.0167187862098217\n",
      "epoch: 15 step: 875, loss is 0.0066175623796880245\n",
      "epoch: 15 step: 876, loss is 0.02985098958015442\n",
      "epoch: 15 step: 877, loss is 0.02163049764931202\n",
      "epoch: 15 step: 878, loss is 0.059670668095350266\n",
      "epoch: 15 step: 879, loss is 0.11141356825828552\n",
      "epoch: 15 step: 880, loss is 0.021073155105113983\n",
      "epoch: 15 step: 881, loss is 0.03166161850094795\n",
      "epoch: 15 step: 882, loss is 0.002558064181357622\n",
      "epoch: 15 step: 883, loss is 0.08708640933036804\n",
      "epoch: 15 step: 884, loss is 0.02335798740386963\n",
      "epoch: 15 step: 885, loss is 0.11692968755960464\n",
      "epoch: 15 step: 886, loss is 0.027479741722345352\n",
      "epoch: 15 step: 887, loss is 0.007381436415016651\n",
      "epoch: 15 step: 888, loss is 0.06030693277716637\n",
      "epoch: 15 step: 889, loss is 0.0712529644370079\n",
      "epoch: 15 step: 890, loss is 0.0198916494846344\n",
      "epoch: 15 step: 891, loss is 0.005386848468333483\n",
      "epoch: 15 step: 892, loss is 0.015189369209110737\n",
      "epoch: 15 step: 893, loss is 0.13505440950393677\n",
      "epoch: 15 step: 894, loss is 0.05042795091867447\n",
      "epoch: 15 step: 895, loss is 0.03814028948545456\n",
      "epoch: 15 step: 896, loss is 0.005579160992056131\n",
      "epoch: 15 step: 897, loss is 0.006724490784108639\n",
      "epoch: 15 step: 898, loss is 0.03025217168033123\n",
      "epoch: 15 step: 899, loss is 0.02032712660729885\n",
      "epoch: 15 step: 900, loss is 0.03722598776221275\n",
      "epoch: 15 step: 901, loss is 0.03296884149312973\n",
      "epoch: 15 step: 902, loss is 0.054804980754852295\n",
      "epoch: 15 step: 903, loss is 0.01611475832760334\n",
      "epoch: 15 step: 904, loss is 0.10979307442903519\n",
      "epoch: 15 step: 905, loss is 0.009262089617550373\n",
      "epoch: 15 step: 906, loss is 0.08247532695531845\n",
      "epoch: 15 step: 907, loss is 0.07706180214881897\n",
      "epoch: 15 step: 908, loss is 0.08613020926713943\n",
      "epoch: 15 step: 909, loss is 0.014497504569590092\n",
      "epoch: 15 step: 910, loss is 0.003512054216116667\n",
      "epoch: 15 step: 911, loss is 0.035952575504779816\n",
      "epoch: 15 step: 912, loss is 0.10903098434209824\n",
      "epoch: 15 step: 913, loss is 0.06011444702744484\n",
      "epoch: 15 step: 914, loss is 0.02640947699546814\n",
      "epoch: 15 step: 915, loss is 0.01643814519047737\n",
      "epoch: 15 step: 916, loss is 0.041014257818460464\n",
      "epoch: 15 step: 917, loss is 0.04789824038743973\n",
      "epoch: 15 step: 918, loss is 0.04438663274049759\n",
      "epoch: 15 step: 919, loss is 0.018607890233397484\n",
      "epoch: 15 step: 920, loss is 0.010450417175889015\n",
      "epoch: 15 step: 921, loss is 0.10575967282056808\n",
      "epoch: 15 step: 922, loss is 0.002354021416977048\n",
      "epoch: 15 step: 923, loss is 0.016005873680114746\n",
      "epoch: 15 step: 924, loss is 0.032050129026174545\n",
      "epoch: 15 step: 925, loss is 0.07724295556545258\n",
      "epoch: 15 step: 926, loss is 0.013913425616919994\n",
      "epoch: 15 step: 927, loss is 0.1332642287015915\n",
      "epoch: 15 step: 928, loss is 0.002231841441243887\n",
      "epoch: 15 step: 929, loss is 0.09461253881454468\n",
      "epoch: 15 step: 930, loss is 0.026021456345915794\n",
      "epoch: 15 step: 931, loss is 0.002638579346239567\n",
      "epoch: 15 step: 932, loss is 0.0520363375544548\n",
      "epoch: 15 step: 933, loss is 0.025566834956407547\n",
      "epoch: 15 step: 934, loss is 0.04164434224367142\n",
      "epoch: 15 step: 935, loss is 0.043702609837055206\n",
      "epoch: 15 step: 936, loss is 0.0161286611109972\n",
      "epoch: 15 step: 937, loss is 0.02120930142700672\n",
      "epoch: 16 step: 1, loss is 0.004729659296572208\n",
      "epoch: 16 step: 2, loss is 0.00935317948460579\n",
      "epoch: 16 step: 3, loss is 0.013435520231723785\n",
      "epoch: 16 step: 4, loss is 0.012406476773321629\n",
      "epoch: 16 step: 5, loss is 0.04782691225409508\n",
      "epoch: 16 step: 6, loss is 0.01802552118897438\n",
      "epoch: 16 step: 7, loss is 0.03858537599444389\n",
      "epoch: 16 step: 8, loss is 0.01067646499723196\n",
      "epoch: 16 step: 9, loss is 0.0022356808185577393\n",
      "epoch: 16 step: 10, loss is 0.006315496750175953\n",
      "epoch: 16 step: 11, loss is 0.01584392786026001\n",
      "epoch: 16 step: 12, loss is 0.05390192195773125\n",
      "epoch: 16 step: 13, loss is 0.006735631730407476\n",
      "epoch: 16 step: 14, loss is 0.06765642762184143\n",
      "epoch: 16 step: 15, loss is 0.02128514088690281\n",
      "epoch: 16 step: 16, loss is 0.04192747175693512\n",
      "epoch: 16 step: 17, loss is 0.021645085886120796\n",
      "epoch: 16 step: 18, loss is 0.003127896226942539\n",
      "epoch: 16 step: 19, loss is 0.012981217354536057\n",
      "epoch: 16 step: 20, loss is 0.040098804980516434\n",
      "epoch: 16 step: 21, loss is 0.003086308017373085\n",
      "epoch: 16 step: 22, loss is 0.0008637076243758202\n",
      "epoch: 16 step: 23, loss is 0.027729609981179237\n",
      "epoch: 16 step: 24, loss is 0.008811410516500473\n",
      "epoch: 16 step: 25, loss is 0.005278067663311958\n",
      "epoch: 16 step: 26, loss is 0.08946814388036728\n",
      "epoch: 16 step: 27, loss is 0.008616378530859947\n",
      "epoch: 16 step: 28, loss is 0.0162387415766716\n",
      "epoch: 16 step: 29, loss is 0.06866971403360367\n",
      "epoch: 16 step: 30, loss is 0.009779070504009724\n",
      "epoch: 16 step: 31, loss is 0.018359513953328133\n",
      "epoch: 16 step: 32, loss is 0.01445720810443163\n",
      "epoch: 16 step: 33, loss is 0.018778912723064423\n",
      "epoch: 16 step: 34, loss is 0.12760108709335327\n",
      "epoch: 16 step: 35, loss is 0.02114247903227806\n",
      "epoch: 16 step: 36, loss is 0.04405149444937706\n",
      "epoch: 16 step: 37, loss is 0.0018632111605256796\n",
      "epoch: 16 step: 38, loss is 0.0005902832490392029\n",
      "epoch: 16 step: 39, loss is 0.012467216700315475\n",
      "epoch: 16 step: 40, loss is 0.034697264432907104\n",
      "epoch: 16 step: 41, loss is 0.042460281401872635\n",
      "epoch: 16 step: 42, loss is 0.006522186566144228\n",
      "epoch: 16 step: 43, loss is 0.0019797536078840494\n",
      "epoch: 16 step: 44, loss is 0.02296978421509266\n",
      "epoch: 16 step: 45, loss is 0.004807517863810062\n",
      "epoch: 16 step: 46, loss is 0.018779149278998375\n",
      "epoch: 16 step: 47, loss is 0.05897832289338112\n",
      "epoch: 16 step: 48, loss is 0.007411489263176918\n",
      "epoch: 16 step: 49, loss is 0.007331342902034521\n",
      "epoch: 16 step: 50, loss is 0.01193825900554657\n",
      "epoch: 16 step: 51, loss is 0.04175801947712898\n",
      "epoch: 16 step: 52, loss is 0.03892313688993454\n",
      "epoch: 16 step: 53, loss is 0.07400233298540115\n",
      "epoch: 16 step: 54, loss is 0.016050556674599648\n",
      "epoch: 16 step: 55, loss is 0.0107572665438056\n",
      "epoch: 16 step: 56, loss is 0.007910870015621185\n",
      "epoch: 16 step: 57, loss is 0.00789871159940958\n",
      "epoch: 16 step: 58, loss is 0.06389020383358002\n",
      "epoch: 16 step: 59, loss is 0.021993281319737434\n",
      "epoch: 16 step: 60, loss is 0.041854895651340485\n",
      "epoch: 16 step: 61, loss is 0.001067867036908865\n",
      "epoch: 16 step: 62, loss is 0.016659902408719063\n",
      "epoch: 16 step: 63, loss is 0.012315365485846996\n",
      "epoch: 16 step: 64, loss is 0.027800122275948524\n",
      "epoch: 16 step: 65, loss is 0.005338441114872694\n",
      "epoch: 16 step: 66, loss is 0.01799354888498783\n",
      "epoch: 16 step: 67, loss is 0.010005556978285313\n",
      "epoch: 16 step: 68, loss is 0.02133515477180481\n",
      "epoch: 16 step: 69, loss is 0.00333052733913064\n",
      "epoch: 16 step: 70, loss is 0.0500507727265358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 71, loss is 0.0025238427333533764\n",
      "epoch: 16 step: 72, loss is 0.014529382809996605\n",
      "epoch: 16 step: 73, loss is 0.0044844928197562695\n",
      "epoch: 16 step: 74, loss is 0.037911828607320786\n",
      "epoch: 16 step: 75, loss is 0.017845580354332924\n",
      "epoch: 16 step: 76, loss is 0.011194591410458088\n",
      "epoch: 16 step: 77, loss is 0.01696736179292202\n",
      "epoch: 16 step: 78, loss is 0.019167909398674965\n",
      "epoch: 16 step: 79, loss is 0.007742116693407297\n",
      "epoch: 16 step: 80, loss is 0.02102503553032875\n",
      "epoch: 16 step: 81, loss is 0.09176060557365417\n",
      "epoch: 16 step: 82, loss is 0.003771890187636018\n",
      "epoch: 16 step: 83, loss is 0.006935157813131809\n",
      "epoch: 16 step: 84, loss is 0.016544891521334648\n",
      "epoch: 16 step: 85, loss is 0.0007522449013777077\n",
      "epoch: 16 step: 86, loss is 0.021334633231163025\n",
      "epoch: 16 step: 87, loss is 0.015098974108695984\n",
      "epoch: 16 step: 88, loss is 0.011306382715702057\n",
      "epoch: 16 step: 89, loss is 0.000900700397323817\n",
      "epoch: 16 step: 90, loss is 0.08263085037469864\n",
      "epoch: 16 step: 91, loss is 0.0022751730866730213\n",
      "epoch: 16 step: 92, loss is 0.0005132418591529131\n",
      "epoch: 16 step: 93, loss is 0.08377131074666977\n",
      "epoch: 16 step: 94, loss is 0.005985083989799023\n",
      "epoch: 16 step: 95, loss is 0.0027196977753192186\n",
      "epoch: 16 step: 96, loss is 0.0033061853609979153\n",
      "epoch: 16 step: 97, loss is 0.0699222981929779\n",
      "epoch: 16 step: 98, loss is 0.007409232668578625\n",
      "epoch: 16 step: 99, loss is 0.027072282508015633\n",
      "epoch: 16 step: 100, loss is 0.007479297462850809\n",
      "epoch: 16 step: 101, loss is 0.0073401848785579205\n",
      "epoch: 16 step: 102, loss is 0.002962622093036771\n",
      "epoch: 16 step: 103, loss is 0.003812360344454646\n",
      "epoch: 16 step: 104, loss is 0.025590525940060616\n",
      "epoch: 16 step: 105, loss is 0.015723178163170815\n",
      "epoch: 16 step: 106, loss is 0.006582051049917936\n",
      "epoch: 16 step: 107, loss is 0.01476473268121481\n",
      "epoch: 16 step: 108, loss is 0.0012286133132874966\n",
      "epoch: 16 step: 109, loss is 0.0015726081328466535\n",
      "epoch: 16 step: 110, loss is 0.0006114864954724908\n",
      "epoch: 16 step: 111, loss is 0.07610848546028137\n",
      "epoch: 16 step: 112, loss is 0.05277079716324806\n",
      "epoch: 16 step: 113, loss is 0.06495477259159088\n",
      "epoch: 16 step: 114, loss is 0.009767623618245125\n",
      "epoch: 16 step: 115, loss is 0.003388964803889394\n",
      "epoch: 16 step: 116, loss is 0.01693316549062729\n",
      "epoch: 16 step: 117, loss is 0.029778460040688515\n",
      "epoch: 16 step: 118, loss is 0.017526693642139435\n",
      "epoch: 16 step: 119, loss is 0.007127336226403713\n",
      "epoch: 16 step: 120, loss is 0.006251988932490349\n",
      "epoch: 16 step: 121, loss is 0.023808179423213005\n",
      "epoch: 16 step: 122, loss is 0.002907396527007222\n",
      "epoch: 16 step: 123, loss is 0.0015933573013171554\n",
      "epoch: 16 step: 124, loss is 0.0006568264216184616\n",
      "epoch: 16 step: 125, loss is 0.001674972358159721\n",
      "epoch: 16 step: 126, loss is 0.004093068186193705\n",
      "epoch: 16 step: 127, loss is 0.0019388305954635143\n",
      "epoch: 16 step: 128, loss is 0.027698896825313568\n",
      "epoch: 16 step: 129, loss is 0.05707968771457672\n",
      "epoch: 16 step: 130, loss is 0.026424875482916832\n",
      "epoch: 16 step: 131, loss is 0.0030011218041181564\n",
      "epoch: 16 step: 132, loss is 0.03824996203184128\n",
      "epoch: 16 step: 133, loss is 0.015918098390102386\n",
      "epoch: 16 step: 134, loss is 0.0014144940068945289\n",
      "epoch: 16 step: 135, loss is 0.03535373508930206\n",
      "epoch: 16 step: 136, loss is 0.0017339115729555488\n",
      "epoch: 16 step: 137, loss is 0.03863004222512245\n",
      "epoch: 16 step: 138, loss is 0.0016419293824583292\n",
      "epoch: 16 step: 139, loss is 0.0023167221806943417\n",
      "epoch: 16 step: 140, loss is 0.007284475956112146\n",
      "epoch: 16 step: 141, loss is 0.0016744780587032437\n",
      "epoch: 16 step: 142, loss is 0.00569693511351943\n",
      "epoch: 16 step: 143, loss is 0.008844534866511822\n",
      "epoch: 16 step: 144, loss is 0.07093973457813263\n",
      "epoch: 16 step: 145, loss is 0.013027932494878769\n",
      "epoch: 16 step: 146, loss is 0.00014328185352496803\n",
      "epoch: 16 step: 147, loss is 0.0010059763444587588\n",
      "epoch: 16 step: 148, loss is 0.007710601668804884\n",
      "epoch: 16 step: 149, loss is 0.014934348873794079\n",
      "epoch: 16 step: 150, loss is 0.003466526046395302\n",
      "epoch: 16 step: 151, loss is 0.0009515306446701288\n",
      "epoch: 16 step: 152, loss is 0.006170213222503662\n",
      "epoch: 16 step: 153, loss is 0.0015215636231005192\n",
      "epoch: 16 step: 154, loss is 0.0014128027250990272\n",
      "epoch: 16 step: 155, loss is 0.018646683543920517\n",
      "epoch: 16 step: 156, loss is 0.02432817779481411\n",
      "epoch: 16 step: 157, loss is 0.05721824988722801\n",
      "epoch: 16 step: 158, loss is 0.0024323242250829935\n",
      "epoch: 16 step: 159, loss is 0.01041489653289318\n",
      "epoch: 16 step: 160, loss is 0.037329599261283875\n",
      "epoch: 16 step: 161, loss is 0.025537438690662384\n",
      "epoch: 16 step: 162, loss is 0.0250460896641016\n",
      "epoch: 16 step: 163, loss is 0.002988348016515374\n",
      "epoch: 16 step: 164, loss is 0.006528011988848448\n",
      "epoch: 16 step: 165, loss is 0.0013004675274714828\n",
      "epoch: 16 step: 166, loss is 0.01518145203590393\n",
      "epoch: 16 step: 167, loss is 0.009124036878347397\n",
      "epoch: 16 step: 168, loss is 0.005819070618599653\n",
      "epoch: 16 step: 169, loss is 0.016831206157803535\n",
      "epoch: 16 step: 170, loss is 0.00288196699693799\n",
      "epoch: 16 step: 171, loss is 0.014677586033940315\n",
      "epoch: 16 step: 172, loss is 0.046321455389261246\n",
      "epoch: 16 step: 173, loss is 0.03301771357655525\n",
      "epoch: 16 step: 174, loss is 0.009443897753953934\n",
      "epoch: 16 step: 175, loss is 0.008760041557252407\n",
      "epoch: 16 step: 176, loss is 0.0020045184064656496\n",
      "epoch: 16 step: 177, loss is 0.014662299305200577\n",
      "epoch: 16 step: 178, loss is 0.006904578767716885\n",
      "epoch: 16 step: 179, loss is 0.0020785140804946423\n",
      "epoch: 16 step: 180, loss is 0.0026216250844299793\n",
      "epoch: 16 step: 181, loss is 0.002121819881722331\n",
      "epoch: 16 step: 182, loss is 0.008509896695613861\n",
      "epoch: 16 step: 183, loss is 0.011394388973712921\n",
      "epoch: 16 step: 184, loss is 0.00040857595740817487\n",
      "epoch: 16 step: 185, loss is 0.0002804750401992351\n",
      "epoch: 16 step: 186, loss is 0.0124997952952981\n",
      "epoch: 16 step: 187, loss is 0.02398916892707348\n",
      "epoch: 16 step: 188, loss is 0.010531092993915081\n",
      "epoch: 16 step: 189, loss is 0.029608113691210747\n",
      "epoch: 16 step: 190, loss is 0.05843918398022652\n",
      "epoch: 16 step: 191, loss is 0.008038688451051712\n",
      "epoch: 16 step: 192, loss is 0.042206794023513794\n",
      "epoch: 16 step: 193, loss is 0.00487388763576746\n",
      "epoch: 16 step: 194, loss is 0.009531528688967228\n",
      "epoch: 16 step: 195, loss is 0.016042646020650864\n",
      "epoch: 16 step: 196, loss is 0.01176689937710762\n",
      "epoch: 16 step: 197, loss is 0.007125456817448139\n",
      "epoch: 16 step: 198, loss is 0.0061952173709869385\n",
      "epoch: 16 step: 199, loss is 0.047176558524370193\n",
      "epoch: 16 step: 200, loss is 0.035615745931863785\n",
      "epoch: 16 step: 201, loss is 0.012340668588876724\n",
      "epoch: 16 step: 202, loss is 0.05204608291387558\n",
      "epoch: 16 step: 203, loss is 0.0019516109023243189\n",
      "epoch: 16 step: 204, loss is 0.001647953991778195\n",
      "epoch: 16 step: 205, loss is 0.003252260386943817\n",
      "epoch: 16 step: 206, loss is 0.01958080753684044\n",
      "epoch: 16 step: 207, loss is 0.01846349611878395\n",
      "epoch: 16 step: 208, loss is 0.004436166025698185\n",
      "epoch: 16 step: 209, loss is 0.008190464228391647\n",
      "epoch: 16 step: 210, loss is 0.0024432113859802485\n",
      "epoch: 16 step: 211, loss is 0.032174039632081985\n",
      "epoch: 16 step: 212, loss is 0.002707053441554308\n",
      "epoch: 16 step: 213, loss is 0.031050531193614006\n",
      "epoch: 16 step: 214, loss is 0.011665272526443005\n",
      "epoch: 16 step: 215, loss is 0.0021197095047682524\n",
      "epoch: 16 step: 216, loss is 0.002295992337167263\n",
      "epoch: 16 step: 217, loss is 0.009745028801262379\n",
      "epoch: 16 step: 218, loss is 0.006363137625157833\n",
      "epoch: 16 step: 219, loss is 0.03775593638420105\n",
      "epoch: 16 step: 220, loss is 0.005808460991829634\n",
      "epoch: 16 step: 221, loss is 0.009170504286885262\n",
      "epoch: 16 step: 222, loss is 0.0025596939958631992\n",
      "epoch: 16 step: 223, loss is 0.007287596818059683\n",
      "epoch: 16 step: 224, loss is 0.002137581119313836\n",
      "epoch: 16 step: 225, loss is 0.015175579115748405\n",
      "epoch: 16 step: 226, loss is 0.003953580744564533\n",
      "epoch: 16 step: 227, loss is 0.013084282167255878\n",
      "epoch: 16 step: 228, loss is 0.02644999511539936\n",
      "epoch: 16 step: 229, loss is 0.0010744768660515547\n",
      "epoch: 16 step: 230, loss is 0.003221754450351\n",
      "epoch: 16 step: 231, loss is 0.001682455651462078\n",
      "epoch: 16 step: 232, loss is 0.015279884450137615\n",
      "epoch: 16 step: 233, loss is 0.028872190043330193\n",
      "epoch: 16 step: 234, loss is 0.036183539777994156\n",
      "epoch: 16 step: 235, loss is 0.008662374690175056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 236, loss is 0.0011163465678691864\n",
      "epoch: 16 step: 237, loss is 0.06516372412443161\n",
      "epoch: 16 step: 238, loss is 0.04257509484887123\n",
      "epoch: 16 step: 239, loss is 0.02319158799946308\n",
      "epoch: 16 step: 240, loss is 0.03102956712245941\n",
      "epoch: 16 step: 241, loss is 0.01700453646481037\n",
      "epoch: 16 step: 242, loss is 0.017400655895471573\n",
      "epoch: 16 step: 243, loss is 0.04352913424372673\n",
      "epoch: 16 step: 244, loss is 0.0002335123426746577\n",
      "epoch: 16 step: 245, loss is 0.12956783175468445\n",
      "epoch: 16 step: 246, loss is 0.0015327123692259192\n",
      "epoch: 16 step: 247, loss is 0.016519946977496147\n",
      "epoch: 16 step: 248, loss is 0.07894834876060486\n",
      "epoch: 16 step: 249, loss is 0.033407554030418396\n",
      "epoch: 16 step: 250, loss is 0.04058914631605148\n",
      "epoch: 16 step: 251, loss is 0.038695190101861954\n",
      "epoch: 16 step: 252, loss is 0.06542349606752396\n",
      "epoch: 16 step: 253, loss is 0.05037200450897217\n",
      "epoch: 16 step: 254, loss is 0.015688253566622734\n",
      "epoch: 16 step: 255, loss is 0.028902273625135422\n",
      "epoch: 16 step: 256, loss is 0.037270866334438324\n",
      "epoch: 16 step: 257, loss is 0.002306864131242037\n",
      "epoch: 16 step: 258, loss is 0.033786334097385406\n",
      "epoch: 16 step: 259, loss is 0.0007140369853004813\n",
      "epoch: 16 step: 260, loss is 0.028637081384658813\n",
      "epoch: 16 step: 261, loss is 0.06388692557811737\n",
      "epoch: 16 step: 262, loss is 0.008278955705463886\n",
      "epoch: 16 step: 263, loss is 0.0013109422288835049\n",
      "epoch: 16 step: 264, loss is 0.038192324340343475\n",
      "epoch: 16 step: 265, loss is 0.07421720027923584\n",
      "epoch: 16 step: 266, loss is 0.007275834213942289\n",
      "epoch: 16 step: 267, loss is 0.006847171112895012\n",
      "epoch: 16 step: 268, loss is 0.025715665891766548\n",
      "epoch: 16 step: 269, loss is 0.0014925331342965364\n",
      "epoch: 16 step: 270, loss is 0.015352636575698853\n",
      "epoch: 16 step: 271, loss is 0.0072748661041259766\n",
      "epoch: 16 step: 272, loss is 0.011153635568916798\n",
      "epoch: 16 step: 273, loss is 0.015951553359627724\n",
      "epoch: 16 step: 274, loss is 0.023052556440234184\n",
      "epoch: 16 step: 275, loss is 0.011375512927770615\n",
      "epoch: 16 step: 276, loss is 0.017360374331474304\n",
      "epoch: 16 step: 277, loss is 0.02475687675178051\n",
      "epoch: 16 step: 278, loss is 0.0006849844357930124\n",
      "epoch: 16 step: 279, loss is 0.012070951983332634\n",
      "epoch: 16 step: 280, loss is 0.011402774602174759\n",
      "epoch: 16 step: 281, loss is 0.0284438319504261\n",
      "epoch: 16 step: 282, loss is 0.007471009157598019\n",
      "epoch: 16 step: 283, loss is 0.047029100358486176\n",
      "epoch: 16 step: 284, loss is 0.002550095319747925\n",
      "epoch: 16 step: 285, loss is 0.0023773317225277424\n",
      "epoch: 16 step: 286, loss is 0.05429068207740784\n",
      "epoch: 16 step: 287, loss is 0.0019553506281226873\n",
      "epoch: 16 step: 288, loss is 0.021480727940797806\n",
      "epoch: 16 step: 289, loss is 0.0028096139430999756\n",
      "epoch: 16 step: 290, loss is 0.03294023871421814\n",
      "epoch: 16 step: 291, loss is 0.006537940353155136\n",
      "epoch: 16 step: 292, loss is 0.11403582990169525\n",
      "epoch: 16 step: 293, loss is 0.005656592547893524\n",
      "epoch: 16 step: 294, loss is 0.0009538816520944238\n",
      "epoch: 16 step: 295, loss is 0.007004481740295887\n",
      "epoch: 16 step: 296, loss is 0.049257077276706696\n",
      "epoch: 16 step: 297, loss is 0.00032110107713378966\n",
      "epoch: 16 step: 298, loss is 0.018251242116093636\n",
      "epoch: 16 step: 299, loss is 0.0012200407218188047\n",
      "epoch: 16 step: 300, loss is 0.0036801390815526247\n",
      "epoch: 16 step: 301, loss is 0.024677203968167305\n",
      "epoch: 16 step: 302, loss is 0.01937262713909149\n",
      "epoch: 16 step: 303, loss is 0.002622496336698532\n",
      "epoch: 16 step: 304, loss is 0.00921179261058569\n",
      "epoch: 16 step: 305, loss is 0.0001979675143957138\n",
      "epoch: 16 step: 306, loss is 0.034961868077516556\n",
      "epoch: 16 step: 307, loss is 0.04910349100828171\n",
      "epoch: 16 step: 308, loss is 0.005040856543928385\n",
      "epoch: 16 step: 309, loss is 0.00939573347568512\n",
      "epoch: 16 step: 310, loss is 0.004870170261710882\n",
      "epoch: 16 step: 311, loss is 0.0028603612445294857\n",
      "epoch: 16 step: 312, loss is 0.004336048383265734\n",
      "epoch: 16 step: 313, loss is 0.01930062472820282\n",
      "epoch: 16 step: 314, loss is 0.0033427139278501272\n",
      "epoch: 16 step: 315, loss is 0.021468255668878555\n",
      "epoch: 16 step: 316, loss is 0.007887468673288822\n",
      "epoch: 16 step: 317, loss is 0.0010443152859807014\n",
      "epoch: 16 step: 318, loss is 0.0025936865713447332\n",
      "epoch: 16 step: 319, loss is 0.011790379881858826\n",
      "epoch: 16 step: 320, loss is 0.0009111878462135792\n",
      "epoch: 16 step: 321, loss is 0.01665147952735424\n",
      "epoch: 16 step: 322, loss is 0.020125137642025948\n",
      "epoch: 16 step: 323, loss is 0.036970481276512146\n",
      "epoch: 16 step: 324, loss is 0.0033146559726446867\n",
      "epoch: 16 step: 325, loss is 0.005771460942924023\n",
      "epoch: 16 step: 326, loss is 0.013054799288511276\n",
      "epoch: 16 step: 327, loss is 0.009748425334692001\n",
      "epoch: 16 step: 328, loss is 0.005749556235969067\n",
      "epoch: 16 step: 329, loss is 0.005250929854810238\n",
      "epoch: 16 step: 330, loss is 0.03604794293642044\n",
      "epoch: 16 step: 331, loss is 0.0007049027481116354\n",
      "epoch: 16 step: 332, loss is 0.0005836114869453013\n",
      "epoch: 16 step: 333, loss is 0.0020246689673513174\n",
      "epoch: 16 step: 334, loss is 0.02761930413544178\n",
      "epoch: 16 step: 335, loss is 0.0006996411830186844\n",
      "epoch: 16 step: 336, loss is 0.003751394571736455\n",
      "epoch: 16 step: 337, loss is 0.012531262822449207\n",
      "epoch: 16 step: 338, loss is 0.00305655924603343\n",
      "epoch: 16 step: 339, loss is 0.0013123563257977366\n",
      "epoch: 16 step: 340, loss is 0.02065626159310341\n",
      "epoch: 16 step: 341, loss is 0.01657634600996971\n",
      "epoch: 16 step: 342, loss is 0.00460664601996541\n",
      "epoch: 16 step: 343, loss is 0.0057981484569609165\n",
      "epoch: 16 step: 344, loss is 0.0738721713423729\n",
      "epoch: 16 step: 345, loss is 0.0058746980503201485\n",
      "epoch: 16 step: 346, loss is 0.0020349733531475067\n",
      "epoch: 16 step: 347, loss is 0.004472764208912849\n",
      "epoch: 16 step: 348, loss is 0.011115357279777527\n",
      "epoch: 16 step: 349, loss is 0.008927580900490284\n",
      "epoch: 16 step: 350, loss is 0.10010109096765518\n",
      "epoch: 16 step: 351, loss is 0.04436362162232399\n",
      "epoch: 16 step: 352, loss is 0.002137580653652549\n",
      "epoch: 16 step: 353, loss is 0.01426466554403305\n",
      "epoch: 16 step: 354, loss is 0.039195287972688675\n",
      "epoch: 16 step: 355, loss is 0.023516908288002014\n",
      "epoch: 16 step: 356, loss is 0.013585502281785011\n",
      "epoch: 16 step: 357, loss is 0.0016347984783351421\n",
      "epoch: 16 step: 358, loss is 0.016332946717739105\n",
      "epoch: 16 step: 359, loss is 0.037156425416469574\n",
      "epoch: 16 step: 360, loss is 0.010625945404171944\n",
      "epoch: 16 step: 361, loss is 0.01939663104712963\n",
      "epoch: 16 step: 362, loss is 0.009021683596074581\n",
      "epoch: 16 step: 363, loss is 0.003785052802413702\n",
      "epoch: 16 step: 364, loss is 0.018513767048716545\n",
      "epoch: 16 step: 365, loss is 0.05924917757511139\n",
      "epoch: 16 step: 366, loss is 0.029996445402503014\n",
      "epoch: 16 step: 367, loss is 0.013430630788207054\n",
      "epoch: 16 step: 368, loss is 0.00881136953830719\n",
      "epoch: 16 step: 369, loss is 0.03540363162755966\n",
      "epoch: 16 step: 370, loss is 0.007460469380021095\n",
      "epoch: 16 step: 371, loss is 0.0015384022844955325\n",
      "epoch: 16 step: 372, loss is 0.012258961796760559\n",
      "epoch: 16 step: 373, loss is 0.0015191553393378854\n",
      "epoch: 16 step: 374, loss is 0.008052323013544083\n",
      "epoch: 16 step: 375, loss is 0.0014384418027475476\n",
      "epoch: 16 step: 376, loss is 0.008473928086459637\n",
      "epoch: 16 step: 377, loss is 0.006216302048414946\n",
      "epoch: 16 step: 378, loss is 0.00018958038708660752\n",
      "epoch: 16 step: 379, loss is 0.013467914424836636\n",
      "epoch: 16 step: 380, loss is 0.04489026218652725\n",
      "epoch: 16 step: 381, loss is 0.006335160229355097\n",
      "epoch: 16 step: 382, loss is 0.0008953851647675037\n",
      "epoch: 16 step: 383, loss is 0.0026401602663099766\n",
      "epoch: 16 step: 384, loss is 0.0011783601948991418\n",
      "epoch: 16 step: 385, loss is 0.00768604502081871\n",
      "epoch: 16 step: 386, loss is 0.0014624246396124363\n",
      "epoch: 16 step: 387, loss is 0.002759686904028058\n",
      "epoch: 16 step: 388, loss is 0.024179380387067795\n",
      "epoch: 16 step: 389, loss is 0.016414828598499298\n",
      "epoch: 16 step: 390, loss is 0.009606963954865932\n",
      "epoch: 16 step: 391, loss is 0.01022370345890522\n",
      "epoch: 16 step: 392, loss is 0.011128651909530163\n",
      "epoch: 16 step: 393, loss is 0.009769067168235779\n",
      "epoch: 16 step: 394, loss is 0.0015931781381368637\n",
      "epoch: 16 step: 395, loss is 0.0012956581776961684\n",
      "epoch: 16 step: 396, loss is 0.024872930720448494\n",
      "epoch: 16 step: 397, loss is 0.006645489949733019\n",
      "epoch: 16 step: 398, loss is 0.08052859455347061\n",
      "epoch: 16 step: 399, loss is 0.0027922019362449646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 400, loss is 0.001199045218527317\n",
      "epoch: 16 step: 401, loss is 0.008682086132466793\n",
      "epoch: 16 step: 402, loss is 0.0034484094940125942\n",
      "epoch: 16 step: 403, loss is 0.0029185295570641756\n",
      "epoch: 16 step: 404, loss is 0.010884488001465797\n",
      "epoch: 16 step: 405, loss is 0.0005338379414752126\n",
      "epoch: 16 step: 406, loss is 0.0019922591745853424\n",
      "epoch: 16 step: 407, loss is 0.020830649882555008\n",
      "epoch: 16 step: 408, loss is 0.057688403874635696\n",
      "epoch: 16 step: 409, loss is 0.01995062828063965\n",
      "epoch: 16 step: 410, loss is 0.005936057772487402\n",
      "epoch: 16 step: 411, loss is 0.006450631655752659\n",
      "epoch: 16 step: 412, loss is 0.09920739382505417\n",
      "epoch: 16 step: 413, loss is 0.013464020565152168\n",
      "epoch: 16 step: 414, loss is 0.000662944104988128\n",
      "epoch: 16 step: 415, loss is 0.0008920277468860149\n",
      "epoch: 16 step: 416, loss is 0.015307583846151829\n",
      "epoch: 16 step: 417, loss is 0.03130751848220825\n",
      "epoch: 16 step: 418, loss is 0.00314309261739254\n",
      "epoch: 16 step: 419, loss is 0.009865391999483109\n",
      "epoch: 16 step: 420, loss is 0.0008158429409377277\n",
      "epoch: 16 step: 421, loss is 0.0026694335974752903\n",
      "epoch: 16 step: 422, loss is 0.03163030371069908\n",
      "epoch: 16 step: 423, loss is 0.004553751554340124\n",
      "epoch: 16 step: 424, loss is 0.006526646204292774\n",
      "epoch: 16 step: 425, loss is 0.0010441754711791873\n",
      "epoch: 16 step: 426, loss is 0.001488376292400062\n",
      "epoch: 16 step: 427, loss is 0.00765736261382699\n",
      "epoch: 16 step: 428, loss is 0.019559161737561226\n",
      "epoch: 16 step: 429, loss is 0.03917143493890762\n",
      "epoch: 16 step: 430, loss is 0.006783267017453909\n",
      "epoch: 16 step: 431, loss is 0.023442352190613747\n",
      "epoch: 16 step: 432, loss is 0.019922375679016113\n",
      "epoch: 16 step: 433, loss is 0.013416074216365814\n",
      "epoch: 16 step: 434, loss is 0.02293381839990616\n",
      "epoch: 16 step: 435, loss is 0.003730625379830599\n",
      "epoch: 16 step: 436, loss is 0.01166875846683979\n",
      "epoch: 16 step: 437, loss is 0.039742954075336456\n",
      "epoch: 16 step: 438, loss is 0.01997511461377144\n",
      "epoch: 16 step: 439, loss is 0.004844130016863346\n",
      "epoch: 16 step: 440, loss is 0.02401133067905903\n",
      "epoch: 16 step: 441, loss is 0.002349335700273514\n",
      "epoch: 16 step: 442, loss is 0.018189439550042152\n",
      "epoch: 16 step: 443, loss is 0.03657158836722374\n",
      "epoch: 16 step: 444, loss is 0.0028518910985440016\n",
      "epoch: 16 step: 445, loss is 0.01850937306880951\n",
      "epoch: 16 step: 446, loss is 0.03455884009599686\n",
      "epoch: 16 step: 447, loss is 0.022833608090877533\n",
      "epoch: 16 step: 448, loss is 0.007674279622733593\n",
      "epoch: 16 step: 449, loss is 0.002220623893663287\n",
      "epoch: 16 step: 450, loss is 0.048640232533216476\n",
      "epoch: 16 step: 451, loss is 0.007315422408282757\n",
      "epoch: 16 step: 452, loss is 0.01110953651368618\n",
      "epoch: 16 step: 453, loss is 0.004353729542344809\n",
      "epoch: 16 step: 454, loss is 0.002720803255215287\n",
      "epoch: 16 step: 455, loss is 0.03955460712313652\n",
      "epoch: 16 step: 456, loss is 0.01887715980410576\n",
      "epoch: 16 step: 457, loss is 0.002466690493747592\n",
      "epoch: 16 step: 458, loss is 0.004174308851361275\n",
      "epoch: 16 step: 459, loss is 0.005200490355491638\n",
      "epoch: 16 step: 460, loss is 0.004054955206811428\n",
      "epoch: 16 step: 461, loss is 0.0012952042743563652\n",
      "epoch: 16 step: 462, loss is 0.043469883501529694\n",
      "epoch: 16 step: 463, loss is 0.02560371533036232\n",
      "epoch: 16 step: 464, loss is 0.008824259042739868\n",
      "epoch: 16 step: 465, loss is 0.13581803441047668\n",
      "epoch: 16 step: 466, loss is 0.017956968396902084\n",
      "epoch: 16 step: 467, loss is 0.052736103534698486\n",
      "epoch: 16 step: 468, loss is 0.01102451328188181\n",
      "epoch: 16 step: 469, loss is 0.08334699273109436\n",
      "epoch: 16 step: 470, loss is 0.0008602052112109959\n",
      "epoch: 16 step: 471, loss is 0.009026257321238518\n",
      "epoch: 16 step: 472, loss is 0.009565688669681549\n",
      "epoch: 16 step: 473, loss is 0.01893635094165802\n",
      "epoch: 16 step: 474, loss is 0.09977395832538605\n",
      "epoch: 16 step: 475, loss is 0.0018521996680647135\n",
      "epoch: 16 step: 476, loss is 0.0076485201716423035\n",
      "epoch: 16 step: 477, loss is 0.02249392680823803\n",
      "epoch: 16 step: 478, loss is 0.015270307660102844\n",
      "epoch: 16 step: 479, loss is 0.006442386191338301\n",
      "epoch: 16 step: 480, loss is 0.03351180627942085\n",
      "epoch: 16 step: 481, loss is 0.0023826982360333204\n",
      "epoch: 16 step: 482, loss is 0.021678458899259567\n",
      "epoch: 16 step: 483, loss is 0.019432026892900467\n",
      "epoch: 16 step: 484, loss is 0.06290148198604584\n",
      "epoch: 16 step: 485, loss is 0.009711907245218754\n",
      "epoch: 16 step: 486, loss is 0.14259383082389832\n",
      "epoch: 16 step: 487, loss is 0.06280039250850677\n",
      "epoch: 16 step: 488, loss is 0.0012802323326468468\n",
      "epoch: 16 step: 489, loss is 0.002226660493761301\n",
      "epoch: 16 step: 490, loss is 0.015912309288978577\n",
      "epoch: 16 step: 491, loss is 0.02943430095911026\n",
      "epoch: 16 step: 492, loss is 0.010805988684296608\n",
      "epoch: 16 step: 493, loss is 0.0037363667506724596\n",
      "epoch: 16 step: 494, loss is 0.0089646615087986\n",
      "epoch: 16 step: 495, loss is 0.00733584351837635\n",
      "epoch: 16 step: 496, loss is 0.006489219609647989\n",
      "epoch: 16 step: 497, loss is 0.030643228441476822\n",
      "epoch: 16 step: 498, loss is 0.04468390345573425\n",
      "epoch: 16 step: 499, loss is 0.011055168695747852\n",
      "epoch: 16 step: 500, loss is 0.04936990514397621\n",
      "epoch: 16 step: 501, loss is 0.008623307570815086\n",
      "epoch: 16 step: 502, loss is 0.056887418031692505\n",
      "epoch: 16 step: 503, loss is 0.020070482045412064\n",
      "epoch: 16 step: 504, loss is 0.05080857500433922\n",
      "epoch: 16 step: 505, loss is 0.016534147784113884\n",
      "epoch: 16 step: 506, loss is 0.011166469193994999\n",
      "epoch: 16 step: 507, loss is 0.0071698990650475025\n",
      "epoch: 16 step: 508, loss is 0.026484688743948936\n",
      "epoch: 16 step: 509, loss is 0.042927030473947525\n",
      "epoch: 16 step: 510, loss is 0.01678459160029888\n",
      "epoch: 16 step: 511, loss is 0.024781906977295876\n",
      "epoch: 16 step: 512, loss is 0.005842819809913635\n",
      "epoch: 16 step: 513, loss is 0.025444017723202705\n",
      "epoch: 16 step: 514, loss is 0.002949007321149111\n",
      "epoch: 16 step: 515, loss is 0.001578476862050593\n",
      "epoch: 16 step: 516, loss is 0.05273429676890373\n",
      "epoch: 16 step: 517, loss is 0.008950585499405861\n",
      "epoch: 16 step: 518, loss is 0.016974112018942833\n",
      "epoch: 16 step: 519, loss is 0.030939841642975807\n",
      "epoch: 16 step: 520, loss is 0.006285602692514658\n",
      "epoch: 16 step: 521, loss is 0.11564701795578003\n",
      "epoch: 16 step: 522, loss is 0.0028429958038032055\n",
      "epoch: 16 step: 523, loss is 0.004520542453974485\n",
      "epoch: 16 step: 524, loss is 0.0034003928303718567\n",
      "epoch: 16 step: 525, loss is 0.0011947555467486382\n",
      "epoch: 16 step: 526, loss is 0.0167775247246027\n",
      "epoch: 16 step: 527, loss is 0.003498025005683303\n",
      "epoch: 16 step: 528, loss is 0.0027807201258838177\n",
      "epoch: 16 step: 529, loss is 0.0778236910700798\n",
      "epoch: 16 step: 530, loss is 0.08830548822879791\n",
      "epoch: 16 step: 531, loss is 0.011602167971432209\n",
      "epoch: 16 step: 532, loss is 0.04435376822948456\n",
      "epoch: 16 step: 533, loss is 0.0023142537102103233\n",
      "epoch: 16 step: 534, loss is 0.009683732874691486\n",
      "epoch: 16 step: 535, loss is 0.007426212541759014\n",
      "epoch: 16 step: 536, loss is 0.007697263732552528\n",
      "epoch: 16 step: 537, loss is 0.0011066029546782374\n",
      "epoch: 16 step: 538, loss is 0.0005621796590276062\n",
      "epoch: 16 step: 539, loss is 0.0100064966827631\n",
      "epoch: 16 step: 540, loss is 0.1138911098241806\n",
      "epoch: 16 step: 541, loss is 0.018929019570350647\n",
      "epoch: 16 step: 542, loss is 0.0007984099211171269\n",
      "epoch: 16 step: 543, loss is 0.03799135982990265\n",
      "epoch: 16 step: 544, loss is 0.0182479415088892\n",
      "epoch: 16 step: 545, loss is 0.06377600133419037\n",
      "epoch: 16 step: 546, loss is 0.010584162548184395\n",
      "epoch: 16 step: 547, loss is 0.010328616946935654\n",
      "epoch: 16 step: 548, loss is 0.089238740503788\n",
      "epoch: 16 step: 549, loss is 0.05465180054306984\n",
      "epoch: 16 step: 550, loss is 0.03361596167087555\n",
      "epoch: 16 step: 551, loss is 0.0057495818473398685\n",
      "epoch: 16 step: 552, loss is 0.0018565343925729394\n",
      "epoch: 16 step: 553, loss is 0.025507260113954544\n",
      "epoch: 16 step: 554, loss is 0.015400280244648457\n",
      "epoch: 16 step: 555, loss is 0.007406686432659626\n",
      "epoch: 16 step: 556, loss is 0.014298531226813793\n",
      "epoch: 16 step: 557, loss is 0.033768851310014725\n",
      "epoch: 16 step: 558, loss is 0.017568184062838554\n",
      "epoch: 16 step: 559, loss is 0.08846689760684967\n",
      "epoch: 16 step: 560, loss is 0.017344892024993896\n",
      "epoch: 16 step: 561, loss is 0.04410344734787941\n",
      "epoch: 16 step: 562, loss is 0.006862367503345013\n",
      "epoch: 16 step: 563, loss is 0.07376226037740707\n",
      "epoch: 16 step: 564, loss is 0.09523648023605347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 565, loss is 0.0312797911465168\n",
      "epoch: 16 step: 566, loss is 0.0028100835625082254\n",
      "epoch: 16 step: 567, loss is 0.07045923173427582\n",
      "epoch: 16 step: 568, loss is 0.0037096908781677485\n",
      "epoch: 16 step: 569, loss is 0.026639873161911964\n",
      "epoch: 16 step: 570, loss is 0.06020836532115936\n",
      "epoch: 16 step: 571, loss is 0.02956128679215908\n",
      "epoch: 16 step: 572, loss is 0.015787318348884583\n",
      "epoch: 16 step: 573, loss is 0.06424874067306519\n",
      "epoch: 16 step: 574, loss is 0.008893342688679695\n",
      "epoch: 16 step: 575, loss is 0.004089762922376394\n",
      "epoch: 16 step: 576, loss is 0.008734509348869324\n",
      "epoch: 16 step: 577, loss is 0.09899837523698807\n",
      "epoch: 16 step: 578, loss is 0.002000445034354925\n",
      "epoch: 16 step: 579, loss is 0.00855326373130083\n",
      "epoch: 16 step: 580, loss is 0.011225634254515171\n",
      "epoch: 16 step: 581, loss is 0.15910008549690247\n",
      "epoch: 16 step: 582, loss is 0.10539694875478745\n",
      "epoch: 16 step: 583, loss is 0.0016023438656702638\n",
      "epoch: 16 step: 584, loss is 0.04747525975108147\n",
      "epoch: 16 step: 585, loss is 0.021084167063236237\n",
      "epoch: 16 step: 586, loss is 0.014484844170510769\n",
      "epoch: 16 step: 587, loss is 0.016179446130990982\n",
      "epoch: 16 step: 588, loss is 0.03918810188770294\n",
      "epoch: 16 step: 589, loss is 0.08115634322166443\n",
      "epoch: 16 step: 590, loss is 0.03566728159785271\n",
      "epoch: 16 step: 591, loss is 0.009277744218707085\n",
      "epoch: 16 step: 592, loss is 0.0016805155901238322\n",
      "epoch: 16 step: 593, loss is 0.07605794072151184\n",
      "epoch: 16 step: 594, loss is 0.011597968637943268\n",
      "epoch: 16 step: 595, loss is 0.029459897428750992\n",
      "epoch: 16 step: 596, loss is 0.048527877777814865\n",
      "epoch: 16 step: 597, loss is 0.12651216983795166\n",
      "epoch: 16 step: 598, loss is 0.02218376100063324\n",
      "epoch: 16 step: 599, loss is 0.07036825269460678\n",
      "epoch: 16 step: 600, loss is 0.017101841047406197\n",
      "epoch: 16 step: 601, loss is 0.037647973746061325\n",
      "epoch: 16 step: 602, loss is 0.032306816428899765\n",
      "epoch: 16 step: 603, loss is 0.0033386710565537214\n",
      "epoch: 16 step: 604, loss is 0.018374549224972725\n",
      "epoch: 16 step: 605, loss is 0.01700187847018242\n",
      "epoch: 16 step: 606, loss is 0.006699297111481428\n",
      "epoch: 16 step: 607, loss is 0.05756668746471405\n",
      "epoch: 16 step: 608, loss is 0.005943181458860636\n",
      "epoch: 16 step: 609, loss is 0.2301560342311859\n",
      "epoch: 16 step: 610, loss is 0.013175824657082558\n",
      "epoch: 16 step: 611, loss is 0.006330450065433979\n",
      "epoch: 16 step: 612, loss is 0.006131104193627834\n",
      "epoch: 16 step: 613, loss is 0.019756827503442764\n",
      "epoch: 16 step: 614, loss is 0.08620165288448334\n",
      "epoch: 16 step: 615, loss is 0.09081228077411652\n",
      "epoch: 16 step: 616, loss is 0.01385614275932312\n",
      "epoch: 16 step: 617, loss is 0.02152058109641075\n",
      "epoch: 16 step: 618, loss is 0.01837862841784954\n",
      "epoch: 16 step: 619, loss is 0.013429480604827404\n",
      "epoch: 16 step: 620, loss is 0.0015477879205718637\n",
      "epoch: 16 step: 621, loss is 0.022667638957500458\n",
      "epoch: 16 step: 622, loss is 0.009530891664326191\n",
      "epoch: 16 step: 623, loss is 0.018323387950658798\n",
      "epoch: 16 step: 624, loss is 0.01091788150370121\n",
      "epoch: 16 step: 625, loss is 0.008472506888210773\n",
      "epoch: 16 step: 626, loss is 0.05277656018733978\n",
      "epoch: 16 step: 627, loss is 0.04582129791378975\n",
      "epoch: 16 step: 628, loss is 0.10760253667831421\n",
      "epoch: 16 step: 629, loss is 0.02150014601647854\n",
      "epoch: 16 step: 630, loss is 0.003527960740029812\n",
      "epoch: 16 step: 631, loss is 0.008190530352294445\n",
      "epoch: 16 step: 632, loss is 0.045995067805051804\n",
      "epoch: 16 step: 633, loss is 0.04085269197821617\n",
      "epoch: 16 step: 634, loss is 0.07197922468185425\n",
      "epoch: 16 step: 635, loss is 0.0017982551362365484\n",
      "epoch: 16 step: 636, loss is 0.009314250200986862\n",
      "epoch: 16 step: 637, loss is 0.04873795807361603\n",
      "epoch: 16 step: 638, loss is 0.001547295949421823\n",
      "epoch: 16 step: 639, loss is 0.04385237768292427\n",
      "epoch: 16 step: 640, loss is 0.007146771997213364\n",
      "epoch: 16 step: 641, loss is 0.003936943598091602\n",
      "epoch: 16 step: 642, loss is 0.014494767412543297\n",
      "epoch: 16 step: 643, loss is 0.005431655794382095\n",
      "epoch: 16 step: 644, loss is 0.0033021573908627033\n",
      "epoch: 16 step: 645, loss is 0.027725934982299805\n",
      "epoch: 16 step: 646, loss is 0.06405000388622284\n",
      "epoch: 16 step: 647, loss is 0.008724119514226913\n",
      "epoch: 16 step: 648, loss is 0.0019831082317978144\n",
      "epoch: 16 step: 649, loss is 0.051087092608213425\n",
      "epoch: 16 step: 650, loss is 0.0020070539321750402\n",
      "epoch: 16 step: 651, loss is 0.004074730910360813\n",
      "epoch: 16 step: 652, loss is 0.006763915531337261\n",
      "epoch: 16 step: 653, loss is 0.03691931068897247\n",
      "epoch: 16 step: 654, loss is 0.0028334492817521095\n",
      "epoch: 16 step: 655, loss is 0.002285533584654331\n",
      "epoch: 16 step: 656, loss is 0.0005642941687256098\n",
      "epoch: 16 step: 657, loss is 0.04748665913939476\n",
      "epoch: 16 step: 658, loss is 0.02250438556075096\n",
      "epoch: 16 step: 659, loss is 0.021461883559823036\n",
      "epoch: 16 step: 660, loss is 0.03364603966474533\n",
      "epoch: 16 step: 661, loss is 0.024174921214580536\n",
      "epoch: 16 step: 662, loss is 0.037252429872751236\n",
      "epoch: 16 step: 663, loss is 0.011867377907037735\n",
      "epoch: 16 step: 664, loss is 0.026982204988598824\n",
      "epoch: 16 step: 665, loss is 0.011199013330042362\n",
      "epoch: 16 step: 666, loss is 0.00970897264778614\n",
      "epoch: 16 step: 667, loss is 0.017143715173006058\n",
      "epoch: 16 step: 668, loss is 0.020269453525543213\n",
      "epoch: 16 step: 669, loss is 0.018229350447654724\n",
      "epoch: 16 step: 670, loss is 0.004618923179805279\n",
      "epoch: 16 step: 671, loss is 0.04906127229332924\n",
      "epoch: 16 step: 672, loss is 0.04555416479706764\n",
      "epoch: 16 step: 673, loss is 0.010454870760440826\n",
      "epoch: 16 step: 674, loss is 0.0937037467956543\n",
      "epoch: 16 step: 675, loss is 0.05521718040108681\n",
      "epoch: 16 step: 676, loss is 0.013107126578688622\n",
      "epoch: 16 step: 677, loss is 0.008536198176443577\n",
      "epoch: 16 step: 678, loss is 0.003916793502867222\n",
      "epoch: 16 step: 679, loss is 0.011975682340562344\n",
      "epoch: 16 step: 680, loss is 0.0022482280619442463\n",
      "epoch: 16 step: 681, loss is 0.023005496710538864\n",
      "epoch: 16 step: 682, loss is 0.002597327809780836\n",
      "epoch: 16 step: 683, loss is 0.004965808242559433\n",
      "epoch: 16 step: 684, loss is 0.04898552969098091\n",
      "epoch: 16 step: 685, loss is 0.03335030749440193\n",
      "epoch: 16 step: 686, loss is 0.014342643320560455\n",
      "epoch: 16 step: 687, loss is 0.003183833323419094\n",
      "epoch: 16 step: 688, loss is 0.01594482734799385\n",
      "epoch: 16 step: 689, loss is 0.006652548909187317\n",
      "epoch: 16 step: 690, loss is 0.0011020206147804856\n",
      "epoch: 16 step: 691, loss is 0.021160492673516273\n",
      "epoch: 16 step: 692, loss is 0.006625705398619175\n",
      "epoch: 16 step: 693, loss is 0.010315097868442535\n",
      "epoch: 16 step: 694, loss is 0.04738525673747063\n",
      "epoch: 16 step: 695, loss is 0.009101612493395805\n",
      "epoch: 16 step: 696, loss is 0.008052060380578041\n",
      "epoch: 16 step: 697, loss is 0.10638497769832611\n",
      "epoch: 16 step: 698, loss is 0.12014449387788773\n",
      "epoch: 16 step: 699, loss is 0.01955362595617771\n",
      "epoch: 16 step: 700, loss is 0.01413007453083992\n",
      "epoch: 16 step: 701, loss is 0.05165715143084526\n",
      "epoch: 16 step: 702, loss is 0.014304662123322487\n",
      "epoch: 16 step: 703, loss is 0.015605770982801914\n",
      "epoch: 16 step: 704, loss is 0.008801613934338093\n",
      "epoch: 16 step: 705, loss is 0.0016178862424567342\n",
      "epoch: 16 step: 706, loss is 0.00601735757663846\n",
      "epoch: 16 step: 707, loss is 0.0012736214557662606\n",
      "epoch: 16 step: 708, loss is 0.02506272867321968\n",
      "epoch: 16 step: 709, loss is 0.009225052781403065\n",
      "epoch: 16 step: 710, loss is 0.013288014568388462\n",
      "epoch: 16 step: 711, loss is 0.009929066523909569\n",
      "epoch: 16 step: 712, loss is 0.01027834601700306\n",
      "epoch: 16 step: 713, loss is 0.013283098116517067\n",
      "epoch: 16 step: 714, loss is 0.09172192215919495\n",
      "epoch: 16 step: 715, loss is 0.009217432700097561\n",
      "epoch: 16 step: 716, loss is 0.009613126516342163\n",
      "epoch: 16 step: 717, loss is 0.019124949350953102\n",
      "epoch: 16 step: 718, loss is 0.00216915225610137\n",
      "epoch: 16 step: 719, loss is 0.09202032536268234\n",
      "epoch: 16 step: 720, loss is 0.003124774433672428\n",
      "epoch: 16 step: 721, loss is 0.049872372299432755\n",
      "epoch: 16 step: 722, loss is 0.011226095259189606\n",
      "epoch: 16 step: 723, loss is 0.10260823369026184\n",
      "epoch: 16 step: 724, loss is 0.0069157425314188\n",
      "epoch: 16 step: 725, loss is 0.07656727731227875\n",
      "epoch: 16 step: 726, loss is 0.02712889201939106\n",
      "epoch: 16 step: 727, loss is 0.07297594845294952\n",
      "epoch: 16 step: 728, loss is 0.023748403415083885\n",
      "epoch: 16 step: 729, loss is 0.017734497785568237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 730, loss is 0.007395964581519365\n",
      "epoch: 16 step: 731, loss is 0.01121903583407402\n",
      "epoch: 16 step: 732, loss is 0.02999810501933098\n",
      "epoch: 16 step: 733, loss is 0.006673169322311878\n",
      "epoch: 16 step: 734, loss is 0.04236993193626404\n",
      "epoch: 16 step: 735, loss is 0.00516220647841692\n",
      "epoch: 16 step: 736, loss is 0.06398428976535797\n",
      "epoch: 16 step: 737, loss is 0.005648637656122446\n",
      "epoch: 16 step: 738, loss is 0.06271853297948837\n",
      "epoch: 16 step: 739, loss is 0.10807790607213974\n",
      "epoch: 16 step: 740, loss is 0.0010213676141574979\n",
      "epoch: 16 step: 741, loss is 0.02557871863245964\n",
      "epoch: 16 step: 742, loss is 0.02600821852684021\n",
      "epoch: 16 step: 743, loss is 0.006732639856636524\n",
      "epoch: 16 step: 744, loss is 0.054703112691640854\n",
      "epoch: 16 step: 745, loss is 0.011087294667959213\n",
      "epoch: 16 step: 746, loss is 0.0010882210917770863\n",
      "epoch: 16 step: 747, loss is 0.002601660555228591\n",
      "epoch: 16 step: 748, loss is 0.1252453476190567\n",
      "epoch: 16 step: 749, loss is 0.08118906617164612\n",
      "epoch: 16 step: 750, loss is 0.003938909154385328\n",
      "epoch: 16 step: 751, loss is 0.00860157236456871\n",
      "epoch: 16 step: 752, loss is 0.007977981120347977\n",
      "epoch: 16 step: 753, loss is 0.03643781691789627\n",
      "epoch: 16 step: 754, loss is 0.040682412683963776\n",
      "epoch: 16 step: 755, loss is 0.0043588485568761826\n",
      "epoch: 16 step: 756, loss is 0.07828084379434586\n",
      "epoch: 16 step: 757, loss is 0.008791782893240452\n",
      "epoch: 16 step: 758, loss is 0.021284280344843864\n",
      "epoch: 16 step: 759, loss is 0.02564035914838314\n",
      "epoch: 16 step: 760, loss is 0.09135380387306213\n",
      "epoch: 16 step: 761, loss is 0.00996685866266489\n",
      "epoch: 16 step: 762, loss is 0.05257315933704376\n",
      "epoch: 16 step: 763, loss is 0.029071344062685966\n",
      "epoch: 16 step: 764, loss is 0.04542015492916107\n",
      "epoch: 16 step: 765, loss is 0.03469832241535187\n",
      "epoch: 16 step: 766, loss is 0.05107492953538895\n",
      "epoch: 16 step: 767, loss is 0.04452112317085266\n",
      "epoch: 16 step: 768, loss is 0.032716672867536545\n",
      "epoch: 16 step: 769, loss is 0.049503143876791\n",
      "epoch: 16 step: 770, loss is 0.0020990215707570314\n",
      "epoch: 16 step: 771, loss is 0.06079661101102829\n",
      "epoch: 16 step: 772, loss is 0.013963461853563786\n",
      "epoch: 16 step: 773, loss is 0.012299204245209694\n",
      "epoch: 16 step: 774, loss is 0.014074914157390594\n",
      "epoch: 16 step: 775, loss is 0.053994547575712204\n",
      "epoch: 16 step: 776, loss is 0.008803999982774258\n",
      "epoch: 16 step: 777, loss is 0.022822756320238113\n",
      "epoch: 16 step: 778, loss is 0.0018201412167400122\n",
      "epoch: 16 step: 779, loss is 0.02572358213365078\n",
      "epoch: 16 step: 780, loss is 0.044299498200416565\n",
      "epoch: 16 step: 781, loss is 0.0024559779558330774\n",
      "epoch: 16 step: 782, loss is 0.048833370208740234\n",
      "epoch: 16 step: 783, loss is 0.0005451401229947805\n",
      "epoch: 16 step: 784, loss is 0.0447041355073452\n",
      "epoch: 16 step: 785, loss is 0.04707089066505432\n",
      "epoch: 16 step: 786, loss is 0.027328839525580406\n",
      "epoch: 16 step: 787, loss is 0.08089853823184967\n",
      "epoch: 16 step: 788, loss is 0.08375632762908936\n",
      "epoch: 16 step: 789, loss is 0.0060043251141905785\n",
      "epoch: 16 step: 790, loss is 0.031503066420555115\n",
      "epoch: 16 step: 791, loss is 0.08469685912132263\n",
      "epoch: 16 step: 792, loss is 0.0179753415286541\n",
      "epoch: 16 step: 793, loss is 0.03249884769320488\n",
      "epoch: 16 step: 794, loss is 0.00593264726921916\n",
      "epoch: 16 step: 795, loss is 0.012410017661750317\n",
      "epoch: 16 step: 796, loss is 0.0720185935497284\n",
      "epoch: 16 step: 797, loss is 0.032768599689006805\n",
      "epoch: 16 step: 798, loss is 0.03141925483942032\n",
      "epoch: 16 step: 799, loss is 0.012658467516303062\n",
      "epoch: 16 step: 800, loss is 0.06375887989997864\n",
      "epoch: 16 step: 801, loss is 0.01050556916743517\n",
      "epoch: 16 step: 802, loss is 0.005730899050831795\n",
      "epoch: 16 step: 803, loss is 0.022453108802437782\n",
      "epoch: 16 step: 804, loss is 0.024878226220607758\n",
      "epoch: 16 step: 805, loss is 0.0034293835051357746\n",
      "epoch: 16 step: 806, loss is 0.08307407796382904\n",
      "epoch: 16 step: 807, loss is 0.14917360246181488\n",
      "epoch: 16 step: 808, loss is 0.02371641807258129\n",
      "epoch: 16 step: 809, loss is 0.08128439635038376\n",
      "epoch: 16 step: 810, loss is 0.007114620413631201\n",
      "epoch: 16 step: 811, loss is 0.04919733852148056\n",
      "epoch: 16 step: 812, loss is 0.011837128549814224\n",
      "epoch: 16 step: 813, loss is 0.013554044999182224\n",
      "epoch: 16 step: 814, loss is 0.014185251668095589\n",
      "epoch: 16 step: 815, loss is 0.004492153413593769\n",
      "epoch: 16 step: 816, loss is 0.024792488664388657\n",
      "epoch: 16 step: 817, loss is 0.04834018647670746\n",
      "epoch: 16 step: 818, loss is 0.025378337129950523\n",
      "epoch: 16 step: 819, loss is 0.04810528829693794\n",
      "epoch: 16 step: 820, loss is 0.044316619634628296\n",
      "epoch: 16 step: 821, loss is 0.010202651843428612\n",
      "epoch: 16 step: 822, loss is 0.004015773069113493\n",
      "epoch: 16 step: 823, loss is 0.03091118112206459\n",
      "epoch: 16 step: 824, loss is 0.013589246198534966\n",
      "epoch: 16 step: 825, loss is 0.05425355210900307\n",
      "epoch: 16 step: 826, loss is 0.020790496841073036\n",
      "epoch: 16 step: 827, loss is 0.04345705360174179\n",
      "epoch: 16 step: 828, loss is 0.01267597358673811\n",
      "epoch: 16 step: 829, loss is 0.004062343388795853\n",
      "epoch: 16 step: 830, loss is 0.00681305630132556\n",
      "epoch: 16 step: 831, loss is 0.005031927023082972\n",
      "epoch: 16 step: 832, loss is 0.0037120652850717306\n",
      "epoch: 16 step: 833, loss is 0.0019454688299447298\n",
      "epoch: 16 step: 834, loss is 0.022155625745654106\n",
      "epoch: 16 step: 835, loss is 0.020009901374578476\n",
      "epoch: 16 step: 836, loss is 0.0044142985716462135\n",
      "epoch: 16 step: 837, loss is 0.013215167447924614\n",
      "epoch: 16 step: 838, loss is 0.0014630380319431424\n",
      "epoch: 16 step: 839, loss is 0.0029327431693673134\n",
      "epoch: 16 step: 840, loss is 0.035708557814359665\n",
      "epoch: 16 step: 841, loss is 0.018052468076348305\n",
      "epoch: 16 step: 842, loss is 0.009214432910084724\n",
      "epoch: 16 step: 843, loss is 0.007775133475661278\n",
      "epoch: 16 step: 844, loss is 0.0009924201294779778\n",
      "epoch: 16 step: 845, loss is 0.05609915405511856\n",
      "epoch: 16 step: 846, loss is 0.027828969061374664\n",
      "epoch: 16 step: 847, loss is 0.08126535266637802\n",
      "epoch: 16 step: 848, loss is 0.004952429328113794\n",
      "epoch: 16 step: 849, loss is 0.07113073021173477\n",
      "epoch: 16 step: 850, loss is 0.038861025124788284\n",
      "epoch: 16 step: 851, loss is 0.0015166470548138022\n",
      "epoch: 16 step: 852, loss is 0.01501766312867403\n",
      "epoch: 16 step: 853, loss is 0.000422765122493729\n",
      "epoch: 16 step: 854, loss is 0.001977603416889906\n",
      "epoch: 16 step: 855, loss is 0.002964668208733201\n",
      "epoch: 16 step: 856, loss is 0.06674116104841232\n",
      "epoch: 16 step: 857, loss is 0.01900063268840313\n",
      "epoch: 16 step: 858, loss is 0.06995368003845215\n",
      "epoch: 16 step: 859, loss is 0.0020657384302467108\n",
      "epoch: 16 step: 860, loss is 0.018084200099110603\n",
      "epoch: 16 step: 861, loss is 0.004556860309094191\n",
      "epoch: 16 step: 862, loss is 0.00026242074090987444\n",
      "epoch: 16 step: 863, loss is 0.0067537748254835606\n",
      "epoch: 16 step: 864, loss is 0.008077419362962246\n",
      "epoch: 16 step: 865, loss is 0.03705215826630592\n",
      "epoch: 16 step: 866, loss is 0.003834131406620145\n",
      "epoch: 16 step: 867, loss is 0.060506079345941544\n",
      "epoch: 16 step: 868, loss is 0.0399019755423069\n",
      "epoch: 16 step: 869, loss is 0.025698384270071983\n",
      "epoch: 16 step: 870, loss is 0.0029445418622344732\n",
      "epoch: 16 step: 871, loss is 0.010110192932188511\n",
      "epoch: 16 step: 872, loss is 0.012469572946429253\n",
      "epoch: 16 step: 873, loss is 0.09801103174686432\n",
      "epoch: 16 step: 874, loss is 0.001065483782440424\n",
      "epoch: 16 step: 875, loss is 0.001236705924384296\n",
      "epoch: 16 step: 876, loss is 0.020453542470932007\n",
      "epoch: 16 step: 877, loss is 0.005711854435503483\n",
      "epoch: 16 step: 878, loss is 0.015482948161661625\n",
      "epoch: 16 step: 879, loss is 0.006822588387876749\n",
      "epoch: 16 step: 880, loss is 0.00010579187073744833\n",
      "epoch: 16 step: 881, loss is 0.006868733558803797\n",
      "epoch: 16 step: 882, loss is 0.004635188262909651\n",
      "epoch: 16 step: 883, loss is 0.02133248932659626\n",
      "epoch: 16 step: 884, loss is 0.00026633404195308685\n",
      "epoch: 16 step: 885, loss is 0.01418716087937355\n",
      "epoch: 16 step: 886, loss is 0.05596749112010002\n",
      "epoch: 16 step: 887, loss is 0.013068681582808495\n",
      "epoch: 16 step: 888, loss is 0.01808885484933853\n",
      "epoch: 16 step: 889, loss is 0.020167961716651917\n",
      "epoch: 16 step: 890, loss is 0.02503827027976513\n",
      "epoch: 16 step: 891, loss is 0.004725264851003885\n",
      "epoch: 16 step: 892, loss is 0.005990046076476574\n",
      "epoch: 16 step: 893, loss is 0.01398425456136465\n",
      "epoch: 16 step: 894, loss is 0.013023070059716702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 895, loss is 0.0069759017787873745\n",
      "epoch: 16 step: 896, loss is 0.00573433842509985\n",
      "epoch: 16 step: 897, loss is 0.013626948930323124\n",
      "epoch: 16 step: 898, loss is 0.0028863423503935337\n",
      "epoch: 16 step: 899, loss is 0.010954156517982483\n",
      "epoch: 16 step: 900, loss is 0.002985592233017087\n",
      "epoch: 16 step: 901, loss is 0.0021295908372849226\n",
      "epoch: 16 step: 902, loss is 0.0010614681523293257\n",
      "epoch: 16 step: 903, loss is 0.034607768058776855\n",
      "epoch: 16 step: 904, loss is 0.014211690984666348\n",
      "epoch: 16 step: 905, loss is 0.0025752303190529346\n",
      "epoch: 16 step: 906, loss is 0.07364636659622192\n",
      "epoch: 16 step: 907, loss is 0.01570843532681465\n",
      "epoch: 16 step: 908, loss is 0.007456854917109013\n",
      "epoch: 16 step: 909, loss is 0.02210441417992115\n",
      "epoch: 16 step: 910, loss is 0.01419486291706562\n",
      "epoch: 16 step: 911, loss is 0.03927525505423546\n",
      "epoch: 16 step: 912, loss is 0.00433114031329751\n",
      "epoch: 16 step: 913, loss is 0.02811324968934059\n",
      "epoch: 16 step: 914, loss is 0.03196973353624344\n",
      "epoch: 16 step: 915, loss is 0.09203867614269257\n",
      "epoch: 16 step: 916, loss is 0.029362481087446213\n",
      "epoch: 16 step: 917, loss is 0.025316283106803894\n",
      "epoch: 16 step: 918, loss is 0.028662629425525665\n",
      "epoch: 16 step: 919, loss is 0.033104654401540756\n",
      "epoch: 16 step: 920, loss is 0.03318700194358826\n",
      "epoch: 16 step: 921, loss is 0.07464490830898285\n",
      "epoch: 16 step: 922, loss is 0.008612655103206635\n",
      "epoch: 16 step: 923, loss is 0.013123943470418453\n",
      "epoch: 16 step: 924, loss is 0.019087694585323334\n",
      "epoch: 16 step: 925, loss is 0.004185163881629705\n",
      "epoch: 16 step: 926, loss is 0.04746032506227493\n",
      "epoch: 16 step: 927, loss is 0.005492751952260733\n",
      "epoch: 16 step: 928, loss is 0.024917926639318466\n",
      "epoch: 16 step: 929, loss is 0.019781803712248802\n",
      "epoch: 16 step: 930, loss is 0.011468459852039814\n",
      "epoch: 16 step: 931, loss is 0.07360005378723145\n",
      "epoch: 16 step: 932, loss is 0.0645093247294426\n",
      "epoch: 16 step: 933, loss is 0.007751699537038803\n",
      "epoch: 16 step: 934, loss is 0.009279608726501465\n",
      "epoch: 16 step: 935, loss is 0.003957344684749842\n",
      "epoch: 16 step: 936, loss is 0.0648428276181221\n",
      "epoch: 16 step: 937, loss is 0.019245890900492668\n",
      "epoch: 17 step: 1, loss is 0.0038575229700654745\n",
      "epoch: 17 step: 2, loss is 0.00044047116534784436\n",
      "epoch: 17 step: 3, loss is 0.00663267495110631\n",
      "epoch: 17 step: 4, loss is 0.0066407546401023865\n",
      "epoch: 17 step: 5, loss is 0.006020580418407917\n",
      "epoch: 17 step: 6, loss is 0.007676698267459869\n",
      "epoch: 17 step: 7, loss is 0.04818812012672424\n",
      "epoch: 17 step: 8, loss is 0.011196938343346119\n",
      "epoch: 17 step: 9, loss is 0.016550889238715172\n",
      "epoch: 17 step: 10, loss is 0.009170853532850742\n",
      "epoch: 17 step: 11, loss is 0.007635469548404217\n",
      "epoch: 17 step: 12, loss is 0.00564143480733037\n",
      "epoch: 17 step: 13, loss is 0.00838282611221075\n",
      "epoch: 17 step: 14, loss is 0.036574587225914\n",
      "epoch: 17 step: 15, loss is 0.10038241744041443\n",
      "epoch: 17 step: 16, loss is 0.01864841766655445\n",
      "epoch: 17 step: 17, loss is 0.0047568222507834435\n",
      "epoch: 17 step: 18, loss is 0.030662020668387413\n",
      "epoch: 17 step: 19, loss is 0.033200569450855255\n",
      "epoch: 17 step: 20, loss is 0.020695868879556656\n",
      "epoch: 17 step: 21, loss is 0.052688293159008026\n",
      "epoch: 17 step: 22, loss is 0.00377928139641881\n",
      "epoch: 17 step: 23, loss is 0.04535455256700516\n",
      "epoch: 17 step: 24, loss is 0.0042352499440312386\n",
      "epoch: 17 step: 25, loss is 0.11037199944257736\n",
      "epoch: 17 step: 26, loss is 0.006785708479583263\n",
      "epoch: 17 step: 27, loss is 0.09628531336784363\n",
      "epoch: 17 step: 28, loss is 0.017421089112758636\n",
      "epoch: 17 step: 29, loss is 0.007837114855647087\n",
      "epoch: 17 step: 30, loss is 0.039815571159124374\n",
      "epoch: 17 step: 31, loss is 0.0029830303974449635\n",
      "epoch: 17 step: 32, loss is 0.02686009742319584\n",
      "epoch: 17 step: 33, loss is 0.040921710431575775\n",
      "epoch: 17 step: 34, loss is 0.018038060516119003\n",
      "epoch: 17 step: 35, loss is 0.027329010888934135\n",
      "epoch: 17 step: 36, loss is 0.01731615886092186\n",
      "epoch: 17 step: 37, loss is 0.0034840614534914494\n",
      "epoch: 17 step: 38, loss is 0.02455933578312397\n",
      "epoch: 17 step: 39, loss is 0.00870154146105051\n",
      "epoch: 17 step: 40, loss is 0.006608868949115276\n",
      "epoch: 17 step: 41, loss is 0.006593573838472366\n",
      "epoch: 17 step: 42, loss is 0.08567716181278229\n",
      "epoch: 17 step: 43, loss is 0.04531470313668251\n",
      "epoch: 17 step: 44, loss is 0.024025090038776398\n",
      "epoch: 17 step: 45, loss is 0.031427763402462006\n",
      "epoch: 17 step: 46, loss is 0.052138861268758774\n",
      "epoch: 17 step: 47, loss is 0.035128574818372726\n",
      "epoch: 17 step: 48, loss is 0.02839581109583378\n",
      "epoch: 17 step: 49, loss is 0.06442537903785706\n",
      "epoch: 17 step: 50, loss is 0.002193003660067916\n",
      "epoch: 17 step: 51, loss is 0.052445754408836365\n",
      "epoch: 17 step: 52, loss is 0.0010966156842187047\n",
      "epoch: 17 step: 53, loss is 0.01187217514961958\n",
      "epoch: 17 step: 54, loss is 0.007913553155958652\n",
      "epoch: 17 step: 55, loss is 0.006151552777737379\n",
      "epoch: 17 step: 56, loss is 0.00018259893113281578\n",
      "epoch: 17 step: 57, loss is 0.01745932176709175\n",
      "epoch: 17 step: 58, loss is 0.005890307482331991\n",
      "epoch: 17 step: 59, loss is 0.032295018434524536\n",
      "epoch: 17 step: 60, loss is 0.019506290555000305\n",
      "epoch: 17 step: 61, loss is 0.0006709391600452363\n",
      "epoch: 17 step: 62, loss is 0.004233353305608034\n",
      "epoch: 17 step: 63, loss is 0.04632200300693512\n",
      "epoch: 17 step: 64, loss is 0.003627427387982607\n",
      "epoch: 17 step: 65, loss is 0.005765238776803017\n",
      "epoch: 17 step: 66, loss is 0.010552491992712021\n",
      "epoch: 17 step: 67, loss is 0.003686104901134968\n",
      "epoch: 17 step: 68, loss is 0.0033868488389998674\n",
      "epoch: 17 step: 69, loss is 0.01894937828183174\n",
      "epoch: 17 step: 70, loss is 0.049398694187402725\n",
      "epoch: 17 step: 71, loss is 0.05079001188278198\n",
      "epoch: 17 step: 72, loss is 0.07121091336011887\n",
      "epoch: 17 step: 73, loss is 0.052292972803115845\n",
      "epoch: 17 step: 74, loss is 0.0018049442442134023\n",
      "epoch: 17 step: 75, loss is 0.0007425455842167139\n",
      "epoch: 17 step: 76, loss is 0.011702680960297585\n",
      "epoch: 17 step: 77, loss is 0.013654875569045544\n",
      "epoch: 17 step: 78, loss is 0.035924799740314484\n",
      "epoch: 17 step: 79, loss is 0.0391492024064064\n",
      "epoch: 17 step: 80, loss is 0.0017313165590167046\n",
      "epoch: 17 step: 81, loss is 0.017264116555452347\n",
      "epoch: 17 step: 82, loss is 0.0012907159980386496\n",
      "epoch: 17 step: 83, loss is 0.06691336631774902\n",
      "epoch: 17 step: 84, loss is 0.0020472160540521145\n",
      "epoch: 17 step: 85, loss is 0.003713379381224513\n",
      "epoch: 17 step: 86, loss is 0.0016958093037828803\n",
      "epoch: 17 step: 87, loss is 0.010421575978398323\n",
      "epoch: 17 step: 88, loss is 0.09144632518291473\n",
      "epoch: 17 step: 89, loss is 0.040441740304231644\n",
      "epoch: 17 step: 90, loss is 0.003051504958420992\n",
      "epoch: 17 step: 91, loss is 0.022468633949756622\n",
      "epoch: 17 step: 92, loss is 0.0022969418205320835\n",
      "epoch: 17 step: 93, loss is 0.010077903978526592\n",
      "epoch: 17 step: 94, loss is 0.006143965758383274\n",
      "epoch: 17 step: 95, loss is 0.01074571069329977\n",
      "epoch: 17 step: 96, loss is 0.004134396556764841\n",
      "epoch: 17 step: 97, loss is 0.0036608469672501087\n",
      "epoch: 17 step: 98, loss is 0.0028991594444960356\n",
      "epoch: 17 step: 99, loss is 0.16445723176002502\n",
      "epoch: 17 step: 100, loss is 0.0008719267207197845\n",
      "epoch: 17 step: 101, loss is 0.0016490176785737276\n",
      "epoch: 17 step: 102, loss is 0.05108635872602463\n",
      "epoch: 17 step: 103, loss is 0.11966299265623093\n",
      "epoch: 17 step: 104, loss is 0.015648115426301956\n",
      "epoch: 17 step: 105, loss is 0.06050022691488266\n",
      "epoch: 17 step: 106, loss is 0.03289403021335602\n",
      "epoch: 17 step: 107, loss is 0.003505897242575884\n",
      "epoch: 17 step: 108, loss is 0.008948786184191704\n",
      "epoch: 17 step: 109, loss is 0.1654418557882309\n",
      "epoch: 17 step: 110, loss is 0.025401122868061066\n",
      "epoch: 17 step: 111, loss is 0.012755388393998146\n",
      "epoch: 17 step: 112, loss is 0.0621342696249485\n",
      "epoch: 17 step: 113, loss is 0.01024965476244688\n",
      "epoch: 17 step: 114, loss is 0.009631938301026821\n",
      "epoch: 17 step: 115, loss is 0.0023278621956706047\n",
      "epoch: 17 step: 116, loss is 0.007089745253324509\n",
      "epoch: 17 step: 117, loss is 0.015177621506154537\n",
      "epoch: 17 step: 118, loss is 0.0013635075883939862\n",
      "epoch: 17 step: 119, loss is 0.0017542896093800664\n",
      "epoch: 17 step: 120, loss is 0.03701094165444374\n",
      "epoch: 17 step: 121, loss is 0.07948683947324753\n",
      "epoch: 17 step: 122, loss is 0.0429837740957737\n",
      "epoch: 17 step: 123, loss is 0.0012713728938251734\n",
      "epoch: 17 step: 124, loss is 0.0036540089640766382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 125, loss is 0.003995012957602739\n",
      "epoch: 17 step: 126, loss is 0.0098370760679245\n",
      "epoch: 17 step: 127, loss is 0.0013083055382594466\n",
      "epoch: 17 step: 128, loss is 0.0025617422070354223\n",
      "epoch: 17 step: 129, loss is 0.002845939015969634\n",
      "epoch: 17 step: 130, loss is 0.004340574610978365\n",
      "epoch: 17 step: 131, loss is 0.12817420065402985\n",
      "epoch: 17 step: 132, loss is 0.0009128227829933167\n",
      "epoch: 17 step: 133, loss is 0.012771577574312687\n",
      "epoch: 17 step: 134, loss is 0.0024359833914786577\n",
      "epoch: 17 step: 135, loss is 0.004632292781025171\n",
      "epoch: 17 step: 136, loss is 0.009960700757801533\n",
      "epoch: 17 step: 137, loss is 0.005123209673911333\n",
      "epoch: 17 step: 138, loss is 0.015416799113154411\n",
      "epoch: 17 step: 139, loss is 0.014092887751758099\n",
      "epoch: 17 step: 140, loss is 0.008043625392019749\n",
      "epoch: 17 step: 141, loss is 0.008695623837411404\n",
      "epoch: 17 step: 142, loss is 0.05245465412735939\n",
      "epoch: 17 step: 143, loss is 0.00530351884663105\n",
      "epoch: 17 step: 144, loss is 0.003114962251856923\n",
      "epoch: 17 step: 145, loss is 0.0034187650308012962\n",
      "epoch: 17 step: 146, loss is 0.01077285222709179\n",
      "epoch: 17 step: 147, loss is 0.002526788506656885\n",
      "epoch: 17 step: 148, loss is 0.004385618958622217\n",
      "epoch: 17 step: 149, loss is 0.002501486334949732\n",
      "epoch: 17 step: 150, loss is 0.0029572495259344578\n",
      "epoch: 17 step: 151, loss is 0.0012432787334546447\n",
      "epoch: 17 step: 152, loss is 0.001289867446757853\n",
      "epoch: 17 step: 153, loss is 0.033843908458948135\n",
      "epoch: 17 step: 154, loss is 0.0013493295991793275\n",
      "epoch: 17 step: 155, loss is 0.009149637073278427\n",
      "epoch: 17 step: 156, loss is 0.013060563243925571\n",
      "epoch: 17 step: 157, loss is 0.0017720286268740892\n",
      "epoch: 17 step: 158, loss is 0.013027037493884563\n",
      "epoch: 17 step: 159, loss is 0.022513171657919884\n",
      "epoch: 17 step: 160, loss is 0.004693356342613697\n",
      "epoch: 17 step: 161, loss is 0.011749189347028732\n",
      "epoch: 17 step: 162, loss is 0.0021537395659834146\n",
      "epoch: 17 step: 163, loss is 0.001710232114419341\n",
      "epoch: 17 step: 164, loss is 0.005453550256788731\n",
      "epoch: 17 step: 165, loss is 0.048002030700445175\n",
      "epoch: 17 step: 166, loss is 0.002374346600845456\n",
      "epoch: 17 step: 167, loss is 0.02862674370408058\n",
      "epoch: 17 step: 168, loss is 0.007598823867738247\n",
      "epoch: 17 step: 169, loss is 0.008580067194998264\n",
      "epoch: 17 step: 170, loss is 0.03555239737033844\n",
      "epoch: 17 step: 171, loss is 0.0028960059862583876\n",
      "epoch: 17 step: 172, loss is 0.004984841216355562\n",
      "epoch: 17 step: 173, loss is 0.02349802479147911\n",
      "epoch: 17 step: 174, loss is 0.027966052293777466\n",
      "epoch: 17 step: 175, loss is 0.010501514188945293\n",
      "epoch: 17 step: 176, loss is 0.014950296841561794\n",
      "epoch: 17 step: 177, loss is 0.0009499125299043953\n",
      "epoch: 17 step: 178, loss is 0.049289509654045105\n",
      "epoch: 17 step: 179, loss is 0.0101256612688303\n",
      "epoch: 17 step: 180, loss is 0.05352180451154709\n",
      "epoch: 17 step: 181, loss is 0.006609381176531315\n",
      "epoch: 17 step: 182, loss is 0.010519417002797127\n",
      "epoch: 17 step: 183, loss is 0.003391663311049342\n",
      "epoch: 17 step: 184, loss is 0.03871301934123039\n",
      "epoch: 17 step: 185, loss is 0.00325477821752429\n",
      "epoch: 17 step: 186, loss is 0.005903720855712891\n",
      "epoch: 17 step: 187, loss is 0.005410749930888414\n",
      "epoch: 17 step: 188, loss is 0.009171297773718834\n",
      "epoch: 17 step: 189, loss is 0.007870159111917019\n",
      "epoch: 17 step: 190, loss is 0.00600708881393075\n",
      "epoch: 17 step: 191, loss is 0.012651128694415092\n",
      "epoch: 17 step: 192, loss is 0.02407263033092022\n",
      "epoch: 17 step: 193, loss is 0.030294572934508324\n",
      "epoch: 17 step: 194, loss is 0.003704020520672202\n",
      "epoch: 17 step: 195, loss is 0.07988901436328888\n",
      "epoch: 17 step: 196, loss is 0.004399458412081003\n",
      "epoch: 17 step: 197, loss is 0.0038795601576566696\n",
      "epoch: 17 step: 198, loss is 0.0029595782980322838\n",
      "epoch: 17 step: 199, loss is 0.024893129244446754\n",
      "epoch: 17 step: 200, loss is 0.015663636848330498\n",
      "epoch: 17 step: 201, loss is 0.00286677572876215\n",
      "epoch: 17 step: 202, loss is 0.0011796513572335243\n",
      "epoch: 17 step: 203, loss is 0.017659498378634453\n",
      "epoch: 17 step: 204, loss is 0.02710767835378647\n",
      "epoch: 17 step: 205, loss is 0.010558930225670338\n",
      "epoch: 17 step: 206, loss is 0.013670096173882484\n",
      "epoch: 17 step: 207, loss is 0.005471646785736084\n",
      "epoch: 17 step: 208, loss is 0.004858291707932949\n",
      "epoch: 17 step: 209, loss is 0.00441503943875432\n",
      "epoch: 17 step: 210, loss is 0.0028496072627604008\n",
      "epoch: 17 step: 211, loss is 0.02878166362643242\n",
      "epoch: 17 step: 212, loss is 0.0018173411954194307\n",
      "epoch: 17 step: 213, loss is 0.004031256306916475\n",
      "epoch: 17 step: 214, loss is 0.0023262775503098965\n",
      "epoch: 17 step: 215, loss is 0.0037050554528832436\n",
      "epoch: 17 step: 216, loss is 0.10632269084453583\n",
      "epoch: 17 step: 217, loss is 0.006003753747791052\n",
      "epoch: 17 step: 218, loss is 0.0009142959606833756\n",
      "epoch: 17 step: 219, loss is 0.01827634684741497\n",
      "epoch: 17 step: 220, loss is 0.006571869365870953\n",
      "epoch: 17 step: 221, loss is 0.00020794308511540294\n",
      "epoch: 17 step: 222, loss is 0.004670209251344204\n",
      "epoch: 17 step: 223, loss is 0.004533887375146151\n",
      "epoch: 17 step: 224, loss is 0.06355497986078262\n",
      "epoch: 17 step: 225, loss is 0.07087970525026321\n",
      "epoch: 17 step: 226, loss is 0.03656526654958725\n",
      "epoch: 17 step: 227, loss is 0.03919434919953346\n",
      "epoch: 17 step: 228, loss is 0.004130908288061619\n",
      "epoch: 17 step: 229, loss is 0.0006588055985048413\n",
      "epoch: 17 step: 230, loss is 0.11105925589799881\n",
      "epoch: 17 step: 231, loss is 0.02260180190205574\n",
      "epoch: 17 step: 232, loss is 0.013615635223686695\n",
      "epoch: 17 step: 233, loss is 0.07292287796735764\n",
      "epoch: 17 step: 234, loss is 0.01011116337031126\n",
      "epoch: 17 step: 235, loss is 0.013369658961892128\n",
      "epoch: 17 step: 236, loss is 0.030919676646590233\n",
      "epoch: 17 step: 237, loss is 0.0035297453869134188\n",
      "epoch: 17 step: 238, loss is 0.023538384586572647\n",
      "epoch: 17 step: 239, loss is 0.006979753728955984\n",
      "epoch: 17 step: 240, loss is 0.025333011522889137\n",
      "epoch: 17 step: 241, loss is 0.010256592184305191\n",
      "epoch: 17 step: 242, loss is 0.011529223993420601\n",
      "epoch: 17 step: 243, loss is 0.01722949743270874\n",
      "epoch: 17 step: 244, loss is 0.000450402672868222\n",
      "epoch: 17 step: 245, loss is 0.018796153366565704\n",
      "epoch: 17 step: 246, loss is 0.06340719014406204\n",
      "epoch: 17 step: 247, loss is 0.007207668386399746\n",
      "epoch: 17 step: 248, loss is 0.007362736389040947\n",
      "epoch: 17 step: 249, loss is 0.009964997880160809\n",
      "epoch: 17 step: 250, loss is 0.20336361229419708\n",
      "epoch: 17 step: 251, loss is 0.036575429141521454\n",
      "epoch: 17 step: 252, loss is 0.02402878925204277\n",
      "epoch: 17 step: 253, loss is 0.019977230578660965\n",
      "epoch: 17 step: 254, loss is 0.030319351702928543\n",
      "epoch: 17 step: 255, loss is 0.026357734575867653\n",
      "epoch: 17 step: 256, loss is 0.006677621975541115\n",
      "epoch: 17 step: 257, loss is 0.007351566106081009\n",
      "epoch: 17 step: 258, loss is 0.0006908129435032606\n",
      "epoch: 17 step: 259, loss is 0.01942799612879753\n",
      "epoch: 17 step: 260, loss is 0.02177503891289234\n",
      "epoch: 17 step: 261, loss is 0.02200509048998356\n",
      "epoch: 17 step: 262, loss is 0.005054431967437267\n",
      "epoch: 17 step: 263, loss is 0.030403893440961838\n",
      "epoch: 17 step: 264, loss is 0.006648452952504158\n",
      "epoch: 17 step: 265, loss is 0.00010696217213990167\n",
      "epoch: 17 step: 266, loss is 0.011888437904417515\n",
      "epoch: 17 step: 267, loss is 0.00011138170521007851\n",
      "epoch: 17 step: 268, loss is 0.002818286418914795\n",
      "epoch: 17 step: 269, loss is 0.0007725522154942155\n",
      "epoch: 17 step: 270, loss is 0.063511922955513\n",
      "epoch: 17 step: 271, loss is 0.0009438691195100546\n",
      "epoch: 17 step: 272, loss is 0.004538454115390778\n",
      "epoch: 17 step: 273, loss is 0.021035799756646156\n",
      "epoch: 17 step: 274, loss is 0.001661123358644545\n",
      "epoch: 17 step: 275, loss is 0.0044892579317092896\n",
      "epoch: 17 step: 276, loss is 0.00045210070675238967\n",
      "epoch: 17 step: 277, loss is 0.00936951581388712\n",
      "epoch: 17 step: 278, loss is 0.006138450000435114\n",
      "epoch: 17 step: 279, loss is 0.01867799460887909\n",
      "epoch: 17 step: 280, loss is 0.05685218796133995\n",
      "epoch: 17 step: 281, loss is 0.010704650543630123\n",
      "epoch: 17 step: 282, loss is 0.03740854188799858\n",
      "epoch: 17 step: 283, loss is 0.0013669478939846158\n",
      "epoch: 17 step: 284, loss is 0.0037473049014806747\n",
      "epoch: 17 step: 285, loss is 0.09793676435947418\n",
      "epoch: 17 step: 286, loss is 0.008688809350132942\n",
      "epoch: 17 step: 287, loss is 0.003322785021737218\n",
      "epoch: 17 step: 288, loss is 0.009807558730244637\n",
      "epoch: 17 step: 289, loss is 0.0005055522196926177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 290, loss is 0.011158584617078304\n",
      "epoch: 17 step: 291, loss is 0.0541958287358284\n",
      "epoch: 17 step: 292, loss is 0.03343508765101433\n",
      "epoch: 17 step: 293, loss is 0.028690481558442116\n",
      "epoch: 17 step: 294, loss is 0.007096505723893642\n",
      "epoch: 17 step: 295, loss is 0.0015171094564720988\n",
      "epoch: 17 step: 296, loss is 0.016452213749289513\n",
      "epoch: 17 step: 297, loss is 0.010815175250172615\n",
      "epoch: 17 step: 298, loss is 0.023800509050488472\n",
      "epoch: 17 step: 299, loss is 0.013560835272073746\n",
      "epoch: 17 step: 300, loss is 0.0028326427564024925\n",
      "epoch: 17 step: 301, loss is 0.005479864776134491\n",
      "epoch: 17 step: 302, loss is 0.010096689686179161\n",
      "epoch: 17 step: 303, loss is 0.0026251748204231262\n",
      "epoch: 17 step: 304, loss is 0.00954100489616394\n",
      "epoch: 17 step: 305, loss is 0.004537466913461685\n",
      "epoch: 17 step: 306, loss is 0.01337393093854189\n",
      "epoch: 17 step: 307, loss is 0.001044640433974564\n",
      "epoch: 17 step: 308, loss is 0.07259748876094818\n",
      "epoch: 17 step: 309, loss is 0.019196080043911934\n",
      "epoch: 17 step: 310, loss is 0.0016528945416212082\n",
      "epoch: 17 step: 311, loss is 0.0009114760323427618\n",
      "epoch: 17 step: 312, loss is 0.001006593811325729\n",
      "epoch: 17 step: 313, loss is 0.001309222774580121\n",
      "epoch: 17 step: 314, loss is 0.007267591543495655\n",
      "epoch: 17 step: 315, loss is 0.0030990487430244684\n",
      "epoch: 17 step: 316, loss is 0.016183815896511078\n",
      "epoch: 17 step: 317, loss is 0.010747049935162067\n",
      "epoch: 17 step: 318, loss is 0.003298328723758459\n",
      "epoch: 17 step: 319, loss is 0.016210265457630157\n",
      "epoch: 17 step: 320, loss is 0.00046781974378973246\n",
      "epoch: 17 step: 321, loss is 0.007188668940216303\n",
      "epoch: 17 step: 322, loss is 0.0034846665803343058\n",
      "epoch: 17 step: 323, loss is 0.020280588418245316\n",
      "epoch: 17 step: 324, loss is 0.004801146220415831\n",
      "epoch: 17 step: 325, loss is 0.0026120746042579412\n",
      "epoch: 17 step: 326, loss is 0.015301511622965336\n",
      "epoch: 17 step: 327, loss is 0.01091443095356226\n",
      "epoch: 17 step: 328, loss is 0.020283980295062065\n",
      "epoch: 17 step: 329, loss is 0.006665089167654514\n",
      "epoch: 17 step: 330, loss is 0.0038101281970739365\n",
      "epoch: 17 step: 331, loss is 0.0023708997759968042\n",
      "epoch: 17 step: 332, loss is 0.043735772371292114\n",
      "epoch: 17 step: 333, loss is 0.007647471036761999\n",
      "epoch: 17 step: 334, loss is 0.002476676367223263\n",
      "epoch: 17 step: 335, loss is 0.00120011274702847\n",
      "epoch: 17 step: 336, loss is 0.025940682739019394\n",
      "epoch: 17 step: 337, loss is 0.009005198255181313\n",
      "epoch: 17 step: 338, loss is 0.04945187643170357\n",
      "epoch: 17 step: 339, loss is 0.009683801792562008\n",
      "epoch: 17 step: 340, loss is 0.02288391813635826\n",
      "epoch: 17 step: 341, loss is 0.011403975076973438\n",
      "epoch: 17 step: 342, loss is 0.07377371191978455\n",
      "epoch: 17 step: 343, loss is 0.047783263027668\n",
      "epoch: 17 step: 344, loss is 0.00755505682900548\n",
      "epoch: 17 step: 345, loss is 0.01410913746803999\n",
      "epoch: 17 step: 346, loss is 0.004083593375980854\n",
      "epoch: 17 step: 347, loss is 0.030851833522319794\n",
      "epoch: 17 step: 348, loss is 0.007238626014441252\n",
      "epoch: 17 step: 349, loss is 0.012724723666906357\n",
      "epoch: 17 step: 350, loss is 0.006388755515217781\n",
      "epoch: 17 step: 351, loss is 0.05202677845954895\n",
      "epoch: 17 step: 352, loss is 0.012617109343409538\n",
      "epoch: 17 step: 353, loss is 0.056518103927373886\n",
      "epoch: 17 step: 354, loss is 0.004899886436760426\n",
      "epoch: 17 step: 355, loss is 0.05328379198908806\n",
      "epoch: 17 step: 356, loss is 0.01907540298998356\n",
      "epoch: 17 step: 357, loss is 0.00044891820289194584\n",
      "epoch: 17 step: 358, loss is 0.015228082425892353\n",
      "epoch: 17 step: 359, loss is 0.016600029543042183\n",
      "epoch: 17 step: 360, loss is 0.0025679722893983126\n",
      "epoch: 17 step: 361, loss is 0.002362919971346855\n",
      "epoch: 17 step: 362, loss is 0.004067150875926018\n",
      "epoch: 17 step: 363, loss is 0.01617744192481041\n",
      "epoch: 17 step: 364, loss is 0.031213194131851196\n",
      "epoch: 17 step: 365, loss is 0.028338516131043434\n",
      "epoch: 17 step: 366, loss is 0.028066745027899742\n",
      "epoch: 17 step: 367, loss is 0.01881333999335766\n",
      "epoch: 17 step: 368, loss is 0.04103076830506325\n",
      "epoch: 17 step: 369, loss is 0.021965399384498596\n",
      "epoch: 17 step: 370, loss is 0.0018645927775651217\n",
      "epoch: 17 step: 371, loss is 0.008353060111403465\n",
      "epoch: 17 step: 372, loss is 0.019564833492040634\n",
      "epoch: 17 step: 373, loss is 0.0011763639049604535\n",
      "epoch: 17 step: 374, loss is 0.03130470588803291\n",
      "epoch: 17 step: 375, loss is 0.09100522845983505\n",
      "epoch: 17 step: 376, loss is 0.003112246049568057\n",
      "epoch: 17 step: 377, loss is 0.02605634555220604\n",
      "epoch: 17 step: 378, loss is 0.010225612670183182\n",
      "epoch: 17 step: 379, loss is 0.033599428832530975\n",
      "epoch: 17 step: 380, loss is 0.006672172341495752\n",
      "epoch: 17 step: 381, loss is 0.001953598577529192\n",
      "epoch: 17 step: 382, loss is 0.010880907066166401\n",
      "epoch: 17 step: 383, loss is 0.010322285816073418\n",
      "epoch: 17 step: 384, loss is 0.01239029597491026\n",
      "epoch: 17 step: 385, loss is 0.004241327755153179\n",
      "epoch: 17 step: 386, loss is 0.0008300823392346501\n",
      "epoch: 17 step: 387, loss is 0.04703991487622261\n",
      "epoch: 17 step: 388, loss is 0.004508586134761572\n",
      "epoch: 17 step: 389, loss is 0.12854914367198944\n",
      "epoch: 17 step: 390, loss is 0.018315725028514862\n",
      "epoch: 17 step: 391, loss is 0.0112635288387537\n",
      "epoch: 17 step: 392, loss is 0.004070177674293518\n",
      "epoch: 17 step: 393, loss is 0.011940398253500462\n",
      "epoch: 17 step: 394, loss is 0.09826856106519699\n",
      "epoch: 17 step: 395, loss is 0.0015663753729313612\n",
      "epoch: 17 step: 396, loss is 0.008423435501754284\n",
      "epoch: 17 step: 397, loss is 0.0029298451263457537\n",
      "epoch: 17 step: 398, loss is 0.008861707523465157\n",
      "epoch: 17 step: 399, loss is 0.012143457308411598\n",
      "epoch: 17 step: 400, loss is 0.0016610148595646024\n",
      "epoch: 17 step: 401, loss is 0.17185229063034058\n",
      "epoch: 17 step: 402, loss is 0.01763371005654335\n",
      "epoch: 17 step: 403, loss is 0.00038898017373867333\n",
      "epoch: 17 step: 404, loss is 0.0038657316472381353\n",
      "epoch: 17 step: 405, loss is 0.03920481726527214\n",
      "epoch: 17 step: 406, loss is 0.00040563600487075746\n",
      "epoch: 17 step: 407, loss is 0.0021238471381366253\n",
      "epoch: 17 step: 408, loss is 0.12201865017414093\n",
      "epoch: 17 step: 409, loss is 0.002722944598644972\n",
      "epoch: 17 step: 410, loss is 0.015915509313344955\n",
      "epoch: 17 step: 411, loss is 0.012556992471218109\n",
      "epoch: 17 step: 412, loss is 0.0037962791975587606\n",
      "epoch: 17 step: 413, loss is 0.004073753487318754\n",
      "epoch: 17 step: 414, loss is 0.01406395435333252\n",
      "epoch: 17 step: 415, loss is 0.0007679015980102122\n",
      "epoch: 17 step: 416, loss is 0.003516195109114051\n",
      "epoch: 17 step: 417, loss is 0.01034822128713131\n",
      "epoch: 17 step: 418, loss is 0.014344874769449234\n",
      "epoch: 17 step: 419, loss is 0.03292427211999893\n",
      "epoch: 17 step: 420, loss is 0.034544140100479126\n",
      "epoch: 17 step: 421, loss is 0.0266624316573143\n",
      "epoch: 17 step: 422, loss is 0.0019150240113958716\n",
      "epoch: 17 step: 423, loss is 0.01394453551620245\n",
      "epoch: 17 step: 424, loss is 0.0038755277637392282\n",
      "epoch: 17 step: 425, loss is 0.007772189099341631\n",
      "epoch: 17 step: 426, loss is 0.06908594816923141\n",
      "epoch: 17 step: 427, loss is 0.004026357084512711\n",
      "epoch: 17 step: 428, loss is 0.006204583682119846\n",
      "epoch: 17 step: 429, loss is 0.011621398851275444\n",
      "epoch: 17 step: 430, loss is 0.016454830765724182\n",
      "epoch: 17 step: 431, loss is 0.06101980432868004\n",
      "epoch: 17 step: 432, loss is 0.0020132088102400303\n",
      "epoch: 17 step: 433, loss is 0.012993366457521915\n",
      "epoch: 17 step: 434, loss is 0.01754576899111271\n",
      "epoch: 17 step: 435, loss is 0.001039211987517774\n",
      "epoch: 17 step: 436, loss is 0.002836346160620451\n",
      "epoch: 17 step: 437, loss is 0.0018810906913131475\n",
      "epoch: 17 step: 438, loss is 0.024324005469679832\n",
      "epoch: 17 step: 439, loss is 0.0016613483894616365\n",
      "epoch: 17 step: 440, loss is 0.0029242108575999737\n",
      "epoch: 17 step: 441, loss is 0.010605685412883759\n",
      "epoch: 17 step: 442, loss is 0.10961966216564178\n",
      "epoch: 17 step: 443, loss is 0.00442849937826395\n",
      "epoch: 17 step: 444, loss is 0.005503226071596146\n",
      "epoch: 17 step: 445, loss is 0.0012891460210084915\n",
      "epoch: 17 step: 446, loss is 0.006809136364609003\n",
      "epoch: 17 step: 447, loss is 0.002986840670928359\n",
      "epoch: 17 step: 448, loss is 0.02223964035511017\n",
      "epoch: 17 step: 449, loss is 0.013762504793703556\n",
      "epoch: 17 step: 450, loss is 0.03431253507733345\n",
      "epoch: 17 step: 451, loss is 0.0012667099945247173\n",
      "epoch: 17 step: 452, loss is 0.014836408197879791\n",
      "epoch: 17 step: 453, loss is 0.058386217802762985\n",
      "epoch: 17 step: 454, loss is 0.0020805364474654198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 455, loss is 0.01815325953066349\n",
      "epoch: 17 step: 456, loss is 0.0029305163770914078\n",
      "epoch: 17 step: 457, loss is 0.04760979488492012\n",
      "epoch: 17 step: 458, loss is 0.004685061518102884\n",
      "epoch: 17 step: 459, loss is 0.0038587080780416727\n",
      "epoch: 17 step: 460, loss is 0.0015230390708893538\n",
      "epoch: 17 step: 461, loss is 0.00851348415017128\n",
      "epoch: 17 step: 462, loss is 0.02696559950709343\n",
      "epoch: 17 step: 463, loss is 0.10111413151025772\n",
      "epoch: 17 step: 464, loss is 0.0033266698010265827\n",
      "epoch: 17 step: 465, loss is 0.0960913598537445\n",
      "epoch: 17 step: 466, loss is 0.006527329795062542\n",
      "epoch: 17 step: 467, loss is 0.016641803085803986\n",
      "epoch: 17 step: 468, loss is 0.0010782195022329688\n",
      "epoch: 17 step: 469, loss is 0.04605498164892197\n",
      "epoch: 17 step: 470, loss is 0.015081830322742462\n",
      "epoch: 17 step: 471, loss is 0.02501627616584301\n",
      "epoch: 17 step: 472, loss is 0.009651442989706993\n",
      "epoch: 17 step: 473, loss is 0.024576541036367416\n",
      "epoch: 17 step: 474, loss is 0.011342338286340237\n",
      "epoch: 17 step: 475, loss is 0.13038121163845062\n",
      "epoch: 17 step: 476, loss is 0.010157308541238308\n",
      "epoch: 17 step: 477, loss is 0.005314650945365429\n",
      "epoch: 17 step: 478, loss is 0.00018728508439380676\n",
      "epoch: 17 step: 479, loss is 0.00044611672637984157\n",
      "epoch: 17 step: 480, loss is 0.03454429656267166\n",
      "epoch: 17 step: 481, loss is 0.0035481080412864685\n",
      "epoch: 17 step: 482, loss is 0.005838773678988218\n",
      "epoch: 17 step: 483, loss is 0.00223440770059824\n",
      "epoch: 17 step: 484, loss is 0.036667514592409134\n",
      "epoch: 17 step: 485, loss is 0.03213980793952942\n",
      "epoch: 17 step: 486, loss is 0.0011117717949673533\n",
      "epoch: 17 step: 487, loss is 0.010663319379091263\n",
      "epoch: 17 step: 488, loss is 0.0032844971865415573\n",
      "epoch: 17 step: 489, loss is 0.0030382925178855658\n",
      "epoch: 17 step: 490, loss is 0.0011219449806958437\n",
      "epoch: 17 step: 491, loss is 0.023713285103440285\n",
      "epoch: 17 step: 492, loss is 0.0006750539760105312\n",
      "epoch: 17 step: 493, loss is 0.003386339172720909\n",
      "epoch: 17 step: 494, loss is 0.005150074604898691\n",
      "epoch: 17 step: 495, loss is 0.0058658248744904995\n",
      "epoch: 17 step: 496, loss is 0.025648564100265503\n",
      "epoch: 17 step: 497, loss is 0.09091055393218994\n",
      "epoch: 17 step: 498, loss is 0.011483428068459034\n",
      "epoch: 17 step: 499, loss is 0.024217115715146065\n",
      "epoch: 17 step: 500, loss is 0.005385199561715126\n",
      "epoch: 17 step: 501, loss is 0.0290389321744442\n",
      "epoch: 17 step: 502, loss is 0.0038545706775039434\n",
      "epoch: 17 step: 503, loss is 0.016549278050661087\n",
      "epoch: 17 step: 504, loss is 0.0004802801995538175\n",
      "epoch: 17 step: 505, loss is 0.0011225007474422455\n",
      "epoch: 17 step: 506, loss is 0.005298049654811621\n",
      "epoch: 17 step: 507, loss is 0.03234848752617836\n",
      "epoch: 17 step: 508, loss is 0.00453817518427968\n",
      "epoch: 17 step: 509, loss is 0.028137579560279846\n",
      "epoch: 17 step: 510, loss is 0.02494714967906475\n",
      "epoch: 17 step: 511, loss is 0.011923790909349918\n",
      "epoch: 17 step: 512, loss is 0.010668603703379631\n",
      "epoch: 17 step: 513, loss is 0.007275265641510487\n",
      "epoch: 17 step: 514, loss is 0.016139784827828407\n",
      "epoch: 17 step: 515, loss is 0.003894487163051963\n",
      "epoch: 17 step: 516, loss is 0.03341154381632805\n",
      "epoch: 17 step: 517, loss is 0.016996102407574654\n",
      "epoch: 17 step: 518, loss is 0.00030801480170339346\n",
      "epoch: 17 step: 519, loss is 0.001825242885388434\n",
      "epoch: 17 step: 520, loss is 0.002656215336173773\n",
      "epoch: 17 step: 521, loss is 0.005933514330536127\n",
      "epoch: 17 step: 522, loss is 0.07461138814687729\n",
      "epoch: 17 step: 523, loss is 0.042659565806388855\n",
      "epoch: 17 step: 524, loss is 0.0011993834050372243\n",
      "epoch: 17 step: 525, loss is 0.07166634500026703\n",
      "epoch: 17 step: 526, loss is 0.0020450667943805456\n",
      "epoch: 17 step: 527, loss is 0.05078260228037834\n",
      "epoch: 17 step: 528, loss is 0.010120406746864319\n",
      "epoch: 17 step: 529, loss is 0.0011244979687035084\n",
      "epoch: 17 step: 530, loss is 0.012019813060760498\n",
      "epoch: 17 step: 531, loss is 0.011934843845665455\n",
      "epoch: 17 step: 532, loss is 0.00947539508342743\n",
      "epoch: 17 step: 533, loss is 0.0016382252797484398\n",
      "epoch: 17 step: 534, loss is 0.027384813874959946\n",
      "epoch: 17 step: 535, loss is 0.04801497608423233\n",
      "epoch: 17 step: 536, loss is 0.03698151558637619\n",
      "epoch: 17 step: 537, loss is 0.003565080463886261\n",
      "epoch: 17 step: 538, loss is 0.03241110220551491\n",
      "epoch: 17 step: 539, loss is 0.01123565062880516\n",
      "epoch: 17 step: 540, loss is 0.0038353754207491875\n",
      "epoch: 17 step: 541, loss is 0.0029339927714318037\n",
      "epoch: 17 step: 542, loss is 0.0049035679548978806\n",
      "epoch: 17 step: 543, loss is 0.0016672407509759068\n",
      "epoch: 17 step: 544, loss is 0.02554873377084732\n",
      "epoch: 17 step: 545, loss is 0.17102089524269104\n",
      "epoch: 17 step: 546, loss is 0.020241672173142433\n",
      "epoch: 17 step: 547, loss is 0.0016819663578644395\n",
      "epoch: 17 step: 548, loss is 0.004961820784956217\n",
      "epoch: 17 step: 549, loss is 0.03445817530155182\n",
      "epoch: 17 step: 550, loss is 0.003908334765583277\n",
      "epoch: 17 step: 551, loss is 0.013788258656859398\n",
      "epoch: 17 step: 552, loss is 0.02172190509736538\n",
      "epoch: 17 step: 553, loss is 0.0038860668428242207\n",
      "epoch: 17 step: 554, loss is 0.003257845062762499\n",
      "epoch: 17 step: 555, loss is 0.002697753021493554\n",
      "epoch: 17 step: 556, loss is 0.0030726096592843533\n",
      "epoch: 17 step: 557, loss is 0.03639943152666092\n",
      "epoch: 17 step: 558, loss is 0.0035474300384521484\n",
      "epoch: 17 step: 559, loss is 0.02157740294933319\n",
      "epoch: 17 step: 560, loss is 0.0021520245354622602\n",
      "epoch: 17 step: 561, loss is 0.0048126839101314545\n",
      "epoch: 17 step: 562, loss is 0.07124421745538712\n",
      "epoch: 17 step: 563, loss is 0.019209714606404305\n",
      "epoch: 17 step: 564, loss is 0.0015155111905187368\n",
      "epoch: 17 step: 565, loss is 0.004168033134192228\n",
      "epoch: 17 step: 566, loss is 0.0004856961313635111\n",
      "epoch: 17 step: 567, loss is 0.005530222784727812\n",
      "epoch: 17 step: 568, loss is 0.12290148437023163\n",
      "epoch: 17 step: 569, loss is 0.033773619681596756\n",
      "epoch: 17 step: 570, loss is 0.02509189397096634\n",
      "epoch: 17 step: 571, loss is 0.004178567323833704\n",
      "epoch: 17 step: 572, loss is 0.008131502196192741\n",
      "epoch: 17 step: 573, loss is 0.006251777056604624\n",
      "epoch: 17 step: 574, loss is 0.030400950461626053\n",
      "epoch: 17 step: 575, loss is 0.004517167340964079\n",
      "epoch: 17 step: 576, loss is 0.034763552248477936\n",
      "epoch: 17 step: 577, loss is 0.020198781043291092\n",
      "epoch: 17 step: 578, loss is 0.005784105975180864\n",
      "epoch: 17 step: 579, loss is 0.0014747765380889177\n",
      "epoch: 17 step: 580, loss is 0.0005001045647077262\n",
      "epoch: 17 step: 581, loss is 0.013210458680987358\n",
      "epoch: 17 step: 582, loss is 0.057444531470537186\n",
      "epoch: 17 step: 583, loss is 0.12998263537883759\n",
      "epoch: 17 step: 584, loss is 0.0008852260652929544\n",
      "epoch: 17 step: 585, loss is 0.006141799967736006\n",
      "epoch: 17 step: 586, loss is 0.003285916754975915\n",
      "epoch: 17 step: 587, loss is 0.007560829631984234\n",
      "epoch: 17 step: 588, loss is 0.0035990476608276367\n",
      "epoch: 17 step: 589, loss is 0.009175189770758152\n",
      "epoch: 17 step: 590, loss is 0.02023964188992977\n",
      "epoch: 17 step: 591, loss is 0.0002613375836517662\n",
      "epoch: 17 step: 592, loss is 0.029246632009744644\n",
      "epoch: 17 step: 593, loss is 0.030531877651810646\n",
      "epoch: 17 step: 594, loss is 0.08645450323820114\n",
      "epoch: 17 step: 595, loss is 0.05232740938663483\n",
      "epoch: 17 step: 596, loss is 0.05371018126606941\n",
      "epoch: 17 step: 597, loss is 0.006869711447507143\n",
      "epoch: 17 step: 598, loss is 0.04537920281291008\n",
      "epoch: 17 step: 599, loss is 0.11352130770683289\n",
      "epoch: 17 step: 600, loss is 0.017374325543642044\n",
      "epoch: 17 step: 601, loss is 0.00333953695371747\n",
      "epoch: 17 step: 602, loss is 0.021278157830238342\n",
      "epoch: 17 step: 603, loss is 0.0002492109197191894\n",
      "epoch: 17 step: 604, loss is 0.007715507876127958\n",
      "epoch: 17 step: 605, loss is 0.0028679065871983767\n",
      "epoch: 17 step: 606, loss is 0.09350217133760452\n",
      "epoch: 17 step: 607, loss is 0.019473707303404808\n",
      "epoch: 17 step: 608, loss is 0.005998985841870308\n",
      "epoch: 17 step: 609, loss is 0.0016353174578398466\n",
      "epoch: 17 step: 610, loss is 0.00665624625980854\n",
      "epoch: 17 step: 611, loss is 0.023269454017281532\n",
      "epoch: 17 step: 612, loss is 0.010017952881753445\n",
      "epoch: 17 step: 613, loss is 0.03760815039277077\n",
      "epoch: 17 step: 614, loss is 0.007976344786584377\n",
      "epoch: 17 step: 615, loss is 0.02354525588452816\n",
      "epoch: 17 step: 616, loss is 0.06901766359806061\n",
      "epoch: 17 step: 617, loss is 0.020947882905602455\n",
      "epoch: 17 step: 618, loss is 0.01032834779471159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 619, loss is 0.011180642992258072\n",
      "epoch: 17 step: 620, loss is 0.0017105229198932648\n",
      "epoch: 17 step: 621, loss is 0.004709488246589899\n",
      "epoch: 17 step: 622, loss is 0.012821856886148453\n",
      "epoch: 17 step: 623, loss is 0.000410924491006881\n",
      "epoch: 17 step: 624, loss is 0.024764327332377434\n",
      "epoch: 17 step: 625, loss is 0.0029004388488829136\n",
      "epoch: 17 step: 626, loss is 0.0646202564239502\n",
      "epoch: 17 step: 627, loss is 0.025128791108727455\n",
      "epoch: 17 step: 628, loss is 0.02672666683793068\n",
      "epoch: 17 step: 629, loss is 0.008969446644186974\n",
      "epoch: 17 step: 630, loss is 0.006253355648368597\n",
      "epoch: 17 step: 631, loss is 0.01640857383608818\n",
      "epoch: 17 step: 632, loss is 0.02453196793794632\n",
      "epoch: 17 step: 633, loss is 0.039670180529356\n",
      "epoch: 17 step: 634, loss is 0.02259047143161297\n",
      "epoch: 17 step: 635, loss is 0.03841553255915642\n",
      "epoch: 17 step: 636, loss is 0.0006872753729112446\n",
      "epoch: 17 step: 637, loss is 0.0005785999237559736\n",
      "epoch: 17 step: 638, loss is 0.009612047113478184\n",
      "epoch: 17 step: 639, loss is 0.011921321973204613\n",
      "epoch: 17 step: 640, loss is 0.011225431226193905\n",
      "epoch: 17 step: 641, loss is 0.08672129362821579\n",
      "epoch: 17 step: 642, loss is 0.0017076688818633556\n",
      "epoch: 17 step: 643, loss is 0.019386006519198418\n",
      "epoch: 17 step: 644, loss is 0.0032042486127465963\n",
      "epoch: 17 step: 645, loss is 0.04507550969719887\n",
      "epoch: 17 step: 646, loss is 0.0029148682951927185\n",
      "epoch: 17 step: 647, loss is 0.008905114606022835\n",
      "epoch: 17 step: 648, loss is 0.010249926708638668\n",
      "epoch: 17 step: 649, loss is 0.008729358203709126\n",
      "epoch: 17 step: 650, loss is 0.03995710611343384\n",
      "epoch: 17 step: 651, loss is 0.0005703941569663584\n",
      "epoch: 17 step: 652, loss is 0.0880005806684494\n",
      "epoch: 17 step: 653, loss is 0.007325704209506512\n",
      "epoch: 17 step: 654, loss is 0.00352319423109293\n",
      "epoch: 17 step: 655, loss is 0.04605701193213463\n",
      "epoch: 17 step: 656, loss is 0.090781569480896\n",
      "epoch: 17 step: 657, loss is 0.006178327836096287\n",
      "epoch: 17 step: 658, loss is 0.02721959725022316\n",
      "epoch: 17 step: 659, loss is 0.007545765023678541\n",
      "epoch: 17 step: 660, loss is 0.005090930033475161\n",
      "epoch: 17 step: 661, loss is 0.004132687579840422\n",
      "epoch: 17 step: 662, loss is 0.022847365587949753\n",
      "epoch: 17 step: 663, loss is 0.0003198350896127522\n",
      "epoch: 17 step: 664, loss is 0.007455313578248024\n",
      "epoch: 17 step: 665, loss is 0.0055732736364007\n",
      "epoch: 17 step: 666, loss is 0.014566103927791119\n",
      "epoch: 17 step: 667, loss is 0.0068264068104326725\n",
      "epoch: 17 step: 668, loss is 0.005925867706537247\n",
      "epoch: 17 step: 669, loss is 0.006177176721394062\n",
      "epoch: 17 step: 670, loss is 0.015160943381488323\n",
      "epoch: 17 step: 671, loss is 0.0035266350023448467\n",
      "epoch: 17 step: 672, loss is 0.0032694372348487377\n",
      "epoch: 17 step: 673, loss is 0.0048002139665186405\n",
      "epoch: 17 step: 674, loss is 6.260291411308572e-05\n",
      "epoch: 17 step: 675, loss is 0.02380373887717724\n",
      "epoch: 17 step: 676, loss is 0.00042712362483143806\n",
      "epoch: 17 step: 677, loss is 0.0006146409432403743\n",
      "epoch: 17 step: 678, loss is 0.007341798860579729\n",
      "epoch: 17 step: 679, loss is 0.01592784933745861\n",
      "epoch: 17 step: 680, loss is 0.01666981354355812\n",
      "epoch: 17 step: 681, loss is 0.0012322495458647609\n",
      "epoch: 17 step: 682, loss is 0.0028989482671022415\n",
      "epoch: 17 step: 683, loss is 0.0021161241456866264\n",
      "epoch: 17 step: 684, loss is 0.009092765860259533\n",
      "epoch: 17 step: 685, loss is 0.001874833949841559\n",
      "epoch: 17 step: 686, loss is 0.002988700056448579\n",
      "epoch: 17 step: 687, loss is 0.00019499943300615996\n",
      "epoch: 17 step: 688, loss is 0.0015377569943666458\n",
      "epoch: 17 step: 689, loss is 0.047436073422431946\n",
      "epoch: 17 step: 690, loss is 0.003438182407990098\n",
      "epoch: 17 step: 691, loss is 0.014508270658552647\n",
      "epoch: 17 step: 692, loss is 0.004462284967303276\n",
      "epoch: 17 step: 693, loss is 0.00576529186218977\n",
      "epoch: 17 step: 694, loss is 0.014015823602676392\n",
      "epoch: 17 step: 695, loss is 0.0023790120612829924\n",
      "epoch: 17 step: 696, loss is 0.023688849061727524\n",
      "epoch: 17 step: 697, loss is 0.005992640275508165\n",
      "epoch: 17 step: 698, loss is 0.010448324494063854\n",
      "epoch: 17 step: 699, loss is 0.02064698189496994\n",
      "epoch: 17 step: 700, loss is 0.006837488152086735\n",
      "epoch: 17 step: 701, loss is 0.012136276811361313\n",
      "epoch: 17 step: 702, loss is 0.006353666074573994\n",
      "epoch: 17 step: 703, loss is 0.018663179129362106\n",
      "epoch: 17 step: 704, loss is 0.01416084449738264\n",
      "epoch: 17 step: 705, loss is 0.0022928686812520027\n",
      "epoch: 17 step: 706, loss is 0.04667564108967781\n",
      "epoch: 17 step: 707, loss is 0.029459796845912933\n",
      "epoch: 17 step: 708, loss is 0.060437463223934174\n",
      "epoch: 17 step: 709, loss is 0.011196494102478027\n",
      "epoch: 17 step: 710, loss is 0.0018251413712278008\n",
      "epoch: 17 step: 711, loss is 0.03783104568719864\n",
      "epoch: 17 step: 712, loss is 0.004212798550724983\n",
      "epoch: 17 step: 713, loss is 0.10120426118373871\n",
      "epoch: 17 step: 714, loss is 0.0018393353093415499\n",
      "epoch: 17 step: 715, loss is 0.11277712136507034\n",
      "epoch: 17 step: 716, loss is 0.04187531769275665\n",
      "epoch: 17 step: 717, loss is 0.00405174819752574\n",
      "epoch: 17 step: 718, loss is 0.025379328057169914\n",
      "epoch: 17 step: 719, loss is 0.10914884507656097\n",
      "epoch: 17 step: 720, loss is 0.020283667370676994\n",
      "epoch: 17 step: 721, loss is 0.02360590174794197\n",
      "epoch: 17 step: 722, loss is 0.0022385630290955305\n",
      "epoch: 17 step: 723, loss is 0.017948659136891365\n",
      "epoch: 17 step: 724, loss is 0.007325427606701851\n",
      "epoch: 17 step: 725, loss is 0.000338787620421499\n",
      "epoch: 17 step: 726, loss is 0.0803249329328537\n",
      "epoch: 17 step: 727, loss is 0.01841815933585167\n",
      "epoch: 17 step: 728, loss is 0.011080198921263218\n",
      "epoch: 17 step: 729, loss is 0.0019987057894468307\n",
      "epoch: 17 step: 730, loss is 0.024648554623126984\n",
      "epoch: 17 step: 731, loss is 0.00842522457242012\n",
      "epoch: 17 step: 732, loss is 0.042596567422151566\n",
      "epoch: 17 step: 733, loss is 0.007334420923143625\n",
      "epoch: 17 step: 734, loss is 0.0011067759478464723\n",
      "epoch: 17 step: 735, loss is 0.046536900103092194\n",
      "epoch: 17 step: 736, loss is 0.0013272830983623862\n",
      "epoch: 17 step: 737, loss is 0.0007284241728484631\n",
      "epoch: 17 step: 738, loss is 0.036372631788253784\n",
      "epoch: 17 step: 739, loss is 0.004811742343008518\n",
      "epoch: 17 step: 740, loss is 0.09114765375852585\n",
      "epoch: 17 step: 741, loss is 0.004892823286354542\n",
      "epoch: 17 step: 742, loss is 0.13540585339069366\n",
      "epoch: 17 step: 743, loss is 0.0580814927816391\n",
      "epoch: 17 step: 744, loss is 0.001652646460570395\n",
      "epoch: 17 step: 745, loss is 0.005689789075404406\n",
      "epoch: 17 step: 746, loss is 0.03635444864630699\n",
      "epoch: 17 step: 747, loss is 0.023500971496105194\n",
      "epoch: 17 step: 748, loss is 0.08181647956371307\n",
      "epoch: 17 step: 749, loss is 0.03450338542461395\n",
      "epoch: 17 step: 750, loss is 0.012823360040783882\n",
      "epoch: 17 step: 751, loss is 0.021494193002581596\n",
      "epoch: 17 step: 752, loss is 0.047643356025218964\n",
      "epoch: 17 step: 753, loss is 0.0033065681345760822\n",
      "epoch: 17 step: 754, loss is 0.05075009912252426\n",
      "epoch: 17 step: 755, loss is 0.0020218766294419765\n",
      "epoch: 17 step: 756, loss is 0.005693615414202213\n",
      "epoch: 17 step: 757, loss is 0.008535407483577728\n",
      "epoch: 17 step: 758, loss is 0.08302117139101028\n",
      "epoch: 17 step: 759, loss is 0.02040781080722809\n",
      "epoch: 17 step: 760, loss is 0.01880260556936264\n",
      "epoch: 17 step: 761, loss is 0.0019358063582330942\n",
      "epoch: 17 step: 762, loss is 0.0578288696706295\n",
      "epoch: 17 step: 763, loss is 0.014087622985243797\n",
      "epoch: 17 step: 764, loss is 0.05977560952305794\n",
      "epoch: 17 step: 765, loss is 0.11309977620840073\n",
      "epoch: 17 step: 766, loss is 0.00961181428283453\n",
      "epoch: 17 step: 767, loss is 0.03625081479549408\n",
      "epoch: 17 step: 768, loss is 0.003367766970768571\n",
      "epoch: 17 step: 769, loss is 0.022924024611711502\n",
      "epoch: 17 step: 770, loss is 0.009781589731574059\n",
      "epoch: 17 step: 771, loss is 0.013716056942939758\n",
      "epoch: 17 step: 772, loss is 0.02319221943616867\n",
      "epoch: 17 step: 773, loss is 0.00436745211482048\n",
      "epoch: 17 step: 774, loss is 0.06120777875185013\n",
      "epoch: 17 step: 775, loss is 0.013411729596555233\n",
      "epoch: 17 step: 776, loss is 0.03995160013437271\n",
      "epoch: 17 step: 777, loss is 0.188181072473526\n",
      "epoch: 17 step: 778, loss is 0.0011595740215852857\n",
      "epoch: 17 step: 779, loss is 0.06420447677373886\n",
      "epoch: 17 step: 780, loss is 0.013099197298288345\n",
      "epoch: 17 step: 781, loss is 0.04420427232980728\n",
      "epoch: 17 step: 782, loss is 0.0268778707832098\n",
      "epoch: 17 step: 783, loss is 0.03188377618789673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 784, loss is 0.029897307977080345\n",
      "epoch: 17 step: 785, loss is 0.014073316939175129\n",
      "epoch: 17 step: 786, loss is 0.011664390563964844\n",
      "epoch: 17 step: 787, loss is 0.11627066135406494\n",
      "epoch: 17 step: 788, loss is 0.0031847746577113867\n",
      "epoch: 17 step: 789, loss is 0.05846903473138809\n",
      "epoch: 17 step: 790, loss is 0.008628997020423412\n",
      "epoch: 17 step: 791, loss is 0.07962870597839355\n",
      "epoch: 17 step: 792, loss is 0.015386665239930153\n",
      "epoch: 17 step: 793, loss is 0.0005067863967269659\n",
      "epoch: 17 step: 794, loss is 0.01865820214152336\n",
      "epoch: 17 step: 795, loss is 0.025875503197312355\n",
      "epoch: 17 step: 796, loss is 0.005320776253938675\n",
      "epoch: 17 step: 797, loss is 0.1922454535961151\n",
      "epoch: 17 step: 798, loss is 0.006676344200968742\n",
      "epoch: 17 step: 799, loss is 0.020424019545316696\n",
      "epoch: 17 step: 800, loss is 0.009734898805618286\n",
      "epoch: 17 step: 801, loss is 0.007237247657030821\n",
      "epoch: 17 step: 802, loss is 0.05122336372733116\n",
      "epoch: 17 step: 803, loss is 0.03290236368775368\n",
      "epoch: 17 step: 804, loss is 0.09112415462732315\n",
      "epoch: 17 step: 805, loss is 0.06544678658246994\n",
      "epoch: 17 step: 806, loss is 0.004891237709671259\n",
      "epoch: 17 step: 807, loss is 0.012500855140388012\n",
      "epoch: 17 step: 808, loss is 0.010267420671880245\n",
      "epoch: 17 step: 809, loss is 0.009173547849059105\n",
      "epoch: 17 step: 810, loss is 0.012727079913020134\n",
      "epoch: 17 step: 811, loss is 0.006565294694155455\n",
      "epoch: 17 step: 812, loss is 0.030883239582180977\n",
      "epoch: 17 step: 813, loss is 0.002365467371419072\n",
      "epoch: 17 step: 814, loss is 0.029225125908851624\n",
      "epoch: 17 step: 815, loss is 0.005500670988112688\n",
      "epoch: 17 step: 816, loss is 0.009265050292015076\n",
      "epoch: 17 step: 817, loss is 0.02565803937613964\n",
      "epoch: 17 step: 818, loss is 0.04011320695281029\n",
      "epoch: 17 step: 819, loss is 0.06571080535650253\n",
      "epoch: 17 step: 820, loss is 0.005292273126542568\n",
      "epoch: 17 step: 821, loss is 0.07678883522748947\n",
      "epoch: 17 step: 822, loss is 0.057628363370895386\n",
      "epoch: 17 step: 823, loss is 0.0033002865966409445\n",
      "epoch: 17 step: 824, loss is 0.004378606099635363\n",
      "epoch: 17 step: 825, loss is 0.0005891782930120826\n",
      "epoch: 17 step: 826, loss is 0.09656912833452225\n",
      "epoch: 17 step: 827, loss is 0.018588537350296974\n",
      "epoch: 17 step: 828, loss is 0.012878200970590115\n",
      "epoch: 17 step: 829, loss is 0.024333951994776726\n",
      "epoch: 17 step: 830, loss is 0.004823945462703705\n",
      "epoch: 17 step: 831, loss is 0.017711110413074493\n",
      "epoch: 17 step: 832, loss is 0.027358729392290115\n",
      "epoch: 17 step: 833, loss is 0.017452800646424294\n",
      "epoch: 17 step: 834, loss is 0.005549195222556591\n",
      "epoch: 17 step: 835, loss is 0.021438509225845337\n",
      "epoch: 17 step: 836, loss is 0.024943973869085312\n",
      "epoch: 17 step: 837, loss is 0.014737013727426529\n",
      "epoch: 17 step: 838, loss is 0.0005911634070798755\n",
      "epoch: 17 step: 839, loss is 0.022424565628170967\n",
      "epoch: 17 step: 840, loss is 0.01683955267071724\n",
      "epoch: 17 step: 841, loss is 0.020045941695570946\n",
      "epoch: 17 step: 842, loss is 0.07525136321783066\n",
      "epoch: 17 step: 843, loss is 0.039299510419368744\n",
      "epoch: 17 step: 844, loss is 0.005629559047520161\n",
      "epoch: 17 step: 845, loss is 0.0035943654365837574\n",
      "epoch: 17 step: 846, loss is 0.003426594892516732\n",
      "epoch: 17 step: 847, loss is 0.06196162477135658\n",
      "epoch: 17 step: 848, loss is 0.00902305357158184\n",
      "epoch: 17 step: 849, loss is 0.051996152848005295\n",
      "epoch: 17 step: 850, loss is 0.028249578550457954\n",
      "epoch: 17 step: 851, loss is 0.002887462032958865\n",
      "epoch: 17 step: 852, loss is 0.12399320304393768\n",
      "epoch: 17 step: 853, loss is 0.0006211850559338927\n",
      "epoch: 17 step: 854, loss is 0.014862656593322754\n",
      "epoch: 17 step: 855, loss is 0.017084607854485512\n",
      "epoch: 17 step: 856, loss is 0.02422579750418663\n",
      "epoch: 17 step: 857, loss is 0.003236934542655945\n",
      "epoch: 17 step: 858, loss is 0.02938258834183216\n",
      "epoch: 17 step: 859, loss is 0.010070417076349258\n",
      "epoch: 17 step: 860, loss is 0.0007973770261742175\n",
      "epoch: 17 step: 861, loss is 0.006857790052890778\n",
      "epoch: 17 step: 862, loss is 0.009450082667171955\n",
      "epoch: 17 step: 863, loss is 0.012352644465863705\n",
      "epoch: 17 step: 864, loss is 0.03143337741494179\n",
      "epoch: 17 step: 865, loss is 0.005124034360051155\n",
      "epoch: 17 step: 866, loss is 0.002580462023615837\n",
      "epoch: 17 step: 867, loss is 0.13825194537639618\n",
      "epoch: 17 step: 868, loss is 0.005422709975391626\n",
      "epoch: 17 step: 869, loss is 0.007431123871356249\n",
      "epoch: 17 step: 870, loss is 0.0024203858338296413\n",
      "epoch: 17 step: 871, loss is 0.0013918097829446197\n",
      "epoch: 17 step: 872, loss is 0.05437607318162918\n",
      "epoch: 17 step: 873, loss is 0.0033136664424091578\n",
      "epoch: 17 step: 874, loss is 0.0035501534584909678\n",
      "epoch: 17 step: 875, loss is 0.003990906290709972\n",
      "epoch: 17 step: 876, loss is 0.03497656062245369\n",
      "epoch: 17 step: 877, loss is 0.030198724940419197\n",
      "epoch: 17 step: 878, loss is 0.029071243479847908\n",
      "epoch: 17 step: 879, loss is 0.06819039583206177\n",
      "epoch: 17 step: 880, loss is 0.008956653997302055\n",
      "epoch: 17 step: 881, loss is 0.009472118690609932\n",
      "epoch: 17 step: 882, loss is 0.0041060419753193855\n",
      "epoch: 17 step: 883, loss is 0.017879586666822433\n",
      "epoch: 17 step: 884, loss is 0.017163708806037903\n",
      "epoch: 17 step: 885, loss is 0.005217215046286583\n",
      "epoch: 17 step: 886, loss is 0.0853419303894043\n",
      "epoch: 17 step: 887, loss is 0.054617542773485184\n",
      "epoch: 17 step: 888, loss is 0.009290244430303574\n",
      "epoch: 17 step: 889, loss is 0.07416876405477524\n",
      "epoch: 17 step: 890, loss is 0.0006154234288260341\n",
      "epoch: 17 step: 891, loss is 0.024031996726989746\n",
      "epoch: 17 step: 892, loss is 0.006384758744388819\n",
      "epoch: 17 step: 893, loss is 0.1282547265291214\n",
      "epoch: 17 step: 894, loss is 0.037669889628887177\n",
      "epoch: 17 step: 895, loss is 0.0006538936286233366\n",
      "epoch: 17 step: 896, loss is 0.10903820395469666\n",
      "epoch: 17 step: 897, loss is 0.019598037004470825\n",
      "epoch: 17 step: 898, loss is 0.02964746579527855\n",
      "epoch: 17 step: 899, loss is 0.003785193432122469\n",
      "epoch: 17 step: 900, loss is 0.005072146654129028\n",
      "epoch: 17 step: 901, loss is 0.011282685212790966\n",
      "epoch: 17 step: 902, loss is 0.030253781005740166\n",
      "epoch: 17 step: 903, loss is 0.04939888417720795\n",
      "epoch: 17 step: 904, loss is 0.017394818365573883\n",
      "epoch: 17 step: 905, loss is 0.0006902386667206883\n",
      "epoch: 17 step: 906, loss is 0.021376341581344604\n",
      "epoch: 17 step: 907, loss is 0.09468399733304977\n",
      "epoch: 17 step: 908, loss is 0.01395333930850029\n",
      "epoch: 17 step: 909, loss is 0.027659496292471886\n",
      "epoch: 17 step: 910, loss is 0.03921305760741234\n",
      "epoch: 17 step: 911, loss is 0.04546233266592026\n",
      "epoch: 17 step: 912, loss is 0.01716489903628826\n",
      "epoch: 17 step: 913, loss is 0.003112146630883217\n",
      "epoch: 17 step: 914, loss is 0.11300063878297806\n",
      "epoch: 17 step: 915, loss is 0.044893842190504074\n",
      "epoch: 17 step: 916, loss is 0.011560038663446903\n",
      "epoch: 17 step: 917, loss is 0.020461581647396088\n",
      "epoch: 17 step: 918, loss is 0.02773495949804783\n",
      "epoch: 17 step: 919, loss is 0.1266198307275772\n",
      "epoch: 17 step: 920, loss is 0.008021135814487934\n",
      "epoch: 17 step: 921, loss is 0.0009739856468513608\n",
      "epoch: 17 step: 922, loss is 0.02840440534055233\n",
      "epoch: 17 step: 923, loss is 0.028995871543884277\n",
      "epoch: 17 step: 924, loss is 0.018747800961136818\n",
      "epoch: 17 step: 925, loss is 0.012098240666091442\n",
      "epoch: 17 step: 926, loss is 0.021440202370285988\n",
      "epoch: 17 step: 927, loss is 0.02362682670354843\n",
      "epoch: 17 step: 928, loss is 0.01337448600679636\n",
      "epoch: 17 step: 929, loss is 0.009372935630381107\n",
      "epoch: 17 step: 930, loss is 0.0038408224936574697\n",
      "epoch: 17 step: 931, loss is 0.0667705088853836\n",
      "epoch: 17 step: 932, loss is 0.08815989643335342\n",
      "epoch: 17 step: 933, loss is 0.06226024031639099\n",
      "epoch: 17 step: 934, loss is 0.00961228460073471\n",
      "epoch: 17 step: 935, loss is 0.03176535665988922\n",
      "epoch: 17 step: 936, loss is 0.005013908725231886\n",
      "epoch: 17 step: 937, loss is 0.007362473756074905\n",
      "epoch: 18 step: 1, loss is 0.08002986013889313\n",
      "epoch: 18 step: 2, loss is 0.019000478088855743\n",
      "epoch: 18 step: 3, loss is 0.003235800424590707\n",
      "epoch: 18 step: 4, loss is 0.014600015245378017\n",
      "epoch: 18 step: 5, loss is 0.0009540864266455173\n",
      "epoch: 18 step: 6, loss is 0.0109425513073802\n",
      "epoch: 18 step: 7, loss is 0.013458320870995522\n",
      "epoch: 18 step: 8, loss is 0.007208844646811485\n",
      "epoch: 18 step: 9, loss is 0.0032329570967704058\n",
      "epoch: 18 step: 10, loss is 0.06519411504268646\n",
      "epoch: 18 step: 11, loss is 0.01103188656270504\n",
      "epoch: 18 step: 12, loss is 0.004008866380900145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 13, loss is 0.03176915645599365\n",
      "epoch: 18 step: 14, loss is 0.0007281149155460298\n",
      "epoch: 18 step: 15, loss is 0.0024156218860298395\n",
      "epoch: 18 step: 16, loss is 0.0009244746179319918\n",
      "epoch: 18 step: 17, loss is 0.0318845696747303\n",
      "epoch: 18 step: 18, loss is 0.004264218732714653\n",
      "epoch: 18 step: 19, loss is 0.0011591895017772913\n",
      "epoch: 18 step: 20, loss is 0.012372849509119987\n",
      "epoch: 18 step: 21, loss is 0.004800464957952499\n",
      "epoch: 18 step: 22, loss is 0.006061537191271782\n",
      "epoch: 18 step: 23, loss is 0.01336122490465641\n",
      "epoch: 18 step: 24, loss is 0.012120060622692108\n",
      "epoch: 18 step: 25, loss is 0.006151304580271244\n",
      "epoch: 18 step: 26, loss is 0.04935901239514351\n",
      "epoch: 18 step: 27, loss is 0.0004743420286104083\n",
      "epoch: 18 step: 28, loss is 0.0036692405119538307\n",
      "epoch: 18 step: 29, loss is 0.05670392885804176\n",
      "epoch: 18 step: 30, loss is 0.03827226907014847\n",
      "epoch: 18 step: 31, loss is 0.007646703161299229\n",
      "epoch: 18 step: 32, loss is 0.008234312757849693\n",
      "epoch: 18 step: 33, loss is 0.0033127819187939167\n",
      "epoch: 18 step: 34, loss is 0.023276684805750847\n",
      "epoch: 18 step: 35, loss is 0.018592702224850655\n",
      "epoch: 18 step: 36, loss is 0.06214557960629463\n",
      "epoch: 18 step: 37, loss is 0.015009595081210136\n",
      "epoch: 18 step: 38, loss is 0.0016655116342008114\n",
      "epoch: 18 step: 39, loss is 0.02284531109035015\n",
      "epoch: 18 step: 40, loss is 0.0018304670229554176\n",
      "epoch: 18 step: 41, loss is 0.015805277973413467\n",
      "epoch: 18 step: 42, loss is 0.02086457796394825\n",
      "epoch: 18 step: 43, loss is 0.0006072295946069062\n",
      "epoch: 18 step: 44, loss is 0.020833222195506096\n",
      "epoch: 18 step: 45, loss is 0.051240235567092896\n",
      "epoch: 18 step: 46, loss is 0.05442877858877182\n",
      "epoch: 18 step: 47, loss is 0.0383530855178833\n",
      "epoch: 18 step: 48, loss is 0.0020158279221504927\n",
      "epoch: 18 step: 49, loss is 0.007808896247297525\n",
      "epoch: 18 step: 50, loss is 0.010777980089187622\n",
      "epoch: 18 step: 51, loss is 0.029370345175266266\n",
      "epoch: 18 step: 52, loss is 0.01744469255208969\n",
      "epoch: 18 step: 53, loss is 0.0004011062846984714\n",
      "epoch: 18 step: 54, loss is 0.0009676730842329562\n",
      "epoch: 18 step: 55, loss is 0.000334101147018373\n",
      "epoch: 18 step: 56, loss is 0.02885640785098076\n",
      "epoch: 18 step: 57, loss is 0.0034770211204886436\n",
      "epoch: 18 step: 58, loss is 0.009628849104046822\n",
      "epoch: 18 step: 59, loss is 0.0009767153533175588\n",
      "epoch: 18 step: 60, loss is 0.019976550713181496\n",
      "epoch: 18 step: 61, loss is 0.055372271686792374\n",
      "epoch: 18 step: 62, loss is 0.02820480428636074\n",
      "epoch: 18 step: 63, loss is 0.00729631120339036\n",
      "epoch: 18 step: 64, loss is 0.03790300339460373\n",
      "epoch: 18 step: 65, loss is 0.15247726440429688\n",
      "epoch: 18 step: 66, loss is 0.03560415655374527\n",
      "epoch: 18 step: 67, loss is 0.00862246286123991\n",
      "epoch: 18 step: 68, loss is 0.005074278451502323\n",
      "epoch: 18 step: 69, loss is 0.005749940872192383\n",
      "epoch: 18 step: 70, loss is 0.002761449199169874\n",
      "epoch: 18 step: 71, loss is 0.025500554591417313\n",
      "epoch: 18 step: 72, loss is 0.04877898097038269\n",
      "epoch: 18 step: 73, loss is 0.016364341601729393\n",
      "epoch: 18 step: 74, loss is 0.010272453539073467\n",
      "epoch: 18 step: 75, loss is 0.007046328857541084\n",
      "epoch: 18 step: 76, loss is 0.03945291042327881\n",
      "epoch: 18 step: 77, loss is 0.005086901597678661\n",
      "epoch: 18 step: 78, loss is 0.007600512355566025\n",
      "epoch: 18 step: 79, loss is 0.020334098488092422\n",
      "epoch: 18 step: 80, loss is 0.015283629298210144\n",
      "epoch: 18 step: 81, loss is 0.0013794784899801016\n",
      "epoch: 18 step: 82, loss is 0.05208227038383484\n",
      "epoch: 18 step: 83, loss is 0.00894249975681305\n",
      "epoch: 18 step: 84, loss is 0.06634680181741714\n",
      "epoch: 18 step: 85, loss is 0.07026754319667816\n",
      "epoch: 18 step: 86, loss is 0.03642316535115242\n",
      "epoch: 18 step: 87, loss is 0.025733929127454758\n",
      "epoch: 18 step: 88, loss is 0.009596959687769413\n",
      "epoch: 18 step: 89, loss is 0.005952358245849609\n",
      "epoch: 18 step: 90, loss is 0.0003532426489982754\n",
      "epoch: 18 step: 91, loss is 0.00953009445220232\n",
      "epoch: 18 step: 92, loss is 0.0032950746826827526\n",
      "epoch: 18 step: 93, loss is 0.04755863919854164\n",
      "epoch: 18 step: 94, loss is 0.057724058628082275\n",
      "epoch: 18 step: 95, loss is 0.024124888703227043\n",
      "epoch: 18 step: 96, loss is 0.02682999148964882\n",
      "epoch: 18 step: 97, loss is 0.06981924921274185\n",
      "epoch: 18 step: 98, loss is 0.004975448362529278\n",
      "epoch: 18 step: 99, loss is 0.0046234894543886185\n",
      "epoch: 18 step: 100, loss is 0.048455048352479935\n",
      "epoch: 18 step: 101, loss is 0.001248917542397976\n",
      "epoch: 18 step: 102, loss is 0.04057074338197708\n",
      "epoch: 18 step: 103, loss is 0.020201370120048523\n",
      "epoch: 18 step: 104, loss is 0.11698451638221741\n",
      "epoch: 18 step: 105, loss is 0.021247103810310364\n",
      "epoch: 18 step: 106, loss is 0.0021787791047245264\n",
      "epoch: 18 step: 107, loss is 0.003854811657220125\n",
      "epoch: 18 step: 108, loss is 0.00821699295192957\n",
      "epoch: 18 step: 109, loss is 0.003076045773923397\n",
      "epoch: 18 step: 110, loss is 0.017260538414120674\n",
      "epoch: 18 step: 111, loss is 0.12769418954849243\n",
      "epoch: 18 step: 112, loss is 0.032071616500616074\n",
      "epoch: 18 step: 113, loss is 0.039909813553094864\n",
      "epoch: 18 step: 114, loss is 0.0019585401751101017\n",
      "epoch: 18 step: 115, loss is 0.014443992637097836\n",
      "epoch: 18 step: 116, loss is 0.014366471208631992\n",
      "epoch: 18 step: 117, loss is 0.03235994279384613\n",
      "epoch: 18 step: 118, loss is 0.004322022665292025\n",
      "epoch: 18 step: 119, loss is 0.028660625219345093\n",
      "epoch: 18 step: 120, loss is 0.028312884271144867\n",
      "epoch: 18 step: 121, loss is 0.024736911058425903\n",
      "epoch: 18 step: 122, loss is 0.004073380958288908\n",
      "epoch: 18 step: 123, loss is 0.004228897858411074\n",
      "epoch: 18 step: 124, loss is 0.01807171106338501\n",
      "epoch: 18 step: 125, loss is 0.012390670366585255\n",
      "epoch: 18 step: 126, loss is 0.04433755949139595\n",
      "epoch: 18 step: 127, loss is 0.0388895608484745\n",
      "epoch: 18 step: 128, loss is 0.005179282743483782\n",
      "epoch: 18 step: 129, loss is 0.01771109737455845\n",
      "epoch: 18 step: 130, loss is 0.0354178324341774\n",
      "epoch: 18 step: 131, loss is 0.0007068102713674307\n",
      "epoch: 18 step: 132, loss is 0.005563393235206604\n",
      "epoch: 18 step: 133, loss is 0.09274014085531235\n",
      "epoch: 18 step: 134, loss is 0.0325966477394104\n",
      "epoch: 18 step: 135, loss is 0.018736807629466057\n",
      "epoch: 18 step: 136, loss is 0.011799400672316551\n",
      "epoch: 18 step: 137, loss is 0.07530441880226135\n",
      "epoch: 18 step: 138, loss is 0.0007418541936203837\n",
      "epoch: 18 step: 139, loss is 0.01572689227759838\n",
      "epoch: 18 step: 140, loss is 0.012082573026418686\n",
      "epoch: 18 step: 141, loss is 0.0027570324018597603\n",
      "epoch: 18 step: 142, loss is 0.0033996475394815207\n",
      "epoch: 18 step: 143, loss is 0.0010045887902379036\n",
      "epoch: 18 step: 144, loss is 0.0021570895332843065\n",
      "epoch: 18 step: 145, loss is 0.049415960907936096\n",
      "epoch: 18 step: 146, loss is 0.013885563239455223\n",
      "epoch: 18 step: 147, loss is 0.00018388287571724504\n",
      "epoch: 18 step: 148, loss is 0.003859545337036252\n",
      "epoch: 18 step: 149, loss is 0.029365181922912598\n",
      "epoch: 18 step: 150, loss is 0.002254231832921505\n",
      "epoch: 18 step: 151, loss is 0.016959307715296745\n",
      "epoch: 18 step: 152, loss is 0.1842091679573059\n",
      "epoch: 18 step: 153, loss is 0.03960120305418968\n",
      "epoch: 18 step: 154, loss is 0.00038696802221238613\n",
      "epoch: 18 step: 155, loss is 0.00542417773976922\n",
      "epoch: 18 step: 156, loss is 0.11371942609548569\n",
      "epoch: 18 step: 157, loss is 0.05298163369297981\n",
      "epoch: 18 step: 158, loss is 0.07657378166913986\n",
      "epoch: 18 step: 159, loss is 0.014868681319057941\n",
      "epoch: 18 step: 160, loss is 0.006102440413087606\n",
      "epoch: 18 step: 161, loss is 0.04331079497933388\n",
      "epoch: 18 step: 162, loss is 0.009441022761166096\n",
      "epoch: 18 step: 163, loss is 0.07972995936870575\n",
      "epoch: 18 step: 164, loss is 0.02505786530673504\n",
      "epoch: 18 step: 165, loss is 0.011246709153056145\n",
      "epoch: 18 step: 166, loss is 0.015557214617729187\n",
      "epoch: 18 step: 167, loss is 0.0712503120303154\n",
      "epoch: 18 step: 168, loss is 0.02828601747751236\n",
      "epoch: 18 step: 169, loss is 0.004132393281906843\n",
      "epoch: 18 step: 170, loss is 0.005092696286737919\n",
      "epoch: 18 step: 171, loss is 0.02391580305993557\n",
      "epoch: 18 step: 172, loss is 0.028569867834448814\n",
      "epoch: 18 step: 173, loss is 0.0021208743564784527\n",
      "epoch: 18 step: 174, loss is 0.009256687946617603\n",
      "epoch: 18 step: 175, loss is 0.059311654418706894\n",
      "epoch: 18 step: 176, loss is 0.007698745466768742\n",
      "epoch: 18 step: 177, loss is 0.06540527939796448\n",
      "epoch: 18 step: 178, loss is 0.017467426136136055\n",
      "epoch: 18 step: 179, loss is 0.0013799451990053058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 180, loss is 0.014576838351786137\n",
      "epoch: 18 step: 181, loss is 0.02720322087407112\n",
      "epoch: 18 step: 182, loss is 0.019237030297517776\n",
      "epoch: 18 step: 183, loss is 0.024226773530244827\n",
      "epoch: 18 step: 184, loss is 0.0032241935841739178\n",
      "epoch: 18 step: 185, loss is 0.01951633207499981\n",
      "epoch: 18 step: 186, loss is 0.015290922485291958\n",
      "epoch: 18 step: 187, loss is 0.0030248670373111963\n",
      "epoch: 18 step: 188, loss is 0.028515411540865898\n",
      "epoch: 18 step: 189, loss is 0.018861887976527214\n",
      "epoch: 18 step: 190, loss is 0.045946717262268066\n",
      "epoch: 18 step: 191, loss is 0.021007610484957695\n",
      "epoch: 18 step: 192, loss is 0.01911165751516819\n",
      "epoch: 18 step: 193, loss is 0.00527373980730772\n",
      "epoch: 18 step: 194, loss is 0.030000930652022362\n",
      "epoch: 18 step: 195, loss is 0.008289122022688389\n",
      "epoch: 18 step: 196, loss is 0.04674854129552841\n",
      "epoch: 18 step: 197, loss is 0.05349420756101608\n",
      "epoch: 18 step: 198, loss is 0.0005502568674273789\n",
      "epoch: 18 step: 199, loss is 0.008162412792444229\n",
      "epoch: 18 step: 200, loss is 0.01859712041914463\n",
      "epoch: 18 step: 201, loss is 0.004030498210340738\n",
      "epoch: 18 step: 202, loss is 0.1848064810037613\n",
      "epoch: 18 step: 203, loss is 0.006028546020388603\n",
      "epoch: 18 step: 204, loss is 0.039951391518116\n",
      "epoch: 18 step: 205, loss is 0.02408500574529171\n",
      "epoch: 18 step: 206, loss is 0.022195393219590187\n",
      "epoch: 18 step: 207, loss is 0.006380325183272362\n",
      "epoch: 18 step: 208, loss is 0.0045727998949587345\n",
      "epoch: 18 step: 209, loss is 0.02318107709288597\n",
      "epoch: 18 step: 210, loss is 0.0015106452628970146\n",
      "epoch: 18 step: 211, loss is 0.004950269591063261\n",
      "epoch: 18 step: 212, loss is 0.0005239640595391393\n",
      "epoch: 18 step: 213, loss is 0.04032193869352341\n",
      "epoch: 18 step: 214, loss is 0.04013846814632416\n",
      "epoch: 18 step: 215, loss is 0.054835062474012375\n",
      "epoch: 18 step: 216, loss is 0.0022894334979355335\n",
      "epoch: 18 step: 217, loss is 0.0002990999200847\n",
      "epoch: 18 step: 218, loss is 0.0007845282671041787\n",
      "epoch: 18 step: 219, loss is 0.003146326867863536\n",
      "epoch: 18 step: 220, loss is 0.014320909976959229\n",
      "epoch: 18 step: 221, loss is 0.011385096237063408\n",
      "epoch: 18 step: 222, loss is 0.047170497477054596\n",
      "epoch: 18 step: 223, loss is 0.014017151668667793\n",
      "epoch: 18 step: 224, loss is 0.005070136860013008\n",
      "epoch: 18 step: 225, loss is 0.0069990442134439945\n",
      "epoch: 18 step: 226, loss is 0.018774835392832756\n",
      "epoch: 18 step: 227, loss is 0.01680230349302292\n",
      "epoch: 18 step: 228, loss is 0.010718862526118755\n",
      "epoch: 18 step: 229, loss is 0.000463636068161577\n",
      "epoch: 18 step: 230, loss is 0.0018399411346763372\n",
      "epoch: 18 step: 231, loss is 0.016269266605377197\n",
      "epoch: 18 step: 232, loss is 0.002094912575557828\n",
      "epoch: 18 step: 233, loss is 0.008528082631528378\n",
      "epoch: 18 step: 234, loss is 0.000636400596704334\n",
      "epoch: 18 step: 235, loss is 0.003051463281735778\n",
      "epoch: 18 step: 236, loss is 0.01671336032450199\n",
      "epoch: 18 step: 237, loss is 0.0075380452908575535\n",
      "epoch: 18 step: 238, loss is 0.00237107602879405\n",
      "epoch: 18 step: 239, loss is 0.053542736917734146\n",
      "epoch: 18 step: 240, loss is 0.008924431167542934\n",
      "epoch: 18 step: 241, loss is 0.0011721377959474921\n",
      "epoch: 18 step: 242, loss is 0.022426584735512733\n",
      "epoch: 18 step: 243, loss is 0.0012065304908901453\n",
      "epoch: 18 step: 244, loss is 0.019762368872761726\n",
      "epoch: 18 step: 245, loss is 0.06862146407365799\n",
      "epoch: 18 step: 246, loss is 0.03280830383300781\n",
      "epoch: 18 step: 247, loss is 0.0018132758559659123\n",
      "epoch: 18 step: 248, loss is 0.04570775851607323\n",
      "epoch: 18 step: 249, loss is 0.019510043784976006\n",
      "epoch: 18 step: 250, loss is 0.023643573746085167\n",
      "epoch: 18 step: 251, loss is 0.011681374162435532\n",
      "epoch: 18 step: 252, loss is 0.020640315487980843\n",
      "epoch: 18 step: 253, loss is 0.020392723381519318\n",
      "epoch: 18 step: 254, loss is 0.005256677512079477\n",
      "epoch: 18 step: 255, loss is 0.005822738632559776\n",
      "epoch: 18 step: 256, loss is 0.005956284236162901\n",
      "epoch: 18 step: 257, loss is 0.029844744130969048\n",
      "epoch: 18 step: 258, loss is 0.009486738592386246\n",
      "epoch: 18 step: 259, loss is 0.00029616066603921354\n",
      "epoch: 18 step: 260, loss is 0.005452884826809168\n",
      "epoch: 18 step: 261, loss is 0.003929451107978821\n",
      "epoch: 18 step: 262, loss is 0.007208239287137985\n",
      "epoch: 18 step: 263, loss is 0.010534071363508701\n",
      "epoch: 18 step: 264, loss is 0.000987100531347096\n",
      "epoch: 18 step: 265, loss is 0.053806547075510025\n",
      "epoch: 18 step: 266, loss is 0.000655930140055716\n",
      "epoch: 18 step: 267, loss is 0.014432771131396294\n",
      "epoch: 18 step: 268, loss is 0.007862348109483719\n",
      "epoch: 18 step: 269, loss is 0.0019612389151006937\n",
      "epoch: 18 step: 270, loss is 0.0005733241559937596\n",
      "epoch: 18 step: 271, loss is 0.01574656553566456\n",
      "epoch: 18 step: 272, loss is 0.029847990721464157\n",
      "epoch: 18 step: 273, loss is 0.04283171519637108\n",
      "epoch: 18 step: 274, loss is 0.03972265496850014\n",
      "epoch: 18 step: 275, loss is 0.0705365389585495\n",
      "epoch: 18 step: 276, loss is 0.0005275188595987856\n",
      "epoch: 18 step: 277, loss is 0.011620588600635529\n",
      "epoch: 18 step: 278, loss is 0.013981429859995842\n",
      "epoch: 18 step: 279, loss is 0.0044199563562870026\n",
      "epoch: 18 step: 280, loss is 0.01224598940461874\n",
      "epoch: 18 step: 281, loss is 0.006724969949573278\n",
      "epoch: 18 step: 282, loss is 0.05586005002260208\n",
      "epoch: 18 step: 283, loss is 0.05392007902264595\n",
      "epoch: 18 step: 284, loss is 0.05806712061166763\n",
      "epoch: 18 step: 285, loss is 0.04854404181241989\n",
      "epoch: 18 step: 286, loss is 0.001979620661586523\n",
      "epoch: 18 step: 287, loss is 0.0015737058129161596\n",
      "epoch: 18 step: 288, loss is 0.005968080833554268\n",
      "epoch: 18 step: 289, loss is 0.005018806084990501\n",
      "epoch: 18 step: 290, loss is 0.00408582529053092\n",
      "epoch: 18 step: 291, loss is 0.02295728772878647\n",
      "epoch: 18 step: 292, loss is 0.012371575459837914\n",
      "epoch: 18 step: 293, loss is 0.0004085721157025546\n",
      "epoch: 18 step: 294, loss is 0.003598991082981229\n",
      "epoch: 18 step: 295, loss is 0.0926651805639267\n",
      "epoch: 18 step: 296, loss is 0.003962911665439606\n",
      "epoch: 18 step: 297, loss is 0.006314543541520834\n",
      "epoch: 18 step: 298, loss is 0.0004905452369712293\n",
      "epoch: 18 step: 299, loss is 0.025876855477690697\n",
      "epoch: 18 step: 300, loss is 0.0011357986368238926\n",
      "epoch: 18 step: 301, loss is 0.004122860264033079\n",
      "epoch: 18 step: 302, loss is 0.004106814973056316\n",
      "epoch: 18 step: 303, loss is 0.030615799129009247\n",
      "epoch: 18 step: 304, loss is 0.012705009430646896\n",
      "epoch: 18 step: 305, loss is 0.03565835580229759\n",
      "epoch: 18 step: 306, loss is 0.13572223484516144\n",
      "epoch: 18 step: 307, loss is 0.006996858865022659\n",
      "epoch: 18 step: 308, loss is 0.01911034621298313\n",
      "epoch: 18 step: 309, loss is 0.006746869534254074\n",
      "epoch: 18 step: 310, loss is 0.005348565522581339\n",
      "epoch: 18 step: 311, loss is 0.0005678922752849758\n",
      "epoch: 18 step: 312, loss is 0.0034216581843793392\n",
      "epoch: 18 step: 313, loss is 0.016538552939891815\n",
      "epoch: 18 step: 314, loss is 0.004966620355844498\n",
      "epoch: 18 step: 315, loss is 0.003083478892222047\n",
      "epoch: 18 step: 316, loss is 0.06211772561073303\n",
      "epoch: 18 step: 317, loss is 0.00586250564083457\n",
      "epoch: 18 step: 318, loss is 0.0019175648922100663\n",
      "epoch: 18 step: 319, loss is 0.0038854023441672325\n",
      "epoch: 18 step: 320, loss is 0.004783346317708492\n",
      "epoch: 18 step: 321, loss is 0.11107681691646576\n",
      "epoch: 18 step: 322, loss is 0.006396130193024874\n",
      "epoch: 18 step: 323, loss is 0.011289202608168125\n",
      "epoch: 18 step: 324, loss is 0.010521397925913334\n",
      "epoch: 18 step: 325, loss is 0.003408134216442704\n",
      "epoch: 18 step: 326, loss is 0.1418529748916626\n",
      "epoch: 18 step: 327, loss is 0.00957279372960329\n",
      "epoch: 18 step: 328, loss is 0.002364116720855236\n",
      "epoch: 18 step: 329, loss is 0.0021242613438516855\n",
      "epoch: 18 step: 330, loss is 0.004378416109830141\n",
      "epoch: 18 step: 331, loss is 0.05567624792456627\n",
      "epoch: 18 step: 332, loss is 0.06452986598014832\n",
      "epoch: 18 step: 333, loss is 0.03792070969939232\n",
      "epoch: 18 step: 334, loss is 0.00039997551357373595\n",
      "epoch: 18 step: 335, loss is 0.09406182914972305\n",
      "epoch: 18 step: 336, loss is 0.03764277324080467\n",
      "epoch: 18 step: 337, loss is 0.0004021820495836437\n",
      "epoch: 18 step: 338, loss is 0.002034649718552828\n",
      "epoch: 18 step: 339, loss is 0.007298226933926344\n",
      "epoch: 18 step: 340, loss is 0.04483914375305176\n",
      "epoch: 18 step: 341, loss is 0.00858413614332676\n",
      "epoch: 18 step: 342, loss is 0.028110288083553314\n",
      "epoch: 18 step: 343, loss is 0.009789150208234787\n",
      "epoch: 18 step: 344, loss is 0.007929806597530842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 345, loss is 0.027686653658747673\n",
      "epoch: 18 step: 346, loss is 0.02117139846086502\n",
      "epoch: 18 step: 347, loss is 0.011729550547897816\n",
      "epoch: 18 step: 348, loss is 0.04288077354431152\n",
      "epoch: 18 step: 349, loss is 0.004072560928761959\n",
      "epoch: 18 step: 350, loss is 0.028712159022688866\n",
      "epoch: 18 step: 351, loss is 0.009364768862724304\n",
      "epoch: 18 step: 352, loss is 0.05356963723897934\n",
      "epoch: 18 step: 353, loss is 0.008601929992437363\n",
      "epoch: 18 step: 354, loss is 0.03190917894244194\n",
      "epoch: 18 step: 355, loss is 0.015241685323417187\n",
      "epoch: 18 step: 356, loss is 0.009231850504875183\n",
      "epoch: 18 step: 357, loss is 0.0007734635146334767\n",
      "epoch: 18 step: 358, loss is 0.01888325996696949\n",
      "epoch: 18 step: 359, loss is 0.010355165228247643\n",
      "epoch: 18 step: 360, loss is 0.0010915513848885894\n",
      "epoch: 18 step: 361, loss is 0.0005005375714972615\n",
      "epoch: 18 step: 362, loss is 0.010294012725353241\n",
      "epoch: 18 step: 363, loss is 0.013691136613488197\n",
      "epoch: 18 step: 364, loss is 0.001818004297092557\n",
      "epoch: 18 step: 365, loss is 0.0016312282532453537\n",
      "epoch: 18 step: 366, loss is 0.0050544398836791515\n",
      "epoch: 18 step: 367, loss is 0.09633180499076843\n",
      "epoch: 18 step: 368, loss is 0.05083877965807915\n",
      "epoch: 18 step: 369, loss is 0.005064019933342934\n",
      "epoch: 18 step: 370, loss is 0.02340896613895893\n",
      "epoch: 18 step: 371, loss is 0.006256572902202606\n",
      "epoch: 18 step: 372, loss is 0.03440342843532562\n",
      "epoch: 18 step: 373, loss is 0.014999357983469963\n",
      "epoch: 18 step: 374, loss is 0.03631378337740898\n",
      "epoch: 18 step: 375, loss is 0.06297291815280914\n",
      "epoch: 18 step: 376, loss is 0.0022848621010780334\n",
      "epoch: 18 step: 377, loss is 0.03147038817405701\n",
      "epoch: 18 step: 378, loss is 0.015864592045545578\n",
      "epoch: 18 step: 379, loss is 0.013675427064299583\n",
      "epoch: 18 step: 380, loss is 0.01361140888184309\n",
      "epoch: 18 step: 381, loss is 0.0040151216089725494\n",
      "epoch: 18 step: 382, loss is 0.004266640171408653\n",
      "epoch: 18 step: 383, loss is 0.0010011738631874323\n",
      "epoch: 18 step: 384, loss is 0.0016794683178886771\n",
      "epoch: 18 step: 385, loss is 0.012726729735732079\n",
      "epoch: 18 step: 386, loss is 0.004581417888402939\n",
      "epoch: 18 step: 387, loss is 0.046298518776893616\n",
      "epoch: 18 step: 388, loss is 0.01013308484107256\n",
      "epoch: 18 step: 389, loss is 0.05257561802864075\n",
      "epoch: 18 step: 390, loss is 0.00263525964692235\n",
      "epoch: 18 step: 391, loss is 0.0029856921173632145\n",
      "epoch: 18 step: 392, loss is 0.00951351597905159\n",
      "epoch: 18 step: 393, loss is 0.007291593588888645\n",
      "epoch: 18 step: 394, loss is 0.021269049495458603\n",
      "epoch: 18 step: 395, loss is 0.007242091000080109\n",
      "epoch: 18 step: 396, loss is 0.006183373276144266\n",
      "epoch: 18 step: 397, loss is 0.055340543389320374\n",
      "epoch: 18 step: 398, loss is 0.00141592463478446\n",
      "epoch: 18 step: 399, loss is 0.036773744970560074\n",
      "epoch: 18 step: 400, loss is 0.006225646007806063\n",
      "epoch: 18 step: 401, loss is 0.021284909918904305\n",
      "epoch: 18 step: 402, loss is 0.028281493112444878\n",
      "epoch: 18 step: 403, loss is 0.18379871547222137\n",
      "epoch: 18 step: 404, loss is 0.015892600640654564\n",
      "epoch: 18 step: 405, loss is 0.008550125174224377\n",
      "epoch: 18 step: 406, loss is 0.003048459067940712\n",
      "epoch: 18 step: 407, loss is 0.009182712994515896\n",
      "epoch: 18 step: 408, loss is 0.0007503751548938453\n",
      "epoch: 18 step: 409, loss is 0.00043266030843369663\n",
      "epoch: 18 step: 410, loss is 0.009028613567352295\n",
      "epoch: 18 step: 411, loss is 0.0036599556915462017\n",
      "epoch: 18 step: 412, loss is 0.014124274253845215\n",
      "epoch: 18 step: 413, loss is 0.0040239146910607815\n",
      "epoch: 18 step: 414, loss is 0.005654601380228996\n",
      "epoch: 18 step: 415, loss is 0.004065239802002907\n",
      "epoch: 18 step: 416, loss is 0.015973880887031555\n",
      "epoch: 18 step: 417, loss is 0.010602994821965694\n",
      "epoch: 18 step: 418, loss is 0.0452749989926815\n",
      "epoch: 18 step: 419, loss is 0.007224959786981344\n",
      "epoch: 18 step: 420, loss is 0.011315439827740192\n",
      "epoch: 18 step: 421, loss is 0.0026343772187829018\n",
      "epoch: 18 step: 422, loss is 0.0037985038943588734\n",
      "epoch: 18 step: 423, loss is 0.010318330489099026\n",
      "epoch: 18 step: 424, loss is 0.02080419473350048\n",
      "epoch: 18 step: 425, loss is 0.0030302670784294605\n",
      "epoch: 18 step: 426, loss is 0.0005701144109480083\n",
      "epoch: 18 step: 427, loss is 0.04590348154306412\n",
      "epoch: 18 step: 428, loss is 0.024113649502396584\n",
      "epoch: 18 step: 429, loss is 0.00811365619301796\n",
      "epoch: 18 step: 430, loss is 0.0017408642452210188\n",
      "epoch: 18 step: 431, loss is 0.0013715261593461037\n",
      "epoch: 18 step: 432, loss is 0.004085839726030827\n",
      "epoch: 18 step: 433, loss is 0.05058052018284798\n",
      "epoch: 18 step: 434, loss is 0.009673302993178368\n",
      "epoch: 18 step: 435, loss is 0.13684946298599243\n",
      "epoch: 18 step: 436, loss is 0.0047352551482617855\n",
      "epoch: 18 step: 437, loss is 0.0011016021016985178\n",
      "epoch: 18 step: 438, loss is 0.0037558034528046846\n",
      "epoch: 18 step: 439, loss is 0.0037440606392920017\n",
      "epoch: 18 step: 440, loss is 0.00032809399999678135\n",
      "epoch: 18 step: 441, loss is 0.0058250934816896915\n",
      "epoch: 18 step: 442, loss is 0.009092111140489578\n",
      "epoch: 18 step: 443, loss is 0.0050071538425982\n",
      "epoch: 18 step: 444, loss is 0.005024127662181854\n",
      "epoch: 18 step: 445, loss is 0.003254676004871726\n",
      "epoch: 18 step: 446, loss is 0.02555977739393711\n",
      "epoch: 18 step: 447, loss is 0.017152175307273865\n",
      "epoch: 18 step: 448, loss is 0.01668684557080269\n",
      "epoch: 18 step: 449, loss is 0.004713748581707478\n",
      "epoch: 18 step: 450, loss is 0.003760435152798891\n",
      "epoch: 18 step: 451, loss is 0.01897849328815937\n",
      "epoch: 18 step: 452, loss is 0.002148279221728444\n",
      "epoch: 18 step: 453, loss is 0.0035445906687527895\n",
      "epoch: 18 step: 454, loss is 0.0006806068122386932\n",
      "epoch: 18 step: 455, loss is 0.0015688713174313307\n",
      "epoch: 18 step: 456, loss is 0.0472525954246521\n",
      "epoch: 18 step: 457, loss is 0.013715364038944244\n",
      "epoch: 18 step: 458, loss is 0.020429635420441628\n",
      "epoch: 18 step: 459, loss is 0.00045704367221333086\n",
      "epoch: 18 step: 460, loss is 0.004405420273542404\n",
      "epoch: 18 step: 461, loss is 0.042216237634420395\n",
      "epoch: 18 step: 462, loss is 0.0017883740365505219\n",
      "epoch: 18 step: 463, loss is 0.022083327174186707\n",
      "epoch: 18 step: 464, loss is 0.010876805521547794\n",
      "epoch: 18 step: 465, loss is 0.006248814053833485\n",
      "epoch: 18 step: 466, loss is 0.011246654205024242\n",
      "epoch: 18 step: 467, loss is 0.003761276137083769\n",
      "epoch: 18 step: 468, loss is 0.014610886573791504\n",
      "epoch: 18 step: 469, loss is 0.0031555236782878637\n",
      "epoch: 18 step: 470, loss is 0.0014629480428993702\n",
      "epoch: 18 step: 471, loss is 0.03479941561818123\n",
      "epoch: 18 step: 472, loss is 0.099664606153965\n",
      "epoch: 18 step: 473, loss is 0.01837744750082493\n",
      "epoch: 18 step: 474, loss is 0.02734282799065113\n",
      "epoch: 18 step: 475, loss is 0.014741157181560993\n",
      "epoch: 18 step: 476, loss is 0.00479061109945178\n",
      "epoch: 18 step: 477, loss is 0.013306992128491402\n",
      "epoch: 18 step: 478, loss is 0.005866681691259146\n",
      "epoch: 18 step: 479, loss is 0.024094101041555405\n",
      "epoch: 18 step: 480, loss is 0.0012695854529738426\n",
      "epoch: 18 step: 481, loss is 0.08317601680755615\n",
      "epoch: 18 step: 482, loss is 9.846902685239911e-05\n",
      "epoch: 18 step: 483, loss is 0.002046788576990366\n",
      "epoch: 18 step: 484, loss is 0.015136625617742538\n",
      "epoch: 18 step: 485, loss is 0.02215351164340973\n",
      "epoch: 18 step: 486, loss is 0.0042468891479074955\n",
      "epoch: 18 step: 487, loss is 0.0031010580714792013\n",
      "epoch: 18 step: 488, loss is 0.05285953730344772\n",
      "epoch: 18 step: 489, loss is 0.002658796263858676\n",
      "epoch: 18 step: 490, loss is 0.004026045557111502\n",
      "epoch: 18 step: 491, loss is 0.002181106945499778\n",
      "epoch: 18 step: 492, loss is 0.03219030797481537\n",
      "epoch: 18 step: 493, loss is 0.047090377658605576\n",
      "epoch: 18 step: 494, loss is 0.027541689574718475\n",
      "epoch: 18 step: 495, loss is 0.10067937523126602\n",
      "epoch: 18 step: 496, loss is 0.010085777379572392\n",
      "epoch: 18 step: 497, loss is 0.05435898154973984\n",
      "epoch: 18 step: 498, loss is 0.007116108201444149\n",
      "epoch: 18 step: 499, loss is 0.004802759736776352\n",
      "epoch: 18 step: 500, loss is 0.005682938266545534\n",
      "epoch: 18 step: 501, loss is 0.0002288651157869026\n",
      "epoch: 18 step: 502, loss is 0.004428409039974213\n",
      "epoch: 18 step: 503, loss is 0.06683560460805893\n",
      "epoch: 18 step: 504, loss is 0.0014437454519793391\n",
      "epoch: 18 step: 505, loss is 0.032158929854631424\n",
      "epoch: 18 step: 506, loss is 0.009574327617883682\n",
      "epoch: 18 step: 507, loss is 0.000840730790514499\n",
      "epoch: 18 step: 508, loss is 0.0019386308267712593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 509, loss is 0.0018837603274732828\n",
      "epoch: 18 step: 510, loss is 0.0025609079748392105\n",
      "epoch: 18 step: 511, loss is 0.03690400347113609\n",
      "epoch: 18 step: 512, loss is 0.0013435415457934141\n",
      "epoch: 18 step: 513, loss is 0.012350421398878098\n",
      "epoch: 18 step: 514, loss is 0.0024016445968300104\n",
      "epoch: 18 step: 515, loss is 0.059435054659843445\n",
      "epoch: 18 step: 516, loss is 0.002362953731790185\n",
      "epoch: 18 step: 517, loss is 0.0015640375204384327\n",
      "epoch: 18 step: 518, loss is 0.0038071656599640846\n",
      "epoch: 18 step: 519, loss is 0.030226994305849075\n",
      "epoch: 18 step: 520, loss is 0.02578640729188919\n",
      "epoch: 18 step: 521, loss is 0.025038333609700203\n",
      "epoch: 18 step: 522, loss is 0.0059081451036036015\n",
      "epoch: 18 step: 523, loss is 0.004427256993949413\n",
      "epoch: 18 step: 524, loss is 0.010081508196890354\n",
      "epoch: 18 step: 525, loss is 0.0059029641561210155\n",
      "epoch: 18 step: 526, loss is 0.005361551884561777\n",
      "epoch: 18 step: 527, loss is 0.0038076131604611874\n",
      "epoch: 18 step: 528, loss is 0.03455726429820061\n",
      "epoch: 18 step: 529, loss is 0.010806013830006123\n",
      "epoch: 18 step: 530, loss is 0.004288662225008011\n",
      "epoch: 18 step: 531, loss is 0.0234473068267107\n",
      "epoch: 18 step: 532, loss is 0.00043146105599589646\n",
      "epoch: 18 step: 533, loss is 0.04125281423330307\n",
      "epoch: 18 step: 534, loss is 0.02592255175113678\n",
      "epoch: 18 step: 535, loss is 0.009193777106702328\n",
      "epoch: 18 step: 536, loss is 0.0099576935172081\n",
      "epoch: 18 step: 537, loss is 0.025741927325725555\n",
      "epoch: 18 step: 538, loss is 0.0215860977768898\n",
      "epoch: 18 step: 539, loss is 0.001243375358171761\n",
      "epoch: 18 step: 540, loss is 0.011791737750172615\n",
      "epoch: 18 step: 541, loss is 0.0014364175731316209\n",
      "epoch: 18 step: 542, loss is 0.009468760341405869\n",
      "epoch: 18 step: 543, loss is 0.003492741845548153\n",
      "epoch: 18 step: 544, loss is 0.000864628585986793\n",
      "epoch: 18 step: 545, loss is 0.022479841485619545\n",
      "epoch: 18 step: 546, loss is 0.0015070227673277259\n",
      "epoch: 18 step: 547, loss is 0.00097653892589733\n",
      "epoch: 18 step: 548, loss is 0.004058536142110825\n",
      "epoch: 18 step: 549, loss is 0.00038041180232539773\n",
      "epoch: 18 step: 550, loss is 0.02397695556282997\n",
      "epoch: 18 step: 551, loss is 0.00011738196189980954\n",
      "epoch: 18 step: 552, loss is 0.0017669391818344593\n",
      "epoch: 18 step: 553, loss is 0.00037109534605406225\n",
      "epoch: 18 step: 554, loss is 0.03627994656562805\n",
      "epoch: 18 step: 555, loss is 0.004472294356673956\n",
      "epoch: 18 step: 556, loss is 0.006149552762508392\n",
      "epoch: 18 step: 557, loss is 0.04311104863882065\n",
      "epoch: 18 step: 558, loss is 0.09231715649366379\n",
      "epoch: 18 step: 559, loss is 0.0337202213704586\n",
      "epoch: 18 step: 560, loss is 0.004529675468802452\n",
      "epoch: 18 step: 561, loss is 0.0050603668205440044\n",
      "epoch: 18 step: 562, loss is 0.004223993048071861\n",
      "epoch: 18 step: 563, loss is 0.014791593886911869\n",
      "epoch: 18 step: 564, loss is 0.01840207353234291\n",
      "epoch: 18 step: 565, loss is 0.004384433384984732\n",
      "epoch: 18 step: 566, loss is 0.03417748585343361\n",
      "epoch: 18 step: 567, loss is 0.0031225767452269793\n",
      "epoch: 18 step: 568, loss is 0.07864294946193695\n",
      "epoch: 18 step: 569, loss is 0.015839548781514168\n",
      "epoch: 18 step: 570, loss is 0.10328089445829391\n",
      "epoch: 18 step: 571, loss is 0.014011177234351635\n",
      "epoch: 18 step: 572, loss is 0.0014089647447690368\n",
      "epoch: 18 step: 573, loss is 0.005018250551074743\n",
      "epoch: 18 step: 574, loss is 0.057720448821783066\n",
      "epoch: 18 step: 575, loss is 0.021461298689246178\n",
      "epoch: 18 step: 576, loss is 0.020757168531417847\n",
      "epoch: 18 step: 577, loss is 0.0011448842706158757\n",
      "epoch: 18 step: 578, loss is 0.007747600320726633\n",
      "epoch: 18 step: 579, loss is 0.061813708394765854\n",
      "epoch: 18 step: 580, loss is 0.03777869790792465\n",
      "epoch: 18 step: 581, loss is 0.08384174108505249\n",
      "epoch: 18 step: 582, loss is 0.003412827616557479\n",
      "epoch: 18 step: 583, loss is 0.06894299387931824\n",
      "epoch: 18 step: 584, loss is 0.040395256131887436\n",
      "epoch: 18 step: 585, loss is 0.0001124686750699766\n",
      "epoch: 18 step: 586, loss is 0.01858401857316494\n",
      "epoch: 18 step: 587, loss is 0.00341910682618618\n",
      "epoch: 18 step: 588, loss is 0.007132165599614382\n",
      "epoch: 18 step: 589, loss is 0.04036093130707741\n",
      "epoch: 18 step: 590, loss is 0.006435937248170376\n",
      "epoch: 18 step: 591, loss is 0.031060220673680305\n",
      "epoch: 18 step: 592, loss is 0.04086269438266754\n",
      "epoch: 18 step: 593, loss is 0.012653897516429424\n",
      "epoch: 18 step: 594, loss is 0.01737883873283863\n",
      "epoch: 18 step: 595, loss is 0.0010156454518437386\n",
      "epoch: 18 step: 596, loss is 0.0015741634415462613\n",
      "epoch: 18 step: 597, loss is 0.009965188801288605\n",
      "epoch: 18 step: 598, loss is 0.04015141353011131\n",
      "epoch: 18 step: 599, loss is 0.003907718230038881\n",
      "epoch: 18 step: 600, loss is 0.04534182325005531\n",
      "epoch: 18 step: 601, loss is 0.06577195972204208\n",
      "epoch: 18 step: 602, loss is 0.017797183245420456\n",
      "epoch: 18 step: 603, loss is 0.02190219610929489\n",
      "epoch: 18 step: 604, loss is 0.1073153018951416\n",
      "epoch: 18 step: 605, loss is 0.07329592853784561\n",
      "epoch: 18 step: 606, loss is 0.025278043001890182\n",
      "epoch: 18 step: 607, loss is 0.004859718959778547\n",
      "epoch: 18 step: 608, loss is 0.030227256938815117\n",
      "epoch: 18 step: 609, loss is 0.010329279117286205\n",
      "epoch: 18 step: 610, loss is 0.03070252016186714\n",
      "epoch: 18 step: 611, loss is 0.0007474764715880156\n",
      "epoch: 18 step: 612, loss is 0.04077240452170372\n",
      "epoch: 18 step: 613, loss is 0.0014192675007507205\n",
      "epoch: 18 step: 614, loss is 0.04146481305360794\n",
      "epoch: 18 step: 615, loss is 0.06956170499324799\n",
      "epoch: 18 step: 616, loss is 0.019728083163499832\n",
      "epoch: 18 step: 617, loss is 0.0017686949577182531\n",
      "epoch: 18 step: 618, loss is 0.025719618424773216\n",
      "epoch: 18 step: 619, loss is 0.013671650551259518\n",
      "epoch: 18 step: 620, loss is 0.01037529669702053\n",
      "epoch: 18 step: 621, loss is 0.008053659461438656\n",
      "epoch: 18 step: 622, loss is 0.045398447662591934\n",
      "epoch: 18 step: 623, loss is 0.010443750768899918\n",
      "epoch: 18 step: 624, loss is 0.07843537628650665\n",
      "epoch: 18 step: 625, loss is 0.018805963918566704\n",
      "epoch: 18 step: 626, loss is 0.02237056940793991\n",
      "epoch: 18 step: 627, loss is 0.0018953020917251706\n",
      "epoch: 18 step: 628, loss is 0.015526602044701576\n",
      "epoch: 18 step: 629, loss is 0.011104300618171692\n",
      "epoch: 18 step: 630, loss is 0.013191537000238895\n",
      "epoch: 18 step: 631, loss is 0.025376804172992706\n",
      "epoch: 18 step: 632, loss is 0.005261021200567484\n",
      "epoch: 18 step: 633, loss is 0.0032858215272426605\n",
      "epoch: 18 step: 634, loss is 0.005590108223259449\n",
      "epoch: 18 step: 635, loss is 0.016804464161396027\n",
      "epoch: 18 step: 636, loss is 0.032062213867902756\n",
      "epoch: 18 step: 637, loss is 0.004677936900407076\n",
      "epoch: 18 step: 638, loss is 0.005716534331440926\n",
      "epoch: 18 step: 639, loss is 0.01976132020354271\n",
      "epoch: 18 step: 640, loss is 0.010150541551411152\n",
      "epoch: 18 step: 641, loss is 0.09511955082416534\n",
      "epoch: 18 step: 642, loss is 0.0038596235681325197\n",
      "epoch: 18 step: 643, loss is 0.1626407951116562\n",
      "epoch: 18 step: 644, loss is 0.010658444836735725\n",
      "epoch: 18 step: 645, loss is 0.0034540488850325346\n",
      "epoch: 18 step: 646, loss is 0.0007890198612585664\n",
      "epoch: 18 step: 647, loss is 0.03260684758424759\n",
      "epoch: 18 step: 648, loss is 0.0034093023277819157\n",
      "epoch: 18 step: 649, loss is 0.0022356577683240175\n",
      "epoch: 18 step: 650, loss is 0.001395878498442471\n",
      "epoch: 18 step: 651, loss is 0.060308296233415604\n",
      "epoch: 18 step: 652, loss is 0.0010724568273872137\n",
      "epoch: 18 step: 653, loss is 0.0004387549706734717\n",
      "epoch: 18 step: 654, loss is 0.06944550573825836\n",
      "epoch: 18 step: 655, loss is 0.0027247783727943897\n",
      "epoch: 18 step: 656, loss is 0.012315778993070126\n",
      "epoch: 18 step: 657, loss is 0.023869283497333527\n",
      "epoch: 18 step: 658, loss is 0.008078882470726967\n",
      "epoch: 18 step: 659, loss is 0.00472981808707118\n",
      "epoch: 18 step: 660, loss is 0.01766026020050049\n",
      "epoch: 18 step: 661, loss is 0.08780494332313538\n",
      "epoch: 18 step: 662, loss is 0.010303899645805359\n",
      "epoch: 18 step: 663, loss is 0.0034879660233855247\n",
      "epoch: 18 step: 664, loss is 0.0016265660524368286\n",
      "epoch: 18 step: 665, loss is 0.015463667921721935\n",
      "epoch: 18 step: 666, loss is 0.04734398424625397\n",
      "epoch: 18 step: 667, loss is 0.0024977459106594324\n",
      "epoch: 18 step: 668, loss is 0.005310789216309786\n",
      "epoch: 18 step: 669, loss is 0.03670778125524521\n",
      "epoch: 18 step: 670, loss is 0.0034359514247626066\n",
      "epoch: 18 step: 671, loss is 0.02430572360754013\n",
      "epoch: 18 step: 672, loss is 0.007649346254765987\n",
      "epoch: 18 step: 673, loss is 0.0011857347562909126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 674, loss is 0.0011781903449445963\n",
      "epoch: 18 step: 675, loss is 0.02212349884212017\n",
      "epoch: 18 step: 676, loss is 0.09200198203325272\n",
      "epoch: 18 step: 677, loss is 0.003052875865250826\n",
      "epoch: 18 step: 678, loss is 0.0003907670616172254\n",
      "epoch: 18 step: 679, loss is 0.12182315438985825\n",
      "epoch: 18 step: 680, loss is 0.05951817333698273\n",
      "epoch: 18 step: 681, loss is 0.025441311299800873\n",
      "epoch: 18 step: 682, loss is 0.034109070897102356\n",
      "epoch: 18 step: 683, loss is 0.018846452236175537\n",
      "epoch: 18 step: 684, loss is 0.031422972679138184\n",
      "epoch: 18 step: 685, loss is 0.053075987845659256\n",
      "epoch: 18 step: 686, loss is 0.06319096684455872\n",
      "epoch: 18 step: 687, loss is 0.0004735876282211393\n",
      "epoch: 18 step: 688, loss is 0.0008265746873803437\n",
      "epoch: 18 step: 689, loss is 0.08022236078977585\n",
      "epoch: 18 step: 690, loss is 0.01753513514995575\n",
      "epoch: 18 step: 691, loss is 0.005950185935944319\n",
      "epoch: 18 step: 692, loss is 0.12291010469198227\n",
      "epoch: 18 step: 693, loss is 0.004435590002685785\n",
      "epoch: 18 step: 694, loss is 0.015535777434706688\n",
      "epoch: 18 step: 695, loss is 0.05519017577171326\n",
      "epoch: 18 step: 696, loss is 0.017913909628987312\n",
      "epoch: 18 step: 697, loss is 0.011263345368206501\n",
      "epoch: 18 step: 698, loss is 0.0007549432921223342\n",
      "epoch: 18 step: 699, loss is 0.030886026099324226\n",
      "epoch: 18 step: 700, loss is 0.001372138038277626\n",
      "epoch: 18 step: 701, loss is 0.001905959565192461\n",
      "epoch: 18 step: 702, loss is 0.0018794347997754812\n",
      "epoch: 18 step: 703, loss is 0.03961031883955002\n",
      "epoch: 18 step: 704, loss is 0.002062686951830983\n",
      "epoch: 18 step: 705, loss is 0.013216911815106869\n",
      "epoch: 18 step: 706, loss is 0.1135958880186081\n",
      "epoch: 18 step: 707, loss is 0.001335561159066856\n",
      "epoch: 18 step: 708, loss is 0.04041622579097748\n",
      "epoch: 18 step: 709, loss is 0.05756828561425209\n",
      "epoch: 18 step: 710, loss is 0.09561150521039963\n",
      "epoch: 18 step: 711, loss is 0.006574724800884724\n",
      "epoch: 18 step: 712, loss is 0.003643748816102743\n",
      "epoch: 18 step: 713, loss is 0.014264081604778767\n",
      "epoch: 18 step: 714, loss is 0.015142432413995266\n",
      "epoch: 18 step: 715, loss is 0.06301124393939972\n",
      "epoch: 18 step: 716, loss is 0.09565689414739609\n",
      "epoch: 18 step: 717, loss is 0.01369943656027317\n",
      "epoch: 18 step: 718, loss is 0.015578349120914936\n",
      "epoch: 18 step: 719, loss is 0.0009937732247635722\n",
      "epoch: 18 step: 720, loss is 0.017777778208255768\n",
      "epoch: 18 step: 721, loss is 0.00399847561493516\n",
      "epoch: 18 step: 722, loss is 0.016078727319836617\n",
      "epoch: 18 step: 723, loss is 0.016103317961096764\n",
      "epoch: 18 step: 724, loss is 0.0013014108408242464\n",
      "epoch: 18 step: 725, loss is 0.011036267504096031\n",
      "epoch: 18 step: 726, loss is 0.03827007859945297\n",
      "epoch: 18 step: 727, loss is 0.08566015958786011\n",
      "epoch: 18 step: 728, loss is 0.0924331471323967\n",
      "epoch: 18 step: 729, loss is 0.0082311499863863\n",
      "epoch: 18 step: 730, loss is 0.0018056518165394664\n",
      "epoch: 18 step: 731, loss is 0.008595531806349754\n",
      "epoch: 18 step: 732, loss is 0.07792409509420395\n",
      "epoch: 18 step: 733, loss is 0.00646357424557209\n",
      "epoch: 18 step: 734, loss is 0.01597970724105835\n",
      "epoch: 18 step: 735, loss is 0.07527191936969757\n",
      "epoch: 18 step: 736, loss is 0.025652464479207993\n",
      "epoch: 18 step: 737, loss is 0.041021257638931274\n",
      "epoch: 18 step: 738, loss is 0.041809652000665665\n",
      "epoch: 18 step: 739, loss is 0.0004884558729827404\n",
      "epoch: 18 step: 740, loss is 0.004628179594874382\n",
      "epoch: 18 step: 741, loss is 0.02165503054857254\n",
      "epoch: 18 step: 742, loss is 0.07586166262626648\n",
      "epoch: 18 step: 743, loss is 0.0175164807587862\n",
      "epoch: 18 step: 744, loss is 0.009167441166937351\n",
      "epoch: 18 step: 745, loss is 0.07647623121738434\n",
      "epoch: 18 step: 746, loss is 0.0026891492307186127\n",
      "epoch: 18 step: 747, loss is 0.004555908497422934\n",
      "epoch: 18 step: 748, loss is 0.022039512172341347\n",
      "epoch: 18 step: 749, loss is 0.04425831139087677\n",
      "epoch: 18 step: 750, loss is 0.0006329648895189166\n",
      "epoch: 18 step: 751, loss is 0.05554196611046791\n",
      "epoch: 18 step: 752, loss is 0.008449241518974304\n",
      "epoch: 18 step: 753, loss is 0.09147276729345322\n",
      "epoch: 18 step: 754, loss is 0.05756578594446182\n",
      "epoch: 18 step: 755, loss is 0.04211486503481865\n",
      "epoch: 18 step: 756, loss is 0.017146723344922066\n",
      "epoch: 18 step: 757, loss is 0.026493171229958534\n",
      "epoch: 18 step: 758, loss is 0.0035196642857044935\n",
      "epoch: 18 step: 759, loss is 0.01841294951736927\n",
      "epoch: 18 step: 760, loss is 0.0011578577104955912\n",
      "epoch: 18 step: 761, loss is 0.001426862319931388\n",
      "epoch: 18 step: 762, loss is 0.0270676389336586\n",
      "epoch: 18 step: 763, loss is 0.037980567663908005\n",
      "epoch: 18 step: 764, loss is 0.0036840590182691813\n",
      "epoch: 18 step: 765, loss is 0.0054067145101726055\n",
      "epoch: 18 step: 766, loss is 0.01660614274442196\n",
      "epoch: 18 step: 767, loss is 0.0017894036136567593\n",
      "epoch: 18 step: 768, loss is 0.049800075590610504\n",
      "epoch: 18 step: 769, loss is 0.11540870368480682\n",
      "epoch: 18 step: 770, loss is 0.009021062403917313\n",
      "epoch: 18 step: 771, loss is 0.0797145813703537\n",
      "epoch: 18 step: 772, loss is 0.0251555684953928\n",
      "epoch: 18 step: 773, loss is 0.010024524293839931\n",
      "epoch: 18 step: 774, loss is 0.006152568385004997\n",
      "epoch: 18 step: 775, loss is 0.019179988652467728\n",
      "epoch: 18 step: 776, loss is 0.009080750867724419\n",
      "epoch: 18 step: 777, loss is 0.09932172298431396\n",
      "epoch: 18 step: 778, loss is 0.05520249158143997\n",
      "epoch: 18 step: 779, loss is 0.10938555002212524\n",
      "epoch: 18 step: 780, loss is 0.01505404431372881\n",
      "epoch: 18 step: 781, loss is 0.020217290148139\n",
      "epoch: 18 step: 782, loss is 0.06719176471233368\n",
      "epoch: 18 step: 783, loss is 0.00911380909383297\n",
      "epoch: 18 step: 784, loss is 0.005592787638306618\n",
      "epoch: 18 step: 785, loss is 0.04799404740333557\n",
      "epoch: 18 step: 786, loss is 0.0022867172956466675\n",
      "epoch: 18 step: 787, loss is 0.0031755915842950344\n",
      "epoch: 18 step: 788, loss is 0.019133485853672028\n",
      "epoch: 18 step: 789, loss is 0.008172278292477131\n",
      "epoch: 18 step: 790, loss is 0.0013922968646511436\n",
      "epoch: 18 step: 791, loss is 0.00039012773777358234\n",
      "epoch: 18 step: 792, loss is 0.000777598877903074\n",
      "epoch: 18 step: 793, loss is 0.030668675899505615\n",
      "epoch: 18 step: 794, loss is 0.004968232940882444\n",
      "epoch: 18 step: 795, loss is 0.0010748925851657987\n",
      "epoch: 18 step: 796, loss is 0.024509094655513763\n",
      "epoch: 18 step: 797, loss is 0.0023618494160473347\n",
      "epoch: 18 step: 798, loss is 0.0072262403555214405\n",
      "epoch: 18 step: 799, loss is 0.06321296840906143\n",
      "epoch: 18 step: 800, loss is 0.0034783685114234686\n",
      "epoch: 18 step: 801, loss is 0.0063917264342308044\n",
      "epoch: 18 step: 802, loss is 0.015177855268120766\n",
      "epoch: 18 step: 803, loss is 0.11653988808393478\n",
      "epoch: 18 step: 804, loss is 0.0012233476154506207\n",
      "epoch: 18 step: 805, loss is 0.046956390142440796\n",
      "epoch: 18 step: 806, loss is 0.001835197675973177\n",
      "epoch: 18 step: 807, loss is 0.022349968552589417\n",
      "epoch: 18 step: 808, loss is 0.040125951170921326\n",
      "epoch: 18 step: 809, loss is 0.030796747654676437\n",
      "epoch: 18 step: 810, loss is 0.10331698507070541\n",
      "epoch: 18 step: 811, loss is 0.0061874608509242535\n",
      "epoch: 18 step: 812, loss is 0.0021805940195918083\n",
      "epoch: 18 step: 813, loss is 0.004370802082121372\n",
      "epoch: 18 step: 814, loss is 0.014232483692467213\n",
      "epoch: 18 step: 815, loss is 0.02458007074892521\n",
      "epoch: 18 step: 816, loss is 0.030613120645284653\n",
      "epoch: 18 step: 817, loss is 0.0012443362502381206\n",
      "epoch: 18 step: 818, loss is 0.002888043876737356\n",
      "epoch: 18 step: 819, loss is 0.07427778840065002\n",
      "epoch: 18 step: 820, loss is 0.014813030138611794\n",
      "epoch: 18 step: 821, loss is 0.04619624465703964\n",
      "epoch: 18 step: 822, loss is 0.003329930594190955\n",
      "epoch: 18 step: 823, loss is 0.010588391683995724\n",
      "epoch: 18 step: 824, loss is 0.04150828719139099\n",
      "epoch: 18 step: 825, loss is 0.015831705182790756\n",
      "epoch: 18 step: 826, loss is 0.012640899047255516\n",
      "epoch: 18 step: 827, loss is 0.0010497469920665026\n",
      "epoch: 18 step: 828, loss is 0.007417955435812473\n",
      "epoch: 18 step: 829, loss is 0.03138551488518715\n",
      "epoch: 18 step: 830, loss is 0.09542907774448395\n",
      "epoch: 18 step: 831, loss is 0.04026107117533684\n",
      "epoch: 18 step: 832, loss is 0.02127339318394661\n",
      "epoch: 18 step: 833, loss is 0.007047321647405624\n",
      "epoch: 18 step: 834, loss is 0.10736992210149765\n",
      "epoch: 18 step: 835, loss is 0.018924547359347343\n",
      "epoch: 18 step: 836, loss is 0.0009251494193449616\n",
      "epoch: 18 step: 837, loss is 0.018059683963656425\n",
      "epoch: 18 step: 838, loss is 0.005694960709661245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 839, loss is 0.015184820629656315\n",
      "epoch: 18 step: 840, loss is 0.0033402051776647568\n",
      "epoch: 18 step: 841, loss is 0.013183142989873886\n",
      "epoch: 18 step: 842, loss is 0.11662024259567261\n",
      "epoch: 18 step: 843, loss is 0.010128526017069817\n",
      "epoch: 18 step: 844, loss is 0.005515669006854296\n",
      "epoch: 18 step: 845, loss is 0.04394691810011864\n",
      "epoch: 18 step: 846, loss is 0.04447000101208687\n",
      "epoch: 18 step: 847, loss is 0.004592022858560085\n",
      "epoch: 18 step: 848, loss is 0.022155823186039925\n",
      "epoch: 18 step: 849, loss is 0.022403812035918236\n",
      "epoch: 18 step: 850, loss is 0.012706815265119076\n",
      "epoch: 18 step: 851, loss is 0.038305673748254776\n",
      "epoch: 18 step: 852, loss is 0.0019358987919986248\n",
      "epoch: 18 step: 853, loss is 0.06665472686290741\n",
      "epoch: 18 step: 854, loss is 0.056537117809057236\n",
      "epoch: 18 step: 855, loss is 0.0040001398883759975\n",
      "epoch: 18 step: 856, loss is 0.034404732286930084\n",
      "epoch: 18 step: 857, loss is 0.027895644307136536\n",
      "epoch: 18 step: 858, loss is 0.013448446057736874\n",
      "epoch: 18 step: 859, loss is 0.013046723790466785\n",
      "epoch: 18 step: 860, loss is 0.002126125618815422\n",
      "epoch: 18 step: 861, loss is 0.07269565016031265\n",
      "epoch: 18 step: 862, loss is 0.038855668157339096\n",
      "epoch: 18 step: 863, loss is 0.007464271038770676\n",
      "epoch: 18 step: 864, loss is 0.005802381783723831\n",
      "epoch: 18 step: 865, loss is 0.0456417016685009\n",
      "epoch: 18 step: 866, loss is 0.05932016670703888\n",
      "epoch: 18 step: 867, loss is 0.0009517834987491369\n",
      "epoch: 18 step: 868, loss is 0.013044063001871109\n",
      "epoch: 18 step: 869, loss is 0.010659296065568924\n",
      "epoch: 18 step: 870, loss is 0.1626550853252411\n",
      "epoch: 18 step: 871, loss is 0.02675515040755272\n",
      "epoch: 18 step: 872, loss is 0.002018075669184327\n",
      "epoch: 18 step: 873, loss is 0.0018899959977716208\n",
      "epoch: 18 step: 874, loss is 0.05625240132212639\n",
      "epoch: 18 step: 875, loss is 0.033325109630823135\n",
      "epoch: 18 step: 876, loss is 0.03666658326983452\n",
      "epoch: 18 step: 877, loss is 0.05022858828306198\n",
      "epoch: 18 step: 878, loss is 0.011429352685809135\n",
      "epoch: 18 step: 879, loss is 0.010145935229957104\n",
      "epoch: 18 step: 880, loss is 0.00024047435726970434\n",
      "epoch: 18 step: 881, loss is 0.027416998520493507\n",
      "epoch: 18 step: 882, loss is 0.007433920167386532\n",
      "epoch: 18 step: 883, loss is 0.013429726473987103\n",
      "epoch: 18 step: 884, loss is 0.03972632810473442\n",
      "epoch: 18 step: 885, loss is 0.007053663954138756\n",
      "epoch: 18 step: 886, loss is 0.01707647554576397\n",
      "epoch: 18 step: 887, loss is 0.040573492646217346\n",
      "epoch: 18 step: 888, loss is 0.13185951113700867\n",
      "epoch: 18 step: 889, loss is 0.01072079036384821\n",
      "epoch: 18 step: 890, loss is 0.003495667828246951\n",
      "epoch: 18 step: 891, loss is 0.004571516532450914\n",
      "epoch: 18 step: 892, loss is 0.03072887286543846\n",
      "epoch: 18 step: 893, loss is 0.0063997977413237095\n",
      "epoch: 18 step: 894, loss is 0.06515061110258102\n",
      "epoch: 18 step: 895, loss is 0.08691580593585968\n",
      "epoch: 18 step: 896, loss is 0.05254445970058441\n",
      "epoch: 18 step: 897, loss is 0.029891498386859894\n",
      "epoch: 18 step: 898, loss is 0.01189239602535963\n",
      "epoch: 18 step: 899, loss is 0.004119989462196827\n",
      "epoch: 18 step: 900, loss is 0.00510718347504735\n",
      "epoch: 18 step: 901, loss is 0.009102217853069305\n",
      "epoch: 18 step: 902, loss is 0.007848023436963558\n",
      "epoch: 18 step: 903, loss is 0.0004412948910612613\n",
      "epoch: 18 step: 904, loss is 0.12661932408809662\n",
      "epoch: 18 step: 905, loss is 0.05423586443066597\n",
      "epoch: 18 step: 906, loss is 0.011248589493334293\n",
      "epoch: 18 step: 907, loss is 0.020437514409422874\n",
      "epoch: 18 step: 908, loss is 0.030179649591445923\n",
      "epoch: 18 step: 909, loss is 0.008593468926846981\n",
      "epoch: 18 step: 910, loss is 0.014003542251884937\n",
      "epoch: 18 step: 911, loss is 0.03789013251662254\n",
      "epoch: 18 step: 912, loss is 0.04031514748930931\n",
      "epoch: 18 step: 913, loss is 0.005616120528429747\n",
      "epoch: 18 step: 914, loss is 0.0006988715613260865\n",
      "epoch: 18 step: 915, loss is 0.02083047851920128\n",
      "epoch: 18 step: 916, loss is 0.14808018505573273\n",
      "epoch: 18 step: 917, loss is 0.015156000852584839\n",
      "epoch: 18 step: 918, loss is 0.005965602118521929\n",
      "epoch: 18 step: 919, loss is 0.04674256220459938\n",
      "epoch: 18 step: 920, loss is 0.008964682929217815\n",
      "epoch: 18 step: 921, loss is 0.002982604783028364\n",
      "epoch: 18 step: 922, loss is 0.00935716088861227\n",
      "epoch: 18 step: 923, loss is 0.012036333791911602\n",
      "epoch: 18 step: 924, loss is 0.00862723495811224\n",
      "epoch: 18 step: 925, loss is 0.04004253074526787\n",
      "epoch: 18 step: 926, loss is 0.030797861516475677\n",
      "epoch: 18 step: 927, loss is 0.002796252490952611\n",
      "epoch: 18 step: 928, loss is 0.05375036969780922\n",
      "epoch: 18 step: 929, loss is 0.0026664696633815765\n",
      "epoch: 18 step: 930, loss is 0.011562191881239414\n",
      "epoch: 18 step: 931, loss is 0.013417725451290607\n",
      "epoch: 18 step: 932, loss is 0.004442896228283644\n",
      "epoch: 18 step: 933, loss is 0.0017952155321836472\n",
      "epoch: 18 step: 934, loss is 0.008673788979649544\n",
      "epoch: 18 step: 935, loss is 0.006761419586837292\n",
      "epoch: 18 step: 936, loss is 0.008813796564936638\n",
      "epoch: 18 step: 937, loss is 0.0013928761472925544\n",
      "epoch: 19 step: 1, loss is 0.01132852304726839\n",
      "epoch: 19 step: 2, loss is 0.0029356623999774456\n",
      "epoch: 19 step: 3, loss is 0.004715376067906618\n",
      "epoch: 19 step: 4, loss is 0.0017137295799329877\n",
      "epoch: 19 step: 5, loss is 0.00958717055618763\n",
      "epoch: 19 step: 6, loss is 0.003265881445258856\n",
      "epoch: 19 step: 7, loss is 0.007786537520587444\n",
      "epoch: 19 step: 8, loss is 0.0015527664218097925\n",
      "epoch: 19 step: 9, loss is 0.002916252240538597\n",
      "epoch: 19 step: 10, loss is 0.008977269753813744\n",
      "epoch: 19 step: 11, loss is 0.0036492426879704\n",
      "epoch: 19 step: 12, loss is 0.00162703322712332\n",
      "epoch: 19 step: 13, loss is 0.0001223153230967\n",
      "epoch: 19 step: 14, loss is 0.01298967283219099\n",
      "epoch: 19 step: 15, loss is 0.005173115991055965\n",
      "epoch: 19 step: 16, loss is 0.013303213752806187\n",
      "epoch: 19 step: 17, loss is 0.032975755631923676\n",
      "epoch: 19 step: 18, loss is 0.001215997152030468\n",
      "epoch: 19 step: 19, loss is 0.0005408478900790215\n",
      "epoch: 19 step: 20, loss is 0.0009813349461182952\n",
      "epoch: 19 step: 21, loss is 0.03312845528125763\n",
      "epoch: 19 step: 22, loss is 0.0304267555475235\n",
      "epoch: 19 step: 23, loss is 0.0028246338479220867\n",
      "epoch: 19 step: 24, loss is 0.010976479388773441\n",
      "epoch: 19 step: 25, loss is 0.0001782828039722517\n",
      "epoch: 19 step: 26, loss is 0.0060536619275808334\n",
      "epoch: 19 step: 27, loss is 0.00020474758639466017\n",
      "epoch: 19 step: 28, loss is 0.002757506910711527\n",
      "epoch: 19 step: 29, loss is 0.021534593775868416\n",
      "epoch: 19 step: 30, loss is 0.08743371069431305\n",
      "epoch: 19 step: 31, loss is 0.003063485724851489\n",
      "epoch: 19 step: 32, loss is 0.0019412495894357562\n",
      "epoch: 19 step: 33, loss is 0.0031219730153679848\n",
      "epoch: 19 step: 34, loss is 0.0006262394599616528\n",
      "epoch: 19 step: 35, loss is 0.001659911242313683\n",
      "epoch: 19 step: 36, loss is 0.0007092846790328622\n",
      "epoch: 19 step: 37, loss is 0.017508812248706818\n",
      "epoch: 19 step: 38, loss is 0.0008065506699495018\n",
      "epoch: 19 step: 39, loss is 0.004961108323186636\n",
      "epoch: 19 step: 40, loss is 0.006039687432348728\n",
      "epoch: 19 step: 41, loss is 0.00015560090832877904\n",
      "epoch: 19 step: 42, loss is 0.0033274530433118343\n",
      "epoch: 19 step: 43, loss is 0.018102534115314484\n",
      "epoch: 19 step: 44, loss is 0.0017409519059583545\n",
      "epoch: 19 step: 45, loss is 0.004414788447320461\n",
      "epoch: 19 step: 46, loss is 0.0006595299928449094\n",
      "epoch: 19 step: 47, loss is 0.00033413205528631806\n",
      "epoch: 19 step: 48, loss is 0.0027239711489528418\n",
      "epoch: 19 step: 49, loss is 0.0030582540202885866\n",
      "epoch: 19 step: 50, loss is 0.0002697108720894903\n",
      "epoch: 19 step: 51, loss is 0.027021843940019608\n",
      "epoch: 19 step: 52, loss is 0.04424777626991272\n",
      "epoch: 19 step: 53, loss is 0.0009607631945982575\n",
      "epoch: 19 step: 54, loss is 0.04446698725223541\n",
      "epoch: 19 step: 55, loss is 0.006235124077647924\n",
      "epoch: 19 step: 56, loss is 0.0032886858098208904\n",
      "epoch: 19 step: 57, loss is 0.05416180565953255\n",
      "epoch: 19 step: 58, loss is 0.01087480504065752\n",
      "epoch: 19 step: 59, loss is 0.024461600929498672\n",
      "epoch: 19 step: 60, loss is 0.00030596519354730844\n",
      "epoch: 19 step: 61, loss is 0.0006576622254215181\n",
      "epoch: 19 step: 62, loss is 0.005582110956311226\n",
      "epoch: 19 step: 63, loss is 0.0034315402153879404\n",
      "epoch: 19 step: 64, loss is 0.014736929908394814\n",
      "epoch: 19 step: 65, loss is 0.0035549455787986517\n",
      "epoch: 19 step: 66, loss is 0.05863567441701889\n",
      "epoch: 19 step: 67, loss is 0.004198786336928606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 68, loss is 0.001052397070452571\n",
      "epoch: 19 step: 69, loss is 0.0038796619046479464\n",
      "epoch: 19 step: 70, loss is 0.003056684508919716\n",
      "epoch: 19 step: 71, loss is 0.000661150726955384\n",
      "epoch: 19 step: 72, loss is 0.11266593635082245\n",
      "epoch: 19 step: 73, loss is 8.293264545500278e-05\n",
      "epoch: 19 step: 74, loss is 0.0030313259921967983\n",
      "epoch: 19 step: 75, loss is 0.00199284590780735\n",
      "epoch: 19 step: 76, loss is 0.002798140514642\n",
      "epoch: 19 step: 77, loss is 0.0002899899263866246\n",
      "epoch: 19 step: 78, loss is 0.02535530924797058\n",
      "epoch: 19 step: 79, loss is 0.01068192534148693\n",
      "epoch: 19 step: 80, loss is 0.0010594138875603676\n",
      "epoch: 19 step: 81, loss is 0.0049229939468204975\n",
      "epoch: 19 step: 82, loss is 0.005269159097224474\n",
      "epoch: 19 step: 83, loss is 0.0173459704965353\n",
      "epoch: 19 step: 84, loss is 0.014788836240768433\n",
      "epoch: 19 step: 85, loss is 0.0005317280883900821\n",
      "epoch: 19 step: 86, loss is 0.004373807460069656\n",
      "epoch: 19 step: 87, loss is 0.047940295189619064\n",
      "epoch: 19 step: 88, loss is 0.0006482331664301455\n",
      "epoch: 19 step: 89, loss is 0.04188966751098633\n",
      "epoch: 19 step: 90, loss is 0.02433190308511257\n",
      "epoch: 19 step: 91, loss is 0.0037454545963555574\n",
      "epoch: 19 step: 92, loss is 0.15330922603607178\n",
      "epoch: 19 step: 93, loss is 0.010172935202717781\n",
      "epoch: 19 step: 94, loss is 0.004351521842181683\n",
      "epoch: 19 step: 95, loss is 0.000639676465652883\n",
      "epoch: 19 step: 96, loss is 0.0007767005008645356\n",
      "epoch: 19 step: 97, loss is 0.007748791947960854\n",
      "epoch: 19 step: 98, loss is 0.014335842803120613\n",
      "epoch: 19 step: 99, loss is 0.08675730228424072\n",
      "epoch: 19 step: 100, loss is 0.004859442822635174\n",
      "epoch: 19 step: 101, loss is 0.0010033712023869157\n",
      "epoch: 19 step: 102, loss is 0.03632432222366333\n",
      "epoch: 19 step: 103, loss is 0.002830691169947386\n",
      "epoch: 19 step: 104, loss is 0.004297833889722824\n",
      "epoch: 19 step: 105, loss is 0.12095034867525101\n",
      "epoch: 19 step: 106, loss is 0.0022878877352923155\n",
      "epoch: 19 step: 107, loss is 0.0053907097317278385\n",
      "epoch: 19 step: 108, loss is 0.1327228844165802\n",
      "epoch: 19 step: 109, loss is 0.002511167200282216\n",
      "epoch: 19 step: 110, loss is 0.0003383673320058733\n",
      "epoch: 19 step: 111, loss is 0.024929342791438103\n",
      "epoch: 19 step: 112, loss is 0.004296040628105402\n",
      "epoch: 19 step: 113, loss is 0.015721982344985008\n",
      "epoch: 19 step: 114, loss is 0.007150149438530207\n",
      "epoch: 19 step: 115, loss is 0.0032085441052913666\n",
      "epoch: 19 step: 116, loss is 0.0197527427226305\n",
      "epoch: 19 step: 117, loss is 0.014567425474524498\n",
      "epoch: 19 step: 118, loss is 0.00015697690832894295\n",
      "epoch: 19 step: 119, loss is 0.023874670267105103\n",
      "epoch: 19 step: 120, loss is 0.049221765249967575\n",
      "epoch: 19 step: 121, loss is 0.016272250562906265\n",
      "epoch: 19 step: 122, loss is 0.10044089704751968\n",
      "epoch: 19 step: 123, loss is 0.0005210528033785522\n",
      "epoch: 19 step: 124, loss is 0.0021798373199999332\n",
      "epoch: 19 step: 125, loss is 0.003073926316574216\n",
      "epoch: 19 step: 126, loss is 0.002313855569809675\n",
      "epoch: 19 step: 127, loss is 0.008624528534710407\n",
      "epoch: 19 step: 128, loss is 0.03003440611064434\n",
      "epoch: 19 step: 129, loss is 0.0019048824906349182\n",
      "epoch: 19 step: 130, loss is 0.018727440387010574\n",
      "epoch: 19 step: 131, loss is 0.00040107074892148376\n",
      "epoch: 19 step: 132, loss is 0.007400743663311005\n",
      "epoch: 19 step: 133, loss is 0.0013922941870987415\n",
      "epoch: 19 step: 134, loss is 0.0008264881325885653\n",
      "epoch: 19 step: 135, loss is 0.0012407423928380013\n",
      "epoch: 19 step: 136, loss is 0.00244480324909091\n",
      "epoch: 19 step: 137, loss is 0.019140105694532394\n",
      "epoch: 19 step: 138, loss is 0.01732279732823372\n",
      "epoch: 19 step: 139, loss is 0.0015705617843195796\n",
      "epoch: 19 step: 140, loss is 0.01331151369959116\n",
      "epoch: 19 step: 141, loss is 0.00663290498778224\n",
      "epoch: 19 step: 142, loss is 0.01457966212183237\n",
      "epoch: 19 step: 143, loss is 0.0008698084275238216\n",
      "epoch: 19 step: 144, loss is 0.007755179889500141\n",
      "epoch: 19 step: 145, loss is 0.02795610949397087\n",
      "epoch: 19 step: 146, loss is 0.0019676005467772484\n",
      "epoch: 19 step: 147, loss is 0.0007996269268915057\n",
      "epoch: 19 step: 148, loss is 0.05938168615102768\n",
      "epoch: 19 step: 149, loss is 0.007241246290504932\n",
      "epoch: 19 step: 150, loss is 0.001190141891129315\n",
      "epoch: 19 step: 151, loss is 0.002568561350926757\n",
      "epoch: 19 step: 152, loss is 0.007886187173426151\n",
      "epoch: 19 step: 153, loss is 0.0016903196228668094\n",
      "epoch: 19 step: 154, loss is 0.0026596279349178076\n",
      "epoch: 19 step: 155, loss is 0.005738564766943455\n",
      "epoch: 19 step: 156, loss is 0.0055030821822583675\n",
      "epoch: 19 step: 157, loss is 0.00706007843837142\n",
      "epoch: 19 step: 158, loss is 0.0006359639228321612\n",
      "epoch: 19 step: 159, loss is 0.026038430631160736\n",
      "epoch: 19 step: 160, loss is 0.011573142372071743\n",
      "epoch: 19 step: 161, loss is 0.0016411611577495933\n",
      "epoch: 19 step: 162, loss is 0.0016330777434632182\n",
      "epoch: 19 step: 163, loss is 0.001202849904075265\n",
      "epoch: 19 step: 164, loss is 0.0001821782352635637\n",
      "epoch: 19 step: 165, loss is 0.004097193945199251\n",
      "epoch: 19 step: 166, loss is 0.0020645842887461185\n",
      "epoch: 19 step: 167, loss is 0.004819316789507866\n",
      "epoch: 19 step: 168, loss is 0.018375463783740997\n",
      "epoch: 19 step: 169, loss is 0.0005015508504584432\n",
      "epoch: 19 step: 170, loss is 0.002130459528416395\n",
      "epoch: 19 step: 171, loss is 0.0001392019767081365\n",
      "epoch: 19 step: 172, loss is 0.0007202850538305938\n",
      "epoch: 19 step: 173, loss is 0.01599346660077572\n",
      "epoch: 19 step: 174, loss is 0.017035529017448425\n",
      "epoch: 19 step: 175, loss is 0.0015555680729448795\n",
      "epoch: 19 step: 176, loss is 0.00013319820573087782\n",
      "epoch: 19 step: 177, loss is 0.033804044127464294\n",
      "epoch: 19 step: 178, loss is 0.00038480901275761425\n",
      "epoch: 19 step: 179, loss is 0.00998417567461729\n",
      "epoch: 19 step: 180, loss is 0.001025349018163979\n",
      "epoch: 19 step: 181, loss is 0.001529914909042418\n",
      "epoch: 19 step: 182, loss is 3.108259988948703e-05\n",
      "epoch: 19 step: 183, loss is 0.019880499690771103\n",
      "epoch: 19 step: 184, loss is 0.007690338883548975\n",
      "epoch: 19 step: 185, loss is 0.001633387291803956\n",
      "epoch: 19 step: 186, loss is 0.03376205638051033\n",
      "epoch: 19 step: 187, loss is 0.004447635263204575\n",
      "epoch: 19 step: 188, loss is 0.021238185465335846\n",
      "epoch: 19 step: 189, loss is 0.0017172214575111866\n",
      "epoch: 19 step: 190, loss is 0.015517523512244225\n",
      "epoch: 19 step: 191, loss is 0.010788923129439354\n",
      "epoch: 19 step: 192, loss is 0.0003466533380560577\n",
      "epoch: 19 step: 193, loss is 0.001116619911044836\n",
      "epoch: 19 step: 194, loss is 0.017306702211499214\n",
      "epoch: 19 step: 195, loss is 0.002426894148811698\n",
      "epoch: 19 step: 196, loss is 0.0022822762839496136\n",
      "epoch: 19 step: 197, loss is 0.013120723888278008\n",
      "epoch: 19 step: 198, loss is 0.00026709228404797614\n",
      "epoch: 19 step: 199, loss is 0.0021265437826514244\n",
      "epoch: 19 step: 200, loss is 0.0063732038252055645\n",
      "epoch: 19 step: 201, loss is 0.0011838856153190136\n",
      "epoch: 19 step: 202, loss is 0.000978402211330831\n",
      "epoch: 19 step: 203, loss is 0.002596542937681079\n",
      "epoch: 19 step: 204, loss is 0.009801138192415237\n",
      "epoch: 19 step: 205, loss is 0.0005182762979529798\n",
      "epoch: 19 step: 206, loss is 0.000814448983874172\n",
      "epoch: 19 step: 207, loss is 0.029723264276981354\n",
      "epoch: 19 step: 208, loss is 0.0033038111869245768\n",
      "epoch: 19 step: 209, loss is 0.0012112001422792673\n",
      "epoch: 19 step: 210, loss is 0.015227331779897213\n",
      "epoch: 19 step: 211, loss is 0.00024407534510828555\n",
      "epoch: 19 step: 212, loss is 0.004285729490220547\n",
      "epoch: 19 step: 213, loss is 0.0006648668786510825\n",
      "epoch: 19 step: 214, loss is 0.011432607658207417\n",
      "epoch: 19 step: 215, loss is 2.2931311832508072e-05\n",
      "epoch: 19 step: 216, loss is 0.004585407208651304\n",
      "epoch: 19 step: 217, loss is 0.003616177476942539\n",
      "epoch: 19 step: 218, loss is 0.0076082139275968075\n",
      "epoch: 19 step: 219, loss is 0.008864089846611023\n",
      "epoch: 19 step: 220, loss is 0.011288130655884743\n",
      "epoch: 19 step: 221, loss is 0.036108024418354034\n",
      "epoch: 19 step: 222, loss is 0.0013306157197803259\n",
      "epoch: 19 step: 223, loss is 0.002996226539835334\n",
      "epoch: 19 step: 224, loss is 0.00979452021420002\n",
      "epoch: 19 step: 225, loss is 0.0007404737989418209\n",
      "epoch: 19 step: 226, loss is 0.0017017391510307789\n",
      "epoch: 19 step: 227, loss is 0.013749264180660248\n",
      "epoch: 19 step: 228, loss is 0.016521530225872993\n",
      "epoch: 19 step: 229, loss is 0.008073202334344387\n",
      "epoch: 19 step: 230, loss is 0.10031242668628693\n",
      "epoch: 19 step: 231, loss is 0.004057219251990318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 232, loss is 0.019599102437496185\n",
      "epoch: 19 step: 233, loss is 0.0009038486168719828\n",
      "epoch: 19 step: 234, loss is 0.0024571954272687435\n",
      "epoch: 19 step: 235, loss is 0.0003633187443483621\n",
      "epoch: 19 step: 236, loss is 0.0020852084271609783\n",
      "epoch: 19 step: 237, loss is 0.0021652611903846264\n",
      "epoch: 19 step: 238, loss is 0.0004378996090963483\n",
      "epoch: 19 step: 239, loss is 0.005741604603827\n",
      "epoch: 19 step: 240, loss is 0.0005021935212425888\n",
      "epoch: 19 step: 241, loss is 0.007187459617853165\n",
      "epoch: 19 step: 242, loss is 0.007231771480292082\n",
      "epoch: 19 step: 243, loss is 0.0007635279325768352\n",
      "epoch: 19 step: 244, loss is 0.005752935539931059\n",
      "epoch: 19 step: 245, loss is 0.001966774230822921\n",
      "epoch: 19 step: 246, loss is 0.001444926019757986\n",
      "epoch: 19 step: 247, loss is 0.0010211553890258074\n",
      "epoch: 19 step: 248, loss is 0.0007407591911032796\n",
      "epoch: 19 step: 249, loss is 0.0017685663187876344\n",
      "epoch: 19 step: 250, loss is 0.007436492945998907\n",
      "epoch: 19 step: 251, loss is 0.011456111446022987\n",
      "epoch: 19 step: 252, loss is 0.004549537785351276\n",
      "epoch: 19 step: 253, loss is 0.001816273434087634\n",
      "epoch: 19 step: 254, loss is 0.007708137854933739\n",
      "epoch: 19 step: 255, loss is 0.0024637742899358273\n",
      "epoch: 19 step: 256, loss is 0.0005448670126497746\n",
      "epoch: 19 step: 257, loss is 0.0035072758328169584\n",
      "epoch: 19 step: 258, loss is 0.005006659310311079\n",
      "epoch: 19 step: 259, loss is 0.019302379339933395\n",
      "epoch: 19 step: 260, loss is 0.0023625444155186415\n",
      "epoch: 19 step: 261, loss is 0.004234055057168007\n",
      "epoch: 19 step: 262, loss is 0.0009873630478978157\n",
      "epoch: 19 step: 263, loss is 0.000681714154779911\n",
      "epoch: 19 step: 264, loss is 0.0007609618478454649\n",
      "epoch: 19 step: 265, loss is 0.0001657757966313511\n",
      "epoch: 19 step: 266, loss is 0.0009973411215469241\n",
      "epoch: 19 step: 267, loss is 0.002463849261403084\n",
      "epoch: 19 step: 268, loss is 0.09623366594314575\n",
      "epoch: 19 step: 269, loss is 0.010828426107764244\n",
      "epoch: 19 step: 270, loss is 0.0032352840062230825\n",
      "epoch: 19 step: 271, loss is 0.004563807975500822\n",
      "epoch: 19 step: 272, loss is 0.00876830704510212\n",
      "epoch: 19 step: 273, loss is 0.002400751458480954\n",
      "epoch: 19 step: 274, loss is 0.036373063921928406\n",
      "epoch: 19 step: 275, loss is 0.013082960620522499\n",
      "epoch: 19 step: 276, loss is 0.013028115034103394\n",
      "epoch: 19 step: 277, loss is 0.0033327664714306593\n",
      "epoch: 19 step: 278, loss is 0.034633439034223557\n",
      "epoch: 19 step: 279, loss is 0.012061640620231628\n",
      "epoch: 19 step: 280, loss is 0.003996032290160656\n",
      "epoch: 19 step: 281, loss is 0.0002599920844659209\n",
      "epoch: 19 step: 282, loss is 0.003784543601796031\n",
      "epoch: 19 step: 283, loss is 0.0028101105708628893\n",
      "epoch: 19 step: 284, loss is 0.0032066265121102333\n",
      "epoch: 19 step: 285, loss is 0.00030841625994071364\n",
      "epoch: 19 step: 286, loss is 0.003625087905675173\n",
      "epoch: 19 step: 287, loss is 0.0009313260670751333\n",
      "epoch: 19 step: 288, loss is 0.011420800350606441\n",
      "epoch: 19 step: 289, loss is 0.012224748730659485\n",
      "epoch: 19 step: 290, loss is 0.008620296604931355\n",
      "epoch: 19 step: 291, loss is 0.0038558098021894693\n",
      "epoch: 19 step: 292, loss is 0.010532579384744167\n",
      "epoch: 19 step: 293, loss is 0.08767308294773102\n",
      "epoch: 19 step: 294, loss is 0.0006628843257203698\n",
      "epoch: 19 step: 295, loss is 0.004884812980890274\n",
      "epoch: 19 step: 296, loss is 0.007596651092171669\n",
      "epoch: 19 step: 297, loss is 0.009695733897387981\n",
      "epoch: 19 step: 298, loss is 0.03961121663451195\n",
      "epoch: 19 step: 299, loss is 0.0010440406622365117\n",
      "epoch: 19 step: 300, loss is 0.0029617291875183582\n",
      "epoch: 19 step: 301, loss is 0.011792836710810661\n",
      "epoch: 19 step: 302, loss is 0.00023225584300234914\n",
      "epoch: 19 step: 303, loss is 0.003965185023844242\n",
      "epoch: 19 step: 304, loss is 0.003893214976415038\n",
      "epoch: 19 step: 305, loss is 0.0009758641244843602\n",
      "epoch: 19 step: 306, loss is 0.002106977393850684\n",
      "epoch: 19 step: 307, loss is 0.026606254279613495\n",
      "epoch: 19 step: 308, loss is 0.07229718565940857\n",
      "epoch: 19 step: 309, loss is 0.002277944702655077\n",
      "epoch: 19 step: 310, loss is 0.03614058718085289\n",
      "epoch: 19 step: 311, loss is 0.00476525304839015\n",
      "epoch: 19 step: 312, loss is 0.010394173674285412\n",
      "epoch: 19 step: 313, loss is 0.00042525120079517365\n",
      "epoch: 19 step: 314, loss is 0.0257399994879961\n",
      "epoch: 19 step: 315, loss is 0.0016423132037743926\n",
      "epoch: 19 step: 316, loss is 0.00017195151303894818\n",
      "epoch: 19 step: 317, loss is 0.0006695518968626857\n",
      "epoch: 19 step: 318, loss is 0.0026928242295980453\n",
      "epoch: 19 step: 319, loss is 0.000994394184090197\n",
      "epoch: 19 step: 320, loss is 0.008257174864411354\n",
      "epoch: 19 step: 321, loss is 0.02239224873483181\n",
      "epoch: 19 step: 322, loss is 0.004295759368687868\n",
      "epoch: 19 step: 323, loss is 0.002073816489428282\n",
      "epoch: 19 step: 324, loss is 0.003043784759938717\n",
      "epoch: 19 step: 325, loss is 0.010893583297729492\n",
      "epoch: 19 step: 326, loss is 0.08598212897777557\n",
      "epoch: 19 step: 327, loss is 0.00456179678440094\n",
      "epoch: 19 step: 328, loss is 0.00468842126429081\n",
      "epoch: 19 step: 329, loss is 0.005285088904201984\n",
      "epoch: 19 step: 330, loss is 0.020815378054976463\n",
      "epoch: 19 step: 331, loss is 0.039647698402404785\n",
      "epoch: 19 step: 332, loss is 0.009060541167855263\n",
      "epoch: 19 step: 333, loss is 0.0009011799120344222\n",
      "epoch: 19 step: 334, loss is 0.004362736828625202\n",
      "epoch: 19 step: 335, loss is 0.0063008577562868595\n",
      "epoch: 19 step: 336, loss is 0.002207342302426696\n",
      "epoch: 19 step: 337, loss is 0.004453110508620739\n",
      "epoch: 19 step: 338, loss is 0.06598057597875595\n",
      "epoch: 19 step: 339, loss is 0.0028050891123712063\n",
      "epoch: 19 step: 340, loss is 0.0013075501192361116\n",
      "epoch: 19 step: 341, loss is 0.005980958696454763\n",
      "epoch: 19 step: 342, loss is 0.011302592232823372\n",
      "epoch: 19 step: 343, loss is 0.1370062679052353\n",
      "epoch: 19 step: 344, loss is 0.0078047048300504684\n",
      "epoch: 19 step: 345, loss is 0.003520026570186019\n",
      "epoch: 19 step: 346, loss is 0.00795273669064045\n",
      "epoch: 19 step: 347, loss is 0.029349688440561295\n",
      "epoch: 19 step: 348, loss is 0.04348234832286835\n",
      "epoch: 19 step: 349, loss is 0.0012778723612427711\n",
      "epoch: 19 step: 350, loss is 0.002156432718038559\n",
      "epoch: 19 step: 351, loss is 0.00021912870579399168\n",
      "epoch: 19 step: 352, loss is 0.005851182155311108\n",
      "epoch: 19 step: 353, loss is 0.019418135285377502\n",
      "epoch: 19 step: 354, loss is 0.0015591690316796303\n",
      "epoch: 19 step: 355, loss is 0.0016644882271066308\n",
      "epoch: 19 step: 356, loss is 0.0026968985330313444\n",
      "epoch: 19 step: 357, loss is 0.005405264440923929\n",
      "epoch: 19 step: 358, loss is 0.05147838965058327\n",
      "epoch: 19 step: 359, loss is 0.0016201130347326398\n",
      "epoch: 19 step: 360, loss is 0.007590780034661293\n",
      "epoch: 19 step: 361, loss is 0.0024936399422585964\n",
      "epoch: 19 step: 362, loss is 0.0055640097707509995\n",
      "epoch: 19 step: 363, loss is 0.001880118390545249\n",
      "epoch: 19 step: 364, loss is 0.0888495221734047\n",
      "epoch: 19 step: 365, loss is 0.030382344499230385\n",
      "epoch: 19 step: 366, loss is 0.054152753204107285\n",
      "epoch: 19 step: 367, loss is 0.05865266174077988\n",
      "epoch: 19 step: 368, loss is 0.01240801252424717\n",
      "epoch: 19 step: 369, loss is 0.025141021236777306\n",
      "epoch: 19 step: 370, loss is 0.0067775254137814045\n",
      "epoch: 19 step: 371, loss is 0.014622699469327927\n",
      "epoch: 19 step: 372, loss is 0.005684921517968178\n",
      "epoch: 19 step: 373, loss is 0.0025301927234977484\n",
      "epoch: 19 step: 374, loss is 0.005452526267617941\n",
      "epoch: 19 step: 375, loss is 0.002685376675799489\n",
      "epoch: 19 step: 376, loss is 0.001362788607366383\n",
      "epoch: 19 step: 377, loss is 0.0009826270397752523\n",
      "epoch: 19 step: 378, loss is 0.0053182123228907585\n",
      "epoch: 19 step: 379, loss is 0.004391978494822979\n",
      "epoch: 19 step: 380, loss is 0.027801571413874626\n",
      "epoch: 19 step: 381, loss is 0.0002643705520313233\n",
      "epoch: 19 step: 382, loss is 0.0018215257441625\n",
      "epoch: 19 step: 383, loss is 0.008128490298986435\n",
      "epoch: 19 step: 384, loss is 0.01898827962577343\n",
      "epoch: 19 step: 385, loss is 0.025828534737229347\n",
      "epoch: 19 step: 386, loss is 0.005286598578095436\n",
      "epoch: 19 step: 387, loss is 0.004255345556885004\n",
      "epoch: 19 step: 388, loss is 0.012751733884215355\n",
      "epoch: 19 step: 389, loss is 0.0009631742723286152\n",
      "epoch: 19 step: 390, loss is 0.347993940114975\n",
      "epoch: 19 step: 391, loss is 0.001647414406761527\n",
      "epoch: 19 step: 392, loss is 0.010317319072782993\n",
      "epoch: 19 step: 393, loss is 0.018526466563344002\n",
      "epoch: 19 step: 394, loss is 0.003330687992274761\n",
      "epoch: 19 step: 395, loss is 0.0005015429924242198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 396, loss is 0.046916838735342026\n",
      "epoch: 19 step: 397, loss is 0.0002394288603682071\n",
      "epoch: 19 step: 398, loss is 0.00815746933221817\n",
      "epoch: 19 step: 399, loss is 0.001190731069073081\n",
      "epoch: 19 step: 400, loss is 0.0003882093878928572\n",
      "epoch: 19 step: 401, loss is 0.00025292267673648894\n",
      "epoch: 19 step: 402, loss is 0.0005730089033022523\n",
      "epoch: 19 step: 403, loss is 0.04435589909553528\n",
      "epoch: 19 step: 404, loss is 0.0030689912382513285\n",
      "epoch: 19 step: 405, loss is 0.008396490477025509\n",
      "epoch: 19 step: 406, loss is 0.002366970293223858\n",
      "epoch: 19 step: 407, loss is 0.003107490250840783\n",
      "epoch: 19 step: 408, loss is 0.0021616965532302856\n",
      "epoch: 19 step: 409, loss is 0.028184479102492332\n",
      "epoch: 19 step: 410, loss is 0.0008746919338591397\n",
      "epoch: 19 step: 411, loss is 0.005371773615479469\n",
      "epoch: 19 step: 412, loss is 0.00030851736664772034\n",
      "epoch: 19 step: 413, loss is 0.0009761720430105925\n",
      "epoch: 19 step: 414, loss is 0.04188932105898857\n",
      "epoch: 19 step: 415, loss is 0.026527857407927513\n",
      "epoch: 19 step: 416, loss is 0.007591784466058016\n",
      "epoch: 19 step: 417, loss is 0.0023881529923528433\n",
      "epoch: 19 step: 418, loss is 0.008422219194471836\n",
      "epoch: 19 step: 419, loss is 0.11137493699789047\n",
      "epoch: 19 step: 420, loss is 0.000514904095325619\n",
      "epoch: 19 step: 421, loss is 1.770971721271053e-05\n",
      "epoch: 19 step: 422, loss is 0.056342799216508865\n",
      "epoch: 19 step: 423, loss is 0.00012315716594457626\n",
      "epoch: 19 step: 424, loss is 0.0006529270322062075\n",
      "epoch: 19 step: 425, loss is 0.002025824273005128\n",
      "epoch: 19 step: 426, loss is 0.008136235177516937\n",
      "epoch: 19 step: 427, loss is 0.08968330174684525\n",
      "epoch: 19 step: 428, loss is 0.0015231202123686671\n",
      "epoch: 19 step: 429, loss is 0.002486595418304205\n",
      "epoch: 19 step: 430, loss is 0.025506652891635895\n",
      "epoch: 19 step: 431, loss is 0.0014284036587923765\n",
      "epoch: 19 step: 432, loss is 0.010689521208405495\n",
      "epoch: 19 step: 433, loss is 0.008784350007772446\n",
      "epoch: 19 step: 434, loss is 0.1344151347875595\n",
      "epoch: 19 step: 435, loss is 0.001425625872798264\n",
      "epoch: 19 step: 436, loss is 0.007066227030009031\n",
      "epoch: 19 step: 437, loss is 0.02205434814095497\n",
      "epoch: 19 step: 438, loss is 0.009283710271120071\n",
      "epoch: 19 step: 439, loss is 0.010329687036573887\n",
      "epoch: 19 step: 440, loss is 0.005563790909945965\n",
      "epoch: 19 step: 441, loss is 0.003581609111279249\n",
      "epoch: 19 step: 442, loss is 0.0015163219068199396\n",
      "epoch: 19 step: 443, loss is 0.003599578747525811\n",
      "epoch: 19 step: 444, loss is 0.00047437826287932694\n",
      "epoch: 19 step: 445, loss is 0.011479037813842297\n",
      "epoch: 19 step: 446, loss is 0.00028925391961820424\n",
      "epoch: 19 step: 447, loss is 0.0006212001317180693\n",
      "epoch: 19 step: 448, loss is 0.008827264420688152\n",
      "epoch: 19 step: 449, loss is 0.02937859296798706\n",
      "epoch: 19 step: 450, loss is 0.10103974491357803\n",
      "epoch: 19 step: 451, loss is 0.0013963914243504405\n",
      "epoch: 19 step: 452, loss is 0.0030876724049448967\n",
      "epoch: 19 step: 453, loss is 0.00768920686095953\n",
      "epoch: 19 step: 454, loss is 0.009324045851826668\n",
      "epoch: 19 step: 455, loss is 0.04201855883002281\n",
      "epoch: 19 step: 456, loss is 0.0026139465626329184\n",
      "epoch: 19 step: 457, loss is 0.024721866473555565\n",
      "epoch: 19 step: 458, loss is 0.01949685625731945\n",
      "epoch: 19 step: 459, loss is 0.060033705085515976\n",
      "epoch: 19 step: 460, loss is 0.10924184322357178\n",
      "epoch: 19 step: 461, loss is 0.06645002216100693\n",
      "epoch: 19 step: 462, loss is 0.02042423188686371\n",
      "epoch: 19 step: 463, loss is 0.010326029732823372\n",
      "epoch: 19 step: 464, loss is 0.04408971220254898\n",
      "epoch: 19 step: 465, loss is 0.00035460799699649215\n",
      "epoch: 19 step: 466, loss is 0.03610745444893837\n",
      "epoch: 19 step: 467, loss is 0.011407884769141674\n",
      "epoch: 19 step: 468, loss is 0.00764803122729063\n",
      "epoch: 19 step: 469, loss is 0.03986840322613716\n",
      "epoch: 19 step: 470, loss is 0.0016161019448190928\n",
      "epoch: 19 step: 471, loss is 0.004759239964187145\n",
      "epoch: 19 step: 472, loss is 0.021529266610741615\n",
      "epoch: 19 step: 473, loss is 0.006293166894465685\n",
      "epoch: 19 step: 474, loss is 0.006875419989228249\n",
      "epoch: 19 step: 475, loss is 0.10852013528347015\n",
      "epoch: 19 step: 476, loss is 0.041768789291381836\n",
      "epoch: 19 step: 477, loss is 0.005598526448011398\n",
      "epoch: 19 step: 478, loss is 0.002460179850459099\n",
      "epoch: 19 step: 479, loss is 0.057916317135095596\n",
      "epoch: 19 step: 480, loss is 0.04665623977780342\n",
      "epoch: 19 step: 481, loss is 0.010119382292032242\n",
      "epoch: 19 step: 482, loss is 0.0015019937418401241\n",
      "epoch: 19 step: 483, loss is 0.009973331354558468\n",
      "epoch: 19 step: 484, loss is 0.0034901094622910023\n",
      "epoch: 19 step: 485, loss is 0.0026593890506774187\n",
      "epoch: 19 step: 486, loss is 0.00382789084687829\n",
      "epoch: 19 step: 487, loss is 0.0006083264015614986\n",
      "epoch: 19 step: 488, loss is 0.007809253875166178\n",
      "epoch: 19 step: 489, loss is 0.0001867341052275151\n",
      "epoch: 19 step: 490, loss is 0.041665803641080856\n",
      "epoch: 19 step: 491, loss is 0.009853762574493885\n",
      "epoch: 19 step: 492, loss is 0.047676801681518555\n",
      "epoch: 19 step: 493, loss is 0.017338827252388\n",
      "epoch: 19 step: 494, loss is 0.04918687045574188\n",
      "epoch: 19 step: 495, loss is 0.0010041787754744291\n",
      "epoch: 19 step: 496, loss is 0.001148253446444869\n",
      "epoch: 19 step: 497, loss is 0.0012436292599886656\n",
      "epoch: 19 step: 498, loss is 0.0440288670361042\n",
      "epoch: 19 step: 499, loss is 0.009906437247991562\n",
      "epoch: 19 step: 500, loss is 0.0063360752537846565\n",
      "epoch: 19 step: 501, loss is 0.0009490643278695643\n",
      "epoch: 19 step: 502, loss is 0.05157069116830826\n",
      "epoch: 19 step: 503, loss is 0.003652998013421893\n",
      "epoch: 19 step: 504, loss is 0.0006009271019138396\n",
      "epoch: 19 step: 505, loss is 0.06595659255981445\n",
      "epoch: 19 step: 506, loss is 0.003444470465183258\n",
      "epoch: 19 step: 507, loss is 0.004551758989691734\n",
      "epoch: 19 step: 508, loss is 0.0026339637115597725\n",
      "epoch: 19 step: 509, loss is 0.0008725341758690774\n",
      "epoch: 19 step: 510, loss is 0.04834141954779625\n",
      "epoch: 19 step: 511, loss is 0.08906945586204529\n",
      "epoch: 19 step: 512, loss is 0.06043921783566475\n",
      "epoch: 19 step: 513, loss is 0.045178890228271484\n",
      "epoch: 19 step: 514, loss is 0.002414997201412916\n",
      "epoch: 19 step: 515, loss is 0.07819092273712158\n",
      "epoch: 19 step: 516, loss is 0.011608479544520378\n",
      "epoch: 19 step: 517, loss is 0.028389768674969673\n",
      "epoch: 19 step: 518, loss is 0.023457122966647148\n",
      "epoch: 19 step: 519, loss is 0.1273588389158249\n",
      "epoch: 19 step: 520, loss is 0.14524181187152863\n",
      "epoch: 19 step: 521, loss is 0.00110521144233644\n",
      "epoch: 19 step: 522, loss is 0.016446655616164207\n",
      "epoch: 19 step: 523, loss is 0.004119768273085356\n",
      "epoch: 19 step: 524, loss is 0.03721533715724945\n",
      "epoch: 19 step: 525, loss is 0.0009375910158269107\n",
      "epoch: 19 step: 526, loss is 0.005553924012929201\n",
      "epoch: 19 step: 527, loss is 0.06439150124788284\n",
      "epoch: 19 step: 528, loss is 0.03792332485318184\n",
      "epoch: 19 step: 529, loss is 0.08386213332414627\n",
      "epoch: 19 step: 530, loss is 0.020478125661611557\n",
      "epoch: 19 step: 531, loss is 0.03444141149520874\n",
      "epoch: 19 step: 532, loss is 0.0784674659371376\n",
      "epoch: 19 step: 533, loss is 0.009066181257367134\n",
      "epoch: 19 step: 534, loss is 0.0015073242830112576\n",
      "epoch: 19 step: 535, loss is 0.04203937575221062\n",
      "epoch: 19 step: 536, loss is 0.13601365685462952\n",
      "epoch: 19 step: 537, loss is 0.03393620625138283\n",
      "epoch: 19 step: 538, loss is 0.03761505335569382\n",
      "epoch: 19 step: 539, loss is 0.016091324388980865\n",
      "epoch: 19 step: 540, loss is 0.06668964773416519\n",
      "epoch: 19 step: 541, loss is 0.006817925255745649\n",
      "epoch: 19 step: 542, loss is 0.007212506607174873\n",
      "epoch: 19 step: 543, loss is 0.014634394086897373\n",
      "epoch: 19 step: 544, loss is 0.011902159079909325\n",
      "epoch: 19 step: 545, loss is 0.02019854635000229\n",
      "epoch: 19 step: 546, loss is 0.020747432485222816\n",
      "epoch: 19 step: 547, loss is 0.006801333744078875\n",
      "epoch: 19 step: 548, loss is 0.00760806305333972\n",
      "epoch: 19 step: 549, loss is 0.003700926434248686\n",
      "epoch: 19 step: 550, loss is 0.007373346947133541\n",
      "epoch: 19 step: 551, loss is 0.0452500618994236\n",
      "epoch: 19 step: 552, loss is 0.09836281090974808\n",
      "epoch: 19 step: 553, loss is 0.006132634822279215\n",
      "epoch: 19 step: 554, loss is 0.02585146203637123\n",
      "epoch: 19 step: 555, loss is 0.18354147672653198\n",
      "epoch: 19 step: 556, loss is 0.02316731959581375\n",
      "epoch: 19 step: 557, loss is 0.0005640613380819559\n",
      "epoch: 19 step: 558, loss is 0.019821105524897575\n",
      "epoch: 19 step: 559, loss is 0.04093987122178078\n",
      "epoch: 19 step: 560, loss is 0.00751664862036705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 561, loss is 0.008862742222845554\n",
      "epoch: 19 step: 562, loss is 0.03464833274483681\n",
      "epoch: 19 step: 563, loss is 0.010752085596323013\n",
      "epoch: 19 step: 564, loss is 0.004509087651968002\n",
      "epoch: 19 step: 565, loss is 0.13981322944164276\n",
      "epoch: 19 step: 566, loss is 0.00843933317810297\n",
      "epoch: 19 step: 567, loss is 0.011728129349648952\n",
      "epoch: 19 step: 568, loss is 0.11981254070997238\n",
      "epoch: 19 step: 569, loss is 0.0011389660649001598\n",
      "epoch: 19 step: 570, loss is 0.016294293105602264\n",
      "epoch: 19 step: 571, loss is 0.03919482231140137\n",
      "epoch: 19 step: 572, loss is 0.00987557414919138\n",
      "epoch: 19 step: 573, loss is 0.015132520347833633\n",
      "epoch: 19 step: 574, loss is 7.984812691574916e-05\n",
      "epoch: 19 step: 575, loss is 0.008144715800881386\n",
      "epoch: 19 step: 576, loss is 0.004377824254333973\n",
      "epoch: 19 step: 577, loss is 0.039430733770132065\n",
      "epoch: 19 step: 578, loss is 0.011707378551363945\n",
      "epoch: 19 step: 579, loss is 0.011676379479467869\n",
      "epoch: 19 step: 580, loss is 0.0035622240975499153\n",
      "epoch: 19 step: 581, loss is 0.04484960436820984\n",
      "epoch: 19 step: 582, loss is 0.002061012899503112\n",
      "epoch: 19 step: 583, loss is 0.11198730021715164\n",
      "epoch: 19 step: 584, loss is 0.008450702764093876\n",
      "epoch: 19 step: 585, loss is 0.01906416192650795\n",
      "epoch: 19 step: 586, loss is 0.002610734198242426\n",
      "epoch: 19 step: 587, loss is 0.025052925571799278\n",
      "epoch: 19 step: 588, loss is 0.04524212330579758\n",
      "epoch: 19 step: 589, loss is 0.0005029583116993308\n",
      "epoch: 19 step: 590, loss is 0.007415674161165953\n",
      "epoch: 19 step: 591, loss is 0.061747778207063675\n",
      "epoch: 19 step: 592, loss is 0.03383747860789299\n",
      "epoch: 19 step: 593, loss is 0.012310882098972797\n",
      "epoch: 19 step: 594, loss is 0.037279162555933\n",
      "epoch: 19 step: 595, loss is 0.0035081279929727316\n",
      "epoch: 19 step: 596, loss is 0.059351034462451935\n",
      "epoch: 19 step: 597, loss is 0.0017564393347129226\n",
      "epoch: 19 step: 598, loss is 0.12831543385982513\n",
      "epoch: 19 step: 599, loss is 0.059785712510347366\n",
      "epoch: 19 step: 600, loss is 0.0057077547535300255\n",
      "epoch: 19 step: 601, loss is 0.03507596626877785\n",
      "epoch: 19 step: 602, loss is 0.017329899594187737\n",
      "epoch: 19 step: 603, loss is 0.003537855576723814\n",
      "epoch: 19 step: 604, loss is 0.1648884117603302\n",
      "epoch: 19 step: 605, loss is 0.018464675173163414\n",
      "epoch: 19 step: 606, loss is 0.0006920286105014384\n",
      "epoch: 19 step: 607, loss is 0.14532653987407684\n",
      "epoch: 19 step: 608, loss is 0.031018167734146118\n",
      "epoch: 19 step: 609, loss is 0.0273192897439003\n",
      "epoch: 19 step: 610, loss is 0.005883167963474989\n",
      "epoch: 19 step: 611, loss is 0.025338539853692055\n",
      "epoch: 19 step: 612, loss is 0.003224744461476803\n",
      "epoch: 19 step: 613, loss is 0.052130427211523056\n",
      "epoch: 19 step: 614, loss is 0.011274425312876701\n",
      "epoch: 19 step: 615, loss is 0.08899951726198196\n",
      "epoch: 19 step: 616, loss is 0.013732942752540112\n",
      "epoch: 19 step: 617, loss is 0.006045334506779909\n",
      "epoch: 19 step: 618, loss is 0.012286731041967869\n",
      "epoch: 19 step: 619, loss is 0.00048567523481324315\n",
      "epoch: 19 step: 620, loss is 0.005059702321887016\n",
      "epoch: 19 step: 621, loss is 0.0011075681541115046\n",
      "epoch: 19 step: 622, loss is 0.01636348105967045\n",
      "epoch: 19 step: 623, loss is 0.004044026602059603\n",
      "epoch: 19 step: 624, loss is 0.08718696236610413\n",
      "epoch: 19 step: 625, loss is 0.06899114698171616\n",
      "epoch: 19 step: 626, loss is 0.012341374531388283\n",
      "epoch: 19 step: 627, loss is 0.04703431576490402\n",
      "epoch: 19 step: 628, loss is 0.002544431947171688\n",
      "epoch: 19 step: 629, loss is 0.0025936346501111984\n",
      "epoch: 19 step: 630, loss is 0.004380146041512489\n",
      "epoch: 19 step: 631, loss is 0.004950155504047871\n",
      "epoch: 19 step: 632, loss is 0.008244051598012447\n",
      "epoch: 19 step: 633, loss is 0.004634654149413109\n",
      "epoch: 19 step: 634, loss is 0.0697261393070221\n",
      "epoch: 19 step: 635, loss is 0.004847473464906216\n",
      "epoch: 19 step: 636, loss is 0.024523578584194183\n",
      "epoch: 19 step: 637, loss is 0.00537378154695034\n",
      "epoch: 19 step: 638, loss is 0.00048348790733143687\n",
      "epoch: 19 step: 639, loss is 0.009825119748711586\n",
      "epoch: 19 step: 640, loss is 0.0010427157394587994\n",
      "epoch: 19 step: 641, loss is 0.07268271595239639\n",
      "epoch: 19 step: 642, loss is 0.0012977045262232423\n",
      "epoch: 19 step: 643, loss is 0.007604688405990601\n",
      "epoch: 19 step: 644, loss is 0.0028327868785709143\n",
      "epoch: 19 step: 645, loss is 0.05033364146947861\n",
      "epoch: 19 step: 646, loss is 0.026935486122965813\n",
      "epoch: 19 step: 647, loss is 0.02670220099389553\n",
      "epoch: 19 step: 648, loss is 0.0026488839648663998\n",
      "epoch: 19 step: 649, loss is 0.00607598852366209\n",
      "epoch: 19 step: 650, loss is 0.03834937885403633\n",
      "epoch: 19 step: 651, loss is 0.0441904254257679\n",
      "epoch: 19 step: 652, loss is 0.038581106811761856\n",
      "epoch: 19 step: 653, loss is 0.0036730910651385784\n",
      "epoch: 19 step: 654, loss is 0.0024879814591258764\n",
      "epoch: 19 step: 655, loss is 0.0014926379080861807\n",
      "epoch: 19 step: 656, loss is 0.013783247210085392\n",
      "epoch: 19 step: 657, loss is 0.003023458644747734\n",
      "epoch: 19 step: 658, loss is 0.11573641002178192\n",
      "epoch: 19 step: 659, loss is 0.002990632550790906\n",
      "epoch: 19 step: 660, loss is 0.0033718966878950596\n",
      "epoch: 19 step: 661, loss is 0.0011939452961087227\n",
      "epoch: 19 step: 662, loss is 0.02966569922864437\n",
      "epoch: 19 step: 663, loss is 0.006053849123418331\n",
      "epoch: 19 step: 664, loss is 0.007242867723107338\n",
      "epoch: 19 step: 665, loss is 0.0013814870035275817\n",
      "epoch: 19 step: 666, loss is 0.08276110142469406\n",
      "epoch: 19 step: 667, loss is 0.007736786734312773\n",
      "epoch: 19 step: 668, loss is 0.0097813680768013\n",
      "epoch: 19 step: 669, loss is 0.02736303210258484\n",
      "epoch: 19 step: 670, loss is 0.04801488667726517\n",
      "epoch: 19 step: 671, loss is 0.045646023005247116\n",
      "epoch: 19 step: 672, loss is 0.024804897606372833\n",
      "epoch: 19 step: 673, loss is 0.02006678283214569\n",
      "epoch: 19 step: 674, loss is 0.0026793924625962973\n",
      "epoch: 19 step: 675, loss is 0.03467165678739548\n",
      "epoch: 19 step: 676, loss is 0.004230146761983633\n",
      "epoch: 19 step: 677, loss is 0.010337349958717823\n",
      "epoch: 19 step: 678, loss is 0.061249762773513794\n",
      "epoch: 19 step: 679, loss is 0.01745314709842205\n",
      "epoch: 19 step: 680, loss is 0.04966583102941513\n",
      "epoch: 19 step: 681, loss is 0.002134183421730995\n",
      "epoch: 19 step: 682, loss is 0.013528596609830856\n",
      "epoch: 19 step: 683, loss is 0.011752607300877571\n",
      "epoch: 19 step: 684, loss is 0.004024002701044083\n",
      "epoch: 19 step: 685, loss is 0.015960348770022392\n",
      "epoch: 19 step: 686, loss is 0.1286134272813797\n",
      "epoch: 19 step: 687, loss is 0.0155792236328125\n",
      "epoch: 19 step: 688, loss is 0.024022618308663368\n",
      "epoch: 19 step: 689, loss is 0.011321085505187511\n",
      "epoch: 19 step: 690, loss is 0.004786202218383551\n",
      "epoch: 19 step: 691, loss is 0.026754818856716156\n",
      "epoch: 19 step: 692, loss is 0.00900334119796753\n",
      "epoch: 19 step: 693, loss is 0.016618434339761734\n",
      "epoch: 19 step: 694, loss is 0.004046433139592409\n",
      "epoch: 19 step: 695, loss is 0.012674280442297459\n",
      "epoch: 19 step: 696, loss is 0.026654452085494995\n",
      "epoch: 19 step: 697, loss is 0.006472717504948378\n",
      "epoch: 19 step: 698, loss is 0.007771459873765707\n",
      "epoch: 19 step: 699, loss is 0.01201699674129486\n",
      "epoch: 19 step: 700, loss is 0.13315609097480774\n",
      "epoch: 19 step: 701, loss is 0.0005928867612965405\n",
      "epoch: 19 step: 702, loss is 0.00012634882295969874\n",
      "epoch: 19 step: 703, loss is 0.0008982937433756888\n",
      "epoch: 19 step: 704, loss is 0.008943479508161545\n",
      "epoch: 19 step: 705, loss is 0.01515066996216774\n",
      "epoch: 19 step: 706, loss is 0.022088704630732536\n",
      "epoch: 19 step: 707, loss is 0.026663675904273987\n",
      "epoch: 19 step: 708, loss is 0.00719337398186326\n",
      "epoch: 19 step: 709, loss is 0.016918856650590897\n",
      "epoch: 19 step: 710, loss is 0.07366237044334412\n",
      "epoch: 19 step: 711, loss is 0.0005602047313004732\n",
      "epoch: 19 step: 712, loss is 0.01447649858891964\n",
      "epoch: 19 step: 713, loss is 0.003066461766138673\n",
      "epoch: 19 step: 714, loss is 0.04536985605955124\n",
      "epoch: 19 step: 715, loss is 0.0007391947438009083\n",
      "epoch: 19 step: 716, loss is 0.004033832810819149\n",
      "epoch: 19 step: 717, loss is 0.003768316237255931\n",
      "epoch: 19 step: 718, loss is 0.005018758587539196\n",
      "epoch: 19 step: 719, loss is 0.002073116134852171\n",
      "epoch: 19 step: 720, loss is 0.004175035282969475\n",
      "epoch: 19 step: 721, loss is 0.009083908051252365\n",
      "epoch: 19 step: 722, loss is 0.006619905587285757\n",
      "epoch: 19 step: 723, loss is 0.0008039467502385378\n",
      "epoch: 19 step: 724, loss is 0.0023755664005875587\n",
      "epoch: 19 step: 725, loss is 0.00043293825001455843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 726, loss is 0.006886305753141642\n",
      "epoch: 19 step: 727, loss is 0.04610290378332138\n",
      "epoch: 19 step: 728, loss is 0.047841060906648636\n",
      "epoch: 19 step: 729, loss is 0.002363786334171891\n",
      "epoch: 19 step: 730, loss is 0.00023568820324726403\n",
      "epoch: 19 step: 731, loss is 0.0018476509721949697\n",
      "epoch: 19 step: 732, loss is 0.001133683486841619\n",
      "epoch: 19 step: 733, loss is 0.01679377444088459\n",
      "epoch: 19 step: 734, loss is 0.053700730204582214\n",
      "epoch: 19 step: 735, loss is 0.013724654912948608\n",
      "epoch: 19 step: 736, loss is 0.023824656382203102\n",
      "epoch: 19 step: 737, loss is 0.003940112888813019\n",
      "epoch: 19 step: 738, loss is 0.028641216456890106\n",
      "epoch: 19 step: 739, loss is 0.0008129143388941884\n",
      "epoch: 19 step: 740, loss is 0.015188348479568958\n",
      "epoch: 19 step: 741, loss is 0.02925557643175125\n",
      "epoch: 19 step: 742, loss is 0.007069749291986227\n",
      "epoch: 19 step: 743, loss is 0.006485003978013992\n",
      "epoch: 19 step: 744, loss is 0.06567759066820145\n",
      "epoch: 19 step: 745, loss is 0.03698355332016945\n",
      "epoch: 19 step: 746, loss is 0.0037591303698718548\n",
      "epoch: 19 step: 747, loss is 0.0019555394537746906\n",
      "epoch: 19 step: 748, loss is 0.001156276324763894\n",
      "epoch: 19 step: 749, loss is 0.02563875913619995\n",
      "epoch: 19 step: 750, loss is 0.004555874969810247\n",
      "epoch: 19 step: 751, loss is 0.006847095210105181\n",
      "epoch: 19 step: 752, loss is 0.03206731379032135\n",
      "epoch: 19 step: 753, loss is 0.0035909435246139765\n",
      "epoch: 19 step: 754, loss is 0.023569095879793167\n",
      "epoch: 19 step: 755, loss is 0.006499690469354391\n",
      "epoch: 19 step: 756, loss is 0.03362877294421196\n",
      "epoch: 19 step: 757, loss is 0.0032018425408750772\n",
      "epoch: 19 step: 758, loss is 0.04129260033369064\n",
      "epoch: 19 step: 759, loss is 0.041844580322504044\n",
      "epoch: 19 step: 760, loss is 0.010633157566189766\n",
      "epoch: 19 step: 761, loss is 0.03674934059381485\n",
      "epoch: 19 step: 762, loss is 0.00045690135448239744\n",
      "epoch: 19 step: 763, loss is 0.006836764980107546\n",
      "epoch: 19 step: 764, loss is 0.003126731840893626\n",
      "epoch: 19 step: 765, loss is 0.044402919709682465\n",
      "epoch: 19 step: 766, loss is 0.0025512254796922207\n",
      "epoch: 19 step: 767, loss is 0.017380457371473312\n",
      "epoch: 19 step: 768, loss is 0.0018839726690202951\n",
      "epoch: 19 step: 769, loss is 0.01588098146021366\n",
      "epoch: 19 step: 770, loss is 0.00041597854578867555\n",
      "epoch: 19 step: 771, loss is 0.006431235931813717\n",
      "epoch: 19 step: 772, loss is 0.008558078669011593\n",
      "epoch: 19 step: 773, loss is 0.0016570108709856868\n",
      "epoch: 19 step: 774, loss is 0.0011690505780279636\n",
      "epoch: 19 step: 775, loss is 0.01763186790049076\n",
      "epoch: 19 step: 776, loss is 0.010680840350687504\n",
      "epoch: 19 step: 777, loss is 0.015045059844851494\n",
      "epoch: 19 step: 778, loss is 0.00423276936635375\n",
      "epoch: 19 step: 779, loss is 0.007409455720335245\n",
      "epoch: 19 step: 780, loss is 0.0008483596611768007\n",
      "epoch: 19 step: 781, loss is 0.08975668996572495\n",
      "epoch: 19 step: 782, loss is 0.02238297089934349\n",
      "epoch: 19 step: 783, loss is 0.0056782313622534275\n",
      "epoch: 19 step: 784, loss is 0.00190434989053756\n",
      "epoch: 19 step: 785, loss is 0.002902544569224119\n",
      "epoch: 19 step: 786, loss is 0.002069267211481929\n",
      "epoch: 19 step: 787, loss is 0.0009052634704858065\n",
      "epoch: 19 step: 788, loss is 0.004636018071323633\n",
      "epoch: 19 step: 789, loss is 0.00014437084610108286\n",
      "epoch: 19 step: 790, loss is 0.008536599576473236\n",
      "epoch: 19 step: 791, loss is 0.006027006544172764\n",
      "epoch: 19 step: 792, loss is 0.02416393719613552\n",
      "epoch: 19 step: 793, loss is 0.0009094494162127376\n",
      "epoch: 19 step: 794, loss is 0.002217676257714629\n",
      "epoch: 19 step: 795, loss is 0.0003115010622423142\n",
      "epoch: 19 step: 796, loss is 0.013143319636583328\n",
      "epoch: 19 step: 797, loss is 0.04492272436618805\n",
      "epoch: 19 step: 798, loss is 0.0016705100424587727\n",
      "epoch: 19 step: 799, loss is 0.0006769619067199528\n",
      "epoch: 19 step: 800, loss is 0.06448373198509216\n",
      "epoch: 19 step: 801, loss is 0.0558762326836586\n",
      "epoch: 19 step: 802, loss is 0.04263262823224068\n",
      "epoch: 19 step: 803, loss is 0.0016580101801082492\n",
      "epoch: 19 step: 804, loss is 0.007992064580321312\n",
      "epoch: 19 step: 805, loss is 0.0038852065335959196\n",
      "epoch: 19 step: 806, loss is 0.004044988192617893\n",
      "epoch: 19 step: 807, loss is 0.08981750160455704\n",
      "epoch: 19 step: 808, loss is 0.023333191871643066\n",
      "epoch: 19 step: 809, loss is 0.0015445738099515438\n",
      "epoch: 19 step: 810, loss is 0.001379585126414895\n",
      "epoch: 19 step: 811, loss is 0.008922090753912926\n",
      "epoch: 19 step: 812, loss is 0.003632640466094017\n",
      "epoch: 19 step: 813, loss is 0.0024669221602380276\n",
      "epoch: 19 step: 814, loss is 0.008029516786336899\n",
      "epoch: 19 step: 815, loss is 0.014815419912338257\n",
      "epoch: 19 step: 816, loss is 0.01828569918870926\n",
      "epoch: 19 step: 817, loss is 0.020848006010055542\n",
      "epoch: 19 step: 818, loss is 0.0036792540922760963\n",
      "epoch: 19 step: 819, loss is 0.01580609567463398\n",
      "epoch: 19 step: 820, loss is 0.056247860193252563\n",
      "epoch: 19 step: 821, loss is 0.026748843491077423\n",
      "epoch: 19 step: 822, loss is 0.0009777810191735625\n",
      "epoch: 19 step: 823, loss is 0.13309115171432495\n",
      "epoch: 19 step: 824, loss is 0.0008433287730440497\n",
      "epoch: 19 step: 825, loss is 0.010156077332794666\n",
      "epoch: 19 step: 826, loss is 0.004906789865344763\n",
      "epoch: 19 step: 827, loss is 0.0004948857240378857\n",
      "epoch: 19 step: 828, loss is 0.0027046434115618467\n",
      "epoch: 19 step: 829, loss is 0.00979671347886324\n",
      "epoch: 19 step: 830, loss is 0.0036655329167842865\n",
      "epoch: 19 step: 831, loss is 0.06205417215824127\n",
      "epoch: 19 step: 832, loss is 0.01092586014419794\n",
      "epoch: 19 step: 833, loss is 0.03249169513583183\n",
      "epoch: 19 step: 834, loss is 0.02663518115878105\n",
      "epoch: 19 step: 835, loss is 0.02401609532535076\n",
      "epoch: 19 step: 836, loss is 0.02170182578265667\n",
      "epoch: 19 step: 837, loss is 0.00204427819699049\n",
      "epoch: 19 step: 838, loss is 0.010177288204431534\n",
      "epoch: 19 step: 839, loss is 0.007975548505783081\n",
      "epoch: 19 step: 840, loss is 0.027833668515086174\n",
      "epoch: 19 step: 841, loss is 0.012503073550760746\n",
      "epoch: 19 step: 842, loss is 0.0038568598683923483\n",
      "epoch: 19 step: 843, loss is 0.0009336968068964779\n",
      "epoch: 19 step: 844, loss is 0.060022983700037\n",
      "epoch: 19 step: 845, loss is 0.0046146768145263195\n",
      "epoch: 19 step: 846, loss is 0.011117137968540192\n",
      "epoch: 19 step: 847, loss is 0.03743705153465271\n",
      "epoch: 19 step: 848, loss is 0.0010487667750567198\n",
      "epoch: 19 step: 849, loss is 0.010996349155902863\n",
      "epoch: 19 step: 850, loss is 0.08501650393009186\n",
      "epoch: 19 step: 851, loss is 0.0010741931619122624\n",
      "epoch: 19 step: 852, loss is 0.015245821326971054\n",
      "epoch: 19 step: 853, loss is 0.005102686118334532\n",
      "epoch: 19 step: 854, loss is 0.0011026501888409257\n",
      "epoch: 19 step: 855, loss is 0.003301764838397503\n",
      "epoch: 19 step: 856, loss is 0.0002960061246994883\n",
      "epoch: 19 step: 857, loss is 0.001291925902478397\n",
      "epoch: 19 step: 858, loss is 0.000379704317310825\n",
      "epoch: 19 step: 859, loss is 0.012699765153229237\n",
      "epoch: 19 step: 860, loss is 0.09754861891269684\n",
      "epoch: 19 step: 861, loss is 0.000692107598297298\n",
      "epoch: 19 step: 862, loss is 0.028265293687582016\n",
      "epoch: 19 step: 863, loss is 0.000881750718690455\n",
      "epoch: 19 step: 864, loss is 0.007055199705064297\n",
      "epoch: 19 step: 865, loss is 0.012836798094213009\n",
      "epoch: 19 step: 866, loss is 0.001468379981815815\n",
      "epoch: 19 step: 867, loss is 0.001870033680461347\n",
      "epoch: 19 step: 868, loss is 0.03788154199719429\n",
      "epoch: 19 step: 869, loss is 0.0019934966694563627\n",
      "epoch: 19 step: 870, loss is 0.021610120311379433\n",
      "epoch: 19 step: 871, loss is 0.004579273518174887\n",
      "epoch: 19 step: 872, loss is 0.0307907871901989\n",
      "epoch: 19 step: 873, loss is 0.0005688586970791221\n",
      "epoch: 19 step: 874, loss is 0.034156862646341324\n",
      "epoch: 19 step: 875, loss is 0.0008760640630498528\n",
      "epoch: 19 step: 876, loss is 0.0031801750883460045\n",
      "epoch: 19 step: 877, loss is 0.0042989193461835384\n",
      "epoch: 19 step: 878, loss is 0.011282882653176785\n",
      "epoch: 19 step: 879, loss is 0.01623581163585186\n",
      "epoch: 19 step: 880, loss is 0.041652340441942215\n",
      "epoch: 19 step: 881, loss is 0.014303206466138363\n",
      "epoch: 19 step: 882, loss is 0.027215026319026947\n",
      "epoch: 19 step: 883, loss is 0.010309825651347637\n",
      "epoch: 19 step: 884, loss is 0.002708174055442214\n",
      "epoch: 19 step: 885, loss is 0.004158154595643282\n",
      "epoch: 19 step: 886, loss is 0.0007152335601858795\n",
      "epoch: 19 step: 887, loss is 0.013605068437755108\n",
      "epoch: 19 step: 888, loss is 0.010989749804139137\n",
      "epoch: 19 step: 889, loss is 0.008134787902235985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 890, loss is 0.013903540559113026\n",
      "epoch: 19 step: 891, loss is 0.011154931038618088\n",
      "epoch: 19 step: 892, loss is 0.0025062712375074625\n",
      "epoch: 19 step: 893, loss is 0.06783075630664825\n",
      "epoch: 19 step: 894, loss is 0.03440624848008156\n",
      "epoch: 19 step: 895, loss is 0.0013642908306792378\n",
      "epoch: 19 step: 896, loss is 0.02195723168551922\n",
      "epoch: 19 step: 897, loss is 0.033902280032634735\n",
      "epoch: 19 step: 898, loss is 0.015570485964417458\n",
      "epoch: 19 step: 899, loss is 0.021860620006918907\n",
      "epoch: 19 step: 900, loss is 0.00424942234531045\n",
      "epoch: 19 step: 901, loss is 0.006333079654723406\n",
      "epoch: 19 step: 902, loss is 0.00019827352662105113\n",
      "epoch: 19 step: 903, loss is 0.0034053383860737085\n",
      "epoch: 19 step: 904, loss is 0.004983216058462858\n",
      "epoch: 19 step: 905, loss is 0.03098939172923565\n",
      "epoch: 19 step: 906, loss is 0.04745875298976898\n",
      "epoch: 19 step: 907, loss is 0.0017134412191808224\n",
      "epoch: 19 step: 908, loss is 0.0008075495716184378\n",
      "epoch: 19 step: 909, loss is 0.12434953451156616\n",
      "epoch: 19 step: 910, loss is 0.004178243689239025\n",
      "epoch: 19 step: 911, loss is 0.0033427784219384193\n",
      "epoch: 19 step: 912, loss is 0.06517016142606735\n",
      "epoch: 19 step: 913, loss is 0.020309068262577057\n",
      "epoch: 19 step: 914, loss is 0.046906229108572006\n",
      "epoch: 19 step: 915, loss is 0.003143040928989649\n",
      "epoch: 19 step: 916, loss is 0.0044534034095704556\n",
      "epoch: 19 step: 917, loss is 0.01285875216126442\n",
      "epoch: 19 step: 918, loss is 0.004467645660042763\n",
      "epoch: 19 step: 919, loss is 0.01014271005988121\n",
      "epoch: 19 step: 920, loss is 0.034102294594049454\n",
      "epoch: 19 step: 921, loss is 0.0021501153241842985\n",
      "epoch: 19 step: 922, loss is 0.09401138126850128\n",
      "epoch: 19 step: 923, loss is 0.017114754766225815\n",
      "epoch: 19 step: 924, loss is 0.002204063581302762\n",
      "epoch: 19 step: 925, loss is 0.0007544292020611465\n",
      "epoch: 19 step: 926, loss is 0.05190611630678177\n",
      "epoch: 19 step: 927, loss is 0.002170038176700473\n",
      "epoch: 19 step: 928, loss is 0.030166413635015488\n",
      "epoch: 19 step: 929, loss is 0.0031015730928629637\n",
      "epoch: 19 step: 930, loss is 0.00035191691131331027\n",
      "epoch: 19 step: 931, loss is 0.027503369376063347\n",
      "epoch: 19 step: 932, loss is 0.03200346231460571\n",
      "epoch: 19 step: 933, loss is 0.0009062503231689334\n",
      "epoch: 19 step: 934, loss is 0.0023034107871353626\n",
      "epoch: 19 step: 935, loss is 0.0009676808258518577\n",
      "epoch: 19 step: 936, loss is 0.0020023039542138577\n",
      "epoch: 19 step: 937, loss is 0.013182432390749454\n",
      "epoch: 20 step: 1, loss is 0.0006597234751097858\n",
      "epoch: 20 step: 2, loss is 0.0010269205085933208\n",
      "epoch: 20 step: 3, loss is 0.013552257791161537\n",
      "epoch: 20 step: 4, loss is 0.13293983042240143\n",
      "epoch: 20 step: 5, loss is 0.00034187076380476356\n",
      "epoch: 20 step: 6, loss is 0.0005262206541374326\n",
      "epoch: 20 step: 7, loss is 0.02146974951028824\n",
      "epoch: 20 step: 8, loss is 0.019599929451942444\n",
      "epoch: 20 step: 9, loss is 0.003638336667791009\n",
      "epoch: 20 step: 10, loss is 0.0004015824815724045\n",
      "epoch: 20 step: 11, loss is 0.0940457135438919\n",
      "epoch: 20 step: 12, loss is 0.02793555147945881\n",
      "epoch: 20 step: 13, loss is 0.015315712429583073\n",
      "epoch: 20 step: 14, loss is 0.0019796111155301332\n",
      "epoch: 20 step: 15, loss is 0.011938104405999184\n",
      "epoch: 20 step: 16, loss is 0.005630896892398596\n",
      "epoch: 20 step: 17, loss is 0.0010468435939401388\n",
      "epoch: 20 step: 18, loss is 0.0004307819763198495\n",
      "epoch: 20 step: 19, loss is 0.02989817038178444\n",
      "epoch: 20 step: 20, loss is 0.020113619044423103\n",
      "epoch: 20 step: 21, loss is 0.005451295990496874\n",
      "epoch: 20 step: 22, loss is 0.001257482566870749\n",
      "epoch: 20 step: 23, loss is 0.003784970613196492\n",
      "epoch: 20 step: 24, loss is 0.01194820087403059\n",
      "epoch: 20 step: 25, loss is 0.06727100163698196\n",
      "epoch: 20 step: 26, loss is 0.07772345840930939\n",
      "epoch: 20 step: 27, loss is 0.0008839973015710711\n",
      "epoch: 20 step: 28, loss is 0.0070519899018108845\n",
      "epoch: 20 step: 29, loss is 0.02754514291882515\n",
      "epoch: 20 step: 30, loss is 0.003874759655445814\n",
      "epoch: 20 step: 31, loss is 0.018412822857499123\n",
      "epoch: 20 step: 32, loss is 0.03243104740977287\n",
      "epoch: 20 step: 33, loss is 0.005784164182841778\n",
      "epoch: 20 step: 34, loss is 0.005080454982817173\n",
      "epoch: 20 step: 35, loss is 0.005171879194676876\n",
      "epoch: 20 step: 36, loss is 0.0002839007065631449\n",
      "epoch: 20 step: 37, loss is 0.0026307597290724516\n",
      "epoch: 20 step: 38, loss is 0.005274852737784386\n",
      "epoch: 20 step: 39, loss is 0.010200109332799911\n",
      "epoch: 20 step: 40, loss is 0.04478547349572182\n",
      "epoch: 20 step: 41, loss is 0.016881408169865608\n",
      "epoch: 20 step: 42, loss is 0.005898064002394676\n",
      "epoch: 20 step: 43, loss is 0.03106321208178997\n",
      "epoch: 20 step: 44, loss is 0.031249532476067543\n",
      "epoch: 20 step: 45, loss is 0.011770765297114849\n",
      "epoch: 20 step: 46, loss is 0.0029112990014255047\n",
      "epoch: 20 step: 47, loss is 0.0015211777063086629\n",
      "epoch: 20 step: 48, loss is 0.003104906529188156\n",
      "epoch: 20 step: 49, loss is 0.010187819600105286\n",
      "epoch: 20 step: 50, loss is 0.011394243687391281\n",
      "epoch: 20 step: 51, loss is 0.00554374884814024\n",
      "epoch: 20 step: 52, loss is 0.0005221511237323284\n",
      "epoch: 20 step: 53, loss is 0.0071772304363548756\n",
      "epoch: 20 step: 54, loss is 0.0009487011702731252\n",
      "epoch: 20 step: 55, loss is 0.015053950250148773\n",
      "epoch: 20 step: 56, loss is 0.0007416039588861167\n",
      "epoch: 20 step: 57, loss is 0.0045868405140936375\n",
      "epoch: 20 step: 58, loss is 0.007701957132667303\n",
      "epoch: 20 step: 59, loss is 0.07049835473299026\n",
      "epoch: 20 step: 60, loss is 0.0009561181650497019\n",
      "epoch: 20 step: 61, loss is 0.09590291976928711\n",
      "epoch: 20 step: 62, loss is 0.0003078949230257422\n",
      "epoch: 20 step: 63, loss is 0.0022198837250471115\n",
      "epoch: 20 step: 64, loss is 0.0040701222606003284\n",
      "epoch: 20 step: 65, loss is 0.02610427886247635\n",
      "epoch: 20 step: 66, loss is 0.00299402279779315\n",
      "epoch: 20 step: 67, loss is 0.0019413052359595895\n",
      "epoch: 20 step: 68, loss is 0.003921540919691324\n",
      "epoch: 20 step: 69, loss is 0.01648687571287155\n",
      "epoch: 20 step: 70, loss is 0.0041150194592773914\n",
      "epoch: 20 step: 71, loss is 0.0033425497822463512\n",
      "epoch: 20 step: 72, loss is 0.00347872544080019\n",
      "epoch: 20 step: 73, loss is 0.0006762798293493688\n",
      "epoch: 20 step: 74, loss is 0.004708486143499613\n",
      "epoch: 20 step: 75, loss is 0.0006983299390412867\n",
      "epoch: 20 step: 76, loss is 0.07329592108726501\n",
      "epoch: 20 step: 77, loss is 0.0023462155368179083\n",
      "epoch: 20 step: 78, loss is 0.00020658111316151917\n",
      "epoch: 20 step: 79, loss is 0.0029104305431246758\n",
      "epoch: 20 step: 80, loss is 0.03255244344472885\n",
      "epoch: 20 step: 81, loss is 0.0022015832364559174\n",
      "epoch: 20 step: 82, loss is 0.00034318125108256936\n",
      "epoch: 20 step: 83, loss is 0.0010024086805060506\n",
      "epoch: 20 step: 84, loss is 0.0002792652521748096\n",
      "epoch: 20 step: 85, loss is 0.024037087336182594\n",
      "epoch: 20 step: 86, loss is 0.0008062431588768959\n",
      "epoch: 20 step: 87, loss is 0.012213717214763165\n",
      "epoch: 20 step: 88, loss is 0.004033486358821392\n",
      "epoch: 20 step: 89, loss is 0.001733690733090043\n",
      "epoch: 20 step: 90, loss is 0.007736505474895239\n",
      "epoch: 20 step: 91, loss is 0.0013185932766646147\n",
      "epoch: 20 step: 92, loss is 0.0032066809944808483\n",
      "epoch: 20 step: 93, loss is 0.003988989163190126\n",
      "epoch: 20 step: 94, loss is 0.0005092615028843284\n",
      "epoch: 20 step: 95, loss is 0.04834963008761406\n",
      "epoch: 20 step: 96, loss is 0.004487896803766489\n",
      "epoch: 20 step: 97, loss is 0.0008112434297800064\n",
      "epoch: 20 step: 98, loss is 0.0004008811083622277\n",
      "epoch: 20 step: 99, loss is 0.0005348508711904287\n",
      "epoch: 20 step: 100, loss is 0.0015420843847095966\n",
      "epoch: 20 step: 101, loss is 0.00250116060487926\n",
      "epoch: 20 step: 102, loss is 0.02246512472629547\n",
      "epoch: 20 step: 103, loss is 0.0018833690555766225\n",
      "epoch: 20 step: 104, loss is 0.001347867539152503\n",
      "epoch: 20 step: 105, loss is 0.022997383028268814\n",
      "epoch: 20 step: 106, loss is 0.009790162555873394\n",
      "epoch: 20 step: 107, loss is 0.010420992970466614\n",
      "epoch: 20 step: 108, loss is 0.0007571385940536857\n",
      "epoch: 20 step: 109, loss is 0.005209232214838266\n",
      "epoch: 20 step: 110, loss is 0.026062695309519768\n",
      "epoch: 20 step: 111, loss is 0.0019755600951611996\n",
      "epoch: 20 step: 112, loss is 0.0059645455330610275\n",
      "epoch: 20 step: 113, loss is 0.0020982897840440273\n",
      "epoch: 20 step: 114, loss is 0.023701917380094528\n",
      "epoch: 20 step: 115, loss is 0.08920357376337051\n",
      "epoch: 20 step: 116, loss is 0.014464718289673328\n",
      "epoch: 20 step: 117, loss is 0.0012562810443341732\n",
      "epoch: 20 step: 118, loss is 0.00010063124500447884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 119, loss is 0.004523018840700388\n",
      "epoch: 20 step: 120, loss is 0.013428829610347748\n",
      "epoch: 20 step: 121, loss is 0.03506901115179062\n",
      "epoch: 20 step: 122, loss is 0.03626074641942978\n",
      "epoch: 20 step: 123, loss is 0.002498251385986805\n",
      "epoch: 20 step: 124, loss is 0.015261807478964329\n",
      "epoch: 20 step: 125, loss is 0.04052693396806717\n",
      "epoch: 20 step: 126, loss is 0.01405202317982912\n",
      "epoch: 20 step: 127, loss is 0.04840659722685814\n",
      "epoch: 20 step: 128, loss is 0.0025712361093610525\n",
      "epoch: 20 step: 129, loss is 0.0009542643674649298\n",
      "epoch: 20 step: 130, loss is 0.0015768363373354077\n",
      "epoch: 20 step: 131, loss is 0.09164580702781677\n",
      "epoch: 20 step: 132, loss is 0.07400791347026825\n",
      "epoch: 20 step: 133, loss is 0.0005020026001147926\n",
      "epoch: 20 step: 134, loss is 0.01040656492114067\n",
      "epoch: 20 step: 135, loss is 0.006305583752691746\n",
      "epoch: 20 step: 136, loss is 0.031809739768505096\n",
      "epoch: 20 step: 137, loss is 0.0006520924507640302\n",
      "epoch: 20 step: 138, loss is 0.0444052517414093\n",
      "epoch: 20 step: 139, loss is 0.015593189746141434\n",
      "epoch: 20 step: 140, loss is 0.1324763149023056\n",
      "epoch: 20 step: 141, loss is 0.11123716086149216\n",
      "epoch: 20 step: 142, loss is 0.024605032056570053\n",
      "epoch: 20 step: 143, loss is 0.0025392293464392424\n",
      "epoch: 20 step: 144, loss is 0.004920327570289373\n",
      "epoch: 20 step: 145, loss is 0.005719300359487534\n",
      "epoch: 20 step: 146, loss is 0.005127444863319397\n",
      "epoch: 20 step: 147, loss is 0.0004975823685526848\n",
      "epoch: 20 step: 148, loss is 0.053699616342782974\n",
      "epoch: 20 step: 149, loss is 0.002853268524631858\n",
      "epoch: 20 step: 150, loss is 0.010268744081258774\n",
      "epoch: 20 step: 151, loss is 0.03262120857834816\n",
      "epoch: 20 step: 152, loss is 0.036881737411022186\n",
      "epoch: 20 step: 153, loss is 0.0020228279754519463\n",
      "epoch: 20 step: 154, loss is 0.0004219933180138469\n",
      "epoch: 20 step: 155, loss is 0.003004577476531267\n",
      "epoch: 20 step: 156, loss is 0.020670227706432343\n",
      "epoch: 20 step: 157, loss is 0.015286905691027641\n",
      "epoch: 20 step: 158, loss is 0.001150915282778442\n",
      "epoch: 20 step: 159, loss is 0.022197848185896873\n",
      "epoch: 20 step: 160, loss is 0.10169611126184464\n",
      "epoch: 20 step: 161, loss is 0.018636519089341164\n",
      "epoch: 20 step: 162, loss is 0.011705121956765652\n",
      "epoch: 20 step: 163, loss is 0.007433795835822821\n",
      "epoch: 20 step: 164, loss is 0.004937526769936085\n",
      "epoch: 20 step: 165, loss is 0.050372663885354996\n",
      "epoch: 20 step: 166, loss is 0.022504178807139397\n",
      "epoch: 20 step: 167, loss is 0.00044829148100689054\n",
      "epoch: 20 step: 168, loss is 0.017301468178629875\n",
      "epoch: 20 step: 169, loss is 8.705045911483467e-05\n",
      "epoch: 20 step: 170, loss is 0.013911464251577854\n",
      "epoch: 20 step: 171, loss is 0.004763606004416943\n",
      "epoch: 20 step: 172, loss is 0.04617471620440483\n",
      "epoch: 20 step: 173, loss is 0.006030108779668808\n",
      "epoch: 20 step: 174, loss is 0.0023822200018912554\n",
      "epoch: 20 step: 175, loss is 0.025267140939831734\n",
      "epoch: 20 step: 176, loss is 0.020015420392155647\n",
      "epoch: 20 step: 177, loss is 0.044985972344875336\n",
      "epoch: 20 step: 178, loss is 0.0005252741393633187\n",
      "epoch: 20 step: 179, loss is 0.028881192207336426\n",
      "epoch: 20 step: 180, loss is 0.004809227306395769\n",
      "epoch: 20 step: 181, loss is 0.0009389865444973111\n",
      "epoch: 20 step: 182, loss is 0.0016942220972850919\n",
      "epoch: 20 step: 183, loss is 0.05728495493531227\n",
      "epoch: 20 step: 184, loss is 0.006006542593240738\n",
      "epoch: 20 step: 185, loss is 0.008614265359938145\n",
      "epoch: 20 step: 186, loss is 0.021148130297660828\n",
      "epoch: 20 step: 187, loss is 0.0031386835034936666\n",
      "epoch: 20 step: 188, loss is 0.00820552185177803\n",
      "epoch: 20 step: 189, loss is 0.0016966243274509907\n",
      "epoch: 20 step: 190, loss is 0.005850852467119694\n",
      "epoch: 20 step: 191, loss is 0.014483015984296799\n",
      "epoch: 20 step: 192, loss is 0.003571694018319249\n",
      "epoch: 20 step: 193, loss is 0.01200070045888424\n",
      "epoch: 20 step: 194, loss is 0.09891301393508911\n",
      "epoch: 20 step: 195, loss is 0.002190148690715432\n",
      "epoch: 20 step: 196, loss is 0.03345885127782822\n",
      "epoch: 20 step: 197, loss is 0.005644350312650204\n",
      "epoch: 20 step: 198, loss is 0.008457389660179615\n",
      "epoch: 20 step: 199, loss is 0.004224870819598436\n",
      "epoch: 20 step: 200, loss is 0.003306599333882332\n",
      "epoch: 20 step: 201, loss is 0.0010235743829980493\n",
      "epoch: 20 step: 202, loss is 0.0019528362900018692\n",
      "epoch: 20 step: 203, loss is 0.0020046986173838377\n",
      "epoch: 20 step: 204, loss is 0.0018664214294403791\n",
      "epoch: 20 step: 205, loss is 0.0022552204318344593\n",
      "epoch: 20 step: 206, loss is 0.036583732813596725\n",
      "epoch: 20 step: 207, loss is 0.014217671938240528\n",
      "epoch: 20 step: 208, loss is 0.03317840397357941\n",
      "epoch: 20 step: 209, loss is 0.0005061302217654884\n",
      "epoch: 20 step: 210, loss is 0.0009090777020901442\n",
      "epoch: 20 step: 211, loss is 0.0011484446004033089\n",
      "epoch: 20 step: 212, loss is 0.005714979954063892\n",
      "epoch: 20 step: 213, loss is 0.0137355150654912\n",
      "epoch: 20 step: 214, loss is 0.0006790077313780785\n",
      "epoch: 20 step: 215, loss is 0.06353358924388885\n",
      "epoch: 20 step: 216, loss is 0.011403970420360565\n",
      "epoch: 20 step: 217, loss is 0.07685492932796478\n",
      "epoch: 20 step: 218, loss is 0.013499799184501171\n",
      "epoch: 20 step: 219, loss is 0.0013489100383594632\n",
      "epoch: 20 step: 220, loss is 0.00014425392146222293\n",
      "epoch: 20 step: 221, loss is 0.06882858276367188\n",
      "epoch: 20 step: 222, loss is 0.0005110247293487191\n",
      "epoch: 20 step: 223, loss is 0.0356324128806591\n",
      "epoch: 20 step: 224, loss is 0.011342473328113556\n",
      "epoch: 20 step: 225, loss is 0.051575060933828354\n",
      "epoch: 20 step: 226, loss is 0.0181210208684206\n",
      "epoch: 20 step: 227, loss is 0.0011160275898873806\n",
      "epoch: 20 step: 228, loss is 0.0017255779821425676\n",
      "epoch: 20 step: 229, loss is 0.0016939218621701002\n",
      "epoch: 20 step: 230, loss is 0.013532537966966629\n",
      "epoch: 20 step: 231, loss is 0.0061792293563485146\n",
      "epoch: 20 step: 232, loss is 0.0019401134923100471\n",
      "epoch: 20 step: 233, loss is 0.011379161849617958\n",
      "epoch: 20 step: 234, loss is 0.009207742288708687\n",
      "epoch: 20 step: 235, loss is 0.0001453567820135504\n",
      "epoch: 20 step: 236, loss is 0.038244619965553284\n",
      "epoch: 20 step: 237, loss is 0.03366430476307869\n",
      "epoch: 20 step: 238, loss is 0.005438544321805239\n",
      "epoch: 20 step: 239, loss is 0.0007838389137759805\n",
      "epoch: 20 step: 240, loss is 0.01726636476814747\n",
      "epoch: 20 step: 241, loss is 0.0035778083838522434\n",
      "epoch: 20 step: 242, loss is 0.03514077886939049\n",
      "epoch: 20 step: 243, loss is 0.0007258109981194139\n",
      "epoch: 20 step: 244, loss is 0.0002985692990478128\n",
      "epoch: 20 step: 245, loss is 0.0023357111494988203\n",
      "epoch: 20 step: 246, loss is 0.012271884828805923\n",
      "epoch: 20 step: 247, loss is 0.056876931339502335\n",
      "epoch: 20 step: 248, loss is 0.019402552396059036\n",
      "epoch: 20 step: 249, loss is 0.012275103479623795\n",
      "epoch: 20 step: 250, loss is 0.025722505524754524\n",
      "epoch: 20 step: 251, loss is 0.005152506288141012\n",
      "epoch: 20 step: 252, loss is 0.030113674700260162\n",
      "epoch: 20 step: 253, loss is 0.00507574575021863\n",
      "epoch: 20 step: 254, loss is 0.00107291666790843\n",
      "epoch: 20 step: 255, loss is 0.010853661224246025\n",
      "epoch: 20 step: 256, loss is 0.0023788963444530964\n",
      "epoch: 20 step: 257, loss is 0.0035342753399163485\n",
      "epoch: 20 step: 258, loss is 0.0009262654348276556\n",
      "epoch: 20 step: 259, loss is 0.0038969204761087894\n",
      "epoch: 20 step: 260, loss is 0.01049042958766222\n",
      "epoch: 20 step: 261, loss is 0.0013001315528526902\n",
      "epoch: 20 step: 262, loss is 0.018547747284173965\n",
      "epoch: 20 step: 263, loss is 0.0014937673695385456\n",
      "epoch: 20 step: 264, loss is 0.04059296473860741\n",
      "epoch: 20 step: 265, loss is 0.004673862364143133\n",
      "epoch: 20 step: 266, loss is 0.021477801725268364\n",
      "epoch: 20 step: 267, loss is 0.0005613434477709234\n",
      "epoch: 20 step: 268, loss is 0.011089693754911423\n",
      "epoch: 20 step: 269, loss is 0.07608631253242493\n",
      "epoch: 20 step: 270, loss is 0.0019406999927014112\n",
      "epoch: 20 step: 271, loss is 0.03365148976445198\n",
      "epoch: 20 step: 272, loss is 0.00415375642478466\n",
      "epoch: 20 step: 273, loss is 0.10535919666290283\n",
      "epoch: 20 step: 274, loss is 0.032560497522354126\n",
      "epoch: 20 step: 275, loss is 0.0009383079595863819\n",
      "epoch: 20 step: 276, loss is 0.00111640733666718\n",
      "epoch: 20 step: 277, loss is 0.001092693186365068\n",
      "epoch: 20 step: 278, loss is 0.03663947805762291\n",
      "epoch: 20 step: 279, loss is 0.0004160588432569057\n",
      "epoch: 20 step: 280, loss is 0.002487780060619116\n",
      "epoch: 20 step: 281, loss is 0.0017877926584333181\n",
      "epoch: 20 step: 282, loss is 0.0006397383403964341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 283, loss is 0.003401843598112464\n",
      "epoch: 20 step: 284, loss is 0.00043978297617286444\n",
      "epoch: 20 step: 285, loss is 0.014417491853237152\n",
      "epoch: 20 step: 286, loss is 0.020503025501966476\n",
      "epoch: 20 step: 287, loss is 0.004059487022459507\n",
      "epoch: 20 step: 288, loss is 0.007839076220989227\n",
      "epoch: 20 step: 289, loss is 0.03204086050391197\n",
      "epoch: 20 step: 290, loss is 0.0038395144511014223\n",
      "epoch: 20 step: 291, loss is 0.0030898465774953365\n",
      "epoch: 20 step: 292, loss is 0.001458034268580377\n",
      "epoch: 20 step: 293, loss is 0.01997245103120804\n",
      "epoch: 20 step: 294, loss is 0.021234072744846344\n",
      "epoch: 20 step: 295, loss is 0.007136613130569458\n",
      "epoch: 20 step: 296, loss is 0.021491877734661102\n",
      "epoch: 20 step: 297, loss is 0.05352627485990524\n",
      "epoch: 20 step: 298, loss is 0.0048435223288834095\n",
      "epoch: 20 step: 299, loss is 0.001051968545652926\n",
      "epoch: 20 step: 300, loss is 0.011651257984340191\n",
      "epoch: 20 step: 301, loss is 0.0020533534698188305\n",
      "epoch: 20 step: 302, loss is 0.011456779204308987\n",
      "epoch: 20 step: 303, loss is 0.0008843399118632078\n",
      "epoch: 20 step: 304, loss is 0.022251572459936142\n",
      "epoch: 20 step: 305, loss is 5.064908691565506e-05\n",
      "epoch: 20 step: 306, loss is 0.11597872525453568\n",
      "epoch: 20 step: 307, loss is 0.00800627563148737\n",
      "epoch: 20 step: 308, loss is 0.002236716914921999\n",
      "epoch: 20 step: 309, loss is 0.002488726982846856\n",
      "epoch: 20 step: 310, loss is 0.07035345584154129\n",
      "epoch: 20 step: 311, loss is 0.021121462807059288\n",
      "epoch: 20 step: 312, loss is 0.004736647941172123\n",
      "epoch: 20 step: 313, loss is 0.000561474182177335\n",
      "epoch: 20 step: 314, loss is 0.007551570422947407\n",
      "epoch: 20 step: 315, loss is 0.003802838269621134\n",
      "epoch: 20 step: 316, loss is 0.0032919938676059246\n",
      "epoch: 20 step: 317, loss is 0.0008309020195156336\n",
      "epoch: 20 step: 318, loss is 0.03609418123960495\n",
      "epoch: 20 step: 319, loss is 0.08436352759599686\n",
      "epoch: 20 step: 320, loss is 0.004853194113820791\n",
      "epoch: 20 step: 321, loss is 0.03698170557618141\n",
      "epoch: 20 step: 322, loss is 0.004784661345183849\n",
      "epoch: 20 step: 323, loss is 0.0044582136906683445\n",
      "epoch: 20 step: 324, loss is 0.00038672779919579625\n",
      "epoch: 20 step: 325, loss is 0.04408537223935127\n",
      "epoch: 20 step: 326, loss is 0.003327075857669115\n",
      "epoch: 20 step: 327, loss is 0.0001396111911162734\n",
      "epoch: 20 step: 328, loss is 0.005418586544692516\n",
      "epoch: 20 step: 329, loss is 0.0016474026488140225\n",
      "epoch: 20 step: 330, loss is 0.02143767848610878\n",
      "epoch: 20 step: 331, loss is 0.010729921981692314\n",
      "epoch: 20 step: 332, loss is 0.007084626704454422\n",
      "epoch: 20 step: 333, loss is 0.0009921143064275384\n",
      "epoch: 20 step: 334, loss is 0.01098824292421341\n",
      "epoch: 20 step: 335, loss is 0.0022029047831892967\n",
      "epoch: 20 step: 336, loss is 0.02497665397822857\n",
      "epoch: 20 step: 337, loss is 0.002514505060389638\n",
      "epoch: 20 step: 338, loss is 0.00012202817015349865\n",
      "epoch: 20 step: 339, loss is 0.02250891737639904\n",
      "epoch: 20 step: 340, loss is 0.004235335160046816\n",
      "epoch: 20 step: 341, loss is 0.0015722847310826182\n",
      "epoch: 20 step: 342, loss is 0.007991147227585316\n",
      "epoch: 20 step: 343, loss is 0.012607605196535587\n",
      "epoch: 20 step: 344, loss is 0.0246793944388628\n",
      "epoch: 20 step: 345, loss is 0.03299273923039436\n",
      "epoch: 20 step: 346, loss is 0.001052578561939299\n",
      "epoch: 20 step: 347, loss is 0.009185487404465675\n",
      "epoch: 20 step: 348, loss is 0.004767127335071564\n",
      "epoch: 20 step: 349, loss is 0.0021477460395544767\n",
      "epoch: 20 step: 350, loss is 0.0032930229790508747\n",
      "epoch: 20 step: 351, loss is 0.003880622098222375\n",
      "epoch: 20 step: 352, loss is 0.03463826701045036\n",
      "epoch: 20 step: 353, loss is 0.002187947742640972\n",
      "epoch: 20 step: 354, loss is 0.00047012994764372706\n",
      "epoch: 20 step: 355, loss is 0.01904262788593769\n",
      "epoch: 20 step: 356, loss is 0.0014325340744107962\n",
      "epoch: 20 step: 357, loss is 0.002014303347095847\n",
      "epoch: 20 step: 358, loss is 0.012059162370860577\n",
      "epoch: 20 step: 359, loss is 0.000996455899439752\n",
      "epoch: 20 step: 360, loss is 0.0023230495862662792\n",
      "epoch: 20 step: 361, loss is 0.0014722138876095414\n",
      "epoch: 20 step: 362, loss is 0.004727807827293873\n",
      "epoch: 20 step: 363, loss is 0.0008667646907269955\n",
      "epoch: 20 step: 364, loss is 0.00010680277773644775\n",
      "epoch: 20 step: 365, loss is 0.009995297528803349\n",
      "epoch: 20 step: 366, loss is 0.003446706337854266\n",
      "epoch: 20 step: 367, loss is 0.0298660509288311\n",
      "epoch: 20 step: 368, loss is 0.019129129126667976\n",
      "epoch: 20 step: 369, loss is 0.0032714514527469873\n",
      "epoch: 20 step: 370, loss is 0.017642896622419357\n",
      "epoch: 20 step: 371, loss is 0.014962994493544102\n",
      "epoch: 20 step: 372, loss is 0.03222173452377319\n",
      "epoch: 20 step: 373, loss is 0.015016725286841393\n",
      "epoch: 20 step: 374, loss is 0.000470782135380432\n",
      "epoch: 20 step: 375, loss is 0.0008526184246875346\n",
      "epoch: 20 step: 376, loss is 0.0016512689180672169\n",
      "epoch: 20 step: 377, loss is 0.0001459440973121673\n",
      "epoch: 20 step: 378, loss is 0.006810677237808704\n",
      "epoch: 20 step: 379, loss is 0.00013302605657372624\n",
      "epoch: 20 step: 380, loss is 0.0007582912221550941\n",
      "epoch: 20 step: 381, loss is 0.03048994392156601\n",
      "epoch: 20 step: 382, loss is 0.028371723368763924\n",
      "epoch: 20 step: 383, loss is 0.0010823763441294432\n",
      "epoch: 20 step: 384, loss is 0.03984878957271576\n",
      "epoch: 20 step: 385, loss is 0.035008564591407776\n",
      "epoch: 20 step: 386, loss is 0.0006435455288738012\n",
      "epoch: 20 step: 387, loss is 0.0002701582561712712\n",
      "epoch: 20 step: 388, loss is 0.012686343863606453\n",
      "epoch: 20 step: 389, loss is 0.0030927935149520636\n",
      "epoch: 20 step: 390, loss is 0.0020082960836589336\n",
      "epoch: 20 step: 391, loss is 0.008182264864444733\n",
      "epoch: 20 step: 392, loss is 0.0010136401979252696\n",
      "epoch: 20 step: 393, loss is 0.051822952926158905\n",
      "epoch: 20 step: 394, loss is 0.0018307568971067667\n",
      "epoch: 20 step: 395, loss is 0.05729072540998459\n",
      "epoch: 20 step: 396, loss is 0.02606392279267311\n",
      "epoch: 20 step: 397, loss is 0.038448408246040344\n",
      "epoch: 20 step: 398, loss is 0.0004397510492708534\n",
      "epoch: 20 step: 399, loss is 0.03532308340072632\n",
      "epoch: 20 step: 400, loss is 0.0018887094920501113\n",
      "epoch: 20 step: 401, loss is 0.003479011356830597\n",
      "epoch: 20 step: 402, loss is 0.003722022520378232\n",
      "epoch: 20 step: 403, loss is 0.007126250304281712\n",
      "epoch: 20 step: 404, loss is 0.000729633029550314\n",
      "epoch: 20 step: 405, loss is 3.42137245752383e-05\n",
      "epoch: 20 step: 406, loss is 0.0005278770113363862\n",
      "epoch: 20 step: 407, loss is 0.0023387791588902473\n",
      "epoch: 20 step: 408, loss is 0.04896962642669678\n",
      "epoch: 20 step: 409, loss is 0.002223273040726781\n",
      "epoch: 20 step: 410, loss is 0.0019263377180323005\n",
      "epoch: 20 step: 411, loss is 0.02595856972038746\n",
      "epoch: 20 step: 412, loss is 0.003252910915762186\n",
      "epoch: 20 step: 413, loss is 0.012579499743878841\n",
      "epoch: 20 step: 414, loss is 0.000969782005995512\n",
      "epoch: 20 step: 415, loss is 0.007220335770398378\n",
      "epoch: 20 step: 416, loss is 0.0662887692451477\n",
      "epoch: 20 step: 417, loss is 0.00035721337189897895\n",
      "epoch: 20 step: 418, loss is 0.007166072726249695\n",
      "epoch: 20 step: 419, loss is 0.000820797577034682\n",
      "epoch: 20 step: 420, loss is 0.0027952762320637703\n",
      "epoch: 20 step: 421, loss is 0.003791207680478692\n",
      "epoch: 20 step: 422, loss is 0.05704037845134735\n",
      "epoch: 20 step: 423, loss is 0.002180671552196145\n",
      "epoch: 20 step: 424, loss is 0.0005345271783880889\n",
      "epoch: 20 step: 425, loss is 0.05513715371489525\n",
      "epoch: 20 step: 426, loss is 0.005659503396600485\n",
      "epoch: 20 step: 427, loss is 0.00011557291873032227\n",
      "epoch: 20 step: 428, loss is 0.011643079109489918\n",
      "epoch: 20 step: 429, loss is 0.010209380649030209\n",
      "epoch: 20 step: 430, loss is 0.022558756172657013\n",
      "epoch: 20 step: 431, loss is 0.007280241698026657\n",
      "epoch: 20 step: 432, loss is 0.0029940938111394644\n",
      "epoch: 20 step: 433, loss is 0.0007530266302637756\n",
      "epoch: 20 step: 434, loss is 0.02403206191956997\n",
      "epoch: 20 step: 435, loss is 0.013467899523675442\n",
      "epoch: 20 step: 436, loss is 0.1851779967546463\n",
      "epoch: 20 step: 437, loss is 0.00026496959617361426\n",
      "epoch: 20 step: 438, loss is 0.006981929764151573\n",
      "epoch: 20 step: 439, loss is 0.001526046427898109\n",
      "epoch: 20 step: 440, loss is 0.005291441455483437\n",
      "epoch: 20 step: 441, loss is 0.0018075580010190606\n",
      "epoch: 20 step: 442, loss is 9.736392530612648e-05\n",
      "epoch: 20 step: 443, loss is 0.06317231059074402\n",
      "epoch: 20 step: 444, loss is 0.0007663398282602429\n",
      "epoch: 20 step: 445, loss is 0.001968106720596552\n",
      "epoch: 20 step: 446, loss is 0.011494922451674938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 447, loss is 0.022066788747906685\n",
      "epoch: 20 step: 448, loss is 0.00700032664462924\n",
      "epoch: 20 step: 449, loss is 0.0004188236780464649\n",
      "epoch: 20 step: 450, loss is 0.00041456855251453817\n",
      "epoch: 20 step: 451, loss is 0.03189920634031296\n",
      "epoch: 20 step: 452, loss is 0.0018050181679427624\n",
      "epoch: 20 step: 453, loss is 0.07252947241067886\n",
      "epoch: 20 step: 454, loss is 0.00360247609205544\n",
      "epoch: 20 step: 455, loss is 0.0002037973899859935\n",
      "epoch: 20 step: 456, loss is 0.0056958552449941635\n",
      "epoch: 20 step: 457, loss is 0.00018257460033055395\n",
      "epoch: 20 step: 458, loss is 0.000889651186298579\n",
      "epoch: 20 step: 459, loss is 0.013315241783857346\n",
      "epoch: 20 step: 460, loss is 0.0023260368034243584\n",
      "epoch: 20 step: 461, loss is 0.0026843983214348555\n",
      "epoch: 20 step: 462, loss is 0.015267538838088512\n",
      "epoch: 20 step: 463, loss is 0.004390442743897438\n",
      "epoch: 20 step: 464, loss is 0.0025423779152333736\n",
      "epoch: 20 step: 465, loss is 0.005712028127163649\n",
      "epoch: 20 step: 466, loss is 0.021131927147507668\n",
      "epoch: 20 step: 467, loss is 0.014591995626688004\n",
      "epoch: 20 step: 468, loss is 0.0041020819917321205\n",
      "epoch: 20 step: 469, loss is 0.014608684927225113\n",
      "epoch: 20 step: 470, loss is 0.12869837880134583\n",
      "epoch: 20 step: 471, loss is 0.012507707811892033\n",
      "epoch: 20 step: 472, loss is 0.009926618076860905\n",
      "epoch: 20 step: 473, loss is 0.015866048634052277\n",
      "epoch: 20 step: 474, loss is 0.003188742557540536\n",
      "epoch: 20 step: 475, loss is 0.0030773901380598545\n",
      "epoch: 20 step: 476, loss is 0.019260773435235023\n",
      "epoch: 20 step: 477, loss is 0.005282887723296881\n",
      "epoch: 20 step: 478, loss is 0.027139337733387947\n",
      "epoch: 20 step: 479, loss is 0.02590290829539299\n",
      "epoch: 20 step: 480, loss is 0.003558097407221794\n",
      "epoch: 20 step: 481, loss is 0.0020952436607331038\n",
      "epoch: 20 step: 482, loss is 0.009415128268301487\n",
      "epoch: 20 step: 483, loss is 0.0027142493054270744\n",
      "epoch: 20 step: 484, loss is 0.004221711773425341\n",
      "epoch: 20 step: 485, loss is 0.011068012565374374\n",
      "epoch: 20 step: 486, loss is 0.004929518327116966\n",
      "epoch: 20 step: 487, loss is 0.01766541786491871\n",
      "epoch: 20 step: 488, loss is 0.018575433641672134\n",
      "epoch: 20 step: 489, loss is 0.007549049332737923\n",
      "epoch: 20 step: 490, loss is 0.012542477808892727\n",
      "epoch: 20 step: 491, loss is 0.0069882068783044815\n",
      "epoch: 20 step: 492, loss is 0.006692417897284031\n",
      "epoch: 20 step: 493, loss is 0.0005788211128674448\n",
      "epoch: 20 step: 494, loss is 0.01257356908172369\n",
      "epoch: 20 step: 495, loss is 0.0014014033367857337\n",
      "epoch: 20 step: 496, loss is 0.000492952938657254\n",
      "epoch: 20 step: 497, loss is 0.011410207487642765\n",
      "epoch: 20 step: 498, loss is 0.055426549166440964\n",
      "epoch: 20 step: 499, loss is 0.03970067575573921\n",
      "epoch: 20 step: 500, loss is 0.008583749644458294\n",
      "epoch: 20 step: 501, loss is 0.03178679943084717\n",
      "epoch: 20 step: 502, loss is 0.0035120032262057066\n",
      "epoch: 20 step: 503, loss is 0.026632973924279213\n",
      "epoch: 20 step: 504, loss is 0.0007389771053567529\n",
      "epoch: 20 step: 505, loss is 0.0001380197354592383\n",
      "epoch: 20 step: 506, loss is 0.012368089519441128\n",
      "epoch: 20 step: 507, loss is 0.006437371019273996\n",
      "epoch: 20 step: 508, loss is 0.0018435664242133498\n",
      "epoch: 20 step: 509, loss is 0.0023180032148957253\n",
      "epoch: 20 step: 510, loss is 0.0021542322356253862\n",
      "epoch: 20 step: 511, loss is 0.018947824835777283\n",
      "epoch: 20 step: 512, loss is 0.0063169789500534534\n",
      "epoch: 20 step: 513, loss is 0.004033732693642378\n",
      "epoch: 20 step: 514, loss is 0.0007758119027130306\n",
      "epoch: 20 step: 515, loss is 0.0009053279645740986\n",
      "epoch: 20 step: 516, loss is 9.002395381685346e-05\n",
      "epoch: 20 step: 517, loss is 0.0005451868055388331\n",
      "epoch: 20 step: 518, loss is 0.009129989892244339\n",
      "epoch: 20 step: 519, loss is 0.034295614808797836\n",
      "epoch: 20 step: 520, loss is 0.001701517729088664\n",
      "epoch: 20 step: 521, loss is 0.0058805253356695175\n",
      "epoch: 20 step: 522, loss is 0.008605089038610458\n",
      "epoch: 20 step: 523, loss is 0.00434431666508317\n",
      "epoch: 20 step: 524, loss is 0.01826697774231434\n",
      "epoch: 20 step: 525, loss is 0.0013316969852894545\n",
      "epoch: 20 step: 526, loss is 0.012182433158159256\n",
      "epoch: 20 step: 527, loss is 0.0076271286234259605\n",
      "epoch: 20 step: 528, loss is 0.012093222700059414\n",
      "epoch: 20 step: 529, loss is 0.0007919573690742254\n",
      "epoch: 20 step: 530, loss is 0.027975890785455704\n",
      "epoch: 20 step: 531, loss is 0.000833845348097384\n",
      "epoch: 20 step: 532, loss is 0.04076243191957474\n",
      "epoch: 20 step: 533, loss is 0.047953031957149506\n",
      "epoch: 20 step: 534, loss is 0.01632312871515751\n",
      "epoch: 20 step: 535, loss is 0.0005890504689887166\n",
      "epoch: 20 step: 536, loss is 0.03630285710096359\n",
      "epoch: 20 step: 537, loss is 0.003575027920305729\n",
      "epoch: 20 step: 538, loss is 0.004911594558507204\n",
      "epoch: 20 step: 539, loss is 0.0026742673944681883\n",
      "epoch: 20 step: 540, loss is 0.05218511447310448\n",
      "epoch: 20 step: 541, loss is 0.03133359178900719\n",
      "epoch: 20 step: 542, loss is 0.009906853549182415\n",
      "epoch: 20 step: 543, loss is 0.08803246170282364\n",
      "epoch: 20 step: 544, loss is 0.0038854090962558985\n",
      "epoch: 20 step: 545, loss is 0.0009049403597600758\n",
      "epoch: 20 step: 546, loss is 0.01055821217596531\n",
      "epoch: 20 step: 547, loss is 0.09550254791975021\n",
      "epoch: 20 step: 548, loss is 0.006098058540374041\n",
      "epoch: 20 step: 549, loss is 0.002476530848070979\n",
      "epoch: 20 step: 550, loss is 0.029762765392661095\n",
      "epoch: 20 step: 551, loss is 0.002010229043662548\n",
      "epoch: 20 step: 552, loss is 0.0002734397421590984\n",
      "epoch: 20 step: 553, loss is 0.01255415752530098\n",
      "epoch: 20 step: 554, loss is 0.0020810263231396675\n",
      "epoch: 20 step: 555, loss is 0.07493197172880173\n",
      "epoch: 20 step: 556, loss is 0.004110140260308981\n",
      "epoch: 20 step: 557, loss is 0.0018059058347716928\n",
      "epoch: 20 step: 558, loss is 0.0041598966345191\n",
      "epoch: 20 step: 559, loss is 0.0005401113303378224\n",
      "epoch: 20 step: 560, loss is 0.010341690853238106\n",
      "epoch: 20 step: 561, loss is 0.013521203771233559\n",
      "epoch: 20 step: 562, loss is 0.08933509886264801\n",
      "epoch: 20 step: 563, loss is 0.0017961644334718585\n",
      "epoch: 20 step: 564, loss is 0.06117413938045502\n",
      "epoch: 20 step: 565, loss is 0.01660311035811901\n",
      "epoch: 20 step: 566, loss is 0.07432152330875397\n",
      "epoch: 20 step: 567, loss is 0.03835485130548477\n",
      "epoch: 20 step: 568, loss is 0.013134108856320381\n",
      "epoch: 20 step: 569, loss is 0.03531097248196602\n",
      "epoch: 20 step: 570, loss is 0.0004835721920244396\n",
      "epoch: 20 step: 571, loss is 0.02710399031639099\n",
      "epoch: 20 step: 572, loss is 0.04167119041085243\n",
      "epoch: 20 step: 573, loss is 0.03381279110908508\n",
      "epoch: 20 step: 574, loss is 0.0020957249216735363\n",
      "epoch: 20 step: 575, loss is 0.06198719143867493\n",
      "epoch: 20 step: 576, loss is 0.005622779484838247\n",
      "epoch: 20 step: 577, loss is 0.01597650535404682\n",
      "epoch: 20 step: 578, loss is 0.004861240275204182\n",
      "epoch: 20 step: 579, loss is 0.020248252898454666\n",
      "epoch: 20 step: 580, loss is 0.003769180504605174\n",
      "epoch: 20 step: 581, loss is 0.019481567665934563\n",
      "epoch: 20 step: 582, loss is 0.0008349939598701894\n",
      "epoch: 20 step: 583, loss is 0.012564857490360737\n",
      "epoch: 20 step: 584, loss is 0.003443869762122631\n",
      "epoch: 20 step: 585, loss is 0.0017346168169751763\n",
      "epoch: 20 step: 586, loss is 0.0030531950760632753\n",
      "epoch: 20 step: 587, loss is 0.0016583746764808893\n",
      "epoch: 20 step: 588, loss is 0.0010595843195915222\n",
      "epoch: 20 step: 589, loss is 0.0009324569255113602\n",
      "epoch: 20 step: 590, loss is 0.004319602157920599\n",
      "epoch: 20 step: 591, loss is 0.008479364216327667\n",
      "epoch: 20 step: 592, loss is 0.026779968291521072\n",
      "epoch: 20 step: 593, loss is 0.041591987013816833\n",
      "epoch: 20 step: 594, loss is 0.012471103109419346\n",
      "epoch: 20 step: 595, loss is 0.007011014968156815\n",
      "epoch: 20 step: 596, loss is 0.03556916490197182\n",
      "epoch: 20 step: 597, loss is 0.00046417777775786817\n",
      "epoch: 20 step: 598, loss is 0.0022945264354348183\n",
      "epoch: 20 step: 599, loss is 0.051341041922569275\n",
      "epoch: 20 step: 600, loss is 0.0005414662882685661\n",
      "epoch: 20 step: 601, loss is 0.003564588725566864\n",
      "epoch: 20 step: 602, loss is 0.0007476040627807379\n",
      "epoch: 20 step: 603, loss is 0.000382117519620806\n",
      "epoch: 20 step: 604, loss is 0.022079801186919212\n",
      "epoch: 20 step: 605, loss is 0.013569752685725689\n",
      "epoch: 20 step: 606, loss is 0.003671807935461402\n",
      "epoch: 20 step: 607, loss is 0.00481302198022604\n",
      "epoch: 20 step: 608, loss is 0.011311665177345276\n",
      "epoch: 20 step: 609, loss is 0.043915409594774246\n",
      "epoch: 20 step: 610, loss is 0.009827964939177036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 611, loss is 0.01667417772114277\n",
      "epoch: 20 step: 612, loss is 0.010216131806373596\n",
      "epoch: 20 step: 613, loss is 0.00989731214940548\n",
      "epoch: 20 step: 614, loss is 0.0009092803811654449\n",
      "epoch: 20 step: 615, loss is 0.005278741009533405\n",
      "epoch: 20 step: 616, loss is 0.0005486256559379399\n",
      "epoch: 20 step: 617, loss is 0.006458043586462736\n",
      "epoch: 20 step: 618, loss is 0.0023983391001820564\n",
      "epoch: 20 step: 619, loss is 0.0014992831274867058\n",
      "epoch: 20 step: 620, loss is 0.0014684019843116403\n",
      "epoch: 20 step: 621, loss is 0.010577468201518059\n",
      "epoch: 20 step: 622, loss is 0.004317118786275387\n",
      "epoch: 20 step: 623, loss is 0.0007231697090901434\n",
      "epoch: 20 step: 624, loss is 0.00628100847825408\n",
      "epoch: 20 step: 625, loss is 0.01151248998939991\n",
      "epoch: 20 step: 626, loss is 0.0001188436581287533\n",
      "epoch: 20 step: 627, loss is 0.00034920492907986045\n",
      "epoch: 20 step: 628, loss is 0.0008125634049065411\n",
      "epoch: 20 step: 629, loss is 0.0013346263440325856\n",
      "epoch: 20 step: 630, loss is 0.001578944968059659\n",
      "epoch: 20 step: 631, loss is 0.00035131946788169444\n",
      "epoch: 20 step: 632, loss is 0.0004137190117035061\n",
      "epoch: 20 step: 633, loss is 0.0007554955082014203\n",
      "epoch: 20 step: 634, loss is 0.02747391164302826\n",
      "epoch: 20 step: 635, loss is 0.0026084198616445065\n",
      "epoch: 20 step: 636, loss is 0.0012107806978747249\n",
      "epoch: 20 step: 637, loss is 0.0044885906390845776\n",
      "epoch: 20 step: 638, loss is 0.00012324588897172362\n",
      "epoch: 20 step: 639, loss is 0.00020631030201911926\n",
      "epoch: 20 step: 640, loss is 0.003824423300102353\n",
      "epoch: 20 step: 641, loss is 0.001092919846996665\n",
      "epoch: 20 step: 642, loss is 4.231976345181465e-05\n",
      "epoch: 20 step: 643, loss is 0.0012858911650255322\n",
      "epoch: 20 step: 644, loss is 0.027925604954361916\n",
      "epoch: 20 step: 645, loss is 0.010638279840350151\n",
      "epoch: 20 step: 646, loss is 0.00476349052041769\n",
      "epoch: 20 step: 647, loss is 0.0033551971428096294\n",
      "epoch: 20 step: 648, loss is 0.002262228401377797\n",
      "epoch: 20 step: 649, loss is 0.007774789817631245\n",
      "epoch: 20 step: 650, loss is 0.017827335745096207\n",
      "epoch: 20 step: 651, loss is 0.00814428087323904\n",
      "epoch: 20 step: 652, loss is 0.00020289773237891495\n",
      "epoch: 20 step: 653, loss is 0.03166448324918747\n",
      "epoch: 20 step: 654, loss is 0.03151782602071762\n",
      "epoch: 20 step: 655, loss is 0.06468547135591507\n",
      "epoch: 20 step: 656, loss is 0.011477514170110226\n",
      "epoch: 20 step: 657, loss is 0.004582837224006653\n",
      "epoch: 20 step: 658, loss is 0.006278345361351967\n",
      "epoch: 20 step: 659, loss is 0.0005610076477751136\n",
      "epoch: 20 step: 660, loss is 0.00022191180323716253\n",
      "epoch: 20 step: 661, loss is 0.0022350989747792482\n",
      "epoch: 20 step: 662, loss is 0.0010039398912340403\n",
      "epoch: 20 step: 663, loss is 0.06580888479948044\n",
      "epoch: 20 step: 664, loss is 0.029012048617005348\n",
      "epoch: 20 step: 665, loss is 0.051557548344135284\n",
      "epoch: 20 step: 666, loss is 0.00470312125980854\n",
      "epoch: 20 step: 667, loss is 0.004521080292761326\n",
      "epoch: 20 step: 668, loss is 0.02753441408276558\n",
      "epoch: 20 step: 669, loss is 0.13884927332401276\n",
      "epoch: 20 step: 670, loss is 0.014723285101354122\n",
      "epoch: 20 step: 671, loss is 0.001576438546180725\n",
      "epoch: 20 step: 672, loss is 0.0013701152056455612\n",
      "epoch: 20 step: 673, loss is 0.006374908145517111\n",
      "epoch: 20 step: 674, loss is 0.02752731740474701\n",
      "epoch: 20 step: 675, loss is 0.0010724053718149662\n",
      "epoch: 20 step: 676, loss is 0.02650686725974083\n",
      "epoch: 20 step: 677, loss is 0.03710488602519035\n",
      "epoch: 20 step: 678, loss is 0.012047871947288513\n",
      "epoch: 20 step: 679, loss is 0.05684887245297432\n",
      "epoch: 20 step: 680, loss is 0.0006413153023459017\n",
      "epoch: 20 step: 681, loss is 0.03524668514728546\n",
      "epoch: 20 step: 682, loss is 0.002602770458906889\n",
      "epoch: 20 step: 683, loss is 0.0002215198619524017\n",
      "epoch: 20 step: 684, loss is 0.0019147138809785247\n",
      "epoch: 20 step: 685, loss is 0.0015592690324410796\n",
      "epoch: 20 step: 686, loss is 0.014231796376407146\n",
      "epoch: 20 step: 687, loss is 0.011310400441288948\n",
      "epoch: 20 step: 688, loss is 0.0004579520027618855\n",
      "epoch: 20 step: 689, loss is 0.012428728863596916\n",
      "epoch: 20 step: 690, loss is 0.0030976696871221066\n",
      "epoch: 20 step: 691, loss is 0.003344898112118244\n",
      "epoch: 20 step: 692, loss is 0.1028003841638565\n",
      "epoch: 20 step: 693, loss is 0.048446305096149445\n",
      "epoch: 20 step: 694, loss is 0.025778086856007576\n",
      "epoch: 20 step: 695, loss is 0.02977052517235279\n",
      "epoch: 20 step: 696, loss is 0.003525124629959464\n",
      "epoch: 20 step: 697, loss is 0.008046611212193966\n",
      "epoch: 20 step: 698, loss is 0.0058395094238221645\n",
      "epoch: 20 step: 699, loss is 0.027199583128094673\n",
      "epoch: 20 step: 700, loss is 0.006460133474320173\n",
      "epoch: 20 step: 701, loss is 0.006878155283629894\n",
      "epoch: 20 step: 702, loss is 0.0018107597716152668\n",
      "epoch: 20 step: 703, loss is 0.006364769767969847\n",
      "epoch: 20 step: 704, loss is 0.020060794427990913\n",
      "epoch: 20 step: 705, loss is 0.0031292522326111794\n",
      "epoch: 20 step: 706, loss is 0.00856903288513422\n",
      "epoch: 20 step: 707, loss is 0.003359180875122547\n",
      "epoch: 20 step: 708, loss is 0.12222059071063995\n",
      "epoch: 20 step: 709, loss is 0.019041940569877625\n",
      "epoch: 20 step: 710, loss is 0.01843872293829918\n",
      "epoch: 20 step: 711, loss is 0.002954807598143816\n",
      "epoch: 20 step: 712, loss is 0.16273704171180725\n",
      "epoch: 20 step: 713, loss is 0.0007122844690456986\n",
      "epoch: 20 step: 714, loss is 0.00022040042676962912\n",
      "epoch: 20 step: 715, loss is 0.02101699262857437\n",
      "epoch: 20 step: 716, loss is 9.863201557891443e-05\n",
      "epoch: 20 step: 717, loss is 0.003942827228456736\n",
      "epoch: 20 step: 718, loss is 0.002226205775514245\n",
      "epoch: 20 step: 719, loss is 0.004607381299138069\n",
      "epoch: 20 step: 720, loss is 0.0022783316671848297\n",
      "epoch: 20 step: 721, loss is 0.006491986569017172\n",
      "epoch: 20 step: 722, loss is 0.15044140815734863\n",
      "epoch: 20 step: 723, loss is 0.0005866463761776686\n",
      "epoch: 20 step: 724, loss is 0.09517235308885574\n",
      "epoch: 20 step: 725, loss is 0.004759309347718954\n",
      "epoch: 20 step: 726, loss is 0.009778546169400215\n",
      "epoch: 20 step: 727, loss is 0.2506231963634491\n",
      "epoch: 20 step: 728, loss is 0.018122466281056404\n",
      "epoch: 20 step: 729, loss is 0.060841355472803116\n",
      "epoch: 20 step: 730, loss is 0.020727358758449554\n",
      "epoch: 20 step: 731, loss is 0.11690916121006012\n",
      "epoch: 20 step: 732, loss is 0.007801393046975136\n",
      "epoch: 20 step: 733, loss is 0.0003378480614628643\n",
      "epoch: 20 step: 734, loss is 0.0007050470449030399\n",
      "epoch: 20 step: 735, loss is 0.016024639829993248\n",
      "epoch: 20 step: 736, loss is 0.0374489389359951\n",
      "epoch: 20 step: 737, loss is 0.009070878848433495\n",
      "epoch: 20 step: 738, loss is 0.0006926027708686888\n",
      "epoch: 20 step: 739, loss is 0.001073397696018219\n",
      "epoch: 20 step: 740, loss is 0.001650685560889542\n",
      "epoch: 20 step: 741, loss is 0.004871748853474855\n",
      "epoch: 20 step: 742, loss is 0.0011543002910912037\n",
      "epoch: 20 step: 743, loss is 0.0037933357525616884\n",
      "epoch: 20 step: 744, loss is 0.021796422079205513\n",
      "epoch: 20 step: 745, loss is 0.006387533154338598\n",
      "epoch: 20 step: 746, loss is 0.0006860456196591258\n",
      "epoch: 20 step: 747, loss is 0.04369049519300461\n",
      "epoch: 20 step: 748, loss is 0.015804044902324677\n",
      "epoch: 20 step: 749, loss is 0.0008180161821655929\n",
      "epoch: 20 step: 750, loss is 0.006544637959450483\n",
      "epoch: 20 step: 751, loss is 0.10922267287969589\n",
      "epoch: 20 step: 752, loss is 0.0007152914768084884\n",
      "epoch: 20 step: 753, loss is 0.0035687191411852837\n",
      "epoch: 20 step: 754, loss is 0.013895699754357338\n",
      "epoch: 20 step: 755, loss is 0.007596888113766909\n",
      "epoch: 20 step: 756, loss is 0.010318562388420105\n",
      "epoch: 20 step: 757, loss is 0.0029618453700095415\n",
      "epoch: 20 step: 758, loss is 0.012214352376759052\n",
      "epoch: 20 step: 759, loss is 0.06571770459413528\n",
      "epoch: 20 step: 760, loss is 0.0009684287360869348\n",
      "epoch: 20 step: 761, loss is 0.029888121411204338\n",
      "epoch: 20 step: 762, loss is 0.04838119447231293\n",
      "epoch: 20 step: 763, loss is 0.0065183863043785095\n",
      "epoch: 20 step: 764, loss is 0.013100231997668743\n",
      "epoch: 20 step: 765, loss is 0.039708033204078674\n",
      "epoch: 20 step: 766, loss is 0.08838092535734177\n",
      "epoch: 20 step: 767, loss is 0.04392365366220474\n",
      "epoch: 20 step: 768, loss is 0.02681301347911358\n",
      "epoch: 20 step: 769, loss is 0.0006326520815491676\n",
      "epoch: 20 step: 770, loss is 0.004418725613504648\n",
      "epoch: 20 step: 771, loss is 0.02620038390159607\n",
      "epoch: 20 step: 772, loss is 0.0462857224047184\n",
      "epoch: 20 step: 773, loss is 0.002428920241072774\n",
      "epoch: 20 step: 774, loss is 0.021552225574851036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 775, loss is 0.0017707851948216558\n",
      "epoch: 20 step: 776, loss is 0.0020205972250550985\n",
      "epoch: 20 step: 777, loss is 0.04260165989398956\n",
      "epoch: 20 step: 778, loss is 0.0025433884002268314\n",
      "epoch: 20 step: 779, loss is 0.009411530569195747\n",
      "epoch: 20 step: 780, loss is 0.00036921686842106283\n",
      "epoch: 20 step: 781, loss is 0.016315151005983353\n",
      "epoch: 20 step: 782, loss is 0.03559642657637596\n",
      "epoch: 20 step: 783, loss is 0.00960406381636858\n",
      "epoch: 20 step: 784, loss is 0.0009104631608352065\n",
      "epoch: 20 step: 785, loss is 0.045900858938694\n",
      "epoch: 20 step: 786, loss is 0.027990905568003654\n",
      "epoch: 20 step: 787, loss is 0.023501859977841377\n",
      "epoch: 20 step: 788, loss is 0.07319848239421844\n",
      "epoch: 20 step: 789, loss is 0.017206206917762756\n",
      "epoch: 20 step: 790, loss is 0.003760273801162839\n",
      "epoch: 20 step: 791, loss is 0.01983848586678505\n",
      "epoch: 20 step: 792, loss is 0.0014770337147638202\n",
      "epoch: 20 step: 793, loss is 0.054190896451473236\n",
      "epoch: 20 step: 794, loss is 0.0009462070884183049\n",
      "epoch: 20 step: 795, loss is 0.002647570101544261\n",
      "epoch: 20 step: 796, loss is 0.001689500524662435\n",
      "epoch: 20 step: 797, loss is 0.00503630843013525\n",
      "epoch: 20 step: 798, loss is 0.019772395491600037\n",
      "epoch: 20 step: 799, loss is 0.008624067530035973\n",
      "epoch: 20 step: 800, loss is 0.006747727748006582\n",
      "epoch: 20 step: 801, loss is 0.02939562313258648\n",
      "epoch: 20 step: 802, loss is 0.0018498520366847515\n",
      "epoch: 20 step: 803, loss is 0.004957950673997402\n",
      "epoch: 20 step: 804, loss is 0.03417390584945679\n",
      "epoch: 20 step: 805, loss is 0.005771653261035681\n",
      "epoch: 20 step: 806, loss is 0.010722452774643898\n",
      "epoch: 20 step: 807, loss is 0.002828338649123907\n",
      "epoch: 20 step: 808, loss is 0.009167246520519257\n",
      "epoch: 20 step: 809, loss is 0.0016766894841566682\n",
      "epoch: 20 step: 810, loss is 0.0147373266518116\n",
      "epoch: 20 step: 811, loss is 0.01187209878116846\n",
      "epoch: 20 step: 812, loss is 0.003552492242306471\n",
      "epoch: 20 step: 813, loss is 0.023982780054211617\n",
      "epoch: 20 step: 814, loss is 0.005477788392454386\n",
      "epoch: 20 step: 815, loss is 0.003190615214407444\n",
      "epoch: 20 step: 816, loss is 0.032407425343990326\n",
      "epoch: 20 step: 817, loss is 0.0007898197509348392\n",
      "epoch: 20 step: 818, loss is 0.006138794124126434\n",
      "epoch: 20 step: 819, loss is 0.00902895350009203\n",
      "epoch: 20 step: 820, loss is 0.003003189340233803\n",
      "epoch: 20 step: 821, loss is 0.013931654393672943\n",
      "epoch: 20 step: 822, loss is 0.00030400310060940683\n",
      "epoch: 20 step: 823, loss is 0.08113668113946915\n",
      "epoch: 20 step: 824, loss is 0.03260018303990364\n",
      "epoch: 20 step: 825, loss is 0.0014372864970937371\n",
      "epoch: 20 step: 826, loss is 0.0013869167305529118\n",
      "epoch: 20 step: 827, loss is 0.01477092131972313\n",
      "epoch: 20 step: 828, loss is 0.0020888822618871927\n",
      "epoch: 20 step: 829, loss is 0.01722034439444542\n",
      "epoch: 20 step: 830, loss is 0.016346251592040062\n",
      "epoch: 20 step: 831, loss is 0.0183037668466568\n",
      "epoch: 20 step: 832, loss is 0.0055324481800198555\n",
      "epoch: 20 step: 833, loss is 0.003803471103310585\n",
      "epoch: 20 step: 834, loss is 0.02016562409698963\n",
      "epoch: 20 step: 835, loss is 0.0024021633435040712\n",
      "epoch: 20 step: 836, loss is 5.615639020106755e-05\n",
      "epoch: 20 step: 837, loss is 0.015253364108502865\n",
      "epoch: 20 step: 838, loss is 0.00647977227345109\n",
      "epoch: 20 step: 839, loss is 0.044597551226615906\n",
      "epoch: 20 step: 840, loss is 0.021144088357686996\n",
      "epoch: 20 step: 841, loss is 0.010662364773452282\n",
      "epoch: 20 step: 842, loss is 0.029598820954561234\n",
      "epoch: 20 step: 843, loss is 0.040829263627529144\n",
      "epoch: 20 step: 844, loss is 0.0004939946229569614\n",
      "epoch: 20 step: 845, loss is 0.00021591107361018658\n",
      "epoch: 20 step: 846, loss is 0.006233195308595896\n",
      "epoch: 20 step: 847, loss is 0.07541803270578384\n",
      "epoch: 20 step: 848, loss is 0.0012733590556308627\n",
      "epoch: 20 step: 849, loss is 0.022114964202046394\n",
      "epoch: 20 step: 850, loss is 0.026888854801654816\n",
      "epoch: 20 step: 851, loss is 0.004914447665214539\n",
      "epoch: 20 step: 852, loss is 0.001255856710486114\n",
      "epoch: 20 step: 853, loss is 0.0033774999901652336\n",
      "epoch: 20 step: 854, loss is 0.00229602656327188\n",
      "epoch: 20 step: 855, loss is 0.015728501603007317\n",
      "epoch: 20 step: 856, loss is 0.005358978174626827\n",
      "epoch: 20 step: 857, loss is 0.003331243060529232\n",
      "epoch: 20 step: 858, loss is 0.0020639889407902956\n",
      "epoch: 20 step: 859, loss is 0.07540539652109146\n",
      "epoch: 20 step: 860, loss is 0.015451054088771343\n",
      "epoch: 20 step: 861, loss is 0.01462812814861536\n",
      "epoch: 20 step: 862, loss is 0.001020867726765573\n",
      "epoch: 20 step: 863, loss is 0.00030372419860213995\n",
      "epoch: 20 step: 864, loss is 0.02642366662621498\n",
      "epoch: 20 step: 865, loss is 0.004132306203246117\n",
      "epoch: 20 step: 866, loss is 0.057247892022132874\n",
      "epoch: 20 step: 867, loss is 0.03793714568018913\n",
      "epoch: 20 step: 868, loss is 0.026203827932476997\n",
      "epoch: 20 step: 869, loss is 0.0003619132621679455\n",
      "epoch: 20 step: 870, loss is 0.0004480493371374905\n",
      "epoch: 20 step: 871, loss is 0.004225253127515316\n",
      "epoch: 20 step: 872, loss is 0.01829826645553112\n",
      "epoch: 20 step: 873, loss is 0.017829839140176773\n",
      "epoch: 20 step: 874, loss is 0.009364694356918335\n",
      "epoch: 20 step: 875, loss is 0.0035678823478519917\n",
      "epoch: 20 step: 876, loss is 0.004294330719858408\n",
      "epoch: 20 step: 877, loss is 0.0006967331282794476\n",
      "epoch: 20 step: 878, loss is 0.050293855369091034\n",
      "epoch: 20 step: 879, loss is 0.002653443953022361\n",
      "epoch: 20 step: 880, loss is 0.002076135715469718\n",
      "epoch: 20 step: 881, loss is 0.00019782678282354027\n",
      "epoch: 20 step: 882, loss is 0.02151893451809883\n",
      "epoch: 20 step: 883, loss is 0.036080434918403625\n",
      "epoch: 20 step: 884, loss is 0.05868064612150192\n",
      "epoch: 20 step: 885, loss is 0.0490272156894207\n",
      "epoch: 20 step: 886, loss is 0.017921658232808113\n",
      "epoch: 20 step: 887, loss is 0.020038917660713196\n",
      "epoch: 20 step: 888, loss is 0.008986013010144234\n",
      "epoch: 20 step: 889, loss is 0.01258924137800932\n",
      "epoch: 20 step: 890, loss is 0.00223473715595901\n",
      "epoch: 20 step: 891, loss is 0.0634009912610054\n",
      "epoch: 20 step: 892, loss is 0.0012452842202037573\n",
      "epoch: 20 step: 893, loss is 0.016899699345231056\n",
      "epoch: 20 step: 894, loss is 0.0008282792987301946\n",
      "epoch: 20 step: 895, loss is 0.008237900212407112\n",
      "epoch: 20 step: 896, loss is 0.03352035954594612\n",
      "epoch: 20 step: 897, loss is 0.01384740974754095\n",
      "epoch: 20 step: 898, loss is 0.033597905188798904\n",
      "epoch: 20 step: 899, loss is 0.05522850528359413\n",
      "epoch: 20 step: 900, loss is 0.028410157188773155\n",
      "epoch: 20 step: 901, loss is 0.000248754455242306\n",
      "epoch: 20 step: 902, loss is 0.0026065977290272713\n",
      "epoch: 20 step: 903, loss is 0.002999181393533945\n",
      "epoch: 20 step: 904, loss is 0.06530911475419998\n",
      "epoch: 20 step: 905, loss is 0.006465560290962458\n",
      "epoch: 20 step: 906, loss is 0.017700374126434326\n",
      "epoch: 20 step: 907, loss is 0.05837904289364815\n",
      "epoch: 20 step: 908, loss is 0.018528947606682777\n",
      "epoch: 20 step: 909, loss is 0.0027742190286517143\n",
      "epoch: 20 step: 910, loss is 0.0010155943455174565\n",
      "epoch: 20 step: 911, loss is 0.005524523556232452\n",
      "epoch: 20 step: 912, loss is 0.01368184108287096\n",
      "epoch: 20 step: 913, loss is 0.017432987689971924\n",
      "epoch: 20 step: 914, loss is 0.008043572306632996\n",
      "epoch: 20 step: 915, loss is 0.02136825956404209\n",
      "epoch: 20 step: 916, loss is 0.0055588302202522755\n",
      "epoch: 20 step: 917, loss is 0.00044258785783313215\n",
      "epoch: 20 step: 918, loss is 0.03937623277306557\n",
      "epoch: 20 step: 919, loss is 0.04125417023897171\n",
      "epoch: 20 step: 920, loss is 0.007428106851875782\n",
      "epoch: 20 step: 921, loss is 0.016667570918798447\n",
      "epoch: 20 step: 922, loss is 0.009202560409903526\n",
      "epoch: 20 step: 923, loss is 0.0033779949881136417\n",
      "epoch: 20 step: 924, loss is 0.02440841868519783\n",
      "epoch: 20 step: 925, loss is 0.013050294481217861\n",
      "epoch: 20 step: 926, loss is 0.0005339303752407432\n",
      "epoch: 20 step: 927, loss is 0.026853807270526886\n",
      "epoch: 20 step: 928, loss is 0.0035653510130941868\n",
      "epoch: 20 step: 929, loss is 0.0289248488843441\n",
      "epoch: 20 step: 930, loss is 0.04780464246869087\n",
      "epoch: 20 step: 931, loss is 0.009154241532087326\n",
      "epoch: 20 step: 932, loss is 0.047928571701049805\n",
      "epoch: 20 step: 933, loss is 0.011792868375778198\n",
      "epoch: 20 step: 934, loss is 0.005565115716308355\n",
      "epoch: 20 step: 935, loss is 0.0007040638010948896\n",
      "epoch: 20 step: 936, loss is 0.004670946393162012\n",
      "epoch: 20 step: 937, loss is 0.08865253627300262\n",
      "epoch: 21 step: 1, loss is 0.016453133895993233\n",
      "epoch: 21 step: 2, loss is 0.007982046343386173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 3, loss is 0.002703478094190359\n",
      "epoch: 21 step: 4, loss is 0.009647578001022339\n",
      "epoch: 21 step: 5, loss is 0.00014574771921616048\n",
      "epoch: 21 step: 6, loss is 0.002990277949720621\n",
      "epoch: 21 step: 7, loss is 0.002639703918248415\n",
      "epoch: 21 step: 8, loss is 0.012595743872225285\n",
      "epoch: 21 step: 9, loss is 0.04775068536400795\n",
      "epoch: 21 step: 10, loss is 0.0033799409866333008\n",
      "epoch: 21 step: 11, loss is 0.00043433081009425223\n",
      "epoch: 21 step: 12, loss is 0.0012941044988110662\n",
      "epoch: 21 step: 13, loss is 0.026495294645428658\n",
      "epoch: 21 step: 14, loss is 0.0006598379695788026\n",
      "epoch: 21 step: 15, loss is 0.015814509242773056\n",
      "epoch: 21 step: 16, loss is 0.011058297008275986\n",
      "epoch: 21 step: 17, loss is 6.152187415864319e-05\n",
      "epoch: 21 step: 18, loss is 0.010668186470866203\n",
      "epoch: 21 step: 19, loss is 0.0035034501925110817\n",
      "epoch: 21 step: 20, loss is 0.027123240754008293\n",
      "epoch: 21 step: 21, loss is 0.004379278048872948\n",
      "epoch: 21 step: 22, loss is 0.00237248744815588\n",
      "epoch: 21 step: 23, loss is 0.021013295277953148\n",
      "epoch: 21 step: 24, loss is 0.0018237727927044034\n",
      "epoch: 21 step: 25, loss is 0.00044537882786244154\n",
      "epoch: 21 step: 26, loss is 0.044521503150463104\n",
      "epoch: 21 step: 27, loss is 0.12943504750728607\n",
      "epoch: 21 step: 28, loss is 0.006347520276904106\n",
      "epoch: 21 step: 29, loss is 0.001555559691041708\n",
      "epoch: 21 step: 30, loss is 0.006901787593960762\n",
      "epoch: 21 step: 31, loss is 0.011229125782847404\n",
      "epoch: 21 step: 32, loss is 0.012465214356780052\n",
      "epoch: 21 step: 33, loss is 0.00045840092934668064\n",
      "epoch: 21 step: 34, loss is 0.0004939423524774611\n",
      "epoch: 21 step: 35, loss is 0.002205249387770891\n",
      "epoch: 21 step: 36, loss is 0.0028648185543715954\n",
      "epoch: 21 step: 37, loss is 0.010263423435389996\n",
      "epoch: 21 step: 38, loss is 0.005184129811823368\n",
      "epoch: 21 step: 39, loss is 0.00361132575199008\n",
      "epoch: 21 step: 40, loss is 0.05164894089102745\n",
      "epoch: 21 step: 41, loss is 0.048108577728271484\n",
      "epoch: 21 step: 42, loss is 0.0006481469026766717\n",
      "epoch: 21 step: 43, loss is 0.0021900904830545187\n",
      "epoch: 21 step: 44, loss is 0.0008298854809254408\n",
      "epoch: 21 step: 45, loss is 0.000919331272598356\n",
      "epoch: 21 step: 46, loss is 0.0021456617396324873\n",
      "epoch: 21 step: 47, loss is 0.00015914766117930412\n",
      "epoch: 21 step: 48, loss is 0.011553342454135418\n",
      "epoch: 21 step: 49, loss is 0.035383354872465134\n",
      "epoch: 21 step: 50, loss is 0.0037891084793955088\n",
      "epoch: 21 step: 51, loss is 0.06609198451042175\n",
      "epoch: 21 step: 52, loss is 0.014056604355573654\n",
      "epoch: 21 step: 53, loss is 0.010216367430984974\n",
      "epoch: 21 step: 54, loss is 7.202923006843776e-05\n",
      "epoch: 21 step: 55, loss is 0.004463109653443098\n",
      "epoch: 21 step: 56, loss is 0.08113711327314377\n",
      "epoch: 21 step: 57, loss is 0.03219768404960632\n",
      "epoch: 21 step: 58, loss is 0.0006874907994642854\n",
      "epoch: 21 step: 59, loss is 0.03437099605798721\n",
      "epoch: 21 step: 60, loss is 0.001760573242790997\n",
      "epoch: 21 step: 61, loss is 0.00040865642949938774\n",
      "epoch: 21 step: 62, loss is 0.0010258503025397658\n",
      "epoch: 21 step: 63, loss is 0.0019359579309821129\n",
      "epoch: 21 step: 64, loss is 0.004656174220144749\n",
      "epoch: 21 step: 65, loss is 0.0017187715275213122\n",
      "epoch: 21 step: 66, loss is 0.002986056264489889\n",
      "epoch: 21 step: 67, loss is 0.0016123951645568013\n",
      "epoch: 21 step: 68, loss is 0.014797193929553032\n",
      "epoch: 21 step: 69, loss is 0.012416329234838486\n",
      "epoch: 21 step: 70, loss is 0.009012928232550621\n",
      "epoch: 21 step: 71, loss is 0.0012962912442162633\n",
      "epoch: 21 step: 72, loss is 0.03752981126308441\n",
      "epoch: 21 step: 73, loss is 0.002442658180370927\n",
      "epoch: 21 step: 74, loss is 0.0022303638979792595\n",
      "epoch: 21 step: 75, loss is 0.01770302839577198\n",
      "epoch: 21 step: 76, loss is 0.0016636934597045183\n",
      "epoch: 21 step: 77, loss is 0.0005696087027899921\n",
      "epoch: 21 step: 78, loss is 0.0025908160023391247\n",
      "epoch: 21 step: 79, loss is 0.01447348203510046\n",
      "epoch: 21 step: 80, loss is 0.006573625840246677\n",
      "epoch: 21 step: 81, loss is 0.002708246698603034\n",
      "epoch: 21 step: 82, loss is 0.003944261930882931\n",
      "epoch: 21 step: 83, loss is 0.0008612906094640493\n",
      "epoch: 21 step: 84, loss is 0.0008032544865272939\n",
      "epoch: 21 step: 85, loss is 0.0028506643138825893\n",
      "epoch: 21 step: 86, loss is 0.016546480357646942\n",
      "epoch: 21 step: 87, loss is 0.0008802800439298153\n",
      "epoch: 21 step: 88, loss is 0.00666492385789752\n",
      "epoch: 21 step: 89, loss is 0.00022999313659965992\n",
      "epoch: 21 step: 90, loss is 0.0002841307723429054\n",
      "epoch: 21 step: 91, loss is 0.004131947178393602\n",
      "epoch: 21 step: 92, loss is 0.0020845606923103333\n",
      "epoch: 21 step: 93, loss is 0.006627610418945551\n",
      "epoch: 21 step: 94, loss is 0.025778360664844513\n",
      "epoch: 21 step: 95, loss is 0.00013443818897940218\n",
      "epoch: 21 step: 96, loss is 0.002643189625814557\n",
      "epoch: 21 step: 97, loss is 0.012145979329943657\n",
      "epoch: 21 step: 98, loss is 0.0136720547452569\n",
      "epoch: 21 step: 99, loss is 0.0013113253517076373\n",
      "epoch: 21 step: 100, loss is 0.029478900134563446\n",
      "epoch: 21 step: 101, loss is 0.014069819822907448\n",
      "epoch: 21 step: 102, loss is 0.010841244831681252\n",
      "epoch: 21 step: 103, loss is 0.0010265602031722665\n",
      "epoch: 21 step: 104, loss is 0.00012599564797710627\n",
      "epoch: 21 step: 105, loss is 0.003549697110429406\n",
      "epoch: 21 step: 106, loss is 0.0028824147302657366\n",
      "epoch: 21 step: 107, loss is 0.0070532807148993015\n",
      "epoch: 21 step: 108, loss is 0.0021311596501618624\n",
      "epoch: 21 step: 109, loss is 0.002216324210166931\n",
      "epoch: 21 step: 110, loss is 0.0035382851492613554\n",
      "epoch: 21 step: 111, loss is 0.04209813103079796\n",
      "epoch: 21 step: 112, loss is 0.004303742200136185\n",
      "epoch: 21 step: 113, loss is 0.00552061852067709\n",
      "epoch: 21 step: 114, loss is 0.008160221390426159\n",
      "epoch: 21 step: 115, loss is 0.038361985236406326\n",
      "epoch: 21 step: 116, loss is 0.001381998648867011\n",
      "epoch: 21 step: 117, loss is 0.004189830273389816\n",
      "epoch: 21 step: 118, loss is 0.001331735635176301\n",
      "epoch: 21 step: 119, loss is 0.00022129139688331634\n",
      "epoch: 21 step: 120, loss is 0.00781945139169693\n",
      "epoch: 21 step: 121, loss is 0.008740509860217571\n",
      "epoch: 21 step: 122, loss is 0.006098771467804909\n",
      "epoch: 21 step: 123, loss is 0.00017022671818267554\n",
      "epoch: 21 step: 124, loss is 0.004861618857830763\n",
      "epoch: 21 step: 125, loss is 0.003818158060312271\n",
      "epoch: 21 step: 126, loss is 0.022690393030643463\n",
      "epoch: 21 step: 127, loss is 0.0036430940963327885\n",
      "epoch: 21 step: 128, loss is 0.01549072191119194\n",
      "epoch: 21 step: 129, loss is 0.0011383115779608488\n",
      "epoch: 21 step: 130, loss is 0.004122976679354906\n",
      "epoch: 21 step: 131, loss is 0.003727175761014223\n",
      "epoch: 21 step: 132, loss is 0.0015690268483012915\n",
      "epoch: 21 step: 133, loss is 0.0029354277066886425\n",
      "epoch: 21 step: 134, loss is 0.033551961183547974\n",
      "epoch: 21 step: 135, loss is 0.0032425704412162304\n",
      "epoch: 21 step: 136, loss is 0.0004976576892659068\n",
      "epoch: 21 step: 137, loss is 0.01084999367594719\n",
      "epoch: 21 step: 138, loss is 0.0023539476096630096\n",
      "epoch: 21 step: 139, loss is 0.0014419812941923738\n",
      "epoch: 21 step: 140, loss is 0.0008900674874894321\n",
      "epoch: 21 step: 141, loss is 0.0004626052104867995\n",
      "epoch: 21 step: 142, loss is 0.0023152523208409548\n",
      "epoch: 21 step: 143, loss is 0.0008935075020417571\n",
      "epoch: 21 step: 144, loss is 0.010610029101371765\n",
      "epoch: 21 step: 145, loss is 0.006051753647625446\n",
      "epoch: 21 step: 146, loss is 0.02901262231171131\n",
      "epoch: 21 step: 147, loss is 0.00955241359770298\n",
      "epoch: 21 step: 148, loss is 0.04247066378593445\n",
      "epoch: 21 step: 149, loss is 0.001195876277051866\n",
      "epoch: 21 step: 150, loss is 0.00019043279462493956\n",
      "epoch: 21 step: 151, loss is 0.0021635545417666435\n",
      "epoch: 21 step: 152, loss is 0.008824425749480724\n",
      "epoch: 21 step: 153, loss is 0.010602661408483982\n",
      "epoch: 21 step: 154, loss is 0.05798660218715668\n",
      "epoch: 21 step: 155, loss is 0.0064878324046730995\n",
      "epoch: 21 step: 156, loss is 0.004073492716997862\n",
      "epoch: 21 step: 157, loss is 0.009699738584458828\n",
      "epoch: 21 step: 158, loss is 0.002221902832388878\n",
      "epoch: 21 step: 159, loss is 0.00733270775526762\n",
      "epoch: 21 step: 160, loss is 0.017144588753581047\n",
      "epoch: 21 step: 161, loss is 0.01885991357266903\n",
      "epoch: 21 step: 162, loss is 0.006037716753780842\n",
      "epoch: 21 step: 163, loss is 0.04144405946135521\n",
      "epoch: 21 step: 164, loss is 0.014239261858165264\n",
      "epoch: 21 step: 165, loss is 0.16102230548858643\n",
      "epoch: 21 step: 166, loss is 0.0034637018106877804\n",
      "epoch: 21 step: 167, loss is 0.0010240772971883416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 168, loss is 3.086450669798069e-05\n",
      "epoch: 21 step: 169, loss is 0.029421381652355194\n",
      "epoch: 21 step: 170, loss is 0.0978831946849823\n",
      "epoch: 21 step: 171, loss is 0.021700456738471985\n",
      "epoch: 21 step: 172, loss is 0.009301056154072285\n",
      "epoch: 21 step: 173, loss is 0.007983099669218063\n",
      "epoch: 21 step: 174, loss is 0.012171266600489616\n",
      "epoch: 21 step: 175, loss is 0.0016813939437270164\n",
      "epoch: 21 step: 176, loss is 0.04236539825797081\n",
      "epoch: 21 step: 177, loss is 0.03971727192401886\n",
      "epoch: 21 step: 178, loss is 0.029787497594952583\n",
      "epoch: 21 step: 179, loss is 0.059003230184316635\n",
      "epoch: 21 step: 180, loss is 0.001941801980137825\n",
      "epoch: 21 step: 181, loss is 0.0626300573348999\n",
      "epoch: 21 step: 182, loss is 0.019692236557602882\n",
      "epoch: 21 step: 183, loss is 0.00136853929143399\n",
      "epoch: 21 step: 184, loss is 0.016844749450683594\n",
      "epoch: 21 step: 185, loss is 0.00290957884863019\n",
      "epoch: 21 step: 186, loss is 0.000730503408703953\n",
      "epoch: 21 step: 187, loss is 0.0008862380054779351\n",
      "epoch: 21 step: 188, loss is 0.0016584659460932016\n",
      "epoch: 21 step: 189, loss is 0.024575794115662575\n",
      "epoch: 21 step: 190, loss is 0.0025718763936311007\n",
      "epoch: 21 step: 191, loss is 0.01785219833254814\n",
      "epoch: 21 step: 192, loss is 0.0004919474595226347\n",
      "epoch: 21 step: 193, loss is 0.0003153060970362276\n",
      "epoch: 21 step: 194, loss is 0.003646373748779297\n",
      "epoch: 21 step: 195, loss is 0.04388485848903656\n",
      "epoch: 21 step: 196, loss is 0.018294639885425568\n",
      "epoch: 21 step: 197, loss is 0.017129695042967796\n",
      "epoch: 21 step: 198, loss is 0.0006616153987124562\n",
      "epoch: 21 step: 199, loss is 0.0014235518174245954\n",
      "epoch: 21 step: 200, loss is 0.0332312248647213\n",
      "epoch: 21 step: 201, loss is 0.11694303154945374\n",
      "epoch: 21 step: 202, loss is 0.001991963479667902\n",
      "epoch: 21 step: 203, loss is 0.026470040902495384\n",
      "epoch: 21 step: 204, loss is 0.023686204105615616\n",
      "epoch: 21 step: 205, loss is 0.020501989871263504\n",
      "epoch: 21 step: 206, loss is 7.068659760989249e-05\n",
      "epoch: 21 step: 207, loss is 0.0037219072692096233\n",
      "epoch: 21 step: 208, loss is 0.0065840063616633415\n",
      "epoch: 21 step: 209, loss is 0.06255179643630981\n",
      "epoch: 21 step: 210, loss is 0.0012396537931635976\n",
      "epoch: 21 step: 211, loss is 0.05946565046906471\n",
      "epoch: 21 step: 212, loss is 0.0005718782776966691\n",
      "epoch: 21 step: 213, loss is 0.013750424608588219\n",
      "epoch: 21 step: 214, loss is 0.003817935474216938\n",
      "epoch: 21 step: 215, loss is 0.013444426469504833\n",
      "epoch: 21 step: 216, loss is 0.006778255570679903\n",
      "epoch: 21 step: 217, loss is 0.006723842583596706\n",
      "epoch: 21 step: 218, loss is 0.0035134530626237392\n",
      "epoch: 21 step: 219, loss is 0.003457579528912902\n",
      "epoch: 21 step: 220, loss is 0.011641398072242737\n",
      "epoch: 21 step: 221, loss is 0.004113639704883099\n",
      "epoch: 21 step: 222, loss is 0.0002473739441484213\n",
      "epoch: 21 step: 223, loss is 0.016848649829626083\n",
      "epoch: 21 step: 224, loss is 0.00953955203294754\n",
      "epoch: 21 step: 225, loss is 0.018473591655492783\n",
      "epoch: 21 step: 226, loss is 0.06505963206291199\n",
      "epoch: 21 step: 227, loss is 0.001778876525349915\n",
      "epoch: 21 step: 228, loss is 0.001446944079361856\n",
      "epoch: 21 step: 229, loss is 0.0013876274460926652\n",
      "epoch: 21 step: 230, loss is 0.00023741627228446305\n",
      "epoch: 21 step: 231, loss is 0.013827341608703136\n",
      "epoch: 21 step: 232, loss is 0.0009257710771635175\n",
      "epoch: 21 step: 233, loss is 0.06951010227203369\n",
      "epoch: 21 step: 234, loss is 0.0006166782113723457\n",
      "epoch: 21 step: 235, loss is 0.001432985533028841\n",
      "epoch: 21 step: 236, loss is 0.009194847196340561\n",
      "epoch: 21 step: 237, loss is 0.006152857095003128\n",
      "epoch: 21 step: 238, loss is 0.0015040539437904954\n",
      "epoch: 21 step: 239, loss is 0.0018954096594825387\n",
      "epoch: 21 step: 240, loss is 0.0006931537063792348\n",
      "epoch: 21 step: 241, loss is 0.005564019549638033\n",
      "epoch: 21 step: 242, loss is 0.0020140723790973425\n",
      "epoch: 21 step: 243, loss is 0.00013054936425760388\n",
      "epoch: 21 step: 244, loss is 0.011180642060935497\n",
      "epoch: 21 step: 245, loss is 0.005141238681972027\n",
      "epoch: 21 step: 246, loss is 0.02069440856575966\n",
      "epoch: 21 step: 247, loss is 0.001696444465778768\n",
      "epoch: 21 step: 248, loss is 0.000551859091501683\n",
      "epoch: 21 step: 249, loss is 0.10740091651678085\n",
      "epoch: 21 step: 250, loss is 0.011279129423201084\n",
      "epoch: 21 step: 251, loss is 0.028439676389098167\n",
      "epoch: 21 step: 252, loss is 0.005696271546185017\n",
      "epoch: 21 step: 253, loss is 0.00018784347048494965\n",
      "epoch: 21 step: 254, loss is 0.0041257417760789394\n",
      "epoch: 21 step: 255, loss is 0.08745385706424713\n",
      "epoch: 21 step: 256, loss is 0.00016478431643918157\n",
      "epoch: 21 step: 257, loss is 0.0019257324747741222\n",
      "epoch: 21 step: 258, loss is 0.0003051863459404558\n",
      "epoch: 21 step: 259, loss is 0.024290215224027634\n",
      "epoch: 21 step: 260, loss is 0.0012792778434231877\n",
      "epoch: 21 step: 261, loss is 0.0022614621557295322\n",
      "epoch: 21 step: 262, loss is 0.015625430271029472\n",
      "epoch: 21 step: 263, loss is 0.0176340714097023\n",
      "epoch: 21 step: 264, loss is 0.02327081374824047\n",
      "epoch: 21 step: 265, loss is 0.0016128573333844543\n",
      "epoch: 21 step: 266, loss is 0.00013034814037382603\n",
      "epoch: 21 step: 267, loss is 0.001228787936270237\n",
      "epoch: 21 step: 268, loss is 0.0052966647781431675\n",
      "epoch: 21 step: 269, loss is 0.00018732626631390303\n",
      "epoch: 21 step: 270, loss is 0.0005302183562889695\n",
      "epoch: 21 step: 271, loss is 0.012021195143461227\n",
      "epoch: 21 step: 272, loss is 0.0005514146760106087\n",
      "epoch: 21 step: 273, loss is 0.03816339001059532\n",
      "epoch: 21 step: 274, loss is 0.0046548969112336636\n",
      "epoch: 21 step: 275, loss is 0.005537443794310093\n",
      "epoch: 21 step: 276, loss is 0.0026308733504265547\n",
      "epoch: 21 step: 277, loss is 0.016981977969408035\n",
      "epoch: 21 step: 278, loss is 0.0012517375871539116\n",
      "epoch: 21 step: 279, loss is 0.016021298244595528\n",
      "epoch: 21 step: 280, loss is 0.004289553035050631\n",
      "epoch: 21 step: 281, loss is 0.019464580342173576\n",
      "epoch: 21 step: 282, loss is 0.0012180227786302567\n",
      "epoch: 21 step: 283, loss is 0.03568214923143387\n",
      "epoch: 21 step: 284, loss is 0.0005916181835345924\n",
      "epoch: 21 step: 285, loss is 0.0010698710102587938\n",
      "epoch: 21 step: 286, loss is 0.002505818847566843\n",
      "epoch: 21 step: 287, loss is 0.011565730907022953\n",
      "epoch: 21 step: 288, loss is 0.004175823647528887\n",
      "epoch: 21 step: 289, loss is 0.010310358367860317\n",
      "epoch: 21 step: 290, loss is 0.0003001915174536407\n",
      "epoch: 21 step: 291, loss is 0.0038438367191702127\n",
      "epoch: 21 step: 292, loss is 0.004222409799695015\n",
      "epoch: 21 step: 293, loss is 0.011602792888879776\n",
      "epoch: 21 step: 294, loss is 0.0009377459646202624\n",
      "epoch: 21 step: 295, loss is 0.03227724879980087\n",
      "epoch: 21 step: 296, loss is 0.00044098077341914177\n",
      "epoch: 21 step: 297, loss is 0.050375714898109436\n",
      "epoch: 21 step: 298, loss is 0.0025706938467919827\n",
      "epoch: 21 step: 299, loss is 0.011518340557813644\n",
      "epoch: 21 step: 300, loss is 0.017592696473002434\n",
      "epoch: 21 step: 301, loss is 0.003394700586795807\n",
      "epoch: 21 step: 302, loss is 0.00120468158274889\n",
      "epoch: 21 step: 303, loss is 0.0003538505989126861\n",
      "epoch: 21 step: 304, loss is 0.002457439200952649\n",
      "epoch: 21 step: 305, loss is 0.00442536873742938\n",
      "epoch: 21 step: 306, loss is 0.001051645609550178\n",
      "epoch: 21 step: 307, loss is 0.0200442373752594\n",
      "epoch: 21 step: 308, loss is 0.0030820765532553196\n",
      "epoch: 21 step: 309, loss is 0.0009144668583758175\n",
      "epoch: 21 step: 310, loss is 0.00017838885833043605\n",
      "epoch: 21 step: 311, loss is 0.03244606405496597\n",
      "epoch: 21 step: 312, loss is 0.00012901601439807564\n",
      "epoch: 21 step: 313, loss is 0.005776050966233015\n",
      "epoch: 21 step: 314, loss is 0.001590652042068541\n",
      "epoch: 21 step: 315, loss is 0.011440041474997997\n",
      "epoch: 21 step: 316, loss is 0.014726711437106133\n",
      "epoch: 21 step: 317, loss is 0.031146962195634842\n",
      "epoch: 21 step: 318, loss is 0.0014690258540213108\n",
      "epoch: 21 step: 319, loss is 0.0051683844067156315\n",
      "epoch: 21 step: 320, loss is 0.009543489664793015\n",
      "epoch: 21 step: 321, loss is 0.005095596890896559\n",
      "epoch: 21 step: 322, loss is 0.0018614124273881316\n",
      "epoch: 21 step: 323, loss is 0.03637920320034027\n",
      "epoch: 21 step: 324, loss is 0.0006990328547544777\n",
      "epoch: 21 step: 325, loss is 0.04557667300105095\n",
      "epoch: 21 step: 326, loss is 0.019807051867246628\n",
      "epoch: 21 step: 327, loss is 0.0009375754161737859\n",
      "epoch: 21 step: 328, loss is 0.001896789763122797\n",
      "epoch: 21 step: 329, loss is 0.012967676855623722\n",
      "epoch: 21 step: 330, loss is 0.0013663321733474731\n",
      "epoch: 21 step: 331, loss is 0.0017969454638659954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 332, loss is 0.004576666280627251\n",
      "epoch: 21 step: 333, loss is 0.014952198602259159\n",
      "epoch: 21 step: 334, loss is 0.003714378923177719\n",
      "epoch: 21 step: 335, loss is 0.005362344905734062\n",
      "epoch: 21 step: 336, loss is 0.002551804296672344\n",
      "epoch: 21 step: 337, loss is 0.0055785225704312325\n",
      "epoch: 21 step: 338, loss is 0.07054086029529572\n",
      "epoch: 21 step: 339, loss is 0.0009493759134784341\n",
      "epoch: 21 step: 340, loss is 0.02908298186957836\n",
      "epoch: 21 step: 341, loss is 0.07630854099988937\n",
      "epoch: 21 step: 342, loss is 3.1003772164694965e-05\n",
      "epoch: 21 step: 343, loss is 0.0007889122352935374\n",
      "epoch: 21 step: 344, loss is 0.001562035409733653\n",
      "epoch: 21 step: 345, loss is 0.00047637784155085683\n",
      "epoch: 21 step: 346, loss is 0.01594408228993416\n",
      "epoch: 21 step: 347, loss is 0.04370443895459175\n",
      "epoch: 21 step: 348, loss is 0.001013924484141171\n",
      "epoch: 21 step: 349, loss is 0.0003750346368178725\n",
      "epoch: 21 step: 350, loss is 0.025001157075166702\n",
      "epoch: 21 step: 351, loss is 0.0721762403845787\n",
      "epoch: 21 step: 352, loss is 0.0016436497680842876\n",
      "epoch: 21 step: 353, loss is 0.004503126256167889\n",
      "epoch: 21 step: 354, loss is 0.038961514830589294\n",
      "epoch: 21 step: 355, loss is 0.009704298339784145\n",
      "epoch: 21 step: 356, loss is 0.0003808972833212465\n",
      "epoch: 21 step: 357, loss is 0.11450477689504623\n",
      "epoch: 21 step: 358, loss is 0.0007164059206843376\n",
      "epoch: 21 step: 359, loss is 0.003686886979267001\n",
      "epoch: 21 step: 360, loss is 0.03667721152305603\n",
      "epoch: 21 step: 361, loss is 0.008154346607625484\n",
      "epoch: 21 step: 362, loss is 0.03354455530643463\n",
      "epoch: 21 step: 363, loss is 0.0022669609170407057\n",
      "epoch: 21 step: 364, loss is 0.05804523825645447\n",
      "epoch: 21 step: 365, loss is 0.00017471465980634093\n",
      "epoch: 21 step: 366, loss is 0.00545415747910738\n",
      "epoch: 21 step: 367, loss is 0.00044593357597477734\n",
      "epoch: 21 step: 368, loss is 0.0011441902024671435\n",
      "epoch: 21 step: 369, loss is 0.010433630086481571\n",
      "epoch: 21 step: 370, loss is 0.011414190754294395\n",
      "epoch: 21 step: 371, loss is 0.14887414872646332\n",
      "epoch: 21 step: 372, loss is 0.012299382127821445\n",
      "epoch: 21 step: 373, loss is 0.05851622670888901\n",
      "epoch: 21 step: 374, loss is 0.00048280920600518584\n",
      "epoch: 21 step: 375, loss is 0.011911356821656227\n",
      "epoch: 21 step: 376, loss is 0.02827601134777069\n",
      "epoch: 21 step: 377, loss is 0.0024838594254106283\n",
      "epoch: 21 step: 378, loss is 0.002847328782081604\n",
      "epoch: 21 step: 379, loss is 0.0455494225025177\n",
      "epoch: 21 step: 380, loss is 0.002672695554792881\n",
      "epoch: 21 step: 381, loss is 0.03822033479809761\n",
      "epoch: 21 step: 382, loss is 0.13045886158943176\n",
      "epoch: 21 step: 383, loss is 0.003054223954677582\n",
      "epoch: 21 step: 384, loss is 0.012239210307598114\n",
      "epoch: 21 step: 385, loss is 0.004511359613388777\n",
      "epoch: 21 step: 386, loss is 0.02463541552424431\n",
      "epoch: 21 step: 387, loss is 0.010016951709985733\n",
      "epoch: 21 step: 388, loss is 0.06425624340772629\n",
      "epoch: 21 step: 389, loss is 0.010504077188670635\n",
      "epoch: 21 step: 390, loss is 0.10758668184280396\n",
      "epoch: 21 step: 391, loss is 0.0016044701915234327\n",
      "epoch: 21 step: 392, loss is 0.014659594744443893\n",
      "epoch: 21 step: 393, loss is 0.03343215212225914\n",
      "epoch: 21 step: 394, loss is 0.0018306884448975325\n",
      "epoch: 21 step: 395, loss is 0.07510095834732056\n",
      "epoch: 21 step: 396, loss is 0.1486792117357254\n",
      "epoch: 21 step: 397, loss is 0.01371070183813572\n",
      "epoch: 21 step: 398, loss is 0.014902480877935886\n",
      "epoch: 21 step: 399, loss is 0.022429022938013077\n",
      "epoch: 21 step: 400, loss is 0.010523822158575058\n",
      "epoch: 21 step: 401, loss is 0.04616886004805565\n",
      "epoch: 21 step: 402, loss is 0.009919543750584126\n",
      "epoch: 21 step: 403, loss is 0.06008024886250496\n",
      "epoch: 21 step: 404, loss is 0.0005996313993819058\n",
      "epoch: 21 step: 405, loss is 0.005108296405524015\n",
      "epoch: 21 step: 406, loss is 0.0005695180734619498\n",
      "epoch: 21 step: 407, loss is 0.0015639234334230423\n",
      "epoch: 21 step: 408, loss is 0.026122888550162315\n",
      "epoch: 21 step: 409, loss is 0.006271529942750931\n",
      "epoch: 21 step: 410, loss is 0.011780784465372562\n",
      "epoch: 21 step: 411, loss is 0.002130327047780156\n",
      "epoch: 21 step: 412, loss is 0.1274758279323578\n",
      "epoch: 21 step: 413, loss is 0.1649997979402542\n",
      "epoch: 21 step: 414, loss is 0.010632062330842018\n",
      "epoch: 21 step: 415, loss is 0.002420544857159257\n",
      "epoch: 21 step: 416, loss is 0.01586242951452732\n",
      "epoch: 21 step: 417, loss is 0.06295577436685562\n",
      "epoch: 21 step: 418, loss is 0.0035061778035014868\n",
      "epoch: 21 step: 419, loss is 0.009961902163922787\n",
      "epoch: 21 step: 420, loss is 0.005660956725478172\n",
      "epoch: 21 step: 421, loss is 0.004412019159644842\n",
      "epoch: 21 step: 422, loss is 0.0012182398932054639\n",
      "epoch: 21 step: 423, loss is 0.0025688635651022196\n",
      "epoch: 21 step: 424, loss is 0.06088120490312576\n",
      "epoch: 21 step: 425, loss is 0.016960371285676956\n",
      "epoch: 21 step: 426, loss is 0.011963831260800362\n",
      "epoch: 21 step: 427, loss is 0.013198524713516235\n",
      "epoch: 21 step: 428, loss is 0.023424748331308365\n",
      "epoch: 21 step: 429, loss is 0.004116336815059185\n",
      "epoch: 21 step: 430, loss is 0.00586175499483943\n",
      "epoch: 21 step: 431, loss is 0.020216669887304306\n",
      "epoch: 21 step: 432, loss is 0.0310154240578413\n",
      "epoch: 21 step: 433, loss is 0.014544203877449036\n",
      "epoch: 21 step: 434, loss is 0.03956837207078934\n",
      "epoch: 21 step: 435, loss is 0.011310631409287453\n",
      "epoch: 21 step: 436, loss is 0.006754916161298752\n",
      "epoch: 21 step: 437, loss is 0.003392226994037628\n",
      "epoch: 21 step: 438, loss is 0.009530040435492992\n",
      "epoch: 21 step: 439, loss is 0.007442771922796965\n",
      "epoch: 21 step: 440, loss is 0.05477440357208252\n",
      "epoch: 21 step: 441, loss is 0.06105046719312668\n",
      "epoch: 21 step: 442, loss is 0.0029017527122050524\n",
      "epoch: 21 step: 443, loss is 0.0035698784049600363\n",
      "epoch: 21 step: 444, loss is 0.0017915286589413881\n",
      "epoch: 21 step: 445, loss is 0.07211384177207947\n",
      "epoch: 21 step: 446, loss is 0.0026773016434162855\n",
      "epoch: 21 step: 447, loss is 0.00013496649626176804\n",
      "epoch: 21 step: 448, loss is 0.04892421141266823\n",
      "epoch: 21 step: 449, loss is 0.002747914521023631\n",
      "epoch: 21 step: 450, loss is 0.013646205887198448\n",
      "epoch: 21 step: 451, loss is 0.015353173948824406\n",
      "epoch: 21 step: 452, loss is 0.0070207021199166775\n",
      "epoch: 21 step: 453, loss is 0.01843263953924179\n",
      "epoch: 21 step: 454, loss is 0.0005622872849926353\n",
      "epoch: 21 step: 455, loss is 0.05318715423345566\n",
      "epoch: 21 step: 456, loss is 0.00932300090789795\n",
      "epoch: 21 step: 457, loss is 0.01199257466942072\n",
      "epoch: 21 step: 458, loss is 0.015732616186141968\n",
      "epoch: 21 step: 459, loss is 0.0011690183309838176\n",
      "epoch: 21 step: 460, loss is 0.02117057517170906\n",
      "epoch: 21 step: 461, loss is 0.002559300046414137\n",
      "epoch: 21 step: 462, loss is 0.013383436016738415\n",
      "epoch: 21 step: 463, loss is 0.0010556891793385148\n",
      "epoch: 21 step: 464, loss is 0.008233657106757164\n",
      "epoch: 21 step: 465, loss is 0.002846489893272519\n",
      "epoch: 21 step: 466, loss is 0.014393944293260574\n",
      "epoch: 21 step: 467, loss is 0.01705855131149292\n",
      "epoch: 21 step: 468, loss is 0.01317081693559885\n",
      "epoch: 21 step: 469, loss is 0.010486734099686146\n",
      "epoch: 21 step: 470, loss is 0.0029018926434218884\n",
      "epoch: 21 step: 471, loss is 0.002041089115664363\n",
      "epoch: 21 step: 472, loss is 0.00044204661389812827\n",
      "epoch: 21 step: 473, loss is 0.007378534879535437\n",
      "epoch: 21 step: 474, loss is 0.01481315866112709\n",
      "epoch: 21 step: 475, loss is 0.002162024611607194\n",
      "epoch: 21 step: 476, loss is 0.001567695289850235\n",
      "epoch: 21 step: 477, loss is 0.011549782007932663\n",
      "epoch: 21 step: 478, loss is 0.0015210333513095975\n",
      "epoch: 21 step: 479, loss is 0.0012920007575303316\n",
      "epoch: 21 step: 480, loss is 0.0013056587195023894\n",
      "epoch: 21 step: 481, loss is 0.003327005309984088\n",
      "epoch: 21 step: 482, loss is 0.00883382186293602\n",
      "epoch: 21 step: 483, loss is 0.0012704780092462897\n",
      "epoch: 21 step: 484, loss is 0.00398271344602108\n",
      "epoch: 21 step: 485, loss is 0.0011048996821045876\n",
      "epoch: 21 step: 486, loss is 0.0005518842954188585\n",
      "epoch: 21 step: 487, loss is 0.01763525791466236\n",
      "epoch: 21 step: 488, loss is 0.04808662086725235\n",
      "epoch: 21 step: 489, loss is 0.0031752074137330055\n",
      "epoch: 21 step: 490, loss is 0.029139762744307518\n",
      "epoch: 21 step: 491, loss is 0.0032544750720262527\n",
      "epoch: 21 step: 492, loss is 0.0007340847514569759\n",
      "epoch: 21 step: 493, loss is 0.05235934630036354\n",
      "epoch: 21 step: 494, loss is 0.0036264085210859776\n",
      "epoch: 21 step: 495, loss is 0.026097888126969337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 496, loss is 0.00239387433975935\n",
      "epoch: 21 step: 497, loss is 0.15488271415233612\n",
      "epoch: 21 step: 498, loss is 0.0004659218539018184\n",
      "epoch: 21 step: 499, loss is 0.0003445350448600948\n",
      "epoch: 21 step: 500, loss is 0.011572097428143024\n",
      "epoch: 21 step: 501, loss is 0.04703667759895325\n",
      "epoch: 21 step: 502, loss is 0.008279845118522644\n",
      "epoch: 21 step: 503, loss is 0.01930657960474491\n",
      "epoch: 21 step: 504, loss is 0.04182702302932739\n",
      "epoch: 21 step: 505, loss is 0.002448530402034521\n",
      "epoch: 21 step: 506, loss is 0.001450811279937625\n",
      "epoch: 21 step: 507, loss is 0.0004065762914251536\n",
      "epoch: 21 step: 508, loss is 0.01940636709332466\n",
      "epoch: 21 step: 509, loss is 0.02106405422091484\n",
      "epoch: 21 step: 510, loss is 0.0010533600579947233\n",
      "epoch: 21 step: 511, loss is 0.0007520874496549368\n",
      "epoch: 21 step: 512, loss is 0.005461646243929863\n",
      "epoch: 21 step: 513, loss is 0.05700904130935669\n",
      "epoch: 21 step: 514, loss is 0.0027852372732013464\n",
      "epoch: 21 step: 515, loss is 0.03595081344246864\n",
      "epoch: 21 step: 516, loss is 0.006903686560690403\n",
      "epoch: 21 step: 517, loss is 0.014271408319473267\n",
      "epoch: 21 step: 518, loss is 0.0072611053474247456\n",
      "epoch: 21 step: 519, loss is 0.03990626335144043\n",
      "epoch: 21 step: 520, loss is 0.0021243267692625523\n",
      "epoch: 21 step: 521, loss is 0.002739774063229561\n",
      "epoch: 21 step: 522, loss is 0.00024785802816040814\n",
      "epoch: 21 step: 523, loss is 0.0003959703026339412\n",
      "epoch: 21 step: 524, loss is 0.0006690510199405253\n",
      "epoch: 21 step: 525, loss is 0.012564399279654026\n",
      "epoch: 21 step: 526, loss is 0.025079963728785515\n",
      "epoch: 21 step: 527, loss is 0.0157002042979002\n",
      "epoch: 21 step: 528, loss is 0.06707491725683212\n",
      "epoch: 21 step: 529, loss is 0.0034216297790408134\n",
      "epoch: 21 step: 530, loss is 0.0015019385609775782\n",
      "epoch: 21 step: 531, loss is 0.03166395425796509\n",
      "epoch: 21 step: 532, loss is 0.004945043008774519\n",
      "epoch: 21 step: 533, loss is 0.009665925987064838\n",
      "epoch: 21 step: 534, loss is 0.013462679460644722\n",
      "epoch: 21 step: 535, loss is 0.011724987998604774\n",
      "epoch: 21 step: 536, loss is 0.00042351093725301325\n",
      "epoch: 21 step: 537, loss is 0.001714959624223411\n",
      "epoch: 21 step: 538, loss is 0.000652216374874115\n",
      "epoch: 21 step: 539, loss is 0.0024388243909925222\n",
      "epoch: 21 step: 540, loss is 0.006731738336384296\n",
      "epoch: 21 step: 541, loss is 0.005455981474369764\n",
      "epoch: 21 step: 542, loss is 0.031939420849084854\n",
      "epoch: 21 step: 543, loss is 0.00022687559248879552\n",
      "epoch: 21 step: 544, loss is 0.03229830041527748\n",
      "epoch: 21 step: 545, loss is 0.04070651903748512\n",
      "epoch: 21 step: 546, loss is 0.009367143735289574\n",
      "epoch: 21 step: 547, loss is 0.011196243576705456\n",
      "epoch: 21 step: 548, loss is 0.08305849879980087\n",
      "epoch: 21 step: 549, loss is 0.008346498012542725\n",
      "epoch: 21 step: 550, loss is 0.0085817975923419\n",
      "epoch: 21 step: 551, loss is 0.0036128275096416473\n",
      "epoch: 21 step: 552, loss is 0.009577703662216663\n",
      "epoch: 21 step: 553, loss is 0.013539372012019157\n",
      "epoch: 21 step: 554, loss is 0.014086173847317696\n",
      "epoch: 21 step: 555, loss is 0.02911311388015747\n",
      "epoch: 21 step: 556, loss is 0.00029113274649716914\n",
      "epoch: 21 step: 557, loss is 0.04240870475769043\n",
      "epoch: 21 step: 558, loss is 0.029431620612740517\n",
      "epoch: 21 step: 559, loss is 0.19240856170654297\n",
      "epoch: 21 step: 560, loss is 0.011117583140730858\n",
      "epoch: 21 step: 561, loss is 0.05704240873456001\n",
      "epoch: 21 step: 562, loss is 0.028405869379639626\n",
      "epoch: 21 step: 563, loss is 0.019748978316783905\n",
      "epoch: 21 step: 564, loss is 0.0007373916450887918\n",
      "epoch: 21 step: 565, loss is 0.0033025408629328012\n",
      "epoch: 21 step: 566, loss is 0.009519089944660664\n",
      "epoch: 21 step: 567, loss is 0.04282722994685173\n",
      "epoch: 21 step: 568, loss is 0.0002967060136143118\n",
      "epoch: 21 step: 569, loss is 0.0003161781933158636\n",
      "epoch: 21 step: 570, loss is 0.0738741010427475\n",
      "epoch: 21 step: 571, loss is 0.023061107844114304\n",
      "epoch: 21 step: 572, loss is 0.03040621057152748\n",
      "epoch: 21 step: 573, loss is 0.12637075781822205\n",
      "epoch: 21 step: 574, loss is 0.007452421821653843\n",
      "epoch: 21 step: 575, loss is 0.008757432922720909\n",
      "epoch: 21 step: 576, loss is 0.02894815057516098\n",
      "epoch: 21 step: 577, loss is 0.0032610842026770115\n",
      "epoch: 21 step: 578, loss is 0.0007602869882248342\n",
      "epoch: 21 step: 579, loss is 0.0006732593756169081\n",
      "epoch: 21 step: 580, loss is 0.03854701668024063\n",
      "epoch: 21 step: 581, loss is 0.0032782559283077717\n",
      "epoch: 21 step: 582, loss is 0.0012779142707586288\n",
      "epoch: 21 step: 583, loss is 0.04297592118382454\n",
      "epoch: 21 step: 584, loss is 0.03183455392718315\n",
      "epoch: 21 step: 585, loss is 0.0016517281765118241\n",
      "epoch: 21 step: 586, loss is 0.013534520752727985\n",
      "epoch: 21 step: 587, loss is 0.012586531229317188\n",
      "epoch: 21 step: 588, loss is 0.003020858857780695\n",
      "epoch: 21 step: 589, loss is 0.018903560936450958\n",
      "epoch: 21 step: 590, loss is 0.005389222875237465\n",
      "epoch: 21 step: 591, loss is 0.001078827423043549\n",
      "epoch: 21 step: 592, loss is 0.08922982215881348\n",
      "epoch: 21 step: 593, loss is 0.0023629802744835615\n",
      "epoch: 21 step: 594, loss is 0.01845632866024971\n",
      "epoch: 21 step: 595, loss is 0.004763506352901459\n",
      "epoch: 21 step: 596, loss is 0.01785069890320301\n",
      "epoch: 21 step: 597, loss is 0.0059512099251151085\n",
      "epoch: 21 step: 598, loss is 0.01795601099729538\n",
      "epoch: 21 step: 599, loss is 0.020799394696950912\n",
      "epoch: 21 step: 600, loss is 0.005234280601143837\n",
      "epoch: 21 step: 601, loss is 0.007075172383338213\n",
      "epoch: 21 step: 602, loss is 0.02325683832168579\n",
      "epoch: 21 step: 603, loss is 0.023807723075151443\n",
      "epoch: 21 step: 604, loss is 0.0014777348842471838\n",
      "epoch: 21 step: 605, loss is 0.002369397319853306\n",
      "epoch: 21 step: 606, loss is 0.005654555279761553\n",
      "epoch: 21 step: 607, loss is 0.010025386698544025\n",
      "epoch: 21 step: 608, loss is 0.01832258328795433\n",
      "epoch: 21 step: 609, loss is 0.0007387032965198159\n",
      "epoch: 21 step: 610, loss is 0.015901794657111168\n",
      "epoch: 21 step: 611, loss is 0.017156751826405525\n",
      "epoch: 21 step: 612, loss is 0.002107120817527175\n",
      "epoch: 21 step: 613, loss is 0.004634837154299021\n",
      "epoch: 21 step: 614, loss is 0.004951123613864183\n",
      "epoch: 21 step: 615, loss is 0.0077284532599151134\n",
      "epoch: 21 step: 616, loss is 0.001842769794166088\n",
      "epoch: 21 step: 617, loss is 0.005269475281238556\n",
      "epoch: 21 step: 618, loss is 0.005342709366232157\n",
      "epoch: 21 step: 619, loss is 0.0011841939995065331\n",
      "epoch: 21 step: 620, loss is 0.03189524635672569\n",
      "epoch: 21 step: 621, loss is 0.025873424485325813\n",
      "epoch: 21 step: 622, loss is 0.002626515692099929\n",
      "epoch: 21 step: 623, loss is 0.07287107408046722\n",
      "epoch: 21 step: 624, loss is 0.004629221744835377\n",
      "epoch: 21 step: 625, loss is 0.041007909923791885\n",
      "epoch: 21 step: 626, loss is 0.007469286676496267\n",
      "epoch: 21 step: 627, loss is 0.011261227540671825\n",
      "epoch: 21 step: 628, loss is 0.006570655386894941\n",
      "epoch: 21 step: 629, loss is 0.01075761392712593\n",
      "epoch: 21 step: 630, loss is 0.0005896169459447265\n",
      "epoch: 21 step: 631, loss is 0.0033270171843469143\n",
      "epoch: 21 step: 632, loss is 0.032794978469610214\n",
      "epoch: 21 step: 633, loss is 0.038877278566360474\n",
      "epoch: 21 step: 634, loss is 0.015721801668405533\n",
      "epoch: 21 step: 635, loss is 0.02007402665913105\n",
      "epoch: 21 step: 636, loss is 0.021199572831392288\n",
      "epoch: 21 step: 637, loss is 0.003142309607937932\n",
      "epoch: 21 step: 638, loss is 0.017934896051883698\n",
      "epoch: 21 step: 639, loss is 0.0001851600973168388\n",
      "epoch: 21 step: 640, loss is 0.0002828097785823047\n",
      "epoch: 21 step: 641, loss is 0.0007415910949930549\n",
      "epoch: 21 step: 642, loss is 0.0005075070075690746\n",
      "epoch: 21 step: 643, loss is 0.0020125247538089752\n",
      "epoch: 21 step: 644, loss is 0.001005403115414083\n",
      "epoch: 21 step: 645, loss is 0.002446795580908656\n",
      "epoch: 21 step: 646, loss is 0.0006829667836427689\n",
      "epoch: 21 step: 647, loss is 0.009188191033899784\n",
      "epoch: 21 step: 648, loss is 0.015523586422204971\n",
      "epoch: 21 step: 649, loss is 0.004767079837620258\n",
      "epoch: 21 step: 650, loss is 0.00420914264395833\n",
      "epoch: 21 step: 651, loss is 0.0011040853569284081\n",
      "epoch: 21 step: 652, loss is 0.0005638544098474085\n",
      "epoch: 21 step: 653, loss is 0.0002564509923104197\n",
      "epoch: 21 step: 654, loss is 0.004251156933605671\n",
      "epoch: 21 step: 655, loss is 0.0016575236804783344\n",
      "epoch: 21 step: 656, loss is 0.022937573492527008\n",
      "epoch: 21 step: 657, loss is 0.019747180864214897\n",
      "epoch: 21 step: 658, loss is 0.0014950977638363838\n",
      "epoch: 21 step: 659, loss is 0.0007399086025543511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 660, loss is 0.0015970398671925068\n",
      "epoch: 21 step: 661, loss is 0.01406832505017519\n",
      "epoch: 21 step: 662, loss is 0.0019319698913022876\n",
      "epoch: 21 step: 663, loss is 0.008588043972849846\n",
      "epoch: 21 step: 664, loss is 0.0026813738513737917\n",
      "epoch: 21 step: 665, loss is 0.006922568194568157\n",
      "epoch: 21 step: 666, loss is 0.006873915437608957\n",
      "epoch: 21 step: 667, loss is 0.0006637789774686098\n",
      "epoch: 21 step: 668, loss is 0.0036496161483228207\n",
      "epoch: 21 step: 669, loss is 0.0024735666811466217\n",
      "epoch: 21 step: 670, loss is 0.009800119325518608\n",
      "epoch: 21 step: 671, loss is 0.0029029587749391794\n",
      "epoch: 21 step: 672, loss is 0.0005173499812372029\n",
      "epoch: 21 step: 673, loss is 0.0018983155023306608\n",
      "epoch: 21 step: 674, loss is 0.0015630583511665463\n",
      "epoch: 21 step: 675, loss is 0.006800979375839233\n",
      "epoch: 21 step: 676, loss is 0.0008862743270583451\n",
      "epoch: 21 step: 677, loss is 0.008037066087126732\n",
      "epoch: 21 step: 678, loss is 0.0002491569030098617\n",
      "epoch: 21 step: 679, loss is 0.0029808294493705034\n",
      "epoch: 21 step: 680, loss is 0.07041256129741669\n",
      "epoch: 21 step: 681, loss is 0.0077550094574689865\n",
      "epoch: 21 step: 682, loss is 0.0062966737896203995\n",
      "epoch: 21 step: 683, loss is 0.008165723644196987\n",
      "epoch: 21 step: 684, loss is 0.016976676881313324\n",
      "epoch: 21 step: 685, loss is 0.04761426895856857\n",
      "epoch: 21 step: 686, loss is 5.362139199860394e-05\n",
      "epoch: 21 step: 687, loss is 0.07152407616376877\n",
      "epoch: 21 step: 688, loss is 0.005977583583444357\n",
      "epoch: 21 step: 689, loss is 0.015803122892975807\n",
      "epoch: 21 step: 690, loss is 0.034518495202064514\n",
      "epoch: 21 step: 691, loss is 0.0033132967073470354\n",
      "epoch: 21 step: 692, loss is 0.17810401320457458\n",
      "epoch: 21 step: 693, loss is 0.039783768355846405\n",
      "epoch: 21 step: 694, loss is 0.1555020660161972\n",
      "epoch: 21 step: 695, loss is 0.01065034233033657\n",
      "epoch: 21 step: 696, loss is 0.00045526071335189044\n",
      "epoch: 21 step: 697, loss is 0.0015025552129372954\n",
      "epoch: 21 step: 698, loss is 0.0020919926464557648\n",
      "epoch: 21 step: 699, loss is 0.02039291337132454\n",
      "epoch: 21 step: 700, loss is 0.028551682829856873\n",
      "epoch: 21 step: 701, loss is 0.026799276471138\n",
      "epoch: 21 step: 702, loss is 0.09488848596811295\n",
      "epoch: 21 step: 703, loss is 0.04769222065806389\n",
      "epoch: 21 step: 704, loss is 0.012718932703137398\n",
      "epoch: 21 step: 705, loss is 0.029288094490766525\n",
      "epoch: 21 step: 706, loss is 0.004346643574535847\n",
      "epoch: 21 step: 707, loss is 0.10604676604270935\n",
      "epoch: 21 step: 708, loss is 0.05429372936487198\n",
      "epoch: 21 step: 709, loss is 0.0035533045884221792\n",
      "epoch: 21 step: 710, loss is 0.014532345347106457\n",
      "epoch: 21 step: 711, loss is 0.0002224445779575035\n",
      "epoch: 21 step: 712, loss is 0.0013949100393801928\n",
      "epoch: 21 step: 713, loss is 0.0008207773789763451\n",
      "epoch: 21 step: 714, loss is 0.011049900203943253\n",
      "epoch: 21 step: 715, loss is 0.006091226823627949\n",
      "epoch: 21 step: 716, loss is 0.0045031215995550156\n",
      "epoch: 21 step: 717, loss is 0.02216494083404541\n",
      "epoch: 21 step: 718, loss is 0.014836414717137814\n",
      "epoch: 21 step: 719, loss is 0.005279289558529854\n",
      "epoch: 21 step: 720, loss is 0.005170051008462906\n",
      "epoch: 21 step: 721, loss is 0.021263685077428818\n",
      "epoch: 21 step: 722, loss is 0.004890344105660915\n",
      "epoch: 21 step: 723, loss is 0.002020270563662052\n",
      "epoch: 21 step: 724, loss is 0.0017725428333505988\n",
      "epoch: 21 step: 725, loss is 0.010396294295787811\n",
      "epoch: 21 step: 726, loss is 0.00380148203112185\n",
      "epoch: 21 step: 727, loss is 0.010103043168783188\n",
      "epoch: 21 step: 728, loss is 0.00035472854506224394\n",
      "epoch: 21 step: 729, loss is 0.0029234036337584257\n",
      "epoch: 21 step: 730, loss is 0.005517883691936731\n",
      "epoch: 21 step: 731, loss is 0.005274973809719086\n",
      "epoch: 21 step: 732, loss is 0.015129991807043552\n",
      "epoch: 21 step: 733, loss is 0.0038401861675083637\n",
      "epoch: 21 step: 734, loss is 0.0030829550232738256\n",
      "epoch: 21 step: 735, loss is 0.004028167575597763\n",
      "epoch: 21 step: 736, loss is 0.001351202023215592\n",
      "epoch: 21 step: 737, loss is 0.019274121150374413\n",
      "epoch: 21 step: 738, loss is 0.0019253726350143552\n",
      "epoch: 21 step: 739, loss is 0.004801618400961161\n",
      "epoch: 21 step: 740, loss is 0.0002523267758078873\n",
      "epoch: 21 step: 741, loss is 0.008750990964472294\n",
      "epoch: 21 step: 742, loss is 0.0013279779814183712\n",
      "epoch: 21 step: 743, loss is 0.020251309499144554\n",
      "epoch: 21 step: 744, loss is 0.055541347712278366\n",
      "epoch: 21 step: 745, loss is 0.0026481151580810547\n",
      "epoch: 21 step: 746, loss is 0.00021455703245010227\n",
      "epoch: 21 step: 747, loss is 0.0010182608384639025\n",
      "epoch: 21 step: 748, loss is 0.0008120746351778507\n",
      "epoch: 21 step: 749, loss is 0.013735856860876083\n",
      "epoch: 21 step: 750, loss is 0.006516138091683388\n",
      "epoch: 21 step: 751, loss is 0.00014021643437445164\n",
      "epoch: 21 step: 752, loss is 0.1328825205564499\n",
      "epoch: 21 step: 753, loss is 0.007358503993600607\n",
      "epoch: 21 step: 754, loss is 0.0017527624731883407\n",
      "epoch: 21 step: 755, loss is 0.029240867123007774\n",
      "epoch: 21 step: 756, loss is 0.0003420828143134713\n",
      "epoch: 21 step: 757, loss is 0.002922018291428685\n",
      "epoch: 21 step: 758, loss is 0.0009189674165099859\n",
      "epoch: 21 step: 759, loss is 0.0008513874490745366\n",
      "epoch: 21 step: 760, loss is 0.009596149437129498\n",
      "epoch: 21 step: 761, loss is 0.002118601929396391\n",
      "epoch: 21 step: 762, loss is 0.031296178698539734\n",
      "epoch: 21 step: 763, loss is 0.0009750730823725462\n",
      "epoch: 21 step: 764, loss is 0.0019741447176784277\n",
      "epoch: 21 step: 765, loss is 0.008788872510194778\n",
      "epoch: 21 step: 766, loss is 0.010639870539307594\n",
      "epoch: 21 step: 767, loss is 0.00048597261775285006\n",
      "epoch: 21 step: 768, loss is 0.01633642427623272\n",
      "epoch: 21 step: 769, loss is 0.03902996703982353\n",
      "epoch: 21 step: 770, loss is 0.0008550717029720545\n",
      "epoch: 21 step: 771, loss is 0.001252087764441967\n",
      "epoch: 21 step: 772, loss is 0.020541859790682793\n",
      "epoch: 21 step: 773, loss is 0.0028621868696063757\n",
      "epoch: 21 step: 774, loss is 0.1296069473028183\n",
      "epoch: 21 step: 775, loss is 0.00015503517352044582\n",
      "epoch: 21 step: 776, loss is 0.14985911548137665\n",
      "epoch: 21 step: 777, loss is 0.009551683440804482\n",
      "epoch: 21 step: 778, loss is 0.008261417038738728\n",
      "epoch: 21 step: 779, loss is 0.0004886368988081813\n",
      "epoch: 21 step: 780, loss is 0.0008295620791614056\n",
      "epoch: 21 step: 781, loss is 0.006205742247402668\n",
      "epoch: 21 step: 782, loss is 0.02082768827676773\n",
      "epoch: 21 step: 783, loss is 0.0005025509162805974\n",
      "epoch: 21 step: 784, loss is 0.057275500148534775\n",
      "epoch: 21 step: 785, loss is 0.013379601761698723\n",
      "epoch: 21 step: 786, loss is 0.059690210968256\n",
      "epoch: 21 step: 787, loss is 0.0005906176520511508\n",
      "epoch: 21 step: 788, loss is 0.05268416926264763\n",
      "epoch: 21 step: 789, loss is 0.0015836956445127726\n",
      "epoch: 21 step: 790, loss is 0.07567201554775238\n",
      "epoch: 21 step: 791, loss is 0.02870975062251091\n",
      "epoch: 21 step: 792, loss is 0.0027944950852543116\n",
      "epoch: 21 step: 793, loss is 0.0867244228720665\n",
      "epoch: 21 step: 794, loss is 0.04912260174751282\n",
      "epoch: 21 step: 795, loss is 0.05541447177529335\n",
      "epoch: 21 step: 796, loss is 0.0025629557203501463\n",
      "epoch: 21 step: 797, loss is 0.0017676560673862696\n",
      "epoch: 21 step: 798, loss is 0.005272123031318188\n",
      "epoch: 21 step: 799, loss is 0.0013870173133909702\n",
      "epoch: 21 step: 800, loss is 0.026276657357811928\n",
      "epoch: 21 step: 801, loss is 0.06181231141090393\n",
      "epoch: 21 step: 802, loss is 0.0022350396029651165\n",
      "epoch: 21 step: 803, loss is 0.0181655865162611\n",
      "epoch: 21 step: 804, loss is 0.016830025240778923\n",
      "epoch: 21 step: 805, loss is 0.0059722913429141045\n",
      "epoch: 21 step: 806, loss is 0.0096428282558918\n",
      "epoch: 21 step: 807, loss is 0.0016871411353349686\n",
      "epoch: 21 step: 808, loss is 0.03555353730916977\n",
      "epoch: 21 step: 809, loss is 0.027403539046645164\n",
      "epoch: 21 step: 810, loss is 0.13113658130168915\n",
      "epoch: 21 step: 811, loss is 0.010389825329184532\n",
      "epoch: 21 step: 812, loss is 0.008342131972312927\n",
      "epoch: 21 step: 813, loss is 0.0659777969121933\n",
      "epoch: 21 step: 814, loss is 0.0012713903561234474\n",
      "epoch: 21 step: 815, loss is 0.013850084505975246\n",
      "epoch: 21 step: 816, loss is 0.007667030207812786\n",
      "epoch: 21 step: 817, loss is 0.00035926199052482843\n",
      "epoch: 21 step: 818, loss is 1.5284296750905924e-05\n",
      "epoch: 21 step: 819, loss is 0.0006315236096270382\n",
      "epoch: 21 step: 820, loss is 0.0422494113445282\n",
      "epoch: 21 step: 821, loss is 0.00482788635417819\n",
      "epoch: 21 step: 822, loss is 0.005075190681964159\n",
      "epoch: 21 step: 823, loss is 0.005722132511436939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 824, loss is 0.003009422682225704\n",
      "epoch: 21 step: 825, loss is 0.0025305363815277815\n",
      "epoch: 21 step: 826, loss is 0.007560660596936941\n",
      "epoch: 21 step: 827, loss is 0.011682011187076569\n",
      "epoch: 21 step: 828, loss is 0.0033041222486644983\n",
      "epoch: 21 step: 829, loss is 0.01074288971722126\n",
      "epoch: 21 step: 830, loss is 0.0035574217326939106\n",
      "epoch: 21 step: 831, loss is 0.00022396576241590083\n",
      "epoch: 21 step: 832, loss is 0.0022045415826141834\n",
      "epoch: 21 step: 833, loss is 0.0051973070949316025\n",
      "epoch: 21 step: 834, loss is 0.0017822911031544209\n",
      "epoch: 21 step: 835, loss is 0.005661715753376484\n",
      "epoch: 21 step: 836, loss is 0.0007264242158271372\n",
      "epoch: 21 step: 837, loss is 0.0017032601172104478\n",
      "epoch: 21 step: 838, loss is 0.1521131843328476\n",
      "epoch: 21 step: 839, loss is 0.015598710626363754\n",
      "epoch: 21 step: 840, loss is 0.0017663311446085572\n",
      "epoch: 21 step: 841, loss is 0.004250215366482735\n",
      "epoch: 21 step: 842, loss is 0.014332952909171581\n",
      "epoch: 21 step: 843, loss is 0.00714719807729125\n",
      "epoch: 21 step: 844, loss is 0.01501043327152729\n",
      "epoch: 21 step: 845, loss is 0.007240363396704197\n",
      "epoch: 21 step: 846, loss is 0.0010742420563474298\n",
      "epoch: 21 step: 847, loss is 0.0011764378286898136\n",
      "epoch: 21 step: 848, loss is 0.0015506376512348652\n",
      "epoch: 21 step: 849, loss is 0.07074721157550812\n",
      "epoch: 21 step: 850, loss is 0.020145226269960403\n",
      "epoch: 21 step: 851, loss is 0.0039023435674607754\n",
      "epoch: 21 step: 852, loss is 0.05885990336537361\n",
      "epoch: 21 step: 853, loss is 0.0017716273432597518\n",
      "epoch: 21 step: 854, loss is 0.0017952341586351395\n",
      "epoch: 21 step: 855, loss is 0.003814543364569545\n",
      "epoch: 21 step: 856, loss is 0.008293263614177704\n",
      "epoch: 21 step: 857, loss is 0.046457283198833466\n",
      "epoch: 21 step: 858, loss is 0.00019465546938590705\n",
      "epoch: 21 step: 859, loss is 0.005370601080358028\n",
      "epoch: 21 step: 860, loss is 0.0027000494301319122\n",
      "epoch: 21 step: 861, loss is 0.0021286066621541977\n",
      "epoch: 21 step: 862, loss is 0.0240312572568655\n",
      "epoch: 21 step: 863, loss is 0.016811953857541084\n",
      "epoch: 21 step: 864, loss is 0.0017983909929171205\n",
      "epoch: 21 step: 865, loss is 0.014639277942478657\n",
      "epoch: 21 step: 866, loss is 0.0020749829709529877\n",
      "epoch: 21 step: 867, loss is 0.004182043019682169\n",
      "epoch: 21 step: 868, loss is 0.0020291688852012157\n",
      "epoch: 21 step: 869, loss is 0.00028611908783204854\n",
      "epoch: 21 step: 870, loss is 0.00027513998793438077\n",
      "epoch: 21 step: 871, loss is 0.0011167193297296762\n",
      "epoch: 21 step: 872, loss is 0.0025317019317299128\n",
      "epoch: 21 step: 873, loss is 0.0015992153203114867\n",
      "epoch: 21 step: 874, loss is 6.596452294616029e-05\n",
      "epoch: 21 step: 875, loss is 0.00160618731752038\n",
      "epoch: 21 step: 876, loss is 0.005378145724534988\n",
      "epoch: 21 step: 877, loss is 0.0002915003860834986\n",
      "epoch: 21 step: 878, loss is 0.004907082766294479\n",
      "epoch: 21 step: 879, loss is 0.03888166323304176\n",
      "epoch: 21 step: 880, loss is 0.00023665303888265043\n",
      "epoch: 21 step: 881, loss is 0.003645408432930708\n",
      "epoch: 21 step: 882, loss is 0.03434516489505768\n",
      "epoch: 21 step: 883, loss is 0.07659562677145004\n",
      "epoch: 21 step: 884, loss is 0.010479546152055264\n",
      "epoch: 21 step: 885, loss is 0.0003629417042247951\n",
      "epoch: 21 step: 886, loss is 0.004049060400575399\n",
      "epoch: 21 step: 887, loss is 0.010788600891828537\n",
      "epoch: 21 step: 888, loss is 0.07229521125555038\n",
      "epoch: 21 step: 889, loss is 0.001286203507333994\n",
      "epoch: 21 step: 890, loss is 0.0006371979834511876\n",
      "epoch: 21 step: 891, loss is 0.020488053560256958\n",
      "epoch: 21 step: 892, loss is 0.009654641151428223\n",
      "epoch: 21 step: 893, loss is 0.07552079111337662\n",
      "epoch: 21 step: 894, loss is 0.01897718943655491\n",
      "epoch: 21 step: 895, loss is 0.016917360946536064\n",
      "epoch: 21 step: 896, loss is 0.0017760084010660648\n",
      "epoch: 21 step: 897, loss is 0.037745118141174316\n",
      "epoch: 21 step: 898, loss is 0.027515958994627\n",
      "epoch: 21 step: 899, loss is 0.008904478512704372\n",
      "epoch: 21 step: 900, loss is 0.005040340591222048\n",
      "epoch: 21 step: 901, loss is 0.008302511647343636\n",
      "epoch: 21 step: 902, loss is 0.03962169960141182\n",
      "epoch: 21 step: 903, loss is 0.03873910382390022\n",
      "epoch: 21 step: 904, loss is 0.001912604901008308\n",
      "epoch: 21 step: 905, loss is 0.01905723661184311\n",
      "epoch: 21 step: 906, loss is 0.007882322184741497\n",
      "epoch: 21 step: 907, loss is 0.0005387085257098079\n",
      "epoch: 21 step: 908, loss is 0.0024352273903787136\n",
      "epoch: 21 step: 909, loss is 0.0031612697057425976\n",
      "epoch: 21 step: 910, loss is 0.00977408792823553\n",
      "epoch: 21 step: 911, loss is 0.0034902170300483704\n",
      "epoch: 21 step: 912, loss is 0.012403564527630806\n",
      "epoch: 21 step: 913, loss is 0.005329907406121492\n",
      "epoch: 21 step: 914, loss is 0.04978739842772484\n",
      "epoch: 21 step: 915, loss is 0.012285161763429642\n",
      "epoch: 21 step: 916, loss is 0.0008339008782058954\n",
      "epoch: 21 step: 917, loss is 0.0009158211760222912\n",
      "epoch: 21 step: 918, loss is 0.006145785562694073\n",
      "epoch: 21 step: 919, loss is 0.006733222398906946\n",
      "epoch: 21 step: 920, loss is 0.0392490029335022\n",
      "epoch: 21 step: 921, loss is 0.001833821996115148\n",
      "epoch: 21 step: 922, loss is 0.015400545671582222\n",
      "epoch: 21 step: 923, loss is 0.011265158653259277\n",
      "epoch: 21 step: 924, loss is 0.002944646403193474\n",
      "epoch: 21 step: 925, loss is 0.002854543272405863\n",
      "epoch: 21 step: 926, loss is 0.00023604481248185039\n",
      "epoch: 21 step: 927, loss is 0.009386436082422733\n",
      "epoch: 21 step: 928, loss is 4.2316831240896136e-05\n",
      "epoch: 21 step: 929, loss is 0.005037038121372461\n",
      "epoch: 21 step: 930, loss is 0.014205804094672203\n",
      "epoch: 21 step: 931, loss is 0.0013840445317327976\n",
      "epoch: 21 step: 932, loss is 0.024274293333292007\n",
      "epoch: 21 step: 933, loss is 0.0019408257212489843\n",
      "epoch: 21 step: 934, loss is 0.0035170940682291985\n",
      "epoch: 21 step: 935, loss is 0.017497651278972626\n",
      "epoch: 21 step: 936, loss is 0.061758194118738174\n",
      "epoch: 21 step: 937, loss is 0.008804271928966045\n",
      "epoch: 22 step: 1, loss is 0.003588195890188217\n",
      "epoch: 22 step: 2, loss is 0.00016728235641494393\n",
      "epoch: 22 step: 3, loss is 0.0018349061720073223\n",
      "epoch: 22 step: 4, loss is 0.00767365749925375\n",
      "epoch: 22 step: 5, loss is 0.013924186117947102\n",
      "epoch: 22 step: 6, loss is 0.03561994805932045\n",
      "epoch: 22 step: 7, loss is 0.0014267967781051993\n",
      "epoch: 22 step: 8, loss is 0.0012791338376700878\n",
      "epoch: 22 step: 9, loss is 0.05009692907333374\n",
      "epoch: 22 step: 10, loss is 0.0006297960062511265\n",
      "epoch: 22 step: 11, loss is 0.009964201599359512\n",
      "epoch: 22 step: 12, loss is 0.008794181048870087\n",
      "epoch: 22 step: 13, loss is 0.0028950648847967386\n",
      "epoch: 22 step: 14, loss is 0.0006762181292288005\n",
      "epoch: 22 step: 15, loss is 0.0024063284508883953\n",
      "epoch: 22 step: 16, loss is 0.004753176122903824\n",
      "epoch: 22 step: 17, loss is 0.00016178742225747555\n",
      "epoch: 22 step: 18, loss is 0.003558018244802952\n",
      "epoch: 22 step: 19, loss is 0.0017870479496195912\n",
      "epoch: 22 step: 20, loss is 0.0004850724362768233\n",
      "epoch: 22 step: 21, loss is 0.0006561091868206859\n",
      "epoch: 22 step: 22, loss is 0.009687654674053192\n",
      "epoch: 22 step: 23, loss is 0.002063550753518939\n",
      "epoch: 22 step: 24, loss is 0.00018350512254983187\n",
      "epoch: 22 step: 25, loss is 0.006591574288904667\n",
      "epoch: 22 step: 26, loss is 0.0170602984726429\n",
      "epoch: 22 step: 27, loss is 0.032546769827604294\n",
      "epoch: 22 step: 28, loss is 0.002067836932837963\n",
      "epoch: 22 step: 29, loss is 0.003414768259972334\n",
      "epoch: 22 step: 30, loss is 0.00032764102797955275\n",
      "epoch: 22 step: 31, loss is 0.0024857032112777233\n",
      "epoch: 22 step: 32, loss is 0.042010754346847534\n",
      "epoch: 22 step: 33, loss is 0.0008530772756785154\n",
      "epoch: 22 step: 34, loss is 0.00086039281450212\n",
      "epoch: 22 step: 35, loss is 0.028709160163998604\n",
      "epoch: 22 step: 36, loss is 0.0004477706679608673\n",
      "epoch: 22 step: 37, loss is 0.001802673446945846\n",
      "epoch: 22 step: 38, loss is 0.0025002092588692904\n",
      "epoch: 22 step: 39, loss is 0.04344739019870758\n",
      "epoch: 22 step: 40, loss is 0.001258429721929133\n",
      "epoch: 22 step: 41, loss is 0.001171741052530706\n",
      "epoch: 22 step: 42, loss is 0.011371254920959473\n",
      "epoch: 22 step: 43, loss is 0.013773120008409023\n",
      "epoch: 22 step: 44, loss is 0.0009332576300948858\n",
      "epoch: 22 step: 45, loss is 0.0012003048323094845\n",
      "epoch: 22 step: 46, loss is 0.00025530310813337564\n",
      "epoch: 22 step: 47, loss is 0.0008818928035907447\n",
      "epoch: 22 step: 48, loss is 0.00319222966209054\n",
      "epoch: 22 step: 49, loss is 0.020689282566308975\n",
      "epoch: 22 step: 50, loss is 0.03555101528763771\n",
      "epoch: 22 step: 51, loss is 0.024469997733831406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 52, loss is 0.0026992163620889187\n",
      "epoch: 22 step: 53, loss is 0.017255842685699463\n",
      "epoch: 22 step: 54, loss is 0.012730282731354237\n",
      "epoch: 22 step: 55, loss is 0.07210244238376617\n",
      "epoch: 22 step: 56, loss is 0.004994534887373447\n",
      "epoch: 22 step: 57, loss is 0.009454876184463501\n",
      "epoch: 22 step: 58, loss is 0.017715640366077423\n",
      "epoch: 22 step: 59, loss is 0.0030420476105064154\n",
      "epoch: 22 step: 60, loss is 0.0020820372737944126\n",
      "epoch: 22 step: 61, loss is 0.0011111142812296748\n",
      "epoch: 22 step: 62, loss is 0.0001264184684259817\n",
      "epoch: 22 step: 63, loss is 0.005215476267039776\n",
      "epoch: 22 step: 64, loss is 0.005312240682542324\n",
      "epoch: 22 step: 65, loss is 0.0005000443197786808\n",
      "epoch: 22 step: 66, loss is 0.00011891056055901572\n",
      "epoch: 22 step: 67, loss is 0.030458876863121986\n",
      "epoch: 22 step: 68, loss is 0.022538112476468086\n",
      "epoch: 22 step: 69, loss is 0.04243508353829384\n",
      "epoch: 22 step: 70, loss is 0.0005429817829281092\n",
      "epoch: 22 step: 71, loss is 0.0025164380203932524\n",
      "epoch: 22 step: 72, loss is 0.02162335440516472\n",
      "epoch: 22 step: 73, loss is 0.0005248680827207863\n",
      "epoch: 22 step: 74, loss is 0.0024890657514333725\n",
      "epoch: 22 step: 75, loss is 0.0011444151168689132\n",
      "epoch: 22 step: 76, loss is 0.003973769955337048\n",
      "epoch: 22 step: 77, loss is 0.0034009332302957773\n",
      "epoch: 22 step: 78, loss is 0.0012155568692833185\n",
      "epoch: 22 step: 79, loss is 0.029455086216330528\n",
      "epoch: 22 step: 80, loss is 0.001741004642099142\n",
      "epoch: 22 step: 81, loss is 0.014152726158499718\n",
      "epoch: 22 step: 82, loss is 0.07349752634763718\n",
      "epoch: 22 step: 83, loss is 0.010929293930530548\n",
      "epoch: 22 step: 84, loss is 0.023341162130236626\n",
      "epoch: 22 step: 85, loss is 0.016068998724222183\n",
      "epoch: 22 step: 86, loss is 0.0036756594199687243\n",
      "epoch: 22 step: 87, loss is 0.001063990406692028\n",
      "epoch: 22 step: 88, loss is 0.002881239866837859\n",
      "epoch: 22 step: 89, loss is 0.00023030108422972262\n",
      "epoch: 22 step: 90, loss is 0.005047708749771118\n",
      "epoch: 22 step: 91, loss is 0.0034256752114742994\n",
      "epoch: 22 step: 92, loss is 0.008489750325679779\n",
      "epoch: 22 step: 93, loss is 0.01660270430147648\n",
      "epoch: 22 step: 94, loss is 0.003990956582129002\n",
      "epoch: 22 step: 95, loss is 0.014477652497589588\n",
      "epoch: 22 step: 96, loss is 0.0007684038719162345\n",
      "epoch: 22 step: 97, loss is 0.0014246474020183086\n",
      "epoch: 22 step: 98, loss is 0.0004034123267047107\n",
      "epoch: 22 step: 99, loss is 0.020726162940263748\n",
      "epoch: 22 step: 100, loss is 0.0033997558057308197\n",
      "epoch: 22 step: 101, loss is 0.0006015943363308907\n",
      "epoch: 22 step: 102, loss is 0.0003477506688795984\n",
      "epoch: 22 step: 103, loss is 0.01785149984061718\n",
      "epoch: 22 step: 104, loss is 0.020362449809908867\n",
      "epoch: 22 step: 105, loss is 0.03585077449679375\n",
      "epoch: 22 step: 106, loss is 0.06302142888307571\n",
      "epoch: 22 step: 107, loss is 0.003428005613386631\n",
      "epoch: 22 step: 108, loss is 0.0021682879887521267\n",
      "epoch: 22 step: 109, loss is 0.009024076163768768\n",
      "epoch: 22 step: 110, loss is 0.04239193722605705\n",
      "epoch: 22 step: 111, loss is 0.008810905739665031\n",
      "epoch: 22 step: 112, loss is 0.004794578067958355\n",
      "epoch: 22 step: 113, loss is 0.0006958440062589943\n",
      "epoch: 22 step: 114, loss is 0.006928442977368832\n",
      "epoch: 22 step: 115, loss is 0.011404945515096188\n",
      "epoch: 22 step: 116, loss is 0.05102268606424332\n",
      "epoch: 22 step: 117, loss is 0.004335926380008459\n",
      "epoch: 22 step: 118, loss is 0.010549720376729965\n",
      "epoch: 22 step: 119, loss is 0.099736787378788\n",
      "epoch: 22 step: 120, loss is 0.0035349430982023478\n",
      "epoch: 22 step: 121, loss is 0.007728679105639458\n",
      "epoch: 22 step: 122, loss is 0.054194461554288864\n",
      "epoch: 22 step: 123, loss is 0.056450214236974716\n",
      "epoch: 22 step: 124, loss is 0.003072591032832861\n",
      "epoch: 22 step: 125, loss is 0.04079277068376541\n",
      "epoch: 22 step: 126, loss is 0.0726596862077713\n",
      "epoch: 22 step: 127, loss is 0.01071276143193245\n",
      "epoch: 22 step: 128, loss is 0.036915138363838196\n",
      "epoch: 22 step: 129, loss is 0.01840762048959732\n",
      "epoch: 22 step: 130, loss is 0.0007465393282473087\n",
      "epoch: 22 step: 131, loss is 0.0005949191399849951\n",
      "epoch: 22 step: 132, loss is 0.0006572816637344658\n",
      "epoch: 22 step: 133, loss is 0.013080364093184471\n",
      "epoch: 22 step: 134, loss is 0.0035254107788205147\n",
      "epoch: 22 step: 135, loss is 0.0016128402203321457\n",
      "epoch: 22 step: 136, loss is 0.018959322944283485\n",
      "epoch: 22 step: 137, loss is 0.01523219607770443\n",
      "epoch: 22 step: 138, loss is 0.0035695566330105066\n",
      "epoch: 22 step: 139, loss is 0.09721413254737854\n",
      "epoch: 22 step: 140, loss is 0.048040613532066345\n",
      "epoch: 22 step: 141, loss is 6.716695497743785e-05\n",
      "epoch: 22 step: 142, loss is 0.012350107543170452\n",
      "epoch: 22 step: 143, loss is 0.026729440316557884\n",
      "epoch: 22 step: 144, loss is 0.007657324429601431\n",
      "epoch: 22 step: 145, loss is 0.023713858798146248\n",
      "epoch: 22 step: 146, loss is 0.01005615759640932\n",
      "epoch: 22 step: 147, loss is 0.008109010756015778\n",
      "epoch: 22 step: 148, loss is 0.042453013360500336\n",
      "epoch: 22 step: 149, loss is 0.024186505004763603\n",
      "epoch: 22 step: 150, loss is 0.006177632603794336\n",
      "epoch: 22 step: 151, loss is 0.019188394770026207\n",
      "epoch: 22 step: 152, loss is 0.029675006866455078\n",
      "epoch: 22 step: 153, loss is 0.0013586728600785136\n",
      "epoch: 22 step: 154, loss is 0.0007648124592378736\n",
      "epoch: 22 step: 155, loss is 0.0018167763482779264\n",
      "epoch: 22 step: 156, loss is 0.0004850646073464304\n",
      "epoch: 22 step: 157, loss is 0.014372408390045166\n",
      "epoch: 22 step: 158, loss is 0.002783688949421048\n",
      "epoch: 22 step: 159, loss is 0.010498596355319023\n",
      "epoch: 22 step: 160, loss is 0.0009369539329782128\n",
      "epoch: 22 step: 161, loss is 0.0010795624693855643\n",
      "epoch: 22 step: 162, loss is 0.029187466949224472\n",
      "epoch: 22 step: 163, loss is 0.024555208161473274\n",
      "epoch: 22 step: 164, loss is 0.0045769475400447845\n",
      "epoch: 22 step: 165, loss is 0.00020534469513222575\n",
      "epoch: 22 step: 166, loss is 0.015644727274775505\n",
      "epoch: 22 step: 167, loss is 0.027857590466737747\n",
      "epoch: 22 step: 168, loss is 0.0008520501432940364\n",
      "epoch: 22 step: 169, loss is 0.03620894253253937\n",
      "epoch: 22 step: 170, loss is 0.010379080660641193\n",
      "epoch: 22 step: 171, loss is 0.0009417863329872489\n",
      "epoch: 22 step: 172, loss is 0.0072726174257695675\n",
      "epoch: 22 step: 173, loss is 0.001181508065201342\n",
      "epoch: 22 step: 174, loss is 0.0020756886806339025\n",
      "epoch: 22 step: 175, loss is 0.004301553126424551\n",
      "epoch: 22 step: 176, loss is 0.043634410947561264\n",
      "epoch: 22 step: 177, loss is 0.013872801326215267\n",
      "epoch: 22 step: 178, loss is 0.01483375858515501\n",
      "epoch: 22 step: 179, loss is 0.0050723799504339695\n",
      "epoch: 22 step: 180, loss is 0.0038153426721692085\n",
      "epoch: 22 step: 181, loss is 0.0033881142735481262\n",
      "epoch: 22 step: 182, loss is 0.02102229930460453\n",
      "epoch: 22 step: 183, loss is 0.005840759724378586\n",
      "epoch: 22 step: 184, loss is 0.08181694149971008\n",
      "epoch: 22 step: 185, loss is 0.045838840305805206\n",
      "epoch: 22 step: 186, loss is 0.008200899697840214\n",
      "epoch: 22 step: 187, loss is 0.007586099673062563\n",
      "epoch: 22 step: 188, loss is 0.060830600559711456\n",
      "epoch: 22 step: 189, loss is 0.00014298783207777888\n",
      "epoch: 22 step: 190, loss is 0.018747206777334213\n",
      "epoch: 22 step: 191, loss is 0.0017967093735933304\n",
      "epoch: 22 step: 192, loss is 0.004481395706534386\n",
      "epoch: 22 step: 193, loss is 0.004967264365404844\n",
      "epoch: 22 step: 194, loss is 0.05016695335507393\n",
      "epoch: 22 step: 195, loss is 0.019384289160370827\n",
      "epoch: 22 step: 196, loss is 0.01011859904974699\n",
      "epoch: 22 step: 197, loss is 0.0005153380916453898\n",
      "epoch: 22 step: 198, loss is 0.00030145401251502335\n",
      "epoch: 22 step: 199, loss is 0.004250501748174429\n",
      "epoch: 22 step: 200, loss is 0.0018505982588976622\n",
      "epoch: 22 step: 201, loss is 0.00038304482586681843\n",
      "epoch: 22 step: 202, loss is 0.0025694849900901318\n",
      "epoch: 22 step: 203, loss is 0.0065076579339802265\n",
      "epoch: 22 step: 204, loss is 0.012672854587435722\n",
      "epoch: 22 step: 205, loss is 9.207064431393519e-05\n",
      "epoch: 22 step: 206, loss is 0.0019991875160485506\n",
      "epoch: 22 step: 207, loss is 0.0014633428072556853\n",
      "epoch: 22 step: 208, loss is 0.011756902560591698\n",
      "epoch: 22 step: 209, loss is 0.0009778034873306751\n",
      "epoch: 22 step: 210, loss is 0.000914894393645227\n",
      "epoch: 22 step: 211, loss is 0.00431410688906908\n",
      "epoch: 22 step: 212, loss is 0.02537463791668415\n",
      "epoch: 22 step: 213, loss is 0.00037993831210769713\n",
      "epoch: 22 step: 214, loss is 0.0008771976572461426\n",
      "epoch: 22 step: 215, loss is 0.006147811189293861\n",
      "epoch: 22 step: 216, loss is 0.00940218660980463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 217, loss is 0.07756294310092926\n",
      "epoch: 22 step: 218, loss is 0.047019798308610916\n",
      "epoch: 22 step: 219, loss is 0.002912976546213031\n",
      "epoch: 22 step: 220, loss is 0.016069909557700157\n",
      "epoch: 22 step: 221, loss is 0.058975476771593094\n",
      "epoch: 22 step: 222, loss is 0.06777165830135345\n",
      "epoch: 22 step: 223, loss is 0.009695612825453281\n",
      "epoch: 22 step: 224, loss is 0.0337047316133976\n",
      "epoch: 22 step: 225, loss is 0.0005496898083947599\n",
      "epoch: 22 step: 226, loss is 0.0058862995356321335\n",
      "epoch: 22 step: 227, loss is 0.013001879677176476\n",
      "epoch: 22 step: 228, loss is 0.0006283375550992787\n",
      "epoch: 22 step: 229, loss is 0.04147835448384285\n",
      "epoch: 22 step: 230, loss is 0.0015685620019212365\n",
      "epoch: 22 step: 231, loss is 0.029491538181900978\n",
      "epoch: 22 step: 232, loss is 0.00043373744119890034\n",
      "epoch: 22 step: 233, loss is 0.0005814858013764024\n",
      "epoch: 22 step: 234, loss is 0.002557144733145833\n",
      "epoch: 22 step: 235, loss is 0.00472706975415349\n",
      "epoch: 22 step: 236, loss is 0.0017784908413887024\n",
      "epoch: 22 step: 237, loss is 0.0023722974583506584\n",
      "epoch: 22 step: 238, loss is 0.002423363272100687\n",
      "epoch: 22 step: 239, loss is 0.10069803893566132\n",
      "epoch: 22 step: 240, loss is 0.013641237281262875\n",
      "epoch: 22 step: 241, loss is 0.0015149805694818497\n",
      "epoch: 22 step: 242, loss is 0.05091080069541931\n",
      "epoch: 22 step: 243, loss is 0.004048941656947136\n",
      "epoch: 22 step: 244, loss is 0.006261328235268593\n",
      "epoch: 22 step: 245, loss is 0.028269175440073013\n",
      "epoch: 22 step: 246, loss is 0.007695560809224844\n",
      "epoch: 22 step: 247, loss is 0.0017276268918067217\n",
      "epoch: 22 step: 248, loss is 0.016697559505701065\n",
      "epoch: 22 step: 249, loss is 0.0007707264740020037\n",
      "epoch: 22 step: 250, loss is 0.006009633187204599\n",
      "epoch: 22 step: 251, loss is 0.0010147563880309463\n",
      "epoch: 22 step: 252, loss is 0.007411058992147446\n",
      "epoch: 22 step: 253, loss is 0.003121434012427926\n",
      "epoch: 22 step: 254, loss is 6.674997712252662e-05\n",
      "epoch: 22 step: 255, loss is 0.009218637831509113\n",
      "epoch: 22 step: 256, loss is 0.013884277082979679\n",
      "epoch: 22 step: 257, loss is 0.002478210721164942\n",
      "epoch: 22 step: 258, loss is 0.0014062122208997607\n",
      "epoch: 22 step: 259, loss is 0.053168028593063354\n",
      "epoch: 22 step: 260, loss is 0.007245461456477642\n",
      "epoch: 22 step: 261, loss is 0.0028591370210051537\n",
      "epoch: 22 step: 262, loss is 0.004867228213697672\n",
      "epoch: 22 step: 263, loss is 0.0013861993793398142\n",
      "epoch: 22 step: 264, loss is 0.0007876192103140056\n",
      "epoch: 22 step: 265, loss is 0.0010477567557245493\n",
      "epoch: 22 step: 266, loss is 0.023146765306591988\n",
      "epoch: 22 step: 267, loss is 0.01246071420609951\n",
      "epoch: 22 step: 268, loss is 0.0017998450202867389\n",
      "epoch: 22 step: 269, loss is 0.003098342102020979\n",
      "epoch: 22 step: 270, loss is 0.009416441433131695\n",
      "epoch: 22 step: 271, loss is 0.004473777022212744\n",
      "epoch: 22 step: 272, loss is 0.003918217960745096\n",
      "epoch: 22 step: 273, loss is 0.012080555781722069\n",
      "epoch: 22 step: 274, loss is 0.03415108099579811\n",
      "epoch: 22 step: 275, loss is 0.0022558639757335186\n",
      "epoch: 22 step: 276, loss is 0.00040856210398487747\n",
      "epoch: 22 step: 277, loss is 0.0009967131773009896\n",
      "epoch: 22 step: 278, loss is 0.0010682842694222927\n",
      "epoch: 22 step: 279, loss is 0.006692043971270323\n",
      "epoch: 22 step: 280, loss is 0.02090795896947384\n",
      "epoch: 22 step: 281, loss is 0.0036540511064231396\n",
      "epoch: 22 step: 282, loss is 0.0013242183485999703\n",
      "epoch: 22 step: 283, loss is 0.0008001246023923159\n",
      "epoch: 22 step: 284, loss is 0.020538682118058205\n",
      "epoch: 22 step: 285, loss is 0.004285445436835289\n",
      "epoch: 22 step: 286, loss is 0.015662631019949913\n",
      "epoch: 22 step: 287, loss is 0.03847480192780495\n",
      "epoch: 22 step: 288, loss is 0.009902681224048138\n",
      "epoch: 22 step: 289, loss is 0.0002195612614741549\n",
      "epoch: 22 step: 290, loss is 0.013197621330618858\n",
      "epoch: 22 step: 291, loss is 0.02478429675102234\n",
      "epoch: 22 step: 292, loss is 0.0022579245269298553\n",
      "epoch: 22 step: 293, loss is 0.01006715651601553\n",
      "epoch: 22 step: 294, loss is 0.0007061578216962516\n",
      "epoch: 22 step: 295, loss is 0.0013094139285385609\n",
      "epoch: 22 step: 296, loss is 0.0033458785619586706\n",
      "epoch: 22 step: 297, loss is 0.019122878089547157\n",
      "epoch: 22 step: 298, loss is 0.010975141078233719\n",
      "epoch: 22 step: 299, loss is 0.025188462808728218\n",
      "epoch: 22 step: 300, loss is 0.0004129112930968404\n",
      "epoch: 22 step: 301, loss is 0.0009214195888489485\n",
      "epoch: 22 step: 302, loss is 0.0036123725585639477\n",
      "epoch: 22 step: 303, loss is 0.005037815775722265\n",
      "epoch: 22 step: 304, loss is 0.0036410100292414427\n",
      "epoch: 22 step: 305, loss is 0.0036849903408437967\n",
      "epoch: 22 step: 306, loss is 0.002585372421890497\n",
      "epoch: 22 step: 307, loss is 0.0015931344823911786\n",
      "epoch: 22 step: 308, loss is 0.002961683552712202\n",
      "epoch: 22 step: 309, loss is 0.0122520811855793\n",
      "epoch: 22 step: 310, loss is 0.017957057803869247\n",
      "epoch: 22 step: 311, loss is 0.05769100785255432\n",
      "epoch: 22 step: 312, loss is 0.010838129557669163\n",
      "epoch: 22 step: 313, loss is 0.0013683972647413611\n",
      "epoch: 22 step: 314, loss is 0.002780171576887369\n",
      "epoch: 22 step: 315, loss is 0.00031723035499453545\n",
      "epoch: 22 step: 316, loss is 0.002526217605918646\n",
      "epoch: 22 step: 317, loss is 0.004408224951475859\n",
      "epoch: 22 step: 318, loss is 0.003767995862290263\n",
      "epoch: 22 step: 319, loss is 0.0018932681996375322\n",
      "epoch: 22 step: 320, loss is 0.00853490550071001\n",
      "epoch: 22 step: 321, loss is 0.06474599987268448\n",
      "epoch: 22 step: 322, loss is 7.521048246417195e-05\n",
      "epoch: 22 step: 323, loss is 0.0010448778048157692\n",
      "epoch: 22 step: 324, loss is 0.005684268660843372\n",
      "epoch: 22 step: 325, loss is 0.012135292403399944\n",
      "epoch: 22 step: 326, loss is 0.019533712416887283\n",
      "epoch: 22 step: 327, loss is 0.0004719956195913255\n",
      "epoch: 22 step: 328, loss is 0.051524486392736435\n",
      "epoch: 22 step: 329, loss is 0.001978945918381214\n",
      "epoch: 22 step: 330, loss is 0.00010111165465787053\n",
      "epoch: 22 step: 331, loss is 0.00014563126023858786\n",
      "epoch: 22 step: 332, loss is 0.01637803204357624\n",
      "epoch: 22 step: 333, loss is 0.0003913248365279287\n",
      "epoch: 22 step: 334, loss is 0.039545197039842606\n",
      "epoch: 22 step: 335, loss is 0.0017664481420069933\n",
      "epoch: 22 step: 336, loss is 0.025118129327893257\n",
      "epoch: 22 step: 337, loss is 9.703402611194178e-06\n",
      "epoch: 22 step: 338, loss is 0.04587525874376297\n",
      "epoch: 22 step: 339, loss is 0.0008878351072780788\n",
      "epoch: 22 step: 340, loss is 0.04696216061711311\n",
      "epoch: 22 step: 341, loss is 0.00514905946329236\n",
      "epoch: 22 step: 342, loss is 0.017392104491591454\n",
      "epoch: 22 step: 343, loss is 0.09559949487447739\n",
      "epoch: 22 step: 344, loss is 0.010436172597110271\n",
      "epoch: 22 step: 345, loss is 0.0025225828867405653\n",
      "epoch: 22 step: 346, loss is 0.0006706986459903419\n",
      "epoch: 22 step: 347, loss is 0.02865583263337612\n",
      "epoch: 22 step: 348, loss is 0.00952870398759842\n",
      "epoch: 22 step: 349, loss is 0.04975235462188721\n",
      "epoch: 22 step: 350, loss is 0.006770719774067402\n",
      "epoch: 22 step: 351, loss is 0.031014999374747276\n",
      "epoch: 22 step: 352, loss is 0.008083716034889221\n",
      "epoch: 22 step: 353, loss is 0.006123705767095089\n",
      "epoch: 22 step: 354, loss is 0.01174695510417223\n",
      "epoch: 22 step: 355, loss is 0.011462755501270294\n",
      "epoch: 22 step: 356, loss is 0.008115130476653576\n",
      "epoch: 22 step: 357, loss is 0.016279947012662888\n",
      "epoch: 22 step: 358, loss is 0.022712258622050285\n",
      "epoch: 22 step: 359, loss is 0.003956939559429884\n",
      "epoch: 22 step: 360, loss is 0.004103885032236576\n",
      "epoch: 22 step: 361, loss is 0.010183273814618587\n",
      "epoch: 22 step: 362, loss is 0.0011740959016606212\n",
      "epoch: 22 step: 363, loss is 0.012319358065724373\n",
      "epoch: 22 step: 364, loss is 0.0011167270131409168\n",
      "epoch: 22 step: 365, loss is 0.02723734825849533\n",
      "epoch: 22 step: 366, loss is 0.00023061914544086903\n",
      "epoch: 22 step: 367, loss is 0.002125772647559643\n",
      "epoch: 22 step: 368, loss is 0.028029365465044975\n",
      "epoch: 22 step: 369, loss is 0.00015310556045733392\n",
      "epoch: 22 step: 370, loss is 0.007190208416432142\n",
      "epoch: 22 step: 371, loss is 0.001975246472284198\n",
      "epoch: 22 step: 372, loss is 0.004652601666748524\n",
      "epoch: 22 step: 373, loss is 0.043455734848976135\n",
      "epoch: 22 step: 374, loss is 0.06644099950790405\n",
      "epoch: 22 step: 375, loss is 0.007691354025155306\n",
      "epoch: 22 step: 376, loss is 0.03832225129008293\n",
      "epoch: 22 step: 377, loss is 0.005869794171303511\n",
      "epoch: 22 step: 378, loss is 0.002943843137472868\n",
      "epoch: 22 step: 379, loss is 0.02234014868736267\n",
      "epoch: 22 step: 380, loss is 0.004832033533602953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 381, loss is 0.01856546476483345\n",
      "epoch: 22 step: 382, loss is 0.0005628553335554898\n",
      "epoch: 22 step: 383, loss is 7.63217976782471e-05\n",
      "epoch: 22 step: 384, loss is 0.0035268221981823444\n",
      "epoch: 22 step: 385, loss is 0.009176032617688179\n",
      "epoch: 22 step: 386, loss is 0.03838424012064934\n",
      "epoch: 22 step: 387, loss is 0.02935757488012314\n",
      "epoch: 22 step: 388, loss is 0.02868126891553402\n",
      "epoch: 22 step: 389, loss is 0.00018460044520907104\n",
      "epoch: 22 step: 390, loss is 0.002500642091035843\n",
      "epoch: 22 step: 391, loss is 0.017205242067575455\n",
      "epoch: 22 step: 392, loss is 0.0022155591286718845\n",
      "epoch: 22 step: 393, loss is 0.0015708347782492638\n",
      "epoch: 22 step: 394, loss is 0.0007365077035501599\n",
      "epoch: 22 step: 395, loss is 0.003543257713317871\n",
      "epoch: 22 step: 396, loss is 0.0007520060753449798\n",
      "epoch: 22 step: 397, loss is 0.001811285619623959\n",
      "epoch: 22 step: 398, loss is 0.13156819343566895\n",
      "epoch: 22 step: 399, loss is 0.012249484658241272\n",
      "epoch: 22 step: 400, loss is 0.014640327543020248\n",
      "epoch: 22 step: 401, loss is 0.014035233296453953\n",
      "epoch: 22 step: 402, loss is 0.0019318510312587023\n",
      "epoch: 22 step: 403, loss is 0.01464992668479681\n",
      "epoch: 22 step: 404, loss is 0.0008415267220698297\n",
      "epoch: 22 step: 405, loss is 0.024174336344003677\n",
      "epoch: 22 step: 406, loss is 0.01775362528860569\n",
      "epoch: 22 step: 407, loss is 0.0003796774835791439\n",
      "epoch: 22 step: 408, loss is 0.008393237367272377\n",
      "epoch: 22 step: 409, loss is 0.004497007932513952\n",
      "epoch: 22 step: 410, loss is 0.00010502707300474867\n",
      "epoch: 22 step: 411, loss is 0.0010740767465904355\n",
      "epoch: 22 step: 412, loss is 0.09803899377584457\n",
      "epoch: 22 step: 413, loss is 0.03586766868829727\n",
      "epoch: 22 step: 414, loss is 0.0037893413100391626\n",
      "epoch: 22 step: 415, loss is 0.014788996428251266\n",
      "epoch: 22 step: 416, loss is 0.015443390235304832\n",
      "epoch: 22 step: 417, loss is 0.0012598511530086398\n",
      "epoch: 22 step: 418, loss is 0.0006899616564624012\n",
      "epoch: 22 step: 419, loss is 0.03304247930645943\n",
      "epoch: 22 step: 420, loss is 0.0014838369097560644\n",
      "epoch: 22 step: 421, loss is 0.0034673516638576984\n",
      "epoch: 22 step: 422, loss is 0.00015361867554020137\n",
      "epoch: 22 step: 423, loss is 0.0008033940684981644\n",
      "epoch: 22 step: 424, loss is 0.043892424553632736\n",
      "epoch: 22 step: 425, loss is 0.038942139595746994\n",
      "epoch: 22 step: 426, loss is 0.005080523435026407\n",
      "epoch: 22 step: 427, loss is 0.009449293836951256\n",
      "epoch: 22 step: 428, loss is 0.004024685360491276\n",
      "epoch: 22 step: 429, loss is 0.01666821725666523\n",
      "epoch: 22 step: 430, loss is 0.0006829021731391549\n",
      "epoch: 22 step: 431, loss is 0.08077803999185562\n",
      "epoch: 22 step: 432, loss is 0.0003736599173862487\n",
      "epoch: 22 step: 433, loss is 0.004216005094349384\n",
      "epoch: 22 step: 434, loss is 0.0008473222842440009\n",
      "epoch: 22 step: 435, loss is 0.0067203775979578495\n",
      "epoch: 22 step: 436, loss is 0.002167204162105918\n",
      "epoch: 22 step: 437, loss is 0.0012554775457829237\n",
      "epoch: 22 step: 438, loss is 5.3689011110691354e-05\n",
      "epoch: 22 step: 439, loss is 0.04988614097237587\n",
      "epoch: 22 step: 440, loss is 0.011972970329225063\n",
      "epoch: 22 step: 441, loss is 0.026817282661795616\n",
      "epoch: 22 step: 442, loss is 0.0322430357336998\n",
      "epoch: 22 step: 443, loss is 0.0034036003053188324\n",
      "epoch: 22 step: 444, loss is 0.009557079523801804\n",
      "epoch: 22 step: 445, loss is 0.01918347179889679\n",
      "epoch: 22 step: 446, loss is 0.0016999985091388226\n",
      "epoch: 22 step: 447, loss is 0.0005678449524566531\n",
      "epoch: 22 step: 448, loss is 0.0005081521230749786\n",
      "epoch: 22 step: 449, loss is 0.006850803270936012\n",
      "epoch: 22 step: 450, loss is 0.00813659094274044\n",
      "epoch: 22 step: 451, loss is 0.00043102301424369216\n",
      "epoch: 22 step: 452, loss is 0.03382294252514839\n",
      "epoch: 22 step: 453, loss is 0.0013228256721049547\n",
      "epoch: 22 step: 454, loss is 0.019244836643338203\n",
      "epoch: 22 step: 455, loss is 0.015925083309412003\n",
      "epoch: 22 step: 456, loss is 0.00037429085932672024\n",
      "epoch: 22 step: 457, loss is 0.002633923664689064\n",
      "epoch: 22 step: 458, loss is 0.028137030079960823\n",
      "epoch: 22 step: 459, loss is 0.0015641043428331614\n",
      "epoch: 22 step: 460, loss is 0.00018982755136676133\n",
      "epoch: 22 step: 461, loss is 0.02005961909890175\n",
      "epoch: 22 step: 462, loss is 0.006216837093234062\n",
      "epoch: 22 step: 463, loss is 0.04569893330335617\n",
      "epoch: 22 step: 464, loss is 0.006775193847715855\n",
      "epoch: 22 step: 465, loss is 0.016741827130317688\n",
      "epoch: 22 step: 466, loss is 0.03173573687672615\n",
      "epoch: 22 step: 467, loss is 0.0011158703127875924\n",
      "epoch: 22 step: 468, loss is 0.0028472391422837973\n",
      "epoch: 22 step: 469, loss is 0.045127928256988525\n",
      "epoch: 22 step: 470, loss is 0.012490704655647278\n",
      "epoch: 22 step: 471, loss is 0.02285013161599636\n",
      "epoch: 22 step: 472, loss is 0.0017514214850962162\n",
      "epoch: 22 step: 473, loss is 0.00037812491063959897\n",
      "epoch: 22 step: 474, loss is 0.00039952059159986675\n",
      "epoch: 22 step: 475, loss is 0.015744471922516823\n",
      "epoch: 22 step: 476, loss is 0.0018799018580466509\n",
      "epoch: 22 step: 477, loss is 0.037277981638908386\n",
      "epoch: 22 step: 478, loss is 0.013307188637554646\n",
      "epoch: 22 step: 479, loss is 0.0017945431172847748\n",
      "epoch: 22 step: 480, loss is 0.0012870833743363619\n",
      "epoch: 22 step: 481, loss is 0.006905485410243273\n",
      "epoch: 22 step: 482, loss is 0.004876881372183561\n",
      "epoch: 22 step: 483, loss is 0.03724253550171852\n",
      "epoch: 22 step: 484, loss is 0.0712786316871643\n",
      "epoch: 22 step: 485, loss is 0.013851949013769627\n",
      "epoch: 22 step: 486, loss is 0.047643035650253296\n",
      "epoch: 22 step: 487, loss is 6.306128489086404e-05\n",
      "epoch: 22 step: 488, loss is 0.015138981863856316\n",
      "epoch: 22 step: 489, loss is 0.03511171415448189\n",
      "epoch: 22 step: 490, loss is 0.020618999376893044\n",
      "epoch: 22 step: 491, loss is 0.06367424875497818\n",
      "epoch: 22 step: 492, loss is 0.0023449058644473553\n",
      "epoch: 22 step: 493, loss is 0.0006625037640333176\n",
      "epoch: 22 step: 494, loss is 0.016393527388572693\n",
      "epoch: 22 step: 495, loss is 0.001559890923090279\n",
      "epoch: 22 step: 496, loss is 0.014762631617486477\n",
      "epoch: 22 step: 497, loss is 0.0027244091033935547\n",
      "epoch: 22 step: 498, loss is 0.00274890405125916\n",
      "epoch: 22 step: 499, loss is 0.067293182015419\n",
      "epoch: 22 step: 500, loss is 0.022823749110102654\n",
      "epoch: 22 step: 501, loss is 0.08122005313634872\n",
      "epoch: 22 step: 502, loss is 0.2270737588405609\n",
      "epoch: 22 step: 503, loss is 0.012611006386578083\n",
      "epoch: 22 step: 504, loss is 0.05789829418063164\n",
      "epoch: 22 step: 505, loss is 0.00021557213040068746\n",
      "epoch: 22 step: 506, loss is 0.041364796459674835\n",
      "epoch: 22 step: 507, loss is 0.0008674458949826658\n",
      "epoch: 22 step: 508, loss is 0.006051321513950825\n",
      "epoch: 22 step: 509, loss is 0.017537729814648628\n",
      "epoch: 22 step: 510, loss is 0.011896987445652485\n",
      "epoch: 22 step: 511, loss is 0.0005156933912076056\n",
      "epoch: 22 step: 512, loss is 0.02574027143418789\n",
      "epoch: 22 step: 513, loss is 0.00037742446875199676\n",
      "epoch: 22 step: 514, loss is 0.009151454083621502\n",
      "epoch: 22 step: 515, loss is 0.0038334378041327\n",
      "epoch: 22 step: 516, loss is 0.002152377041056752\n",
      "epoch: 22 step: 517, loss is 0.014370063319802284\n",
      "epoch: 22 step: 518, loss is 0.032656121999025345\n",
      "epoch: 22 step: 519, loss is 0.08411569893360138\n",
      "epoch: 22 step: 520, loss is 0.00220255134627223\n",
      "epoch: 22 step: 521, loss is 0.01165278535336256\n",
      "epoch: 22 step: 522, loss is 0.00868886150419712\n",
      "epoch: 22 step: 523, loss is 0.0706511065363884\n",
      "epoch: 22 step: 524, loss is 0.007798492908477783\n",
      "epoch: 22 step: 525, loss is 0.08953741937875748\n",
      "epoch: 22 step: 526, loss is 0.02019328996539116\n",
      "epoch: 22 step: 527, loss is 0.001940968562848866\n",
      "epoch: 22 step: 528, loss is 0.0016529709100723267\n",
      "epoch: 22 step: 529, loss is 0.001838934374973178\n",
      "epoch: 22 step: 530, loss is 0.027827609330415726\n",
      "epoch: 22 step: 531, loss is 0.010378952138125896\n",
      "epoch: 22 step: 532, loss is 0.012748979032039642\n",
      "epoch: 22 step: 533, loss is 0.04407519847154617\n",
      "epoch: 22 step: 534, loss is 0.03829648345708847\n",
      "epoch: 22 step: 535, loss is 0.013337116688489914\n",
      "epoch: 22 step: 536, loss is 0.005388486199080944\n",
      "epoch: 22 step: 537, loss is 0.006984673906117678\n",
      "epoch: 22 step: 538, loss is 0.004616136197000742\n",
      "epoch: 22 step: 539, loss is 0.04587974771857262\n",
      "epoch: 22 step: 540, loss is 0.053827106952667236\n",
      "epoch: 22 step: 541, loss is 0.039240043610334396\n",
      "epoch: 22 step: 542, loss is 0.002860310487449169\n",
      "epoch: 22 step: 543, loss is 0.07916152477264404\n",
      "epoch: 22 step: 544, loss is 0.06841415166854858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 545, loss is 0.024403005838394165\n",
      "epoch: 22 step: 546, loss is 0.024287264794111252\n",
      "epoch: 22 step: 547, loss is 0.003780550789088011\n",
      "epoch: 22 step: 548, loss is 0.0004134378978051245\n",
      "epoch: 22 step: 549, loss is 0.07034371048212051\n",
      "epoch: 22 step: 550, loss is 0.025175029411911964\n",
      "epoch: 22 step: 551, loss is 0.009119880385696888\n",
      "epoch: 22 step: 552, loss is 0.00625773286446929\n",
      "epoch: 22 step: 553, loss is 0.024990743026137352\n",
      "epoch: 22 step: 554, loss is 0.1613481193780899\n",
      "epoch: 22 step: 555, loss is 0.0641632229089737\n",
      "epoch: 22 step: 556, loss is 0.00041157694067806005\n",
      "epoch: 22 step: 557, loss is 0.0013264742447063327\n",
      "epoch: 22 step: 558, loss is 0.003049498423933983\n",
      "epoch: 22 step: 559, loss is 0.00018316731438972056\n",
      "epoch: 22 step: 560, loss is 0.0001018464972730726\n",
      "epoch: 22 step: 561, loss is 0.00837038829922676\n",
      "epoch: 22 step: 562, loss is 0.019344981759786606\n",
      "epoch: 22 step: 563, loss is 0.013943578116595745\n",
      "epoch: 22 step: 564, loss is 0.0931440144777298\n",
      "epoch: 22 step: 565, loss is 0.003912217449396849\n",
      "epoch: 22 step: 566, loss is 0.0007026000530458987\n",
      "epoch: 22 step: 567, loss is 0.008086356334388256\n",
      "epoch: 22 step: 568, loss is 0.13453632593154907\n",
      "epoch: 22 step: 569, loss is 0.055131249129772186\n",
      "epoch: 22 step: 570, loss is 0.014964677393436432\n",
      "epoch: 22 step: 571, loss is 0.01180906593799591\n",
      "epoch: 22 step: 572, loss is 0.0014428086578845978\n",
      "epoch: 22 step: 573, loss is 0.0013456401647999883\n",
      "epoch: 22 step: 574, loss is 0.00432615214958787\n",
      "epoch: 22 step: 575, loss is 0.010804952122271061\n",
      "epoch: 22 step: 576, loss is 0.004900696221739054\n",
      "epoch: 22 step: 577, loss is 0.012841750867664814\n",
      "epoch: 22 step: 578, loss is 0.028252875432372093\n",
      "epoch: 22 step: 579, loss is 0.000622918305452913\n",
      "epoch: 22 step: 580, loss is 0.010212235152721405\n",
      "epoch: 22 step: 581, loss is 0.004890536889433861\n",
      "epoch: 22 step: 582, loss is 0.02339884638786316\n",
      "epoch: 22 step: 583, loss is 0.0026643844321370125\n",
      "epoch: 22 step: 584, loss is 0.0006643602973781526\n",
      "epoch: 22 step: 585, loss is 0.004889336880296469\n",
      "epoch: 22 step: 586, loss is 0.06080314889550209\n",
      "epoch: 22 step: 587, loss is 0.018098611384630203\n",
      "epoch: 22 step: 588, loss is 0.007454267702996731\n",
      "epoch: 22 step: 589, loss is 0.061370883136987686\n",
      "epoch: 22 step: 590, loss is 0.003985120914876461\n",
      "epoch: 22 step: 591, loss is 0.0008112119976431131\n",
      "epoch: 22 step: 592, loss is 0.02782437950372696\n",
      "epoch: 22 step: 593, loss is 0.05004804581403732\n",
      "epoch: 22 step: 594, loss is 0.016606763005256653\n",
      "epoch: 22 step: 595, loss is 0.009025388397276402\n",
      "epoch: 22 step: 596, loss is 0.021729299798607826\n",
      "epoch: 22 step: 597, loss is 0.0005975398235023022\n",
      "epoch: 22 step: 598, loss is 0.0004763099714182317\n",
      "epoch: 22 step: 599, loss is 0.015299086458981037\n",
      "epoch: 22 step: 600, loss is 0.0006990598631091416\n",
      "epoch: 22 step: 601, loss is 0.005237765144556761\n",
      "epoch: 22 step: 602, loss is 0.006200695410370827\n",
      "epoch: 22 step: 603, loss is 0.0004085441178176552\n",
      "epoch: 22 step: 604, loss is 0.0012185410596430302\n",
      "epoch: 22 step: 605, loss is 0.003583003068342805\n",
      "epoch: 22 step: 606, loss is 0.0026659092400223017\n",
      "epoch: 22 step: 607, loss is 0.021647609770298004\n",
      "epoch: 22 step: 608, loss is 0.00035061073140241206\n",
      "epoch: 22 step: 609, loss is 0.023690348491072655\n",
      "epoch: 22 step: 610, loss is 0.05489961802959442\n",
      "epoch: 22 step: 611, loss is 0.010340784676373005\n",
      "epoch: 22 step: 612, loss is 0.011832553893327713\n",
      "epoch: 22 step: 613, loss is 0.004980785772204399\n",
      "epoch: 22 step: 614, loss is 0.0003992784477304667\n",
      "epoch: 22 step: 615, loss is 0.01163063757121563\n",
      "epoch: 22 step: 616, loss is 0.004122234880924225\n",
      "epoch: 22 step: 617, loss is 0.00801070500165224\n",
      "epoch: 22 step: 618, loss is 0.007695139851421118\n",
      "epoch: 22 step: 619, loss is 0.0053252289071679115\n",
      "epoch: 22 step: 620, loss is 0.06619718670845032\n",
      "epoch: 22 step: 621, loss is 0.003355405991896987\n",
      "epoch: 22 step: 622, loss is 0.0005433186306618154\n",
      "epoch: 22 step: 623, loss is 0.09628623723983765\n",
      "epoch: 22 step: 624, loss is 0.010291775688529015\n",
      "epoch: 22 step: 625, loss is 0.004135602619498968\n",
      "epoch: 22 step: 626, loss is 0.01771191507577896\n",
      "epoch: 22 step: 627, loss is 0.0034314990043640137\n",
      "epoch: 22 step: 628, loss is 0.013760216534137726\n",
      "epoch: 22 step: 629, loss is 0.002679397352039814\n",
      "epoch: 22 step: 630, loss is 0.021377254277467728\n",
      "epoch: 22 step: 631, loss is 0.022607237100601196\n",
      "epoch: 22 step: 632, loss is 0.0014670336386188865\n",
      "epoch: 22 step: 633, loss is 0.10026582330465317\n",
      "epoch: 22 step: 634, loss is 0.030270611867308617\n",
      "epoch: 22 step: 635, loss is 0.0031995908357203007\n",
      "epoch: 22 step: 636, loss is 0.002554700244218111\n",
      "epoch: 22 step: 637, loss is 0.014639767818152905\n",
      "epoch: 22 step: 638, loss is 0.0005359495407901704\n",
      "epoch: 22 step: 639, loss is 0.001307943370193243\n",
      "epoch: 22 step: 640, loss is 2.4074450266198255e-05\n",
      "epoch: 22 step: 641, loss is 0.0010119873331859708\n",
      "epoch: 22 step: 642, loss is 0.0016914579318836331\n",
      "epoch: 22 step: 643, loss is 0.0023419582284986973\n",
      "epoch: 22 step: 644, loss is 0.018733326345682144\n",
      "epoch: 22 step: 645, loss is 0.13864348828792572\n",
      "epoch: 22 step: 646, loss is 0.002122101839631796\n",
      "epoch: 22 step: 647, loss is 0.02819342166185379\n",
      "epoch: 22 step: 648, loss is 0.007026792969554663\n",
      "epoch: 22 step: 649, loss is 0.009333573281764984\n",
      "epoch: 22 step: 650, loss is 0.00541200116276741\n",
      "epoch: 22 step: 651, loss is 0.008621346205472946\n",
      "epoch: 22 step: 652, loss is 0.0014722703490406275\n",
      "epoch: 22 step: 653, loss is 0.013126020319759846\n",
      "epoch: 22 step: 654, loss is 0.001483335392549634\n",
      "epoch: 22 step: 655, loss is 0.02420983836054802\n",
      "epoch: 22 step: 656, loss is 0.003123810049146414\n",
      "epoch: 22 step: 657, loss is 0.004960176069289446\n",
      "epoch: 22 step: 658, loss is 0.044149309396743774\n",
      "epoch: 22 step: 659, loss is 0.003975079394876957\n",
      "epoch: 22 step: 660, loss is 0.06049303337931633\n",
      "epoch: 22 step: 661, loss is 0.0040607997216284275\n",
      "epoch: 22 step: 662, loss is 0.0005277422606013715\n",
      "epoch: 22 step: 663, loss is 0.006689314264804125\n",
      "epoch: 22 step: 664, loss is 0.00044781289761886\n",
      "epoch: 22 step: 665, loss is 0.024035722017288208\n",
      "epoch: 22 step: 666, loss is 0.008808841928839684\n",
      "epoch: 22 step: 667, loss is 0.0014318712055683136\n",
      "epoch: 22 step: 668, loss is 0.002704324433580041\n",
      "epoch: 22 step: 669, loss is 0.006063173525035381\n",
      "epoch: 22 step: 670, loss is 0.011160309426486492\n",
      "epoch: 22 step: 671, loss is 0.06735823303461075\n",
      "epoch: 22 step: 672, loss is 0.0029949871823191643\n",
      "epoch: 22 step: 673, loss is 0.07766969501972198\n",
      "epoch: 22 step: 674, loss is 0.002562058623880148\n",
      "epoch: 22 step: 675, loss is 0.06186521425843239\n",
      "epoch: 22 step: 676, loss is 0.06677530705928802\n",
      "epoch: 22 step: 677, loss is 0.045865148305892944\n",
      "epoch: 22 step: 678, loss is 0.028580259531736374\n",
      "epoch: 22 step: 679, loss is 0.02044217474758625\n",
      "epoch: 22 step: 680, loss is 0.003576388582587242\n",
      "epoch: 22 step: 681, loss is 0.0018337744986638427\n",
      "epoch: 22 step: 682, loss is 0.004222486168146133\n",
      "epoch: 22 step: 683, loss is 0.0005745136877521873\n",
      "epoch: 22 step: 684, loss is 0.008212151005864143\n",
      "epoch: 22 step: 685, loss is 0.0005465145804919302\n",
      "epoch: 22 step: 686, loss is 0.005728897172957659\n",
      "epoch: 22 step: 687, loss is 0.04022486135363579\n",
      "epoch: 22 step: 688, loss is 0.052702534943819046\n",
      "epoch: 22 step: 689, loss is 0.008428419008851051\n",
      "epoch: 22 step: 690, loss is 0.004605413880199194\n",
      "epoch: 22 step: 691, loss is 0.018402647227048874\n",
      "epoch: 22 step: 692, loss is 0.007418383378535509\n",
      "epoch: 22 step: 693, loss is 0.014313961379230022\n",
      "epoch: 22 step: 694, loss is 0.006979514379054308\n",
      "epoch: 22 step: 695, loss is 0.0003998584870714694\n",
      "epoch: 22 step: 696, loss is 0.03884214162826538\n",
      "epoch: 22 step: 697, loss is 0.043418653309345245\n",
      "epoch: 22 step: 698, loss is 0.023024125024676323\n",
      "epoch: 22 step: 699, loss is 0.0010685428278520703\n",
      "epoch: 22 step: 700, loss is 0.07753324508666992\n",
      "epoch: 22 step: 701, loss is 0.04254458099603653\n",
      "epoch: 22 step: 702, loss is 0.0008659284212626517\n",
      "epoch: 22 step: 703, loss is 0.11980956792831421\n",
      "epoch: 22 step: 704, loss is 0.0005187899805605412\n",
      "epoch: 22 step: 705, loss is 0.0021149348467588425\n",
      "epoch: 22 step: 706, loss is 0.024623766541481018\n",
      "epoch: 22 step: 707, loss is 0.11444714665412903\n",
      "epoch: 22 step: 708, loss is 0.05308924987912178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 709, loss is 0.007343934383243322\n",
      "epoch: 22 step: 710, loss is 0.006710985209792852\n",
      "epoch: 22 step: 711, loss is 0.0013104485115036368\n",
      "epoch: 22 step: 712, loss is 0.026621125638484955\n",
      "epoch: 22 step: 713, loss is 0.0019013849087059498\n",
      "epoch: 22 step: 714, loss is 0.0010444611543789506\n",
      "epoch: 22 step: 715, loss is 0.007645057048648596\n",
      "epoch: 22 step: 716, loss is 0.01306195929646492\n",
      "epoch: 22 step: 717, loss is 0.010003919713199139\n",
      "epoch: 22 step: 718, loss is 0.0013557204511016607\n",
      "epoch: 22 step: 719, loss is 0.0028595367912203074\n",
      "epoch: 22 step: 720, loss is 0.012585914693772793\n",
      "epoch: 22 step: 721, loss is 0.10862646251916885\n",
      "epoch: 22 step: 722, loss is 0.07391728460788727\n",
      "epoch: 22 step: 723, loss is 0.0012429371709004045\n",
      "epoch: 22 step: 724, loss is 0.04098396375775337\n",
      "epoch: 22 step: 725, loss is 0.024819646030664444\n",
      "epoch: 22 step: 726, loss is 0.07395437359809875\n",
      "epoch: 22 step: 727, loss is 0.05477811023592949\n",
      "epoch: 22 step: 728, loss is 0.019893471151590347\n",
      "epoch: 22 step: 729, loss is 0.000752968539018184\n",
      "epoch: 22 step: 730, loss is 0.0014169466448947787\n",
      "epoch: 22 step: 731, loss is 0.00379717699252069\n",
      "epoch: 22 step: 732, loss is 0.0033514569513499737\n",
      "epoch: 22 step: 733, loss is 0.00047593293129466474\n",
      "epoch: 22 step: 734, loss is 0.084222711622715\n",
      "epoch: 22 step: 735, loss is 0.07747985422611237\n",
      "epoch: 22 step: 736, loss is 0.04458453878760338\n",
      "epoch: 22 step: 737, loss is 0.00342397834174335\n",
      "epoch: 22 step: 738, loss is 0.009492271579802036\n",
      "epoch: 22 step: 739, loss is 0.049848657101392746\n",
      "epoch: 22 step: 740, loss is 0.06763102859258652\n",
      "epoch: 22 step: 741, loss is 0.004733160603791475\n",
      "epoch: 22 step: 742, loss is 0.00490944366902113\n",
      "epoch: 22 step: 743, loss is 0.0033539864234626293\n",
      "epoch: 22 step: 744, loss is 0.0029177735559642315\n",
      "epoch: 22 step: 745, loss is 0.013541236519813538\n",
      "epoch: 22 step: 746, loss is 0.012559900991618633\n",
      "epoch: 22 step: 747, loss is 0.029977332800626755\n",
      "epoch: 22 step: 748, loss is 0.11138976365327835\n",
      "epoch: 22 step: 749, loss is 0.01847291737794876\n",
      "epoch: 22 step: 750, loss is 0.05674133077263832\n",
      "epoch: 22 step: 751, loss is 0.012408863753080368\n",
      "epoch: 22 step: 752, loss is 0.07675912231206894\n",
      "epoch: 22 step: 753, loss is 0.10057960450649261\n",
      "epoch: 22 step: 754, loss is 0.008262877352535725\n",
      "epoch: 22 step: 755, loss is 0.0016727083129808307\n",
      "epoch: 22 step: 756, loss is 0.07105623185634613\n",
      "epoch: 22 step: 757, loss is 0.00035328345256857574\n",
      "epoch: 22 step: 758, loss is 0.022419212386012077\n",
      "epoch: 22 step: 759, loss is 0.04478311911225319\n",
      "epoch: 22 step: 760, loss is 0.00046190948341973126\n",
      "epoch: 22 step: 761, loss is 0.018543925136327744\n",
      "epoch: 22 step: 762, loss is 0.0024128034710884094\n",
      "epoch: 22 step: 763, loss is 0.002168321516364813\n",
      "epoch: 22 step: 764, loss is 0.018737152218818665\n",
      "epoch: 22 step: 765, loss is 0.0008965928573161364\n",
      "epoch: 22 step: 766, loss is 0.012252694927155972\n",
      "epoch: 22 step: 767, loss is 0.0010890742996707559\n",
      "epoch: 22 step: 768, loss is 0.0008522285497747362\n",
      "epoch: 22 step: 769, loss is 0.01653387024998665\n",
      "epoch: 22 step: 770, loss is 0.008102227933704853\n",
      "epoch: 22 step: 771, loss is 0.04594333469867706\n",
      "epoch: 22 step: 772, loss is 0.0010025323135778308\n",
      "epoch: 22 step: 773, loss is 0.012510587461292744\n",
      "epoch: 22 step: 774, loss is 0.0002462194242980331\n",
      "epoch: 22 step: 775, loss is 0.02039310149848461\n",
      "epoch: 22 step: 776, loss is 0.03775724023580551\n",
      "epoch: 22 step: 777, loss is 0.0013072279980406165\n",
      "epoch: 22 step: 778, loss is 0.00025060345069505274\n",
      "epoch: 22 step: 779, loss is 0.0020449080038815737\n",
      "epoch: 22 step: 780, loss is 0.05038731172680855\n",
      "epoch: 22 step: 781, loss is 0.021687384694814682\n",
      "epoch: 22 step: 782, loss is 0.030414478853344917\n",
      "epoch: 22 step: 783, loss is 0.039608120918273926\n",
      "epoch: 22 step: 784, loss is 0.003132822923362255\n",
      "epoch: 22 step: 785, loss is 0.053916387259960175\n",
      "epoch: 22 step: 786, loss is 0.0035712821409106255\n",
      "epoch: 22 step: 787, loss is 5.0038892368320376e-05\n",
      "epoch: 22 step: 788, loss is 0.0031146362889558077\n",
      "epoch: 22 step: 789, loss is 0.0008281468763016164\n",
      "epoch: 22 step: 790, loss is 0.005868630018085241\n",
      "epoch: 22 step: 791, loss is 0.0004998103831894696\n",
      "epoch: 22 step: 792, loss is 0.0014327422250062227\n",
      "epoch: 22 step: 793, loss is 0.006568699609488249\n",
      "epoch: 22 step: 794, loss is 0.0009684116230346262\n",
      "epoch: 22 step: 795, loss is 0.0013331201625987887\n",
      "epoch: 22 step: 796, loss is 0.044607847929000854\n",
      "epoch: 22 step: 797, loss is 0.00916736014187336\n",
      "epoch: 22 step: 798, loss is 0.00018953204562421888\n",
      "epoch: 22 step: 799, loss is 0.003126101801171899\n",
      "epoch: 22 step: 800, loss is 0.00113677850458771\n",
      "epoch: 22 step: 801, loss is 0.032388146966695786\n",
      "epoch: 22 step: 802, loss is 0.00010953671153401956\n",
      "epoch: 22 step: 803, loss is 0.010121096856892109\n",
      "epoch: 22 step: 804, loss is 0.04259922727942467\n",
      "epoch: 22 step: 805, loss is 0.005650039296597242\n",
      "epoch: 22 step: 806, loss is 0.0025368293281644583\n",
      "epoch: 22 step: 807, loss is 0.0062742289155721664\n",
      "epoch: 22 step: 808, loss is 0.024931902065873146\n",
      "epoch: 22 step: 809, loss is 0.0004971266025677323\n",
      "epoch: 22 step: 810, loss is 6.333970668492839e-05\n",
      "epoch: 22 step: 811, loss is 0.0203685499727726\n",
      "epoch: 22 step: 812, loss is 0.05461614951491356\n",
      "epoch: 22 step: 813, loss is 0.00044826639350503683\n",
      "epoch: 22 step: 814, loss is 0.01329395268112421\n",
      "epoch: 22 step: 815, loss is 0.00014643553004134446\n",
      "epoch: 22 step: 816, loss is 0.0032240517903119326\n",
      "epoch: 22 step: 817, loss is 0.004178072325885296\n",
      "epoch: 22 step: 818, loss is 0.0008225747151300311\n",
      "epoch: 22 step: 819, loss is 0.03448459878563881\n",
      "epoch: 22 step: 820, loss is 0.001117622829042375\n",
      "epoch: 22 step: 821, loss is 0.0013542920351028442\n",
      "epoch: 22 step: 822, loss is 0.008629505522549152\n",
      "epoch: 22 step: 823, loss is 0.06377150118350983\n",
      "epoch: 22 step: 824, loss is 0.008152537047863007\n",
      "epoch: 22 step: 825, loss is 0.003990714438259602\n",
      "epoch: 22 step: 826, loss is 0.0003057390567846596\n",
      "epoch: 22 step: 827, loss is 0.08813174068927765\n",
      "epoch: 22 step: 828, loss is 0.06571751832962036\n",
      "epoch: 22 step: 829, loss is 0.004520667716860771\n",
      "epoch: 22 step: 830, loss is 0.003600906580686569\n",
      "epoch: 22 step: 831, loss is 0.022953320294618607\n",
      "epoch: 22 step: 832, loss is 0.0005412214086391032\n",
      "epoch: 22 step: 833, loss is 0.016979917883872986\n",
      "epoch: 22 step: 834, loss is 0.0005622838507406414\n",
      "epoch: 22 step: 835, loss is 0.012466070242226124\n",
      "epoch: 22 step: 836, loss is 0.006075419019907713\n",
      "epoch: 22 step: 837, loss is 0.19133764505386353\n",
      "epoch: 22 step: 838, loss is 0.0018997408915311098\n",
      "epoch: 22 step: 839, loss is 0.001590715255588293\n",
      "epoch: 22 step: 840, loss is 0.007171597331762314\n",
      "epoch: 22 step: 841, loss is 0.03317023441195488\n",
      "epoch: 22 step: 842, loss is 0.05991499871015549\n",
      "epoch: 22 step: 843, loss is 0.0005256165750324726\n",
      "epoch: 22 step: 844, loss is 0.00982271321117878\n",
      "epoch: 22 step: 845, loss is 0.01531333476305008\n",
      "epoch: 22 step: 846, loss is 0.0695481076836586\n",
      "epoch: 22 step: 847, loss is 0.015122661367058754\n",
      "epoch: 22 step: 848, loss is 0.022554993629455566\n",
      "epoch: 22 step: 849, loss is 0.000371503148926422\n",
      "epoch: 22 step: 850, loss is 0.054052747786045074\n",
      "epoch: 22 step: 851, loss is 0.0016859163297340274\n",
      "epoch: 22 step: 852, loss is 0.005592642817646265\n",
      "epoch: 22 step: 853, loss is 0.004893168341368437\n",
      "epoch: 22 step: 854, loss is 0.03966367989778519\n",
      "epoch: 22 step: 855, loss is 0.04077545553445816\n",
      "epoch: 22 step: 856, loss is 0.027678728103637695\n",
      "epoch: 22 step: 857, loss is 0.019365202635526657\n",
      "epoch: 22 step: 858, loss is 0.0013286781031638384\n",
      "epoch: 22 step: 859, loss is 0.004863246809691191\n",
      "epoch: 22 step: 860, loss is 0.006081530824303627\n",
      "epoch: 22 step: 861, loss is 0.025849368423223495\n",
      "epoch: 22 step: 862, loss is 0.014517744071781635\n",
      "epoch: 22 step: 863, loss is 0.005904210731387138\n",
      "epoch: 22 step: 864, loss is 0.032169684767723083\n",
      "epoch: 22 step: 865, loss is 0.0912892073392868\n",
      "epoch: 22 step: 866, loss is 0.07630258798599243\n",
      "epoch: 22 step: 867, loss is 0.004181407857686281\n",
      "epoch: 22 step: 868, loss is 0.0017372521106153727\n",
      "epoch: 22 step: 869, loss is 0.0012487481581047177\n",
      "epoch: 22 step: 870, loss is 0.019953187555074692\n",
      "epoch: 22 step: 871, loss is 0.00965522974729538\n",
      "epoch: 22 step: 872, loss is 0.003123257076367736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 873, loss is 0.0025117911864072084\n",
      "epoch: 22 step: 874, loss is 0.027153169736266136\n",
      "epoch: 22 step: 875, loss is 0.06646072119474411\n",
      "epoch: 22 step: 876, loss is 0.14773766696453094\n",
      "epoch: 22 step: 877, loss is 0.03505543991923332\n",
      "epoch: 22 step: 878, loss is 0.0005910416366532445\n",
      "epoch: 22 step: 879, loss is 0.016161803156137466\n",
      "epoch: 22 step: 880, loss is 0.0017181284492835402\n",
      "epoch: 22 step: 881, loss is 0.0027813503984361887\n",
      "epoch: 22 step: 882, loss is 0.015000048093497753\n",
      "epoch: 22 step: 883, loss is 0.003971651662141085\n",
      "epoch: 22 step: 884, loss is 0.010115642100572586\n",
      "epoch: 22 step: 885, loss is 0.0006535998545587063\n",
      "epoch: 22 step: 886, loss is 0.02365904115140438\n",
      "epoch: 22 step: 887, loss is 0.057154521346092224\n",
      "epoch: 22 step: 888, loss is 0.039924122393131256\n",
      "epoch: 22 step: 889, loss is 0.001108668278902769\n",
      "epoch: 22 step: 890, loss is 0.00130311562679708\n",
      "epoch: 22 step: 891, loss is 0.024921255186200142\n",
      "epoch: 22 step: 892, loss is 0.015107928775250912\n",
      "epoch: 22 step: 893, loss is 0.0019984953105449677\n",
      "epoch: 22 step: 894, loss is 0.004713526926934719\n",
      "epoch: 22 step: 895, loss is 0.0009792178170755506\n",
      "epoch: 22 step: 896, loss is 0.005086620803922415\n",
      "epoch: 22 step: 897, loss is 0.060658350586891174\n",
      "epoch: 22 step: 898, loss is 0.062097709625959396\n",
      "epoch: 22 step: 899, loss is 0.022765446454286575\n",
      "epoch: 22 step: 900, loss is 0.04607792943716049\n",
      "epoch: 22 step: 901, loss is 0.03663140535354614\n",
      "epoch: 22 step: 902, loss is 0.037009187042713165\n",
      "epoch: 22 step: 903, loss is 0.003331752959638834\n",
      "epoch: 22 step: 904, loss is 0.009101673029363155\n",
      "epoch: 22 step: 905, loss is 0.00859674159437418\n",
      "epoch: 22 step: 906, loss is 0.002929002046585083\n",
      "epoch: 22 step: 907, loss is 0.01718747429549694\n",
      "epoch: 22 step: 908, loss is 0.001829840475693345\n",
      "epoch: 22 step: 909, loss is 0.004680075217038393\n",
      "epoch: 22 step: 910, loss is 0.0011472963960841298\n",
      "epoch: 22 step: 911, loss is 0.011826909147202969\n",
      "epoch: 22 step: 912, loss is 0.006586634553968906\n",
      "epoch: 22 step: 913, loss is 0.0011865660781040788\n",
      "epoch: 22 step: 914, loss is 0.00519268773496151\n",
      "epoch: 22 step: 915, loss is 0.0018739223014563322\n",
      "epoch: 22 step: 916, loss is 0.0006738179363310337\n",
      "epoch: 22 step: 917, loss is 0.004004745278507471\n",
      "epoch: 22 step: 918, loss is 0.07151331752538681\n",
      "epoch: 22 step: 919, loss is 0.018184227868914604\n",
      "epoch: 22 step: 920, loss is 0.10632625967264175\n",
      "epoch: 22 step: 921, loss is 0.023691290989518166\n",
      "epoch: 22 step: 922, loss is 0.007385031320154667\n",
      "epoch: 22 step: 923, loss is 0.01024987269192934\n",
      "epoch: 22 step: 924, loss is 0.0004866726230829954\n",
      "epoch: 22 step: 925, loss is 0.001913436339236796\n",
      "epoch: 22 step: 926, loss is 0.0011532894568517804\n",
      "epoch: 22 step: 927, loss is 0.07017102092504501\n",
      "epoch: 22 step: 928, loss is 0.015599170699715614\n",
      "epoch: 22 step: 929, loss is 0.0032729709055274725\n",
      "epoch: 22 step: 930, loss is 0.04042641818523407\n",
      "epoch: 22 step: 931, loss is 0.0010715314419940114\n",
      "epoch: 22 step: 932, loss is 0.06893406808376312\n",
      "epoch: 22 step: 933, loss is 0.0028959158807992935\n",
      "epoch: 22 step: 934, loss is 0.013133576139807701\n",
      "epoch: 22 step: 935, loss is 0.004748036619275808\n",
      "epoch: 22 step: 936, loss is 0.013134065084159374\n",
      "epoch: 22 step: 937, loss is 0.004149206914007664\n",
      "epoch: 23 step: 1, loss is 0.00481892004609108\n",
      "epoch: 23 step: 2, loss is 0.0059822797775268555\n",
      "epoch: 23 step: 3, loss is 0.00041218436672352254\n",
      "epoch: 23 step: 4, loss is 0.014823171310126781\n",
      "epoch: 23 step: 5, loss is 0.005826711189001799\n",
      "epoch: 23 step: 6, loss is 0.004535337444394827\n",
      "epoch: 23 step: 7, loss is 0.01164706889539957\n",
      "epoch: 23 step: 8, loss is 0.0080774687230587\n",
      "epoch: 23 step: 9, loss is 0.0033890390768647194\n",
      "epoch: 23 step: 10, loss is 0.0003338480310048908\n",
      "epoch: 23 step: 11, loss is 0.06891417503356934\n",
      "epoch: 23 step: 12, loss is 0.0007757011917419732\n",
      "epoch: 23 step: 13, loss is 0.00018776138313114643\n",
      "epoch: 23 step: 14, loss is 0.0008746610837988555\n",
      "epoch: 23 step: 15, loss is 0.008337358012795448\n",
      "epoch: 23 step: 16, loss is 0.00037911534309387207\n",
      "epoch: 23 step: 17, loss is 0.02812352403998375\n",
      "epoch: 23 step: 18, loss is 0.0007057407055981457\n",
      "epoch: 23 step: 19, loss is 0.008807815611362457\n",
      "epoch: 23 step: 20, loss is 0.01866944506764412\n",
      "epoch: 23 step: 21, loss is 0.007081638090312481\n",
      "epoch: 23 step: 22, loss is 0.001738606602884829\n",
      "epoch: 23 step: 23, loss is 0.008327349089086056\n",
      "epoch: 23 step: 24, loss is 0.012199364602565765\n",
      "epoch: 23 step: 25, loss is 0.06635165959596634\n",
      "epoch: 23 step: 26, loss is 0.005160861182957888\n",
      "epoch: 23 step: 27, loss is 0.003033617977052927\n",
      "epoch: 23 step: 28, loss is 0.01410038210451603\n",
      "epoch: 23 step: 29, loss is 0.010163484141230583\n",
      "epoch: 23 step: 30, loss is 0.0004466802638489753\n",
      "epoch: 23 step: 31, loss is 0.0003118239692412317\n",
      "epoch: 23 step: 32, loss is 0.0006886458140797913\n",
      "epoch: 23 step: 33, loss is 0.005846330896019936\n",
      "epoch: 23 step: 34, loss is 1.5302755855373107e-05\n",
      "epoch: 23 step: 35, loss is 0.00064519711304456\n",
      "epoch: 23 step: 36, loss is 0.0028216931968927383\n",
      "epoch: 23 step: 37, loss is 0.007114359177649021\n",
      "epoch: 23 step: 38, loss is 0.0472855344414711\n",
      "epoch: 23 step: 39, loss is 0.05062902346253395\n",
      "epoch: 23 step: 40, loss is 0.013854268938302994\n",
      "epoch: 23 step: 41, loss is 0.0005688639939762652\n",
      "epoch: 23 step: 42, loss is 0.001421889872290194\n",
      "epoch: 23 step: 43, loss is 0.00776398740708828\n",
      "epoch: 23 step: 44, loss is 0.0074347080662846565\n",
      "epoch: 23 step: 45, loss is 0.029827211052179337\n",
      "epoch: 23 step: 46, loss is 0.030501410365104675\n",
      "epoch: 23 step: 47, loss is 0.002050256822258234\n",
      "epoch: 23 step: 48, loss is 0.010301736183464527\n",
      "epoch: 23 step: 49, loss is 0.010125313885509968\n",
      "epoch: 23 step: 50, loss is 0.031238511204719543\n",
      "epoch: 23 step: 51, loss is 0.005636883899569511\n",
      "epoch: 23 step: 52, loss is 0.004769453313201666\n",
      "epoch: 23 step: 53, loss is 0.019697224721312523\n",
      "epoch: 23 step: 54, loss is 0.01764034293591976\n",
      "epoch: 23 step: 55, loss is 0.008176160976290703\n",
      "epoch: 23 step: 56, loss is 0.023750433698296547\n",
      "epoch: 23 step: 57, loss is 0.016364002600312233\n",
      "epoch: 23 step: 58, loss is 0.00961893331259489\n",
      "epoch: 23 step: 59, loss is 0.0006231266306713223\n",
      "epoch: 23 step: 60, loss is 0.0015327691799029708\n",
      "epoch: 23 step: 61, loss is 0.015617383643984795\n",
      "epoch: 23 step: 62, loss is 0.0007361025200225413\n",
      "epoch: 23 step: 63, loss is 0.0014725364744663239\n",
      "epoch: 23 step: 64, loss is 0.0006745042628608644\n",
      "epoch: 23 step: 65, loss is 0.0006480853189714253\n",
      "epoch: 23 step: 66, loss is 0.0013214353239163756\n",
      "epoch: 23 step: 67, loss is 0.0018785930005833507\n",
      "epoch: 23 step: 68, loss is 0.0012356971856206656\n",
      "epoch: 23 step: 69, loss is 0.0011110022896900773\n",
      "epoch: 23 step: 70, loss is 5.0619571993593127e-05\n",
      "epoch: 23 step: 71, loss is 0.02143843285739422\n",
      "epoch: 23 step: 72, loss is 0.05447544902563095\n",
      "epoch: 23 step: 73, loss is 0.008712699636816978\n",
      "epoch: 23 step: 74, loss is 0.00047604076098650694\n",
      "epoch: 23 step: 75, loss is 0.0011085233418270946\n",
      "epoch: 23 step: 76, loss is 0.0031574589665979147\n",
      "epoch: 23 step: 77, loss is 0.007866322062909603\n",
      "epoch: 23 step: 78, loss is 0.017777953296899796\n",
      "epoch: 23 step: 79, loss is 0.013710135594010353\n",
      "epoch: 23 step: 80, loss is 0.013843408785760403\n",
      "epoch: 23 step: 81, loss is 3.3007163438014686e-05\n",
      "epoch: 23 step: 82, loss is 0.0028638888616114855\n",
      "epoch: 23 step: 83, loss is 0.0011145116295665503\n",
      "epoch: 23 step: 84, loss is 0.0014145714230835438\n",
      "epoch: 23 step: 85, loss is 0.003377223154529929\n",
      "epoch: 23 step: 86, loss is 0.005793023854494095\n",
      "epoch: 23 step: 87, loss is 0.0002407081046840176\n",
      "epoch: 23 step: 88, loss is 0.005131384823471308\n",
      "epoch: 23 step: 89, loss is 0.003937109373509884\n",
      "epoch: 23 step: 90, loss is 0.000337938719894737\n",
      "epoch: 23 step: 91, loss is 0.040607769042253494\n",
      "epoch: 23 step: 92, loss is 0.0023602305445820093\n",
      "epoch: 23 step: 93, loss is 0.0004752340028062463\n",
      "epoch: 23 step: 94, loss is 0.00018799526151269674\n",
      "epoch: 23 step: 95, loss is 7.898702460806817e-05\n",
      "epoch: 23 step: 96, loss is 0.0011690204264596105\n",
      "epoch: 23 step: 97, loss is 0.002865902381017804\n",
      "epoch: 23 step: 98, loss is 0.0005115302046760917\n",
      "epoch: 23 step: 99, loss is 2.3970118490979075e-05\n",
      "epoch: 23 step: 100, loss is 0.010625666007399559\n",
      "epoch: 23 step: 101, loss is 0.0035126912407577038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 102, loss is 0.014679157175123692\n",
      "epoch: 23 step: 103, loss is 0.0007787860231474042\n",
      "epoch: 23 step: 104, loss is 0.0011556845856830478\n",
      "epoch: 23 step: 105, loss is 0.0023506497964262962\n",
      "epoch: 23 step: 106, loss is 0.00029682632884941995\n",
      "epoch: 23 step: 107, loss is 0.002906324341893196\n",
      "epoch: 23 step: 108, loss is 0.006080255378037691\n",
      "epoch: 23 step: 109, loss is 0.0033901920542120934\n",
      "epoch: 23 step: 110, loss is 0.00238187238574028\n",
      "epoch: 23 step: 111, loss is 0.0004043433873448521\n",
      "epoch: 23 step: 112, loss is 0.003112792270258069\n",
      "epoch: 23 step: 113, loss is 0.0004224600561428815\n",
      "epoch: 23 step: 114, loss is 0.003272033529356122\n",
      "epoch: 23 step: 115, loss is 0.0005372487939894199\n",
      "epoch: 23 step: 116, loss is 0.0024217539466917515\n",
      "epoch: 23 step: 117, loss is 0.01872311532497406\n",
      "epoch: 23 step: 118, loss is 0.016438782215118408\n",
      "epoch: 23 step: 119, loss is 0.017261989414691925\n",
      "epoch: 23 step: 120, loss is 0.0005279781180433929\n",
      "epoch: 23 step: 121, loss is 0.007797408849000931\n",
      "epoch: 23 step: 122, loss is 0.02793583832681179\n",
      "epoch: 23 step: 123, loss is 0.007847175002098083\n",
      "epoch: 23 step: 124, loss is 0.003689670702442527\n",
      "epoch: 23 step: 125, loss is 0.018000435084104538\n",
      "epoch: 23 step: 126, loss is 0.0024380686227232218\n",
      "epoch: 23 step: 127, loss is 0.0003337410162203014\n",
      "epoch: 23 step: 128, loss is 0.014108279719948769\n",
      "epoch: 23 step: 129, loss is 0.0008153854869306087\n",
      "epoch: 23 step: 130, loss is 6.362464046105742e-05\n",
      "epoch: 23 step: 131, loss is 0.040290240198373795\n",
      "epoch: 23 step: 132, loss is 0.0001831313711591065\n",
      "epoch: 23 step: 133, loss is 0.006532194558531046\n",
      "epoch: 23 step: 134, loss is 0.007101054769009352\n",
      "epoch: 23 step: 135, loss is 0.003242375096306205\n",
      "epoch: 23 step: 136, loss is 0.00038883794331923127\n",
      "epoch: 23 step: 137, loss is 0.0017431187443435192\n",
      "epoch: 23 step: 138, loss is 0.0346689447760582\n",
      "epoch: 23 step: 139, loss is 0.0010086538968607783\n",
      "epoch: 23 step: 140, loss is 0.004994827322661877\n",
      "epoch: 23 step: 141, loss is 0.007186941336840391\n",
      "epoch: 23 step: 142, loss is 0.0039098127745091915\n",
      "epoch: 23 step: 143, loss is 0.00029228313360363245\n",
      "epoch: 23 step: 144, loss is 0.022800611332058907\n",
      "epoch: 23 step: 145, loss is 0.09748333692550659\n",
      "epoch: 23 step: 146, loss is 0.005860623903572559\n",
      "epoch: 23 step: 147, loss is 0.014742178842425346\n",
      "epoch: 23 step: 148, loss is 0.020132258534431458\n",
      "epoch: 23 step: 149, loss is 0.036915723234415054\n",
      "epoch: 23 step: 150, loss is 0.08846725523471832\n",
      "epoch: 23 step: 151, loss is 0.013666142709553242\n",
      "epoch: 23 step: 152, loss is 0.010046626441180706\n",
      "epoch: 23 step: 153, loss is 0.03428175672888756\n",
      "epoch: 23 step: 154, loss is 0.003583499463275075\n",
      "epoch: 23 step: 155, loss is 0.0011180005967617035\n",
      "epoch: 23 step: 156, loss is 0.0021969026420265436\n",
      "epoch: 23 step: 157, loss is 0.012844903394579887\n",
      "epoch: 23 step: 158, loss is 0.004410733003169298\n",
      "epoch: 23 step: 159, loss is 0.00263205380178988\n",
      "epoch: 23 step: 160, loss is 0.0020981866400688887\n",
      "epoch: 23 step: 161, loss is 0.0065755536779761314\n",
      "epoch: 23 step: 162, loss is 0.01093302946537733\n",
      "epoch: 23 step: 163, loss is 0.04573548212647438\n",
      "epoch: 23 step: 164, loss is 0.021284358575940132\n",
      "epoch: 23 step: 165, loss is 0.00475947093218565\n",
      "epoch: 23 step: 166, loss is 0.013198604807257652\n",
      "epoch: 23 step: 167, loss is 0.022064417600631714\n",
      "epoch: 23 step: 168, loss is 0.028107674792408943\n",
      "epoch: 23 step: 169, loss is 0.009830501861870289\n",
      "epoch: 23 step: 170, loss is 0.00043742184061557055\n",
      "epoch: 23 step: 171, loss is 0.00233614188618958\n",
      "epoch: 23 step: 172, loss is 0.007363650016486645\n",
      "epoch: 23 step: 173, loss is 0.00011706210352713242\n",
      "epoch: 23 step: 174, loss is 0.003188912756741047\n",
      "epoch: 23 step: 175, loss is 0.002565080299973488\n",
      "epoch: 23 step: 176, loss is 0.0007833075942471623\n",
      "epoch: 23 step: 177, loss is 0.03640410676598549\n",
      "epoch: 23 step: 178, loss is 0.0004907987313345075\n",
      "epoch: 23 step: 179, loss is 0.002186192898079753\n",
      "epoch: 23 step: 180, loss is 0.013009466230869293\n",
      "epoch: 23 step: 181, loss is 0.00020232611859682947\n",
      "epoch: 23 step: 182, loss is 0.010124173015356064\n",
      "epoch: 23 step: 183, loss is 0.00836777314543724\n",
      "epoch: 23 step: 184, loss is 5.08515331603121e-05\n",
      "epoch: 23 step: 185, loss is 0.0003739928943105042\n",
      "epoch: 23 step: 186, loss is 0.0002667118969839066\n",
      "epoch: 23 step: 187, loss is 0.009324299171566963\n",
      "epoch: 23 step: 188, loss is 0.0449506901204586\n",
      "epoch: 23 step: 189, loss is 0.005808556918054819\n",
      "epoch: 23 step: 190, loss is 0.021610412746667862\n",
      "epoch: 23 step: 191, loss is 0.003977758344262838\n",
      "epoch: 23 step: 192, loss is 0.008947345428168774\n",
      "epoch: 23 step: 193, loss is 0.0016497047618031502\n",
      "epoch: 23 step: 194, loss is 0.021846331655979156\n",
      "epoch: 23 step: 195, loss is 0.0015333958435803652\n",
      "epoch: 23 step: 196, loss is 0.009209319949150085\n",
      "epoch: 23 step: 197, loss is 0.0033980607986450195\n",
      "epoch: 23 step: 198, loss is 0.0023972855415195227\n",
      "epoch: 23 step: 199, loss is 0.013267542235553265\n",
      "epoch: 23 step: 200, loss is 0.05922428146004677\n",
      "epoch: 23 step: 201, loss is 0.0029825614765286446\n",
      "epoch: 23 step: 202, loss is 7.280620047822595e-05\n",
      "epoch: 23 step: 203, loss is 0.0003894456895068288\n",
      "epoch: 23 step: 204, loss is 0.008221297524869442\n",
      "epoch: 23 step: 205, loss is 0.0028584932442754507\n",
      "epoch: 23 step: 206, loss is 0.03343198448419571\n",
      "epoch: 23 step: 207, loss is 0.007300482131540775\n",
      "epoch: 23 step: 208, loss is 0.0077395252883434296\n",
      "epoch: 23 step: 209, loss is 0.00030958434217609465\n",
      "epoch: 23 step: 210, loss is 0.001708343275822699\n",
      "epoch: 23 step: 211, loss is 0.004350296687334776\n",
      "epoch: 23 step: 212, loss is 0.011432193219661713\n",
      "epoch: 23 step: 213, loss is 0.0020214098040014505\n",
      "epoch: 23 step: 214, loss is 0.013192006386816502\n",
      "epoch: 23 step: 215, loss is 0.03904468193650246\n",
      "epoch: 23 step: 216, loss is 0.015655294060707092\n",
      "epoch: 23 step: 217, loss is 0.00047259271377697587\n",
      "epoch: 23 step: 218, loss is 0.0005672427942045033\n",
      "epoch: 23 step: 219, loss is 0.0018586977384984493\n",
      "epoch: 23 step: 220, loss is 0.0107133062556386\n",
      "epoch: 23 step: 221, loss is 0.0002924973960034549\n",
      "epoch: 23 step: 222, loss is 0.0011198405409231782\n",
      "epoch: 23 step: 223, loss is 0.007376072928309441\n",
      "epoch: 23 step: 224, loss is 0.0010204389691352844\n",
      "epoch: 23 step: 225, loss is 0.005684551317244768\n",
      "epoch: 23 step: 226, loss is 0.010472946800291538\n",
      "epoch: 23 step: 227, loss is 0.02340276539325714\n",
      "epoch: 23 step: 228, loss is 0.0003664156247396022\n",
      "epoch: 23 step: 229, loss is 0.001413086662068963\n",
      "epoch: 23 step: 230, loss is 0.09383547306060791\n",
      "epoch: 23 step: 231, loss is 0.0072454409673810005\n",
      "epoch: 23 step: 232, loss is 0.0003438029089011252\n",
      "epoch: 23 step: 233, loss is 0.0007980087539181113\n",
      "epoch: 23 step: 234, loss is 2.432448854960967e-05\n",
      "epoch: 23 step: 235, loss is 0.006919936742633581\n",
      "epoch: 23 step: 236, loss is 0.0038854191079735756\n",
      "epoch: 23 step: 237, loss is 9.085977217182517e-05\n",
      "epoch: 23 step: 238, loss is 0.007401075679808855\n",
      "epoch: 23 step: 239, loss is 0.0007029501721262932\n",
      "epoch: 23 step: 240, loss is 0.0164762195199728\n",
      "epoch: 23 step: 241, loss is 0.0053832815028727055\n",
      "epoch: 23 step: 242, loss is 0.000745544908568263\n",
      "epoch: 23 step: 243, loss is 0.0007182650733739138\n",
      "epoch: 23 step: 244, loss is 0.0015948620857670903\n",
      "epoch: 23 step: 245, loss is 0.0005744664231315255\n",
      "epoch: 23 step: 246, loss is 4.832681224797852e-05\n",
      "epoch: 23 step: 247, loss is 0.008340995758771896\n",
      "epoch: 23 step: 248, loss is 0.030403492972254753\n",
      "epoch: 23 step: 249, loss is 0.0058342646807432175\n",
      "epoch: 23 step: 250, loss is 0.0027630040422081947\n",
      "epoch: 23 step: 251, loss is 0.0025342213921248913\n",
      "epoch: 23 step: 252, loss is 0.006045600865036249\n",
      "epoch: 23 step: 253, loss is 4.015690865344368e-05\n",
      "epoch: 23 step: 254, loss is 0.015470487996935844\n",
      "epoch: 23 step: 255, loss is 0.00013701488205697387\n",
      "epoch: 23 step: 256, loss is 0.003601074917241931\n",
      "epoch: 23 step: 257, loss is 0.027732620015740395\n",
      "epoch: 23 step: 258, loss is 0.00020425039110705256\n",
      "epoch: 23 step: 259, loss is 0.000766927725635469\n",
      "epoch: 23 step: 260, loss is 0.003213324351236224\n",
      "epoch: 23 step: 261, loss is 0.07881713658571243\n",
      "epoch: 23 step: 262, loss is 5.795726247015409e-05\n",
      "epoch: 23 step: 263, loss is 0.006293581333011389\n",
      "epoch: 23 step: 264, loss is 0.0007794848061166704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 265, loss is 0.0011653926922008395\n",
      "epoch: 23 step: 266, loss is 0.0007799396989867091\n",
      "epoch: 23 step: 267, loss is 0.020697439089417458\n",
      "epoch: 23 step: 268, loss is 0.027392586693167686\n",
      "epoch: 23 step: 269, loss is 0.00813866313546896\n",
      "epoch: 23 step: 270, loss is 0.0006502745673060417\n",
      "epoch: 23 step: 271, loss is 0.012456384487450123\n",
      "epoch: 23 step: 272, loss is 0.006196372676640749\n",
      "epoch: 23 step: 273, loss is 0.00012678132043220103\n",
      "epoch: 23 step: 274, loss is 0.18390707671642303\n",
      "epoch: 23 step: 275, loss is 0.0024252235889434814\n",
      "epoch: 23 step: 276, loss is 0.017200514674186707\n",
      "epoch: 23 step: 277, loss is 0.0006719744415022433\n",
      "epoch: 23 step: 278, loss is 0.04608888179063797\n",
      "epoch: 23 step: 279, loss is 0.001710813376121223\n",
      "epoch: 23 step: 280, loss is 0.000609962735325098\n",
      "epoch: 23 step: 281, loss is 0.004085599444806576\n",
      "epoch: 23 step: 282, loss is 0.010824709199368954\n",
      "epoch: 23 step: 283, loss is 0.00033710995921865106\n",
      "epoch: 23 step: 284, loss is 0.0025173183530569077\n",
      "epoch: 23 step: 285, loss is 0.017759380862116814\n",
      "epoch: 23 step: 286, loss is 0.011753708124160767\n",
      "epoch: 23 step: 287, loss is 0.08122814446687698\n",
      "epoch: 23 step: 288, loss is 0.008838227950036526\n",
      "epoch: 23 step: 289, loss is 0.001183376181870699\n",
      "epoch: 23 step: 290, loss is 0.052574120461940765\n",
      "epoch: 23 step: 291, loss is 0.03399873524904251\n",
      "epoch: 23 step: 292, loss is 0.0006609461270272732\n",
      "epoch: 23 step: 293, loss is 0.0010587829165160656\n",
      "epoch: 23 step: 294, loss is 0.00022371635714080185\n",
      "epoch: 23 step: 295, loss is 0.004445620812475681\n",
      "epoch: 23 step: 296, loss is 0.0007984262192621827\n",
      "epoch: 23 step: 297, loss is 0.00798766314983368\n",
      "epoch: 23 step: 298, loss is 0.009295320138335228\n",
      "epoch: 23 step: 299, loss is 0.0128471739590168\n",
      "epoch: 23 step: 300, loss is 0.004719374235719442\n",
      "epoch: 23 step: 301, loss is 0.0017305159708485007\n",
      "epoch: 23 step: 302, loss is 0.022616839036345482\n",
      "epoch: 23 step: 303, loss is 0.0403883121907711\n",
      "epoch: 23 step: 304, loss is 0.0015661446377635002\n",
      "epoch: 23 step: 305, loss is 0.0014513584319502115\n",
      "epoch: 23 step: 306, loss is 0.07258999347686768\n",
      "epoch: 23 step: 307, loss is 0.0004173829802311957\n",
      "epoch: 23 step: 308, loss is 0.022805314511060715\n",
      "epoch: 23 step: 309, loss is 0.025273293256759644\n",
      "epoch: 23 step: 310, loss is 0.008158889599144459\n",
      "epoch: 23 step: 311, loss is 0.006873973645269871\n",
      "epoch: 23 step: 312, loss is 0.008782407268881798\n",
      "epoch: 23 step: 313, loss is 0.02092142403125763\n",
      "epoch: 23 step: 314, loss is 0.016163140535354614\n",
      "epoch: 23 step: 315, loss is 0.00018572343105915934\n",
      "epoch: 23 step: 316, loss is 0.0047034528106451035\n",
      "epoch: 23 step: 317, loss is 0.024892469868063927\n",
      "epoch: 23 step: 318, loss is 0.001405684044584632\n",
      "epoch: 23 step: 319, loss is 0.0024598627351224422\n",
      "epoch: 23 step: 320, loss is 0.008369465358555317\n",
      "epoch: 23 step: 321, loss is 0.01635359600186348\n",
      "epoch: 23 step: 322, loss is 0.012897774577140808\n",
      "epoch: 23 step: 323, loss is 0.001313886372372508\n",
      "epoch: 23 step: 324, loss is 0.002282676985487342\n",
      "epoch: 23 step: 325, loss is 0.0005093539948575199\n",
      "epoch: 23 step: 326, loss is 0.006246655248105526\n",
      "epoch: 23 step: 327, loss is 0.005653500556945801\n",
      "epoch: 23 step: 328, loss is 0.06966925412416458\n",
      "epoch: 23 step: 329, loss is 0.016938477754592896\n",
      "epoch: 23 step: 330, loss is 0.05816018953919411\n",
      "epoch: 23 step: 331, loss is 0.007079149596393108\n",
      "epoch: 23 step: 332, loss is 0.0037924835924059153\n",
      "epoch: 23 step: 333, loss is 0.0010547204874455929\n",
      "epoch: 23 step: 334, loss is 0.023353999480605125\n",
      "epoch: 23 step: 335, loss is 0.0021649152040481567\n",
      "epoch: 23 step: 336, loss is 0.0008789211278781295\n",
      "epoch: 23 step: 337, loss is 3.398071203264408e-05\n",
      "epoch: 23 step: 338, loss is 0.001681714435108006\n",
      "epoch: 23 step: 339, loss is 0.004454934503883123\n",
      "epoch: 23 step: 340, loss is 0.012852782383561134\n",
      "epoch: 23 step: 341, loss is 0.0002425137790851295\n",
      "epoch: 23 step: 342, loss is 0.0021301659289747477\n",
      "epoch: 23 step: 343, loss is 0.00016426312504336238\n",
      "epoch: 23 step: 344, loss is 0.0010130126029253006\n",
      "epoch: 23 step: 345, loss is 0.00026574876392260194\n",
      "epoch: 23 step: 346, loss is 0.006809699349105358\n",
      "epoch: 23 step: 347, loss is 0.055973198264837265\n",
      "epoch: 23 step: 348, loss is 0.00046489210217259824\n",
      "epoch: 23 step: 349, loss is 0.007056926377117634\n",
      "epoch: 23 step: 350, loss is 0.012112966738641262\n",
      "epoch: 23 step: 351, loss is 0.0023023271933197975\n",
      "epoch: 23 step: 352, loss is 0.0017314464785158634\n",
      "epoch: 23 step: 353, loss is 0.0021457457914948463\n",
      "epoch: 23 step: 354, loss is 0.004527812357991934\n",
      "epoch: 23 step: 355, loss is 3.555784496711567e-05\n",
      "epoch: 23 step: 356, loss is 0.010932001285254955\n",
      "epoch: 23 step: 357, loss is 0.009237435646355152\n",
      "epoch: 23 step: 358, loss is 0.04549866542220116\n",
      "epoch: 23 step: 359, loss is 0.01938612572848797\n",
      "epoch: 23 step: 360, loss is 0.07118233293294907\n",
      "epoch: 23 step: 361, loss is 0.0015469538047909737\n",
      "epoch: 23 step: 362, loss is 0.006013399921357632\n",
      "epoch: 23 step: 363, loss is 0.005428676027804613\n",
      "epoch: 23 step: 364, loss is 0.015889877453446388\n",
      "epoch: 23 step: 365, loss is 0.00013398882583715022\n",
      "epoch: 23 step: 366, loss is 0.000480659247841686\n",
      "epoch: 23 step: 367, loss is 0.0007554225157946348\n",
      "epoch: 23 step: 368, loss is 0.0060058338567614555\n",
      "epoch: 23 step: 369, loss is 0.006276305299252272\n",
      "epoch: 23 step: 370, loss is 0.1310897022485733\n",
      "epoch: 23 step: 371, loss is 0.0032372765708714724\n",
      "epoch: 23 step: 372, loss is 0.0001807529479265213\n",
      "epoch: 23 step: 373, loss is 0.0008634385885670781\n",
      "epoch: 23 step: 374, loss is 0.011810509487986565\n",
      "epoch: 23 step: 375, loss is 0.0012363712303340435\n",
      "epoch: 23 step: 376, loss is 0.0017194852698594332\n",
      "epoch: 23 step: 377, loss is 0.006969404872506857\n",
      "epoch: 23 step: 378, loss is 0.05374652147293091\n",
      "epoch: 23 step: 379, loss is 0.007695205509662628\n",
      "epoch: 23 step: 380, loss is 0.02882041595876217\n",
      "epoch: 23 step: 381, loss is 0.009421832859516144\n",
      "epoch: 23 step: 382, loss is 0.03290065750479698\n",
      "epoch: 23 step: 383, loss is 0.0031967228278517723\n",
      "epoch: 23 step: 384, loss is 0.007599961012601852\n",
      "epoch: 23 step: 385, loss is 0.0032313638366758823\n",
      "epoch: 23 step: 386, loss is 0.055281106382608414\n",
      "epoch: 23 step: 387, loss is 0.0025042828638106585\n",
      "epoch: 23 step: 388, loss is 0.006986770313233137\n",
      "epoch: 23 step: 389, loss is 2.9154545700293966e-05\n",
      "epoch: 23 step: 390, loss is 0.0019311989890411496\n",
      "epoch: 23 step: 391, loss is 0.0021171432454138994\n",
      "epoch: 23 step: 392, loss is 0.023730214685201645\n",
      "epoch: 23 step: 393, loss is 0.000917547382414341\n",
      "epoch: 23 step: 394, loss is 0.0010941284708678722\n",
      "epoch: 23 step: 395, loss is 0.00326801766641438\n",
      "epoch: 23 step: 396, loss is 0.0010498457122594118\n",
      "epoch: 23 step: 397, loss is 0.00396285206079483\n",
      "epoch: 23 step: 398, loss is 3.754482895601541e-05\n",
      "epoch: 23 step: 399, loss is 0.06468544900417328\n",
      "epoch: 23 step: 400, loss is 0.0014856485649943352\n",
      "epoch: 23 step: 401, loss is 0.012657864019274712\n",
      "epoch: 23 step: 402, loss is 0.01757224276661873\n",
      "epoch: 23 step: 403, loss is 0.03775971755385399\n",
      "epoch: 23 step: 404, loss is 0.0060806600376963615\n",
      "epoch: 23 step: 405, loss is 0.026033634319901466\n",
      "epoch: 23 step: 406, loss is 0.07540907710790634\n",
      "epoch: 23 step: 407, loss is 0.0004734089015983045\n",
      "epoch: 23 step: 408, loss is 0.051033828407526016\n",
      "epoch: 23 step: 409, loss is 9.671821317169815e-06\n",
      "epoch: 23 step: 410, loss is 0.0013675629161298275\n",
      "epoch: 23 step: 411, loss is 0.02803519368171692\n",
      "epoch: 23 step: 412, loss is 0.011346927843987942\n",
      "epoch: 23 step: 413, loss is 0.004125322215259075\n",
      "epoch: 23 step: 414, loss is 0.026459552347660065\n",
      "epoch: 23 step: 415, loss is 0.0014856199268251657\n",
      "epoch: 23 step: 416, loss is 0.013117095455527306\n",
      "epoch: 23 step: 417, loss is 0.006856248248368502\n",
      "epoch: 23 step: 418, loss is 0.0013015858130529523\n",
      "epoch: 23 step: 419, loss is 0.0075236535631120205\n",
      "epoch: 23 step: 420, loss is 0.003749720985069871\n",
      "epoch: 23 step: 421, loss is 0.0003987112722825259\n",
      "epoch: 23 step: 422, loss is 0.008347215130925179\n",
      "epoch: 23 step: 423, loss is 0.00019820677698589861\n",
      "epoch: 23 step: 424, loss is 0.0008768453844822943\n",
      "epoch: 23 step: 425, loss is 0.023439999669790268\n",
      "epoch: 23 step: 426, loss is 0.008047589100897312\n",
      "epoch: 23 step: 427, loss is 0.007349926512688398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 428, loss is 0.008042866364121437\n",
      "epoch: 23 step: 429, loss is 0.0001081718728528358\n",
      "epoch: 23 step: 430, loss is 0.027899140492081642\n",
      "epoch: 23 step: 431, loss is 0.006273304112255573\n",
      "epoch: 23 step: 432, loss is 0.0018185714725404978\n",
      "epoch: 23 step: 433, loss is 0.0004227914323564619\n",
      "epoch: 23 step: 434, loss is 0.0006461785524152219\n",
      "epoch: 23 step: 435, loss is 0.008789933286607265\n",
      "epoch: 23 step: 436, loss is 0.014636680483818054\n",
      "epoch: 23 step: 437, loss is 0.009058619849383831\n",
      "epoch: 23 step: 438, loss is 1.5288211216102354e-05\n",
      "epoch: 23 step: 439, loss is 0.0014282491756603122\n",
      "epoch: 23 step: 440, loss is 0.0003660910006146878\n",
      "epoch: 23 step: 441, loss is 0.004928285256028175\n",
      "epoch: 23 step: 442, loss is 0.053344108164310455\n",
      "epoch: 23 step: 443, loss is 0.0003282177203800529\n",
      "epoch: 23 step: 444, loss is 0.0010501736542209983\n",
      "epoch: 23 step: 445, loss is 4.284600800019689e-05\n",
      "epoch: 23 step: 446, loss is 0.038489047437906265\n",
      "epoch: 23 step: 447, loss is 0.0027546980418264866\n",
      "epoch: 23 step: 448, loss is 0.00041676597902551293\n",
      "epoch: 23 step: 449, loss is 0.02452545426785946\n",
      "epoch: 23 step: 450, loss is 0.0015158546157181263\n",
      "epoch: 23 step: 451, loss is 0.03237588703632355\n",
      "epoch: 23 step: 452, loss is 0.00020799024787265807\n",
      "epoch: 23 step: 453, loss is 0.033083245158195496\n",
      "epoch: 23 step: 454, loss is 0.0007983499672263861\n",
      "epoch: 23 step: 455, loss is 0.03703417256474495\n",
      "epoch: 23 step: 456, loss is 0.013230910524725914\n",
      "epoch: 23 step: 457, loss is 0.03029760532081127\n",
      "epoch: 23 step: 458, loss is 0.011480345390737057\n",
      "epoch: 23 step: 459, loss is 0.17618529498577118\n",
      "epoch: 23 step: 460, loss is 0.003755523357540369\n",
      "epoch: 23 step: 461, loss is 0.001865366124548018\n",
      "epoch: 23 step: 462, loss is 0.00020966217562090605\n",
      "epoch: 23 step: 463, loss is 0.024017268791794777\n",
      "epoch: 23 step: 464, loss is 0.00011890356836374849\n",
      "epoch: 23 step: 465, loss is 0.0016623666742816567\n",
      "epoch: 23 step: 466, loss is 0.015591679140925407\n",
      "epoch: 23 step: 467, loss is 0.057150691747665405\n",
      "epoch: 23 step: 468, loss is 0.00025716787786222994\n",
      "epoch: 23 step: 469, loss is 0.003765512490645051\n",
      "epoch: 23 step: 470, loss is 0.0018169297836720943\n",
      "epoch: 23 step: 471, loss is 0.0006852246588096023\n",
      "epoch: 23 step: 472, loss is 0.11121954768896103\n",
      "epoch: 23 step: 473, loss is 0.008723347447812557\n",
      "epoch: 23 step: 474, loss is 0.007234382908791304\n",
      "epoch: 23 step: 475, loss is 0.00849953107535839\n",
      "epoch: 23 step: 476, loss is 0.0033786259591579437\n",
      "epoch: 23 step: 477, loss is 0.09351608902215958\n",
      "epoch: 23 step: 478, loss is 0.029196541756391525\n",
      "epoch: 23 step: 479, loss is 0.0061170500703155994\n",
      "epoch: 23 step: 480, loss is 0.0001884547236841172\n",
      "epoch: 23 step: 481, loss is 0.0059504215605556965\n",
      "epoch: 23 step: 482, loss is 0.006872785743325949\n",
      "epoch: 23 step: 483, loss is 0.0015347461448982358\n",
      "epoch: 23 step: 484, loss is 0.054108552634716034\n",
      "epoch: 23 step: 485, loss is 0.0020356150344014168\n",
      "epoch: 23 step: 486, loss is 0.0010521222138777375\n",
      "epoch: 23 step: 487, loss is 0.0019122138619422913\n",
      "epoch: 23 step: 488, loss is 0.012289268895983696\n",
      "epoch: 23 step: 489, loss is 0.0002401457168161869\n",
      "epoch: 23 step: 490, loss is 0.0040136659517884254\n",
      "epoch: 23 step: 491, loss is 0.0027579269371926785\n",
      "epoch: 23 step: 492, loss is 0.009430375881493092\n",
      "epoch: 23 step: 493, loss is 0.013005624525249004\n",
      "epoch: 23 step: 494, loss is 0.0018740746891126037\n",
      "epoch: 23 step: 495, loss is 0.003656309563666582\n",
      "epoch: 23 step: 496, loss is 0.06993025541305542\n",
      "epoch: 23 step: 497, loss is 0.0004602680273819715\n",
      "epoch: 23 step: 498, loss is 0.0005273863207548857\n",
      "epoch: 23 step: 499, loss is 0.00029153499053791165\n",
      "epoch: 23 step: 500, loss is 8.514791261404753e-05\n",
      "epoch: 23 step: 501, loss is 0.0004891629214398563\n",
      "epoch: 23 step: 502, loss is 0.0039742495864629745\n",
      "epoch: 23 step: 503, loss is 0.00031593238236382604\n",
      "epoch: 23 step: 504, loss is 0.00892470870167017\n",
      "epoch: 23 step: 505, loss is 0.0016241646371781826\n",
      "epoch: 23 step: 506, loss is 0.056610140949487686\n",
      "epoch: 23 step: 507, loss is 0.0061996616423130035\n",
      "epoch: 23 step: 508, loss is 0.02904769591987133\n",
      "epoch: 23 step: 509, loss is 0.016594985499978065\n",
      "epoch: 23 step: 510, loss is 0.002017566002905369\n",
      "epoch: 23 step: 511, loss is 0.001566292718052864\n",
      "epoch: 23 step: 512, loss is 0.001936520915478468\n",
      "epoch: 23 step: 513, loss is 0.00016904629592318088\n",
      "epoch: 23 step: 514, loss is 0.03702351450920105\n",
      "epoch: 23 step: 515, loss is 0.002187282545492053\n",
      "epoch: 23 step: 516, loss is 0.023866139352321625\n",
      "epoch: 23 step: 517, loss is 0.005531220231205225\n",
      "epoch: 23 step: 518, loss is 0.001730583026073873\n",
      "epoch: 23 step: 519, loss is 0.00825060997158289\n",
      "epoch: 23 step: 520, loss is 0.00013724056771025062\n",
      "epoch: 23 step: 521, loss is 0.0020169205963611603\n",
      "epoch: 23 step: 522, loss is 0.01140595693141222\n",
      "epoch: 23 step: 523, loss is 0.0038557506632059813\n",
      "epoch: 23 step: 524, loss is 0.010079571045935154\n",
      "epoch: 23 step: 525, loss is 0.0006579129258170724\n",
      "epoch: 23 step: 526, loss is 0.002470713574439287\n",
      "epoch: 23 step: 527, loss is 0.015477028675377369\n",
      "epoch: 23 step: 528, loss is 0.023065969347953796\n",
      "epoch: 23 step: 529, loss is 0.0035856065806001425\n",
      "epoch: 23 step: 530, loss is 0.025851944461464882\n",
      "epoch: 23 step: 531, loss is 0.002780957380309701\n",
      "epoch: 23 step: 532, loss is 0.006445297505706549\n",
      "epoch: 23 step: 533, loss is 0.0006750223110429943\n",
      "epoch: 23 step: 534, loss is 0.0006583537324331701\n",
      "epoch: 23 step: 535, loss is 0.00044771278044208884\n",
      "epoch: 23 step: 536, loss is 0.011716327629983425\n",
      "epoch: 23 step: 537, loss is 0.00011018926306860521\n",
      "epoch: 23 step: 538, loss is 0.01653027907013893\n",
      "epoch: 23 step: 539, loss is 0.018813587725162506\n",
      "epoch: 23 step: 540, loss is 0.004854387138038874\n",
      "epoch: 23 step: 541, loss is 0.0007924949168227613\n",
      "epoch: 23 step: 542, loss is 0.003113098908215761\n",
      "epoch: 23 step: 543, loss is 0.0001971886376850307\n",
      "epoch: 23 step: 544, loss is 0.029042262583971024\n",
      "epoch: 23 step: 545, loss is 0.058739736676216125\n",
      "epoch: 23 step: 546, loss is 0.0003883379977196455\n",
      "epoch: 23 step: 547, loss is 0.004182572942227125\n",
      "epoch: 23 step: 548, loss is 0.01885305345058441\n",
      "epoch: 23 step: 549, loss is 0.001271453220397234\n",
      "epoch: 23 step: 550, loss is 0.014814832247793674\n",
      "epoch: 23 step: 551, loss is 0.0013609501766040921\n",
      "epoch: 23 step: 552, loss is 0.014303424395620823\n",
      "epoch: 23 step: 553, loss is 0.058187518268823624\n",
      "epoch: 23 step: 554, loss is 0.0006162748322822154\n",
      "epoch: 23 step: 555, loss is 0.042174987494945526\n",
      "epoch: 23 step: 556, loss is 0.016139861196279526\n",
      "epoch: 23 step: 557, loss is 0.00276182615198195\n",
      "epoch: 23 step: 558, loss is 2.223502451670356e-05\n",
      "epoch: 23 step: 559, loss is 0.0004319330328144133\n",
      "epoch: 23 step: 560, loss is 0.00010876508895307779\n",
      "epoch: 23 step: 561, loss is 0.0065754251554608345\n",
      "epoch: 23 step: 562, loss is 0.0008920130785554647\n",
      "epoch: 23 step: 563, loss is 0.0020986569579690695\n",
      "epoch: 23 step: 564, loss is 0.01660970225930214\n",
      "epoch: 23 step: 565, loss is 0.054044149816036224\n",
      "epoch: 23 step: 566, loss is 0.0008112376090139151\n",
      "epoch: 23 step: 567, loss is 0.014281831681728363\n",
      "epoch: 23 step: 568, loss is 0.026690809056162834\n",
      "epoch: 23 step: 569, loss is 0.000245844479650259\n",
      "epoch: 23 step: 570, loss is 0.0009109863312914968\n",
      "epoch: 23 step: 571, loss is 0.0005080453702248633\n",
      "epoch: 23 step: 572, loss is 0.002058765385299921\n",
      "epoch: 23 step: 573, loss is 0.03354385867714882\n",
      "epoch: 23 step: 574, loss is 0.010577024891972542\n",
      "epoch: 23 step: 575, loss is 0.0007609118474647403\n",
      "epoch: 23 step: 576, loss is 0.02541258931159973\n",
      "epoch: 23 step: 577, loss is 0.015661610290408134\n",
      "epoch: 23 step: 578, loss is 0.00013159200898371637\n",
      "epoch: 23 step: 579, loss is 0.020058510825037956\n",
      "epoch: 23 step: 580, loss is 0.005152469500899315\n",
      "epoch: 23 step: 581, loss is 0.000657593656796962\n",
      "epoch: 23 step: 582, loss is 0.006370152812451124\n",
      "epoch: 23 step: 583, loss is 0.0020320317707955837\n",
      "epoch: 23 step: 584, loss is 0.0042084199376404285\n",
      "epoch: 23 step: 585, loss is 0.07747957855463028\n",
      "epoch: 23 step: 586, loss is 4.980958692613058e-06\n",
      "epoch: 23 step: 587, loss is 0.007741048000752926\n",
      "epoch: 23 step: 588, loss is 0.00021751245367340744\n",
      "epoch: 23 step: 589, loss is 0.03095378167927265\n",
      "epoch: 23 step: 590, loss is 0.09765408933162689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 591, loss is 0.00027171612600795925\n",
      "epoch: 23 step: 592, loss is 0.0009972676634788513\n",
      "epoch: 23 step: 593, loss is 0.0037352154031395912\n",
      "epoch: 23 step: 594, loss is 0.0031666187569499016\n",
      "epoch: 23 step: 595, loss is 0.029658054932951927\n",
      "epoch: 23 step: 596, loss is 5.309269909048453e-05\n",
      "epoch: 23 step: 597, loss is 0.00012029281060677022\n",
      "epoch: 23 step: 598, loss is 0.00039029939216561615\n",
      "epoch: 23 step: 599, loss is 0.0003373391227796674\n",
      "epoch: 23 step: 600, loss is 0.004073937889188528\n",
      "epoch: 23 step: 601, loss is 0.007477490231394768\n",
      "epoch: 23 step: 602, loss is 0.0076395124197006226\n",
      "epoch: 23 step: 603, loss is 0.07655662298202515\n",
      "epoch: 23 step: 604, loss is 0.0047194454818964005\n",
      "epoch: 23 step: 605, loss is 0.005181481596082449\n",
      "epoch: 23 step: 606, loss is 0.001821448327973485\n",
      "epoch: 23 step: 607, loss is 3.8236757973209023e-05\n",
      "epoch: 23 step: 608, loss is 0.0679546445608139\n",
      "epoch: 23 step: 609, loss is 0.15985260903835297\n",
      "epoch: 23 step: 610, loss is 0.011976826936006546\n",
      "epoch: 23 step: 611, loss is 0.0022447360679507256\n",
      "epoch: 23 step: 612, loss is 0.00017481838585808873\n",
      "epoch: 23 step: 613, loss is 9.280912490794435e-05\n",
      "epoch: 23 step: 614, loss is 0.0019705984741449356\n",
      "epoch: 23 step: 615, loss is 0.047940611839294434\n",
      "epoch: 23 step: 616, loss is 0.00020386482356116176\n",
      "epoch: 23 step: 617, loss is 0.027435580268502235\n",
      "epoch: 23 step: 618, loss is 0.010893543250858784\n",
      "epoch: 23 step: 619, loss is 0.0041440934874117374\n",
      "epoch: 23 step: 620, loss is 0.12965835630893707\n",
      "epoch: 23 step: 621, loss is 0.12952817976474762\n",
      "epoch: 23 step: 622, loss is 0.04623842239379883\n",
      "epoch: 23 step: 623, loss is 0.004396710079163313\n",
      "epoch: 23 step: 624, loss is 0.0019626051653176546\n",
      "epoch: 23 step: 625, loss is 4.663559593609534e-05\n",
      "epoch: 23 step: 626, loss is 0.05989970266819\n",
      "epoch: 23 step: 627, loss is 0.007001080084592104\n",
      "epoch: 23 step: 628, loss is 0.0037473030388355255\n",
      "epoch: 23 step: 629, loss is 0.03089931793510914\n",
      "epoch: 23 step: 630, loss is 0.0014292632695287466\n",
      "epoch: 23 step: 631, loss is 0.01572517119348049\n",
      "epoch: 23 step: 632, loss is 0.02799985744059086\n",
      "epoch: 23 step: 633, loss is 0.1443978250026703\n",
      "epoch: 23 step: 634, loss is 0.011837498284876347\n",
      "epoch: 23 step: 635, loss is 0.023580633103847504\n",
      "epoch: 23 step: 636, loss is 0.007291050627827644\n",
      "epoch: 23 step: 637, loss is 0.02118305303156376\n",
      "epoch: 23 step: 638, loss is 0.0219745934009552\n",
      "epoch: 23 step: 639, loss is 0.038248855620622635\n",
      "epoch: 23 step: 640, loss is 0.00687808683142066\n",
      "epoch: 23 step: 641, loss is 0.04697299003601074\n",
      "epoch: 23 step: 642, loss is 0.006507304962724447\n",
      "epoch: 23 step: 643, loss is 0.006078771781176329\n",
      "epoch: 23 step: 644, loss is 0.041773442178964615\n",
      "epoch: 23 step: 645, loss is 0.0035794477444142103\n",
      "epoch: 23 step: 646, loss is 0.0026181673165410757\n",
      "epoch: 23 step: 647, loss is 0.001468850881792605\n",
      "epoch: 23 step: 648, loss is 0.004103394225239754\n",
      "epoch: 23 step: 649, loss is 0.03975025191903114\n",
      "epoch: 23 step: 650, loss is 0.004604138899594545\n",
      "epoch: 23 step: 651, loss is 0.09760084003210068\n",
      "epoch: 23 step: 652, loss is 0.01861243136227131\n",
      "epoch: 23 step: 653, loss is 0.00584667082875967\n",
      "epoch: 23 step: 654, loss is 0.004169064108282328\n",
      "epoch: 23 step: 655, loss is 0.030081067234277725\n",
      "epoch: 23 step: 656, loss is 0.004034722223877907\n",
      "epoch: 23 step: 657, loss is 0.0696667730808258\n",
      "epoch: 23 step: 658, loss is 0.04186168313026428\n",
      "epoch: 23 step: 659, loss is 0.0008338725892826915\n",
      "epoch: 23 step: 660, loss is 0.04150514677166939\n",
      "epoch: 23 step: 661, loss is 0.0026351050473749638\n",
      "epoch: 23 step: 662, loss is 0.011490915901958942\n",
      "epoch: 23 step: 663, loss is 0.0006199028575792909\n",
      "epoch: 23 step: 664, loss is 0.005216095596551895\n",
      "epoch: 23 step: 665, loss is 0.0005095248343423009\n",
      "epoch: 23 step: 666, loss is 0.008650148287415504\n",
      "epoch: 23 step: 667, loss is 0.0009045224287547171\n",
      "epoch: 23 step: 668, loss is 0.006061222404241562\n",
      "epoch: 23 step: 669, loss is 0.0011822684900835156\n",
      "epoch: 23 step: 670, loss is 0.07565406709909439\n",
      "epoch: 23 step: 671, loss is 0.06919319927692413\n",
      "epoch: 23 step: 672, loss is 0.010767320170998573\n",
      "epoch: 23 step: 673, loss is 0.0021481523290276527\n",
      "epoch: 23 step: 674, loss is 0.007103700190782547\n",
      "epoch: 23 step: 675, loss is 0.0017992834327742457\n",
      "epoch: 23 step: 676, loss is 0.041200052946805954\n",
      "epoch: 23 step: 677, loss is 0.003722509602084756\n",
      "epoch: 23 step: 678, loss is 0.0034355169627815485\n",
      "epoch: 23 step: 679, loss is 0.0020209848880767822\n",
      "epoch: 23 step: 680, loss is 0.005558720324188471\n",
      "epoch: 23 step: 681, loss is 0.08393594622612\n",
      "epoch: 23 step: 682, loss is 0.0013439710019156337\n",
      "epoch: 23 step: 683, loss is 0.013979533687233925\n",
      "epoch: 23 step: 684, loss is 0.031946297734975815\n",
      "epoch: 23 step: 685, loss is 0.08883408457040787\n",
      "epoch: 23 step: 686, loss is 0.008002172224223614\n",
      "epoch: 23 step: 687, loss is 0.07593263685703278\n",
      "epoch: 23 step: 688, loss is 0.004480416886508465\n",
      "epoch: 23 step: 689, loss is 0.00033891882048919797\n",
      "epoch: 23 step: 690, loss is 0.006676239427179098\n",
      "epoch: 23 step: 691, loss is 0.003344316966831684\n",
      "epoch: 23 step: 692, loss is 0.02340058796107769\n",
      "epoch: 23 step: 693, loss is 0.040781810879707336\n",
      "epoch: 23 step: 694, loss is 0.021913042291998863\n",
      "epoch: 23 step: 695, loss is 0.004723289981484413\n",
      "epoch: 23 step: 696, loss is 0.002491217805072665\n",
      "epoch: 23 step: 697, loss is 0.03873894363641739\n",
      "epoch: 23 step: 698, loss is 0.0009461627341806889\n",
      "epoch: 23 step: 699, loss is 0.0003322922275401652\n",
      "epoch: 23 step: 700, loss is 0.04355129599571228\n",
      "epoch: 23 step: 701, loss is 0.13475154340267181\n",
      "epoch: 23 step: 702, loss is 0.019919399172067642\n",
      "epoch: 23 step: 703, loss is 0.000367023196304217\n",
      "epoch: 23 step: 704, loss is 0.012415807694196701\n",
      "epoch: 23 step: 705, loss is 0.0018638381734490395\n",
      "epoch: 23 step: 706, loss is 0.048555757850408554\n",
      "epoch: 23 step: 707, loss is 0.012005722150206566\n",
      "epoch: 23 step: 708, loss is 0.000703723169863224\n",
      "epoch: 23 step: 709, loss is 0.02133721113204956\n",
      "epoch: 23 step: 710, loss is 0.0015440909191966057\n",
      "epoch: 23 step: 711, loss is 0.0021154023706912994\n",
      "epoch: 23 step: 712, loss is 0.012522989884018898\n",
      "epoch: 23 step: 713, loss is 0.0013728425838053226\n",
      "epoch: 23 step: 714, loss is 0.022719431668519974\n",
      "epoch: 23 step: 715, loss is 0.0019060836639255285\n",
      "epoch: 23 step: 716, loss is 0.0004256876418367028\n",
      "epoch: 23 step: 717, loss is 0.15094052255153656\n",
      "epoch: 23 step: 718, loss is 0.0020205688197165728\n",
      "epoch: 23 step: 719, loss is 0.034811049699783325\n",
      "epoch: 23 step: 720, loss is 0.0010693948715925217\n",
      "epoch: 23 step: 721, loss is 0.06382759660482407\n",
      "epoch: 23 step: 722, loss is 0.02738833613693714\n",
      "epoch: 23 step: 723, loss is 0.017170313745737076\n",
      "epoch: 23 step: 724, loss is 0.003843456506729126\n",
      "epoch: 23 step: 725, loss is 0.0022726100869476795\n",
      "epoch: 23 step: 726, loss is 0.0024871034547686577\n",
      "epoch: 23 step: 727, loss is 0.019110651686787605\n",
      "epoch: 23 step: 728, loss is 0.013859562575817108\n",
      "epoch: 23 step: 729, loss is 0.014542470686137676\n",
      "epoch: 23 step: 730, loss is 0.00881014484912157\n",
      "epoch: 23 step: 731, loss is 0.0007581643876619637\n",
      "epoch: 23 step: 732, loss is 0.006242935545742512\n",
      "epoch: 23 step: 733, loss is 0.04020775482058525\n",
      "epoch: 23 step: 734, loss is 0.007070953957736492\n",
      "epoch: 23 step: 735, loss is 0.010507459752261639\n",
      "epoch: 23 step: 736, loss is 0.02544701285660267\n",
      "epoch: 23 step: 737, loss is 0.0011133205844089389\n",
      "epoch: 23 step: 738, loss is 0.007646986283361912\n",
      "epoch: 23 step: 739, loss is 0.012079269625246525\n",
      "epoch: 23 step: 740, loss is 0.005467138718813658\n",
      "epoch: 23 step: 741, loss is 0.014620990492403507\n",
      "epoch: 23 step: 742, loss is 0.03979998826980591\n",
      "epoch: 23 step: 743, loss is 0.0029049087315797806\n",
      "epoch: 23 step: 744, loss is 0.004260765388607979\n",
      "epoch: 23 step: 745, loss is 0.007396424189209938\n",
      "epoch: 23 step: 746, loss is 0.0057455869391560555\n",
      "epoch: 23 step: 747, loss is 0.0025789428036659956\n",
      "epoch: 23 step: 748, loss is 0.019186144694685936\n",
      "epoch: 23 step: 749, loss is 0.042671460658311844\n",
      "epoch: 23 step: 750, loss is 0.0006281257374212146\n",
      "epoch: 23 step: 751, loss is 0.002719895914196968\n",
      "epoch: 23 step: 752, loss is 0.0001199180114781484\n",
      "epoch: 23 step: 753, loss is 0.0012680052313953638\n",
      "epoch: 23 step: 754, loss is 0.0837099626660347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 755, loss is 0.016914865002036095\n",
      "epoch: 23 step: 756, loss is 0.01621323637664318\n",
      "epoch: 23 step: 757, loss is 0.0002233366685686633\n",
      "epoch: 23 step: 758, loss is 0.00229933625087142\n",
      "epoch: 23 step: 759, loss is 0.004805827047675848\n",
      "epoch: 23 step: 760, loss is 0.0018239339115098119\n",
      "epoch: 23 step: 761, loss is 0.03969398885965347\n",
      "epoch: 23 step: 762, loss is 0.01192817185074091\n",
      "epoch: 23 step: 763, loss is 0.024299398064613342\n",
      "epoch: 23 step: 764, loss is 0.0006420069839805365\n",
      "epoch: 23 step: 765, loss is 0.022434581071138382\n",
      "epoch: 23 step: 766, loss is 0.0016370267840102315\n",
      "epoch: 23 step: 767, loss is 0.03203345090150833\n",
      "epoch: 23 step: 768, loss is 0.031962864100933075\n",
      "epoch: 23 step: 769, loss is 0.22358784079551697\n",
      "epoch: 23 step: 770, loss is 0.00810365378856659\n",
      "epoch: 23 step: 771, loss is 0.02328072115778923\n",
      "epoch: 23 step: 772, loss is 0.00147915817797184\n",
      "epoch: 23 step: 773, loss is 0.01627771183848381\n",
      "epoch: 23 step: 774, loss is 0.005817717872560024\n",
      "epoch: 23 step: 775, loss is 0.0008330906857736409\n",
      "epoch: 23 step: 776, loss is 0.07925485074520111\n",
      "epoch: 23 step: 777, loss is 0.0001782073377398774\n",
      "epoch: 23 step: 778, loss is 0.024048559367656708\n",
      "epoch: 23 step: 779, loss is 0.0016388240037485957\n",
      "epoch: 23 step: 780, loss is 0.01191052608191967\n",
      "epoch: 23 step: 781, loss is 0.008068989031016827\n",
      "epoch: 23 step: 782, loss is 0.021739281713962555\n",
      "epoch: 23 step: 783, loss is 0.0703357458114624\n",
      "epoch: 23 step: 784, loss is 0.0005241425824351609\n",
      "epoch: 23 step: 785, loss is 0.0010539272334426641\n",
      "epoch: 23 step: 786, loss is 0.0004583793634083122\n",
      "epoch: 23 step: 787, loss is 0.04227609932422638\n",
      "epoch: 23 step: 788, loss is 0.016783926635980606\n",
      "epoch: 23 step: 789, loss is 0.005299747921526432\n",
      "epoch: 23 step: 790, loss is 0.014188412576913834\n",
      "epoch: 23 step: 791, loss is 0.0034229422453790903\n",
      "epoch: 23 step: 792, loss is 0.014927725307643414\n",
      "epoch: 23 step: 793, loss is 0.011231868527829647\n",
      "epoch: 23 step: 794, loss is 0.0204546470195055\n",
      "epoch: 23 step: 795, loss is 0.0455731563270092\n",
      "epoch: 23 step: 796, loss is 0.0016313840169459581\n",
      "epoch: 23 step: 797, loss is 0.0007924349629320204\n",
      "epoch: 23 step: 798, loss is 0.10292056947946548\n",
      "epoch: 23 step: 799, loss is 0.005501456558704376\n",
      "epoch: 23 step: 800, loss is 0.014683439396321774\n",
      "epoch: 23 step: 801, loss is 0.0036584537010639906\n",
      "epoch: 23 step: 802, loss is 0.0011309005785733461\n",
      "epoch: 23 step: 803, loss is 0.009381189942359924\n",
      "epoch: 23 step: 804, loss is 0.00015386365703307092\n",
      "epoch: 23 step: 805, loss is 8.83492102730088e-05\n",
      "epoch: 23 step: 806, loss is 0.00013183732517063618\n",
      "epoch: 23 step: 807, loss is 0.0015613120049238205\n",
      "epoch: 23 step: 808, loss is 0.0003317225200589746\n",
      "epoch: 23 step: 809, loss is 0.0018246739637106657\n",
      "epoch: 23 step: 810, loss is 0.00022064837685320526\n",
      "epoch: 23 step: 811, loss is 0.03376815840601921\n",
      "epoch: 23 step: 812, loss is 0.0025683073326945305\n",
      "epoch: 23 step: 813, loss is 0.0035776447039097548\n",
      "epoch: 23 step: 814, loss is 0.0032610823400318623\n",
      "epoch: 23 step: 815, loss is 0.006567103788256645\n",
      "epoch: 23 step: 816, loss is 0.0021540115121752024\n",
      "epoch: 23 step: 817, loss is 0.003333731321617961\n",
      "epoch: 23 step: 818, loss is 0.0025431630201637745\n",
      "epoch: 23 step: 819, loss is 0.017244761809706688\n",
      "epoch: 23 step: 820, loss is 0.03276662155985832\n",
      "epoch: 23 step: 821, loss is 0.0654730573296547\n",
      "epoch: 23 step: 822, loss is 0.0029635357204824686\n",
      "epoch: 23 step: 823, loss is 0.0069011058658361435\n",
      "epoch: 23 step: 824, loss is 0.00028661670512519777\n",
      "epoch: 23 step: 825, loss is 0.012748820707201958\n",
      "epoch: 23 step: 826, loss is 0.012583748437464237\n",
      "epoch: 23 step: 827, loss is 0.013942341320216656\n",
      "epoch: 23 step: 828, loss is 0.049294281750917435\n",
      "epoch: 23 step: 829, loss is 0.0013685084413737059\n",
      "epoch: 23 step: 830, loss is 0.0014379031490534544\n",
      "epoch: 23 step: 831, loss is 0.003639735747128725\n",
      "epoch: 23 step: 832, loss is 0.0066559878177940845\n",
      "epoch: 23 step: 833, loss is 0.0020485727582126856\n",
      "epoch: 23 step: 834, loss is 0.015367561019957066\n",
      "epoch: 23 step: 835, loss is 0.006997226271778345\n",
      "epoch: 23 step: 836, loss is 0.011193184182047844\n",
      "epoch: 23 step: 837, loss is 0.06018039211630821\n",
      "epoch: 23 step: 838, loss is 0.016800308600068092\n",
      "epoch: 23 step: 839, loss is 0.005766152869910002\n",
      "epoch: 23 step: 840, loss is 0.000328687863657251\n",
      "epoch: 23 step: 841, loss is 0.0016984357498586178\n",
      "epoch: 23 step: 842, loss is 0.004560047294944525\n",
      "epoch: 23 step: 843, loss is 0.0008670311653986573\n",
      "epoch: 23 step: 844, loss is 0.0003979154280386865\n",
      "epoch: 23 step: 845, loss is 0.0056889294646680355\n",
      "epoch: 23 step: 846, loss is 0.00045690586557611823\n",
      "epoch: 23 step: 847, loss is 0.0014718493912369013\n",
      "epoch: 23 step: 848, loss is 0.029068851843476295\n",
      "epoch: 23 step: 849, loss is 0.0035730646923184395\n",
      "epoch: 23 step: 850, loss is 0.041833993047475815\n",
      "epoch: 23 step: 851, loss is 0.01085120439529419\n",
      "epoch: 23 step: 852, loss is 0.08819276094436646\n",
      "epoch: 23 step: 853, loss is 0.006264877039939165\n",
      "epoch: 23 step: 854, loss is 0.0050421589985489845\n",
      "epoch: 23 step: 855, loss is 0.05475562810897827\n",
      "epoch: 23 step: 856, loss is 0.0009406822500750422\n",
      "epoch: 23 step: 857, loss is 0.04753509536385536\n",
      "epoch: 23 step: 858, loss is 0.006530159153044224\n",
      "epoch: 23 step: 859, loss is 0.01439684722572565\n",
      "epoch: 23 step: 860, loss is 0.010600941255688667\n",
      "epoch: 23 step: 861, loss is 0.03149488940834999\n",
      "epoch: 23 step: 862, loss is 0.031224047765135765\n",
      "epoch: 23 step: 863, loss is 0.013427723199129105\n",
      "epoch: 23 step: 864, loss is 0.0025245905853807926\n",
      "epoch: 23 step: 865, loss is 0.028047312051057816\n",
      "epoch: 23 step: 866, loss is 0.0035693596582859755\n",
      "epoch: 23 step: 867, loss is 0.0029815409798175097\n",
      "epoch: 23 step: 868, loss is 0.06755461543798447\n",
      "epoch: 23 step: 869, loss is 0.0007908057305030525\n",
      "epoch: 23 step: 870, loss is 0.012547187507152557\n",
      "epoch: 23 step: 871, loss is 0.017244430258870125\n",
      "epoch: 23 step: 872, loss is 0.016639182344079018\n",
      "epoch: 23 step: 873, loss is 0.0012226440012454987\n",
      "epoch: 23 step: 874, loss is 0.014968271367251873\n",
      "epoch: 23 step: 875, loss is 0.0002890249015763402\n",
      "epoch: 23 step: 876, loss is 0.002143525518476963\n",
      "epoch: 23 step: 877, loss is 0.0010072224540635943\n",
      "epoch: 23 step: 878, loss is 0.013532343320548534\n",
      "epoch: 23 step: 879, loss is 0.004601382650434971\n",
      "epoch: 23 step: 880, loss is 0.003946853801608086\n",
      "epoch: 23 step: 881, loss is 0.018505148589611053\n",
      "epoch: 23 step: 882, loss is 0.010570414364337921\n",
      "epoch: 23 step: 883, loss is 0.00043401875882409513\n",
      "epoch: 23 step: 884, loss is 0.01879289373755455\n",
      "epoch: 23 step: 885, loss is 0.006951556541025639\n",
      "epoch: 23 step: 886, loss is 0.0009674608591012657\n",
      "epoch: 23 step: 887, loss is 0.0002175742556573823\n",
      "epoch: 23 step: 888, loss is 6.457303243223578e-05\n",
      "epoch: 23 step: 889, loss is 0.0058045536279678345\n",
      "epoch: 23 step: 890, loss is 0.09956145286560059\n",
      "epoch: 23 step: 891, loss is 0.0014238287694752216\n",
      "epoch: 23 step: 892, loss is 0.005306905601173639\n",
      "epoch: 23 step: 893, loss is 0.0001598301314515993\n",
      "epoch: 23 step: 894, loss is 0.014276444911956787\n",
      "epoch: 23 step: 895, loss is 9.518269507680088e-05\n",
      "epoch: 23 step: 896, loss is 0.0019900542683899403\n",
      "epoch: 23 step: 897, loss is 0.0005449517630040646\n",
      "epoch: 23 step: 898, loss is 0.017143189907073975\n",
      "epoch: 23 step: 899, loss is 0.0071736667305231094\n",
      "epoch: 23 step: 900, loss is 0.0016873313579708338\n",
      "epoch: 23 step: 901, loss is 0.08428554236888885\n",
      "epoch: 23 step: 902, loss is 0.006244489457458258\n",
      "epoch: 23 step: 903, loss is 0.007881425321102142\n",
      "epoch: 23 step: 904, loss is 0.0031768849585205317\n",
      "epoch: 23 step: 905, loss is 0.0007575931958854198\n",
      "epoch: 23 step: 906, loss is 0.0030304810497909784\n",
      "epoch: 23 step: 907, loss is 0.004083083476871252\n",
      "epoch: 23 step: 908, loss is 0.0002984491002280265\n",
      "epoch: 23 step: 909, loss is 0.016397159546613693\n",
      "epoch: 23 step: 910, loss is 0.0027862899005413055\n",
      "epoch: 23 step: 911, loss is 0.00593970064073801\n",
      "epoch: 23 step: 912, loss is 0.001211051014252007\n",
      "epoch: 23 step: 913, loss is 0.00016761446022428572\n",
      "epoch: 23 step: 914, loss is 0.0004795726854354143\n",
      "epoch: 23 step: 915, loss is 0.009243171662092209\n",
      "epoch: 23 step: 916, loss is 0.024579886347055435\n",
      "epoch: 23 step: 917, loss is 0.018533460795879364\n",
      "epoch: 23 step: 918, loss is 0.04149017482995987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 919, loss is 0.09573192894458771\n",
      "epoch: 23 step: 920, loss is 0.0033801780082285404\n",
      "epoch: 23 step: 921, loss is 0.002085261046886444\n",
      "epoch: 23 step: 922, loss is 0.0010770863154903054\n",
      "epoch: 23 step: 923, loss is 0.00013693529763258994\n",
      "epoch: 23 step: 924, loss is 0.0012317077489569783\n",
      "epoch: 23 step: 925, loss is 0.00010200538963545114\n",
      "epoch: 23 step: 926, loss is 0.005430950783193111\n",
      "epoch: 23 step: 927, loss is 0.003473303746432066\n",
      "epoch: 23 step: 928, loss is 0.005714569706469774\n",
      "epoch: 23 step: 929, loss is 0.0007882158970460296\n",
      "epoch: 23 step: 930, loss is 0.001490121241658926\n",
      "epoch: 23 step: 931, loss is 0.05946364626288414\n",
      "epoch: 23 step: 932, loss is 0.0001983056281460449\n",
      "epoch: 23 step: 933, loss is 0.1622750610113144\n",
      "epoch: 23 step: 934, loss is 0.01919388584792614\n",
      "epoch: 23 step: 935, loss is 0.0157675389200449\n",
      "epoch: 23 step: 936, loss is 0.08482621610164642\n",
      "epoch: 23 step: 937, loss is 0.008548608981072903\n",
      "epoch: 24 step: 1, loss is 0.023196320980787277\n",
      "epoch: 24 step: 2, loss is 9.417017281521112e-05\n",
      "epoch: 24 step: 3, loss is 0.006534885615110397\n",
      "epoch: 24 step: 4, loss is 0.002340909093618393\n",
      "epoch: 24 step: 5, loss is 0.0028887703083455563\n",
      "epoch: 24 step: 6, loss is 0.0662565603852272\n",
      "epoch: 24 step: 7, loss is 0.0642070397734642\n",
      "epoch: 24 step: 8, loss is 0.02889273129403591\n",
      "epoch: 24 step: 9, loss is 0.0014342981157824397\n",
      "epoch: 24 step: 10, loss is 0.008180323988199234\n",
      "epoch: 24 step: 11, loss is 0.004905226640403271\n",
      "epoch: 24 step: 12, loss is 0.03449064493179321\n",
      "epoch: 24 step: 13, loss is 0.0015533354599028826\n",
      "epoch: 24 step: 14, loss is 0.008449496701359749\n",
      "epoch: 24 step: 15, loss is 0.00025794049724936485\n",
      "epoch: 24 step: 16, loss is 0.02924864925444126\n",
      "epoch: 24 step: 17, loss is 0.0014185284962877631\n",
      "epoch: 24 step: 18, loss is 0.022651178762316704\n",
      "epoch: 24 step: 19, loss is 0.003705178387463093\n",
      "epoch: 24 step: 20, loss is 0.002124849474057555\n",
      "epoch: 24 step: 21, loss is 0.00020905611745547503\n",
      "epoch: 24 step: 22, loss is 0.01027816254645586\n",
      "epoch: 24 step: 23, loss is 0.0038391887210309505\n",
      "epoch: 24 step: 24, loss is 0.011515787802636623\n",
      "epoch: 24 step: 25, loss is 0.0015913712559267879\n",
      "epoch: 24 step: 26, loss is 0.03861339017748833\n",
      "epoch: 24 step: 27, loss is 0.0003858161799144\n",
      "epoch: 24 step: 28, loss is 0.00023717024305369705\n",
      "epoch: 24 step: 29, loss is 0.003495386801660061\n",
      "epoch: 24 step: 30, loss is 0.005065383855253458\n",
      "epoch: 24 step: 31, loss is 0.00248103984631598\n",
      "epoch: 24 step: 32, loss is 0.010508351027965546\n",
      "epoch: 24 step: 33, loss is 0.001575556118041277\n",
      "epoch: 24 step: 34, loss is 0.008027446456253529\n",
      "epoch: 24 step: 35, loss is 0.005634929519146681\n",
      "epoch: 24 step: 36, loss is 0.014269929379224777\n",
      "epoch: 24 step: 37, loss is 0.0009715947089716792\n",
      "epoch: 24 step: 38, loss is 0.010604551061987877\n",
      "epoch: 24 step: 39, loss is 0.001826896914280951\n",
      "epoch: 24 step: 40, loss is 0.006653610151261091\n",
      "epoch: 24 step: 41, loss is 0.005934925749897957\n",
      "epoch: 24 step: 42, loss is 0.017682161182165146\n",
      "epoch: 24 step: 43, loss is 0.0017921883845701814\n",
      "epoch: 24 step: 44, loss is 0.0022382140159606934\n",
      "epoch: 24 step: 45, loss is 0.005674762651324272\n",
      "epoch: 24 step: 46, loss is 0.005152234807610512\n",
      "epoch: 24 step: 47, loss is 0.001581675955094397\n",
      "epoch: 24 step: 48, loss is 0.008285457268357277\n",
      "epoch: 24 step: 49, loss is 0.008284457959234715\n",
      "epoch: 24 step: 50, loss is 0.0055745490826666355\n",
      "epoch: 24 step: 51, loss is 9.82773708528839e-05\n",
      "epoch: 24 step: 52, loss is 0.0005874505150131881\n",
      "epoch: 24 step: 53, loss is 0.03174145519733429\n",
      "epoch: 24 step: 54, loss is 0.0005213387194089592\n",
      "epoch: 24 step: 55, loss is 0.0031422574538737535\n",
      "epoch: 24 step: 56, loss is 0.013816424645483494\n",
      "epoch: 24 step: 57, loss is 0.00013216803199611604\n",
      "epoch: 24 step: 58, loss is 0.06671377271413803\n",
      "epoch: 24 step: 59, loss is 0.0010563344694674015\n",
      "epoch: 24 step: 60, loss is 0.0012304135598242283\n",
      "epoch: 24 step: 61, loss is 0.00034301381674595177\n",
      "epoch: 24 step: 62, loss is 0.0005134562379680574\n",
      "epoch: 24 step: 63, loss is 0.005489226430654526\n",
      "epoch: 24 step: 64, loss is 0.05292540788650513\n",
      "epoch: 24 step: 65, loss is 0.00017527356976643205\n",
      "epoch: 24 step: 66, loss is 0.0006154368747957051\n",
      "epoch: 24 step: 67, loss is 0.0008689140086062253\n",
      "epoch: 24 step: 68, loss is 0.00047927850391715765\n",
      "epoch: 24 step: 69, loss is 0.004531048703938723\n",
      "epoch: 24 step: 70, loss is 0.0012063978938385844\n",
      "epoch: 24 step: 71, loss is 0.030065327882766724\n",
      "epoch: 24 step: 72, loss is 0.003987815231084824\n",
      "epoch: 24 step: 73, loss is 0.001753116026520729\n",
      "epoch: 24 step: 74, loss is 0.006497133523225784\n",
      "epoch: 24 step: 75, loss is 3.8234265957726166e-05\n",
      "epoch: 24 step: 76, loss is 0.03294973447918892\n",
      "epoch: 24 step: 77, loss is 0.09500115364789963\n",
      "epoch: 24 step: 78, loss is 0.004777798894792795\n",
      "epoch: 24 step: 79, loss is 0.0014678144361823797\n",
      "epoch: 24 step: 80, loss is 0.00029757633456029\n",
      "epoch: 24 step: 81, loss is 0.015920843929052353\n",
      "epoch: 24 step: 82, loss is 0.01166720874607563\n",
      "epoch: 24 step: 83, loss is 0.0003966482763644308\n",
      "epoch: 24 step: 84, loss is 0.0021093515679240227\n",
      "epoch: 24 step: 85, loss is 0.014156045392155647\n",
      "epoch: 24 step: 86, loss is 0.0029862020164728165\n",
      "epoch: 24 step: 87, loss is 0.0025386910419911146\n",
      "epoch: 24 step: 88, loss is 0.00170897226780653\n",
      "epoch: 24 step: 89, loss is 0.04842539131641388\n",
      "epoch: 24 step: 90, loss is 0.0018867027247324586\n",
      "epoch: 24 step: 91, loss is 0.08635129034519196\n",
      "epoch: 24 step: 92, loss is 0.0004070317663718015\n",
      "epoch: 24 step: 93, loss is 0.04336846247315407\n",
      "epoch: 24 step: 94, loss is 0.12587898969650269\n",
      "epoch: 24 step: 95, loss is 0.0012672415468841791\n",
      "epoch: 24 step: 96, loss is 0.04632914066314697\n",
      "epoch: 24 step: 97, loss is 0.00019985933613497764\n",
      "epoch: 24 step: 98, loss is 0.009157340042293072\n",
      "epoch: 24 step: 99, loss is 0.0017126327147707343\n",
      "epoch: 24 step: 100, loss is 0.01155941840261221\n",
      "epoch: 24 step: 101, loss is 0.00750301918014884\n",
      "epoch: 24 step: 102, loss is 0.000651512120384723\n",
      "epoch: 24 step: 103, loss is 0.001369750825688243\n",
      "epoch: 24 step: 104, loss is 0.0017994685331359506\n",
      "epoch: 24 step: 105, loss is 0.09128815680742264\n",
      "epoch: 24 step: 106, loss is 0.019734488800168037\n",
      "epoch: 24 step: 107, loss is 0.010130547918379307\n",
      "epoch: 24 step: 108, loss is 0.02036173641681671\n",
      "epoch: 24 step: 109, loss is 0.035646483302116394\n",
      "epoch: 24 step: 110, loss is 0.0001301133306697011\n",
      "epoch: 24 step: 111, loss is 0.001793803065083921\n",
      "epoch: 24 step: 112, loss is 0.00014380856009665877\n",
      "epoch: 24 step: 113, loss is 0.0717691034078598\n",
      "epoch: 24 step: 114, loss is 0.00452932994812727\n",
      "epoch: 24 step: 115, loss is 0.004697736352682114\n",
      "epoch: 24 step: 116, loss is 0.0009571966947987676\n",
      "epoch: 24 step: 117, loss is 0.0011785444803535938\n",
      "epoch: 24 step: 118, loss is 0.0005357782356441021\n",
      "epoch: 24 step: 119, loss is 0.002877816092222929\n",
      "epoch: 24 step: 120, loss is 0.039942238479852676\n",
      "epoch: 24 step: 121, loss is 0.03327791765332222\n",
      "epoch: 24 step: 122, loss is 0.000508864235598594\n",
      "epoch: 24 step: 123, loss is 0.013968979939818382\n",
      "epoch: 24 step: 124, loss is 0.00011257153528276831\n",
      "epoch: 24 step: 125, loss is 0.047099024057388306\n",
      "epoch: 24 step: 126, loss is 0.01543966494500637\n",
      "epoch: 24 step: 127, loss is 0.00103942456189543\n",
      "epoch: 24 step: 128, loss is 0.0007063157390803099\n",
      "epoch: 24 step: 129, loss is 0.002381023019552231\n",
      "epoch: 24 step: 130, loss is 0.0014285093639045954\n",
      "epoch: 24 step: 131, loss is 0.04404940828680992\n",
      "epoch: 24 step: 132, loss is 6.656381447101012e-05\n",
      "epoch: 24 step: 133, loss is 0.03930870071053505\n",
      "epoch: 24 step: 134, loss is 0.001888003433123231\n",
      "epoch: 24 step: 135, loss is 0.015774382278323174\n",
      "epoch: 24 step: 136, loss is 0.05253083258867264\n",
      "epoch: 24 step: 137, loss is 0.08148586750030518\n",
      "epoch: 24 step: 138, loss is 0.03867099806666374\n",
      "epoch: 24 step: 139, loss is 0.0025727967731654644\n",
      "epoch: 24 step: 140, loss is 0.0021120677702128887\n",
      "epoch: 24 step: 141, loss is 0.04431445524096489\n",
      "epoch: 24 step: 142, loss is 0.0004202627460472286\n",
      "epoch: 24 step: 143, loss is 0.01078862976282835\n",
      "epoch: 24 step: 144, loss is 0.03474712371826172\n",
      "epoch: 24 step: 145, loss is 0.00041700416477397084\n",
      "epoch: 24 step: 146, loss is 0.0005466653383336961\n",
      "epoch: 24 step: 147, loss is 0.00952929351478815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 148, loss is 0.015756018459796906\n",
      "epoch: 24 step: 149, loss is 0.09675666689872742\n",
      "epoch: 24 step: 150, loss is 0.0014973533106967807\n",
      "epoch: 24 step: 151, loss is 0.0008947070455178618\n",
      "epoch: 24 step: 152, loss is 0.004618948791176081\n",
      "epoch: 24 step: 153, loss is 0.054702598601579666\n",
      "epoch: 24 step: 154, loss is 0.004387658089399338\n",
      "epoch: 24 step: 155, loss is 0.009930960834026337\n",
      "epoch: 24 step: 156, loss is 0.008162621408700943\n",
      "epoch: 24 step: 157, loss is 0.025768933817744255\n",
      "epoch: 24 step: 158, loss is 0.01268089096993208\n",
      "epoch: 24 step: 159, loss is 0.062057990580797195\n",
      "epoch: 24 step: 160, loss is 0.0016338557470589876\n",
      "epoch: 24 step: 161, loss is 0.0011734310537576675\n",
      "epoch: 24 step: 162, loss is 0.005882504861801863\n",
      "epoch: 24 step: 163, loss is 0.043128687888383865\n",
      "epoch: 24 step: 164, loss is 0.004740532487630844\n",
      "epoch: 24 step: 165, loss is 0.004466830752789974\n",
      "epoch: 24 step: 166, loss is 0.01804276555776596\n",
      "epoch: 24 step: 167, loss is 0.00576816825196147\n",
      "epoch: 24 step: 168, loss is 0.007003152742981911\n",
      "epoch: 24 step: 169, loss is 0.01596428081393242\n",
      "epoch: 24 step: 170, loss is 0.0021308381110429764\n",
      "epoch: 24 step: 171, loss is 0.028981084004044533\n",
      "epoch: 24 step: 172, loss is 0.006069814320653677\n",
      "epoch: 24 step: 173, loss is 0.0005146068870089948\n",
      "epoch: 24 step: 174, loss is 0.00026742261252366006\n",
      "epoch: 24 step: 175, loss is 0.03314061835408211\n",
      "epoch: 24 step: 176, loss is 0.002073399955406785\n",
      "epoch: 24 step: 177, loss is 0.03342839702963829\n",
      "epoch: 24 step: 178, loss is 0.027239501476287842\n",
      "epoch: 24 step: 179, loss is 0.006870797835290432\n",
      "epoch: 24 step: 180, loss is 0.0502936989068985\n",
      "epoch: 24 step: 181, loss is 0.003115418367087841\n",
      "epoch: 24 step: 182, loss is 0.016177881509065628\n",
      "epoch: 24 step: 183, loss is 0.002118995413184166\n",
      "epoch: 24 step: 184, loss is 0.04092719405889511\n",
      "epoch: 24 step: 185, loss is 0.005054224748164415\n",
      "epoch: 24 step: 186, loss is 0.01295510120689869\n",
      "epoch: 24 step: 187, loss is 0.005693578161299229\n",
      "epoch: 24 step: 188, loss is 0.0017235670238733292\n",
      "epoch: 24 step: 189, loss is 0.020118746906518936\n",
      "epoch: 24 step: 190, loss is 0.0011589848436415195\n",
      "epoch: 24 step: 191, loss is 0.033455729484558105\n",
      "epoch: 24 step: 192, loss is 0.001911834697239101\n",
      "epoch: 24 step: 193, loss is 0.0010153487091884017\n",
      "epoch: 24 step: 194, loss is 0.002251323778182268\n",
      "epoch: 24 step: 195, loss is 0.0018552802503108978\n",
      "epoch: 24 step: 196, loss is 0.054312944412231445\n",
      "epoch: 24 step: 197, loss is 0.0004500588693190366\n",
      "epoch: 24 step: 198, loss is 0.0006260191439650953\n",
      "epoch: 24 step: 199, loss is 0.0012574775610119104\n",
      "epoch: 24 step: 200, loss is 0.02312944456934929\n",
      "epoch: 24 step: 201, loss is 0.0002152540546376258\n",
      "epoch: 24 step: 202, loss is 0.003910648170858622\n",
      "epoch: 24 step: 203, loss is 0.008249383419752121\n",
      "epoch: 24 step: 204, loss is 0.0001256082468898967\n",
      "epoch: 24 step: 205, loss is 0.0022514485754072666\n",
      "epoch: 24 step: 206, loss is 0.0029495684430003166\n",
      "epoch: 24 step: 207, loss is 0.0023734529968351126\n",
      "epoch: 24 step: 208, loss is 0.004199010785669088\n",
      "epoch: 24 step: 209, loss is 0.0023571066558361053\n",
      "epoch: 24 step: 210, loss is 0.00229125889018178\n",
      "epoch: 24 step: 211, loss is 0.04584063962101936\n",
      "epoch: 24 step: 212, loss is 0.012857665307819843\n",
      "epoch: 24 step: 213, loss is 0.011756310239434242\n",
      "epoch: 24 step: 214, loss is 0.0008359888452105224\n",
      "epoch: 24 step: 215, loss is 0.034855037927627563\n",
      "epoch: 24 step: 216, loss is 0.004936146549880505\n",
      "epoch: 24 step: 217, loss is 0.0005883933044970036\n",
      "epoch: 24 step: 218, loss is 0.07772143930196762\n",
      "epoch: 24 step: 219, loss is 0.016575945541262627\n",
      "epoch: 24 step: 220, loss is 0.0009232783922925591\n",
      "epoch: 24 step: 221, loss is 0.0008446805877611041\n",
      "epoch: 24 step: 222, loss is 0.0008092907373793423\n",
      "epoch: 24 step: 223, loss is 0.0028452780097723007\n",
      "epoch: 24 step: 224, loss is 0.0015543715562671423\n",
      "epoch: 24 step: 225, loss is 0.0012413949007168412\n",
      "epoch: 24 step: 226, loss is 0.0006697239587083459\n",
      "epoch: 24 step: 227, loss is 0.00017506492440588772\n",
      "epoch: 24 step: 228, loss is 0.04852236062288284\n",
      "epoch: 24 step: 229, loss is 0.0014402659144252539\n",
      "epoch: 24 step: 230, loss is 0.00018746915156953037\n",
      "epoch: 24 step: 231, loss is 0.007316252216696739\n",
      "epoch: 24 step: 232, loss is 0.042336683720350266\n",
      "epoch: 24 step: 233, loss is 0.00011210997763555497\n",
      "epoch: 24 step: 234, loss is 0.003147309413179755\n",
      "epoch: 24 step: 235, loss is 0.0053015658631920815\n",
      "epoch: 24 step: 236, loss is 0.012352443300187588\n",
      "epoch: 24 step: 237, loss is 0.000594250566791743\n",
      "epoch: 24 step: 238, loss is 0.002696490613743663\n",
      "epoch: 24 step: 239, loss is 0.004609859082847834\n",
      "epoch: 24 step: 240, loss is 0.006909408140927553\n",
      "epoch: 24 step: 241, loss is 0.014335873536765575\n",
      "epoch: 24 step: 242, loss is 0.0002510966151021421\n",
      "epoch: 24 step: 243, loss is 0.0047241924330592155\n",
      "epoch: 24 step: 244, loss is 0.0001189346075989306\n",
      "epoch: 24 step: 245, loss is 0.0012410846538841724\n",
      "epoch: 24 step: 246, loss is 0.004468235652893782\n",
      "epoch: 24 step: 247, loss is 0.0024751583114266396\n",
      "epoch: 24 step: 248, loss is 9.755779319675639e-05\n",
      "epoch: 24 step: 249, loss is 0.0007460765773430467\n",
      "epoch: 24 step: 250, loss is 0.007007997017353773\n",
      "epoch: 24 step: 251, loss is 0.0001568684383528307\n",
      "epoch: 24 step: 252, loss is 0.004358804784715176\n",
      "epoch: 24 step: 253, loss is 0.008992630057036877\n",
      "epoch: 24 step: 254, loss is 0.004611372947692871\n",
      "epoch: 24 step: 255, loss is 0.11588900536298752\n",
      "epoch: 24 step: 256, loss is 0.002115159295499325\n",
      "epoch: 24 step: 257, loss is 0.0042402977123856544\n",
      "epoch: 24 step: 258, loss is 0.004349800292402506\n",
      "epoch: 24 step: 259, loss is 0.0247064009308815\n",
      "epoch: 24 step: 260, loss is 0.0005088835605420172\n",
      "epoch: 24 step: 261, loss is 0.007134113926440477\n",
      "epoch: 24 step: 262, loss is 0.002198014408349991\n",
      "epoch: 24 step: 263, loss is 0.04908325523138046\n",
      "epoch: 24 step: 264, loss is 0.003499268554151058\n",
      "epoch: 24 step: 265, loss is 0.0022431903053075075\n",
      "epoch: 24 step: 266, loss is 0.00011906703002750874\n",
      "epoch: 24 step: 267, loss is 0.0008383062668144703\n",
      "epoch: 24 step: 268, loss is 0.1596439778804779\n",
      "epoch: 24 step: 269, loss is 0.0011884920531883836\n",
      "epoch: 24 step: 270, loss is 0.001776157645508647\n",
      "epoch: 24 step: 271, loss is 0.002771268133074045\n",
      "epoch: 24 step: 272, loss is 0.05721702799201012\n",
      "epoch: 24 step: 273, loss is 0.0007082084193825722\n",
      "epoch: 24 step: 274, loss is 0.03127703070640564\n",
      "epoch: 24 step: 275, loss is 0.001140599139034748\n",
      "epoch: 24 step: 276, loss is 0.007617409806698561\n",
      "epoch: 24 step: 277, loss is 0.047477636486291885\n",
      "epoch: 24 step: 278, loss is 0.01743229292333126\n",
      "epoch: 24 step: 279, loss is 0.026512205600738525\n",
      "epoch: 24 step: 280, loss is 0.0011788373813033104\n",
      "epoch: 24 step: 281, loss is 0.008338228799402714\n",
      "epoch: 24 step: 282, loss is 0.00012412594514898956\n",
      "epoch: 24 step: 283, loss is 0.0038768730591982603\n",
      "epoch: 24 step: 284, loss is 0.00046321749687194824\n",
      "epoch: 24 step: 285, loss is 0.056001998484134674\n",
      "epoch: 24 step: 286, loss is 0.00022582683595828712\n",
      "epoch: 24 step: 287, loss is 0.0023644622415304184\n",
      "epoch: 24 step: 288, loss is 0.043610427528619766\n",
      "epoch: 24 step: 289, loss is 0.006047927308827639\n",
      "epoch: 24 step: 290, loss is 0.0005685686483047903\n",
      "epoch: 24 step: 291, loss is 0.00018796944641508162\n",
      "epoch: 24 step: 292, loss is 0.013230311684310436\n",
      "epoch: 24 step: 293, loss is 0.0003956838045269251\n",
      "epoch: 24 step: 294, loss is 0.0014557404210790992\n",
      "epoch: 24 step: 295, loss is 0.00015308175352402031\n",
      "epoch: 24 step: 296, loss is 0.0006331703043542802\n",
      "epoch: 24 step: 297, loss is 0.013809537515044212\n",
      "epoch: 24 step: 298, loss is 0.0007631981861777604\n",
      "epoch: 24 step: 299, loss is 0.0043722279369831085\n",
      "epoch: 24 step: 300, loss is 0.00370492204092443\n",
      "epoch: 24 step: 301, loss is 0.0009595601004548371\n",
      "epoch: 24 step: 302, loss is 0.10217469185590744\n",
      "epoch: 24 step: 303, loss is 0.000835150945931673\n",
      "epoch: 24 step: 304, loss is 0.04473491758108139\n",
      "epoch: 24 step: 305, loss is 0.004414070397615433\n",
      "epoch: 24 step: 306, loss is 0.0014669562224298716\n",
      "epoch: 24 step: 307, loss is 0.0075589497573673725\n",
      "epoch: 24 step: 308, loss is 0.000321687082760036\n",
      "epoch: 24 step: 309, loss is 0.008127080276608467\n",
      "epoch: 24 step: 310, loss is 0.0009460836299695075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 311, loss is 0.004633394535630941\n",
      "epoch: 24 step: 312, loss is 0.0005946096498519182\n",
      "epoch: 24 step: 313, loss is 0.0010683737928047776\n",
      "epoch: 24 step: 314, loss is 0.0013131157029420137\n",
      "epoch: 24 step: 315, loss is 0.019432436674833298\n",
      "epoch: 24 step: 316, loss is 0.0008943708962760866\n",
      "epoch: 24 step: 317, loss is 0.029951421543955803\n",
      "epoch: 24 step: 318, loss is 0.026617534458637238\n",
      "epoch: 24 step: 319, loss is 0.0015973245026543736\n",
      "epoch: 24 step: 320, loss is 0.0005360866780392826\n",
      "epoch: 24 step: 321, loss is 0.0010539188515394926\n",
      "epoch: 24 step: 322, loss is 0.0003894484252668917\n",
      "epoch: 24 step: 323, loss is 0.00011505910515552387\n",
      "epoch: 24 step: 324, loss is 0.12359614670276642\n",
      "epoch: 24 step: 325, loss is 0.02008746936917305\n",
      "epoch: 24 step: 326, loss is 0.007018284872174263\n",
      "epoch: 24 step: 327, loss is 0.0029663473833352327\n",
      "epoch: 24 step: 328, loss is 0.0023026077542454004\n",
      "epoch: 24 step: 329, loss is 0.0010938168270513415\n",
      "epoch: 24 step: 330, loss is 0.0013922016369178891\n",
      "epoch: 24 step: 331, loss is 0.004880645778030157\n",
      "epoch: 24 step: 332, loss is 0.009311451576650143\n",
      "epoch: 24 step: 333, loss is 0.0004093397583346814\n",
      "epoch: 24 step: 334, loss is 0.006112393923103809\n",
      "epoch: 24 step: 335, loss is 0.0001278768468182534\n",
      "epoch: 24 step: 336, loss is 0.018404798582196236\n",
      "epoch: 24 step: 337, loss is 0.0003604310913942754\n",
      "epoch: 24 step: 338, loss is 0.0007004484068602324\n",
      "epoch: 24 step: 339, loss is 0.017265740782022476\n",
      "epoch: 24 step: 340, loss is 0.0033413029741495848\n",
      "epoch: 24 step: 341, loss is 0.016576716676354408\n",
      "epoch: 24 step: 342, loss is 0.0046311793848872185\n",
      "epoch: 24 step: 343, loss is 0.0011830369476228952\n",
      "epoch: 24 step: 344, loss is 0.002089275512844324\n",
      "epoch: 24 step: 345, loss is 0.015341190621256828\n",
      "epoch: 24 step: 346, loss is 0.00792858749628067\n",
      "epoch: 24 step: 347, loss is 0.006186270155012608\n",
      "epoch: 24 step: 348, loss is 0.0030681139323860407\n",
      "epoch: 24 step: 349, loss is 0.00019813081598840654\n",
      "epoch: 24 step: 350, loss is 0.012719858437776566\n",
      "epoch: 24 step: 351, loss is 0.00012238624913152307\n",
      "epoch: 24 step: 352, loss is 0.0008789036655798554\n",
      "epoch: 24 step: 353, loss is 3.9167825889308006e-05\n",
      "epoch: 24 step: 354, loss is 0.008299414068460464\n",
      "epoch: 24 step: 355, loss is 0.0013061629142612219\n",
      "epoch: 24 step: 356, loss is 0.01423339918255806\n",
      "epoch: 24 step: 357, loss is 7.55072760512121e-05\n",
      "epoch: 24 step: 358, loss is 0.0003352088970132172\n",
      "epoch: 24 step: 359, loss is 0.02122591808438301\n",
      "epoch: 24 step: 360, loss is 0.06815946847200394\n",
      "epoch: 24 step: 361, loss is 0.000188085890840739\n",
      "epoch: 24 step: 362, loss is 0.0008804031531326473\n",
      "epoch: 24 step: 363, loss is 0.0009974854765459895\n",
      "epoch: 24 step: 364, loss is 0.009948081336915493\n",
      "epoch: 24 step: 365, loss is 8.096411329461262e-05\n",
      "epoch: 24 step: 366, loss is 0.00052992207929492\n",
      "epoch: 24 step: 367, loss is 0.04344861954450607\n",
      "epoch: 24 step: 368, loss is 0.0032193586230278015\n",
      "epoch: 24 step: 369, loss is 0.016202574595808983\n",
      "epoch: 24 step: 370, loss is 0.0008028671727515757\n",
      "epoch: 24 step: 371, loss is 0.0007577937794849277\n",
      "epoch: 24 step: 372, loss is 0.010108336806297302\n",
      "epoch: 24 step: 373, loss is 0.0005865865387022495\n",
      "epoch: 24 step: 374, loss is 0.0007239844417199492\n",
      "epoch: 24 step: 375, loss is 0.017102086916565895\n",
      "epoch: 24 step: 376, loss is 0.00020742512424476445\n",
      "epoch: 24 step: 377, loss is 0.00261307624168694\n",
      "epoch: 24 step: 378, loss is 0.003806796856224537\n",
      "epoch: 24 step: 379, loss is 0.0024471015203744173\n",
      "epoch: 24 step: 380, loss is 0.016552308574318886\n",
      "epoch: 24 step: 381, loss is 0.009922552853822708\n",
      "epoch: 24 step: 382, loss is 0.004032867029309273\n",
      "epoch: 24 step: 383, loss is 0.0036824923008680344\n",
      "epoch: 24 step: 384, loss is 0.028803903609514236\n",
      "epoch: 24 step: 385, loss is 0.000381591817131266\n",
      "epoch: 24 step: 386, loss is 0.01383821852505207\n",
      "epoch: 24 step: 387, loss is 0.0010456766467541456\n",
      "epoch: 24 step: 388, loss is 0.007277635857462883\n",
      "epoch: 24 step: 389, loss is 0.0004326991329435259\n",
      "epoch: 24 step: 390, loss is 0.02948756143450737\n",
      "epoch: 24 step: 391, loss is 0.00013900127669330686\n",
      "epoch: 24 step: 392, loss is 0.0012062751920893788\n",
      "epoch: 24 step: 393, loss is 0.1243358850479126\n",
      "epoch: 24 step: 394, loss is 0.03985638543963432\n",
      "epoch: 24 step: 395, loss is 0.026860561221837997\n",
      "epoch: 24 step: 396, loss is 0.0016436693258583546\n",
      "epoch: 24 step: 397, loss is 0.0010573453037068248\n",
      "epoch: 24 step: 398, loss is 0.02062331885099411\n",
      "epoch: 24 step: 399, loss is 0.0004305157926864922\n",
      "epoch: 24 step: 400, loss is 0.004495390225201845\n",
      "epoch: 24 step: 401, loss is 0.026578951627016068\n",
      "epoch: 24 step: 402, loss is 0.008897123858332634\n",
      "epoch: 24 step: 403, loss is 0.0011398820206522942\n",
      "epoch: 24 step: 404, loss is 0.2736746370792389\n",
      "epoch: 24 step: 405, loss is 0.042455900460481644\n",
      "epoch: 24 step: 406, loss is 0.017959076911211014\n",
      "epoch: 24 step: 407, loss is 0.0008291731355711818\n",
      "epoch: 24 step: 408, loss is 0.0033726519905030727\n",
      "epoch: 24 step: 409, loss is 0.016731873154640198\n",
      "epoch: 24 step: 410, loss is 0.008153515867888927\n",
      "epoch: 24 step: 411, loss is 0.0048767756670713425\n",
      "epoch: 24 step: 412, loss is 0.003931822720915079\n",
      "epoch: 24 step: 413, loss is 0.01083353627473116\n",
      "epoch: 24 step: 414, loss is 0.00490556051954627\n",
      "epoch: 24 step: 415, loss is 0.00950414314866066\n",
      "epoch: 24 step: 416, loss is 0.00039180839667096734\n",
      "epoch: 24 step: 417, loss is 0.0029733150731772184\n",
      "epoch: 24 step: 418, loss is 0.0006171117420308292\n",
      "epoch: 24 step: 419, loss is 0.03711008280515671\n",
      "epoch: 24 step: 420, loss is 0.045382365584373474\n",
      "epoch: 24 step: 421, loss is 0.0022420715540647507\n",
      "epoch: 24 step: 422, loss is 0.030585918575525284\n",
      "epoch: 24 step: 423, loss is 0.0004214729997329414\n",
      "epoch: 24 step: 424, loss is 0.010320741683244705\n",
      "epoch: 24 step: 425, loss is 0.04312032088637352\n",
      "epoch: 24 step: 426, loss is 0.0005122216534800828\n",
      "epoch: 24 step: 427, loss is 0.019613441079854965\n",
      "epoch: 24 step: 428, loss is 0.006317052990198135\n",
      "epoch: 24 step: 429, loss is 0.10153669118881226\n",
      "epoch: 24 step: 430, loss is 0.03986045718193054\n",
      "epoch: 24 step: 431, loss is 0.005250397603958845\n",
      "epoch: 24 step: 432, loss is 0.004143683705478907\n",
      "epoch: 24 step: 433, loss is 0.015840401872992516\n",
      "epoch: 24 step: 434, loss is 0.00032466475386172533\n",
      "epoch: 24 step: 435, loss is 0.0012278722133487463\n",
      "epoch: 24 step: 436, loss is 0.000860437867231667\n",
      "epoch: 24 step: 437, loss is 0.020982740446925163\n",
      "epoch: 24 step: 438, loss is 0.02735055796802044\n",
      "epoch: 24 step: 439, loss is 0.0019201557151973248\n",
      "epoch: 24 step: 440, loss is 0.014636984094977379\n",
      "epoch: 24 step: 441, loss is 0.008547774516046047\n",
      "epoch: 24 step: 442, loss is 0.0011292751878499985\n",
      "epoch: 24 step: 443, loss is 0.004399682395160198\n",
      "epoch: 24 step: 444, loss is 0.0028601004742085934\n",
      "epoch: 24 step: 445, loss is 0.00791723933070898\n",
      "epoch: 24 step: 446, loss is 8.709870598977432e-05\n",
      "epoch: 24 step: 447, loss is 0.0004937721532769501\n",
      "epoch: 24 step: 448, loss is 0.0015458938432857394\n",
      "epoch: 24 step: 449, loss is 0.0011634038528427482\n",
      "epoch: 24 step: 450, loss is 0.006942901760339737\n",
      "epoch: 24 step: 451, loss is 0.09479531645774841\n",
      "epoch: 24 step: 452, loss is 0.0008744950755499303\n",
      "epoch: 24 step: 453, loss is 0.0013512595323845744\n",
      "epoch: 24 step: 454, loss is 0.01869223639369011\n",
      "epoch: 24 step: 455, loss is 0.006877327337861061\n",
      "epoch: 24 step: 456, loss is 0.00041472393786534667\n",
      "epoch: 24 step: 457, loss is 0.02050863392651081\n",
      "epoch: 24 step: 458, loss is 0.0028227423317730427\n",
      "epoch: 24 step: 459, loss is 0.01484766136854887\n",
      "epoch: 24 step: 460, loss is 0.0021234217565506697\n",
      "epoch: 24 step: 461, loss is 0.012867179699242115\n",
      "epoch: 24 step: 462, loss is 9.313562622992322e-05\n",
      "epoch: 24 step: 463, loss is 0.03165695071220398\n",
      "epoch: 24 step: 464, loss is 0.06910973787307739\n",
      "epoch: 24 step: 465, loss is 0.003370503196492791\n",
      "epoch: 24 step: 466, loss is 0.00029952492332085967\n",
      "epoch: 24 step: 467, loss is 0.0003762165433727205\n",
      "epoch: 24 step: 468, loss is 0.00475178100168705\n",
      "epoch: 24 step: 469, loss is 0.004236698616296053\n",
      "epoch: 24 step: 470, loss is 0.01646103523671627\n",
      "epoch: 24 step: 471, loss is 0.014135243371129036\n",
      "epoch: 24 step: 472, loss is 0.08053639531135559\n",
      "epoch: 24 step: 473, loss is 0.01088505145162344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 474, loss is 0.008805993013083935\n",
      "epoch: 24 step: 475, loss is 0.001259436015971005\n",
      "epoch: 24 step: 476, loss is 0.09451330453157425\n",
      "epoch: 24 step: 477, loss is 0.056396398693323135\n",
      "epoch: 24 step: 478, loss is 0.10526344180107117\n",
      "epoch: 24 step: 479, loss is 0.001296514761634171\n",
      "epoch: 24 step: 480, loss is 0.0015031853690743446\n",
      "epoch: 24 step: 481, loss is 0.0017778815235942602\n",
      "epoch: 24 step: 482, loss is 0.01568714529275894\n",
      "epoch: 24 step: 483, loss is 0.0022906966041773558\n",
      "epoch: 24 step: 484, loss is 0.019633041694760323\n",
      "epoch: 24 step: 485, loss is 0.014406201429665089\n",
      "epoch: 24 step: 486, loss is 0.09995227307081223\n",
      "epoch: 24 step: 487, loss is 0.07138287276029587\n",
      "epoch: 24 step: 488, loss is 0.024203458800911903\n",
      "epoch: 24 step: 489, loss is 0.004007100127637386\n",
      "epoch: 24 step: 490, loss is 0.014353282749652863\n",
      "epoch: 24 step: 491, loss is 0.0760321170091629\n",
      "epoch: 24 step: 492, loss is 0.09151657670736313\n",
      "epoch: 24 step: 493, loss is 0.004054082091897726\n",
      "epoch: 24 step: 494, loss is 0.017749279737472534\n",
      "epoch: 24 step: 495, loss is 0.034220676869153976\n",
      "epoch: 24 step: 496, loss is 0.00219132611528039\n",
      "epoch: 24 step: 497, loss is 0.002277806866914034\n",
      "epoch: 24 step: 498, loss is 0.002529293531551957\n",
      "epoch: 24 step: 499, loss is 0.035787515342235565\n",
      "epoch: 24 step: 500, loss is 0.0068938229233026505\n",
      "epoch: 24 step: 501, loss is 0.010767408646643162\n",
      "epoch: 24 step: 502, loss is 0.0013297107070684433\n",
      "epoch: 24 step: 503, loss is 0.004032200202345848\n",
      "epoch: 24 step: 504, loss is 0.013480279594659805\n",
      "epoch: 24 step: 505, loss is 0.01634236052632332\n",
      "epoch: 24 step: 506, loss is 0.06326466798782349\n",
      "epoch: 24 step: 507, loss is 0.043264634907245636\n",
      "epoch: 24 step: 508, loss is 0.014309144578874111\n",
      "epoch: 24 step: 509, loss is 0.09960119426250458\n",
      "epoch: 24 step: 510, loss is 0.04392441734671593\n",
      "epoch: 24 step: 511, loss is 0.0008698176243342459\n",
      "epoch: 24 step: 512, loss is 0.005963814444839954\n",
      "epoch: 24 step: 513, loss is 0.13170775771141052\n",
      "epoch: 24 step: 514, loss is 0.0012246188707649708\n",
      "epoch: 24 step: 515, loss is 0.015644341707229614\n",
      "epoch: 24 step: 516, loss is 0.11500908434391022\n",
      "epoch: 24 step: 517, loss is 0.0013735878746956587\n",
      "epoch: 24 step: 518, loss is 0.01291502546519041\n",
      "epoch: 24 step: 519, loss is 0.002460853662341833\n",
      "epoch: 24 step: 520, loss is 0.015023631975054741\n",
      "epoch: 24 step: 521, loss is 0.023407747969031334\n",
      "epoch: 24 step: 522, loss is 0.00606458680704236\n",
      "epoch: 24 step: 523, loss is 0.00437853904440999\n",
      "epoch: 24 step: 524, loss is 0.004354284144937992\n",
      "epoch: 24 step: 525, loss is 0.007864358834922314\n",
      "epoch: 24 step: 526, loss is 0.00040190204163081944\n",
      "epoch: 24 step: 527, loss is 0.0006821417482569814\n",
      "epoch: 24 step: 528, loss is 0.01234471332281828\n",
      "epoch: 24 step: 529, loss is 0.013445070944726467\n",
      "epoch: 24 step: 530, loss is 0.029449107125401497\n",
      "epoch: 24 step: 531, loss is 0.0005308600957505405\n",
      "epoch: 24 step: 532, loss is 0.0008820421644486487\n",
      "epoch: 24 step: 533, loss is 0.000653208524454385\n",
      "epoch: 24 step: 534, loss is 0.01406930387020111\n",
      "epoch: 24 step: 535, loss is 0.002160142408683896\n",
      "epoch: 24 step: 536, loss is 0.0014273301931098104\n",
      "epoch: 24 step: 537, loss is 0.0004010941192973405\n",
      "epoch: 24 step: 538, loss is 0.0013783586909994483\n",
      "epoch: 24 step: 539, loss is 0.00535963661968708\n",
      "epoch: 24 step: 540, loss is 0.01943141222000122\n",
      "epoch: 24 step: 541, loss is 0.0026437274646013975\n",
      "epoch: 24 step: 542, loss is 0.0046095228753983974\n",
      "epoch: 24 step: 543, loss is 0.010498675517737865\n",
      "epoch: 24 step: 544, loss is 0.0050122095271945\n",
      "epoch: 24 step: 545, loss is 0.002355226082727313\n",
      "epoch: 24 step: 546, loss is 0.0015579762402921915\n",
      "epoch: 24 step: 547, loss is 0.0011891056783497334\n",
      "epoch: 24 step: 548, loss is 0.0021156934089958668\n",
      "epoch: 24 step: 549, loss is 0.005029203835874796\n",
      "epoch: 24 step: 550, loss is 0.000606193789280951\n",
      "epoch: 24 step: 551, loss is 4.045337118441239e-05\n",
      "epoch: 24 step: 552, loss is 0.0007644536672160029\n",
      "epoch: 24 step: 553, loss is 0.0019195738714188337\n",
      "epoch: 24 step: 554, loss is 0.011664684861898422\n",
      "epoch: 24 step: 555, loss is 0.00013967452105134726\n",
      "epoch: 24 step: 556, loss is 7.384485070360824e-05\n",
      "epoch: 24 step: 557, loss is 0.0006464565522037446\n",
      "epoch: 24 step: 558, loss is 0.019372928887605667\n",
      "epoch: 24 step: 559, loss is 0.0011671166867017746\n",
      "epoch: 24 step: 560, loss is 0.007221601437777281\n",
      "epoch: 24 step: 561, loss is 0.016858799383044243\n",
      "epoch: 24 step: 562, loss is 0.0009930970845744014\n",
      "epoch: 24 step: 563, loss is 0.0041831680573523045\n",
      "epoch: 24 step: 564, loss is 0.009428457356989384\n",
      "epoch: 24 step: 565, loss is 0.0009632902219891548\n",
      "epoch: 24 step: 566, loss is 0.00027399990358389914\n",
      "epoch: 24 step: 567, loss is 0.0013361932942643762\n",
      "epoch: 24 step: 568, loss is 0.03503774479031563\n",
      "epoch: 24 step: 569, loss is 0.1005394235253334\n",
      "epoch: 24 step: 570, loss is 0.0005519611295312643\n",
      "epoch: 24 step: 571, loss is 0.0016332495724782348\n",
      "epoch: 24 step: 572, loss is 0.007612871937453747\n",
      "epoch: 24 step: 573, loss is 0.02107393369078636\n",
      "epoch: 24 step: 574, loss is 7.088606798788533e-05\n",
      "epoch: 24 step: 575, loss is 0.0005378209752961993\n",
      "epoch: 24 step: 576, loss is 0.030166540294885635\n",
      "epoch: 24 step: 577, loss is 0.011238249018788338\n",
      "epoch: 24 step: 578, loss is 0.007508180104196072\n",
      "epoch: 24 step: 579, loss is 0.03442469239234924\n",
      "epoch: 24 step: 580, loss is 0.06571957468986511\n",
      "epoch: 24 step: 581, loss is 0.002784215146675706\n",
      "epoch: 24 step: 582, loss is 0.08655033260583878\n",
      "epoch: 24 step: 583, loss is 0.0007284990279003978\n",
      "epoch: 24 step: 584, loss is 0.011240324005484581\n",
      "epoch: 24 step: 585, loss is 0.010750798508524895\n",
      "epoch: 24 step: 586, loss is 0.0006507673533633351\n",
      "epoch: 24 step: 587, loss is 0.004563906695693731\n",
      "epoch: 24 step: 588, loss is 0.0002464450371917337\n",
      "epoch: 24 step: 589, loss is 0.01037545595318079\n",
      "epoch: 24 step: 590, loss is 0.0025453087873756886\n",
      "epoch: 24 step: 591, loss is 0.008719071745872498\n",
      "epoch: 24 step: 592, loss is 0.017011011019349098\n",
      "epoch: 24 step: 593, loss is 0.000605462584644556\n",
      "epoch: 24 step: 594, loss is 0.003219307167455554\n",
      "epoch: 24 step: 595, loss is 0.007682835217565298\n",
      "epoch: 24 step: 596, loss is 0.002109708497300744\n",
      "epoch: 24 step: 597, loss is 0.05843304842710495\n",
      "epoch: 24 step: 598, loss is 0.0003408025950193405\n",
      "epoch: 24 step: 599, loss is 0.009068500250577927\n",
      "epoch: 24 step: 600, loss is 0.0030537366401404142\n",
      "epoch: 24 step: 601, loss is 0.011651953682303429\n",
      "epoch: 24 step: 602, loss is 0.0003212403680663556\n",
      "epoch: 24 step: 603, loss is 0.0007037838222458959\n",
      "epoch: 24 step: 604, loss is 0.004590952303260565\n",
      "epoch: 24 step: 605, loss is 0.0012065025512129068\n",
      "epoch: 24 step: 606, loss is 0.016898760572075844\n",
      "epoch: 24 step: 607, loss is 0.0005742558278143406\n",
      "epoch: 24 step: 608, loss is 0.02520415373146534\n",
      "epoch: 24 step: 609, loss is 0.03630717098712921\n",
      "epoch: 24 step: 610, loss is 0.0027917169500142336\n",
      "epoch: 24 step: 611, loss is 0.0001500320213381201\n",
      "epoch: 24 step: 612, loss is 0.0400785394012928\n",
      "epoch: 24 step: 613, loss is 0.002504986012354493\n",
      "epoch: 24 step: 614, loss is 0.11226077377796173\n",
      "epoch: 24 step: 615, loss is 0.0008205081103369594\n",
      "epoch: 24 step: 616, loss is 0.0004145264974795282\n",
      "epoch: 24 step: 617, loss is 0.009247536770999432\n",
      "epoch: 24 step: 618, loss is 0.001145233865827322\n",
      "epoch: 24 step: 619, loss is 0.00030435377266258\n",
      "epoch: 24 step: 620, loss is 0.012814510613679886\n",
      "epoch: 24 step: 621, loss is 0.0026619513519108295\n",
      "epoch: 24 step: 622, loss is 0.008153234608471394\n",
      "epoch: 24 step: 623, loss is 0.0012129914248362184\n",
      "epoch: 24 step: 624, loss is 0.010278469882905483\n",
      "epoch: 24 step: 625, loss is 0.0032652097288519144\n",
      "epoch: 24 step: 626, loss is 0.007350980304181576\n",
      "epoch: 24 step: 627, loss is 0.003090976271778345\n",
      "epoch: 24 step: 628, loss is 0.041301775723695755\n",
      "epoch: 24 step: 629, loss is 0.0005057313246652484\n",
      "epoch: 24 step: 630, loss is 0.0002431980101391673\n",
      "epoch: 24 step: 631, loss is 0.001190110808238387\n",
      "epoch: 24 step: 632, loss is 0.027732504531741142\n",
      "epoch: 24 step: 633, loss is 0.01611383631825447\n",
      "epoch: 24 step: 634, loss is 0.015208335593342781\n",
      "epoch: 24 step: 635, loss is 0.005320151802152395\n",
      "epoch: 24 step: 636, loss is 0.0003236742631997913\n",
      "epoch: 24 step: 637, loss is 0.05296963453292847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 638, loss is 0.0006130116526037455\n",
      "epoch: 24 step: 639, loss is 0.02388428896665573\n",
      "epoch: 24 step: 640, loss is 0.01306266151368618\n",
      "epoch: 24 step: 641, loss is 0.0004154884081799537\n",
      "epoch: 24 step: 642, loss is 0.0003712959587574005\n",
      "epoch: 24 step: 643, loss is 0.010629490949213505\n",
      "epoch: 24 step: 644, loss is 0.06711684167385101\n",
      "epoch: 24 step: 645, loss is 0.00014241845929063857\n",
      "epoch: 24 step: 646, loss is 0.011545154266059399\n",
      "epoch: 24 step: 647, loss is 0.008839664980769157\n",
      "epoch: 24 step: 648, loss is 0.009969794191420078\n",
      "epoch: 24 step: 649, loss is 0.03507377207279205\n",
      "epoch: 24 step: 650, loss is 0.015088068321347237\n",
      "epoch: 24 step: 651, loss is 1.316250109084649e-05\n",
      "epoch: 24 step: 652, loss is 0.032188571989536285\n",
      "epoch: 24 step: 653, loss is 0.005188108421862125\n",
      "epoch: 24 step: 654, loss is 0.0274253711104393\n",
      "epoch: 24 step: 655, loss is 0.00024125311756506562\n",
      "epoch: 24 step: 656, loss is 0.0001503180101281032\n",
      "epoch: 24 step: 657, loss is 8.307117968797684e-05\n",
      "epoch: 24 step: 658, loss is 0.0030070426873862743\n",
      "epoch: 24 step: 659, loss is 0.06084860861301422\n",
      "epoch: 24 step: 660, loss is 0.0030241685453802347\n",
      "epoch: 24 step: 661, loss is 0.046257875859737396\n",
      "epoch: 24 step: 662, loss is 0.0004932776209898293\n",
      "epoch: 24 step: 663, loss is 6.654270691797137e-05\n",
      "epoch: 24 step: 664, loss is 0.01844947598874569\n",
      "epoch: 24 step: 665, loss is 0.10537546873092651\n",
      "epoch: 24 step: 666, loss is 0.012903333641588688\n",
      "epoch: 24 step: 667, loss is 0.022173414006829262\n",
      "epoch: 24 step: 668, loss is 0.016663094982504845\n",
      "epoch: 24 step: 669, loss is 0.0014652370009571314\n",
      "epoch: 24 step: 670, loss is 0.005888517014682293\n",
      "epoch: 24 step: 671, loss is 0.0009634376619942486\n",
      "epoch: 24 step: 672, loss is 0.0147422906011343\n",
      "epoch: 24 step: 673, loss is 0.005759242922067642\n",
      "epoch: 24 step: 674, loss is 0.04764268174767494\n",
      "epoch: 24 step: 675, loss is 0.03243535757064819\n",
      "epoch: 24 step: 676, loss is 0.06143127754330635\n",
      "epoch: 24 step: 677, loss is 0.027005935087800026\n",
      "epoch: 24 step: 678, loss is 0.0040458678267896175\n",
      "epoch: 24 step: 679, loss is 0.003972440958023071\n",
      "epoch: 24 step: 680, loss is 0.001923455623909831\n",
      "epoch: 24 step: 681, loss is 0.0029943143017590046\n",
      "epoch: 24 step: 682, loss is 0.002802930073812604\n",
      "epoch: 24 step: 683, loss is 0.09841970354318619\n",
      "epoch: 24 step: 684, loss is 0.001990637741982937\n",
      "epoch: 24 step: 685, loss is 0.015393629670143127\n",
      "epoch: 24 step: 686, loss is 0.07547570765018463\n",
      "epoch: 24 step: 687, loss is 0.0021130049135535955\n",
      "epoch: 24 step: 688, loss is 0.01365501806139946\n",
      "epoch: 24 step: 689, loss is 0.0008977866382338107\n",
      "epoch: 24 step: 690, loss is 0.03850872069597244\n",
      "epoch: 24 step: 691, loss is 0.00305143091827631\n",
      "epoch: 24 step: 692, loss is 7.980594818945974e-05\n",
      "epoch: 24 step: 693, loss is 0.03271142393350601\n",
      "epoch: 24 step: 694, loss is 0.0002491293416824192\n",
      "epoch: 24 step: 695, loss is 0.03161094710230827\n",
      "epoch: 24 step: 696, loss is 0.06337186694145203\n",
      "epoch: 24 step: 697, loss is 0.0014612640952691436\n",
      "epoch: 24 step: 698, loss is 0.009742212481796741\n",
      "epoch: 24 step: 699, loss is 0.019800126552581787\n",
      "epoch: 24 step: 700, loss is 0.019557064399123192\n",
      "epoch: 24 step: 701, loss is 0.0482613779604435\n",
      "epoch: 24 step: 702, loss is 0.04606505110859871\n",
      "epoch: 24 step: 703, loss is 0.02115636318922043\n",
      "epoch: 24 step: 704, loss is 0.0034943101927638054\n",
      "epoch: 24 step: 705, loss is 0.0019056572346016765\n",
      "epoch: 24 step: 706, loss is 0.0032846052199602127\n",
      "epoch: 24 step: 707, loss is 0.0001129465235862881\n",
      "epoch: 24 step: 708, loss is 0.03273673728108406\n",
      "epoch: 24 step: 709, loss is 0.0003480113227851689\n",
      "epoch: 24 step: 710, loss is 0.00918959267437458\n",
      "epoch: 24 step: 711, loss is 0.0003812208888120949\n",
      "epoch: 24 step: 712, loss is 0.00021344251581467688\n",
      "epoch: 24 step: 713, loss is 0.001659638830460608\n",
      "epoch: 24 step: 714, loss is 0.03265375271439552\n",
      "epoch: 24 step: 715, loss is 0.007910826243460178\n",
      "epoch: 24 step: 716, loss is 0.031335923820734024\n",
      "epoch: 24 step: 717, loss is 0.016810288652777672\n",
      "epoch: 24 step: 718, loss is 0.0005221775500103831\n",
      "epoch: 24 step: 719, loss is 0.000627335102763027\n",
      "epoch: 24 step: 720, loss is 0.014827021397650242\n",
      "epoch: 24 step: 721, loss is 0.0008781597134657204\n",
      "epoch: 24 step: 722, loss is 0.01559033989906311\n",
      "epoch: 24 step: 723, loss is 0.017190735787153244\n",
      "epoch: 24 step: 724, loss is 0.00233214208856225\n",
      "epoch: 24 step: 725, loss is 0.05526020750403404\n",
      "epoch: 24 step: 726, loss is 0.00683203199878335\n",
      "epoch: 24 step: 727, loss is 0.01987963356077671\n",
      "epoch: 24 step: 728, loss is 0.008074374869465828\n",
      "epoch: 24 step: 729, loss is 0.0037891881074756384\n",
      "epoch: 24 step: 730, loss is 0.0013368072686716914\n",
      "epoch: 24 step: 731, loss is 0.04944045469164848\n",
      "epoch: 24 step: 732, loss is 0.010557708330452442\n",
      "epoch: 24 step: 733, loss is 0.002233046106994152\n",
      "epoch: 24 step: 734, loss is 0.007442794740200043\n",
      "epoch: 24 step: 735, loss is 0.002370215719565749\n",
      "epoch: 24 step: 736, loss is 0.005349434446543455\n",
      "epoch: 24 step: 737, loss is 0.001425746362656355\n",
      "epoch: 24 step: 738, loss is 0.0025939105544239283\n",
      "epoch: 24 step: 739, loss is 0.006665007211267948\n",
      "epoch: 24 step: 740, loss is 0.14333128929138184\n",
      "epoch: 24 step: 741, loss is 0.0029396689496934414\n",
      "epoch: 24 step: 742, loss is 0.018408622592687607\n",
      "epoch: 24 step: 743, loss is 0.0008649342344142497\n",
      "epoch: 24 step: 744, loss is 0.0001723006134852767\n",
      "epoch: 24 step: 745, loss is 6.640063656959683e-05\n",
      "epoch: 24 step: 746, loss is 0.00012321224494371563\n",
      "epoch: 24 step: 747, loss is 0.00086773227667436\n",
      "epoch: 24 step: 748, loss is 7.873425056459382e-05\n",
      "epoch: 24 step: 749, loss is 0.04335133731365204\n",
      "epoch: 24 step: 750, loss is 0.013238414190709591\n",
      "epoch: 24 step: 751, loss is 0.00010594433842925355\n",
      "epoch: 24 step: 752, loss is 0.058473940938711166\n",
      "epoch: 24 step: 753, loss is 0.06771562248468399\n",
      "epoch: 24 step: 754, loss is 0.0002524216833990067\n",
      "epoch: 24 step: 755, loss is 0.04376586899161339\n",
      "epoch: 24 step: 756, loss is 0.009351006709039211\n",
      "epoch: 24 step: 757, loss is 0.00033114475081674755\n",
      "epoch: 24 step: 758, loss is 0.02650655433535576\n",
      "epoch: 24 step: 759, loss is 0.06861559301614761\n",
      "epoch: 24 step: 760, loss is 0.026082955300807953\n",
      "epoch: 24 step: 761, loss is 0.012387750670313835\n",
      "epoch: 24 step: 762, loss is 0.0017084762221202254\n",
      "epoch: 24 step: 763, loss is 0.07841074466705322\n",
      "epoch: 24 step: 764, loss is 0.006177640054374933\n",
      "epoch: 24 step: 765, loss is 0.019939042627811432\n",
      "epoch: 24 step: 766, loss is 0.005943950731307268\n",
      "epoch: 24 step: 767, loss is 0.0058103688061237335\n",
      "epoch: 24 step: 768, loss is 0.0015510187949985266\n",
      "epoch: 24 step: 769, loss is 0.03683827817440033\n",
      "epoch: 24 step: 770, loss is 0.0037479533348232508\n",
      "epoch: 24 step: 771, loss is 0.11860004812479019\n",
      "epoch: 24 step: 772, loss is 0.0632794052362442\n",
      "epoch: 24 step: 773, loss is 7.817969162715599e-05\n",
      "epoch: 24 step: 774, loss is 0.0045875259675085545\n",
      "epoch: 24 step: 775, loss is 0.09566610306501389\n",
      "epoch: 24 step: 776, loss is 0.04577053338289261\n",
      "epoch: 24 step: 777, loss is 0.002848471747711301\n",
      "epoch: 24 step: 778, loss is 0.01963386870920658\n",
      "epoch: 24 step: 779, loss is 0.014410849660634995\n",
      "epoch: 24 step: 780, loss is 0.004583402536809444\n",
      "epoch: 24 step: 781, loss is 0.0025307589676231146\n",
      "epoch: 24 step: 782, loss is 0.07330076396465302\n",
      "epoch: 24 step: 783, loss is 0.01029793731868267\n",
      "epoch: 24 step: 784, loss is 0.0019371688831597567\n",
      "epoch: 24 step: 785, loss is 0.002198213944211602\n",
      "epoch: 24 step: 786, loss is 0.018549155443906784\n",
      "epoch: 24 step: 787, loss is 0.023357639089226723\n",
      "epoch: 24 step: 788, loss is 0.00020702494657598436\n",
      "epoch: 24 step: 789, loss is 0.007449437398463488\n",
      "epoch: 24 step: 790, loss is 0.005411946214735508\n",
      "epoch: 24 step: 791, loss is 0.006087133660912514\n",
      "epoch: 24 step: 792, loss is 0.005435370374470949\n",
      "epoch: 24 step: 793, loss is 0.06970833986997604\n",
      "epoch: 24 step: 794, loss is 0.0020284883212298155\n",
      "epoch: 24 step: 795, loss is 0.011445042677223682\n",
      "epoch: 24 step: 796, loss is 4.8944155423669145e-05\n",
      "epoch: 24 step: 797, loss is 0.011594031006097794\n",
      "epoch: 24 step: 798, loss is 0.0001551844470668584\n",
      "epoch: 24 step: 799, loss is 0.00189215037971735\n",
      "epoch: 24 step: 800, loss is 0.05049813911318779\n",
      "epoch: 24 step: 801, loss is 0.0037441300228238106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 802, loss is 0.08120527118444443\n",
      "epoch: 24 step: 803, loss is 0.18672478199005127\n",
      "epoch: 24 step: 804, loss is 0.039758484810590744\n",
      "epoch: 24 step: 805, loss is 0.0013512425357475877\n",
      "epoch: 24 step: 806, loss is 0.001984949456527829\n",
      "epoch: 24 step: 807, loss is 0.0004702500009443611\n",
      "epoch: 24 step: 808, loss is 0.0009767544688656926\n",
      "epoch: 24 step: 809, loss is 0.00581827387213707\n",
      "epoch: 24 step: 810, loss is 0.0005060613038949668\n",
      "epoch: 24 step: 811, loss is 0.000971378933172673\n",
      "epoch: 24 step: 812, loss is 0.0019491601269692183\n",
      "epoch: 24 step: 813, loss is 0.01715753600001335\n",
      "epoch: 24 step: 814, loss is 0.052961938083171844\n",
      "epoch: 24 step: 815, loss is 0.010888362303376198\n",
      "epoch: 24 step: 816, loss is 0.14709432423114777\n",
      "epoch: 24 step: 817, loss is 0.0028309207409620285\n",
      "epoch: 24 step: 818, loss is 0.0005322194192558527\n",
      "epoch: 24 step: 819, loss is 0.0019435801077634096\n",
      "epoch: 24 step: 820, loss is 0.11799237877130508\n",
      "epoch: 24 step: 821, loss is 0.010322106070816517\n",
      "epoch: 24 step: 822, loss is 0.01794886216521263\n",
      "epoch: 24 step: 823, loss is 0.04928647354245186\n",
      "epoch: 24 step: 824, loss is 0.0020181313157081604\n",
      "epoch: 24 step: 825, loss is 0.0003787571913562715\n",
      "epoch: 24 step: 826, loss is 0.023029986768960953\n",
      "epoch: 24 step: 827, loss is 0.02632335014641285\n",
      "epoch: 24 step: 828, loss is 0.020033666864037514\n",
      "epoch: 24 step: 829, loss is 0.007023792713880539\n",
      "epoch: 24 step: 830, loss is 0.01960381120443344\n",
      "epoch: 24 step: 831, loss is 0.10624554008245468\n",
      "epoch: 24 step: 832, loss is 0.006203820463269949\n",
      "epoch: 24 step: 833, loss is 0.07410790026187897\n",
      "epoch: 24 step: 834, loss is 0.0509837344288826\n",
      "epoch: 24 step: 835, loss is 0.0021112121175974607\n",
      "epoch: 24 step: 836, loss is 0.00913919322192669\n",
      "epoch: 24 step: 837, loss is 0.0034718033857643604\n",
      "epoch: 24 step: 838, loss is 0.1322868913412094\n",
      "epoch: 24 step: 839, loss is 0.0008010463789105415\n",
      "epoch: 24 step: 840, loss is 0.046896543353796005\n",
      "epoch: 24 step: 841, loss is 0.02600504644215107\n",
      "epoch: 24 step: 842, loss is 0.0006692215683870018\n",
      "epoch: 24 step: 843, loss is 0.026695119217038155\n",
      "epoch: 24 step: 844, loss is 0.0007005772786214948\n",
      "epoch: 24 step: 845, loss is 0.05763427913188934\n",
      "epoch: 24 step: 846, loss is 0.050211504101753235\n",
      "epoch: 24 step: 847, loss is 0.003351740539073944\n",
      "epoch: 24 step: 848, loss is 0.06001117080450058\n",
      "epoch: 24 step: 849, loss is 0.002730089472606778\n",
      "epoch: 24 step: 850, loss is 0.03339754790067673\n",
      "epoch: 24 step: 851, loss is 0.03596219792962074\n",
      "epoch: 24 step: 852, loss is 0.0009263342944905162\n",
      "epoch: 24 step: 853, loss is 0.0004004286602139473\n",
      "epoch: 24 step: 854, loss is 0.00033332552993670106\n",
      "epoch: 24 step: 855, loss is 0.003252871334552765\n",
      "epoch: 24 step: 856, loss is 0.007177487947046757\n",
      "epoch: 24 step: 857, loss is 0.003519157413393259\n",
      "epoch: 24 step: 858, loss is 0.05196555331349373\n",
      "epoch: 24 step: 859, loss is 0.005804392974823713\n",
      "epoch: 24 step: 860, loss is 0.003987227100878954\n",
      "epoch: 24 step: 861, loss is 0.002275434322655201\n",
      "epoch: 24 step: 862, loss is 0.008825322613120079\n",
      "epoch: 24 step: 863, loss is 0.0017258482985198498\n",
      "epoch: 24 step: 864, loss is 3.837786061922088e-05\n",
      "epoch: 24 step: 865, loss is 0.061971016228199005\n",
      "epoch: 24 step: 866, loss is 0.023055754601955414\n",
      "epoch: 24 step: 867, loss is 0.0010720101417973638\n",
      "epoch: 24 step: 868, loss is 0.008540160953998566\n",
      "epoch: 24 step: 869, loss is 0.01493687741458416\n",
      "epoch: 24 step: 870, loss is 0.005686995107680559\n",
      "epoch: 24 step: 871, loss is 0.0009468046482652426\n",
      "epoch: 24 step: 872, loss is 0.01868966594338417\n",
      "epoch: 24 step: 873, loss is 0.02396613173186779\n",
      "epoch: 24 step: 874, loss is 0.057905055582523346\n",
      "epoch: 24 step: 875, loss is 0.002517042448744178\n",
      "epoch: 24 step: 876, loss is 0.009494170546531677\n",
      "epoch: 24 step: 877, loss is 0.026352986693382263\n",
      "epoch: 24 step: 878, loss is 0.004235358908772469\n",
      "epoch: 24 step: 879, loss is 0.005224886350333691\n",
      "epoch: 24 step: 880, loss is 0.00880003534257412\n",
      "epoch: 24 step: 881, loss is 3.3518481359351426e-05\n",
      "epoch: 24 step: 882, loss is 0.057292912155389786\n",
      "epoch: 24 step: 883, loss is 0.00791940651834011\n",
      "epoch: 24 step: 884, loss is 0.003230957081541419\n",
      "epoch: 24 step: 885, loss is 0.053317729383707047\n",
      "epoch: 24 step: 886, loss is 0.012781689874827862\n",
      "epoch: 24 step: 887, loss is 0.0070011499337852\n",
      "epoch: 24 step: 888, loss is 0.006034554447978735\n",
      "epoch: 24 step: 889, loss is 0.0011857012286782265\n",
      "epoch: 24 step: 890, loss is 0.026990236714482307\n",
      "epoch: 24 step: 891, loss is 0.004033963195979595\n",
      "epoch: 24 step: 892, loss is 0.0038381293416023254\n",
      "epoch: 24 step: 893, loss is 0.05597527697682381\n",
      "epoch: 24 step: 894, loss is 0.004378055222332478\n",
      "epoch: 24 step: 895, loss is 0.04759587347507477\n",
      "epoch: 24 step: 896, loss is 0.011639468371868134\n",
      "epoch: 24 step: 897, loss is 0.003567753592506051\n",
      "epoch: 24 step: 898, loss is 0.14267638325691223\n",
      "epoch: 24 step: 899, loss is 0.004737367387861013\n",
      "epoch: 24 step: 900, loss is 0.003420010907575488\n",
      "epoch: 24 step: 901, loss is 0.00327350664883852\n",
      "epoch: 24 step: 902, loss is 0.03310747444629669\n",
      "epoch: 24 step: 903, loss is 0.0004116033960599452\n",
      "epoch: 24 step: 904, loss is 0.036724306643009186\n",
      "epoch: 24 step: 905, loss is 0.08979254215955734\n",
      "epoch: 24 step: 906, loss is 0.057064469903707504\n",
      "epoch: 24 step: 907, loss is 0.0024866664316505194\n",
      "epoch: 24 step: 908, loss is 0.0006049980875104666\n",
      "epoch: 24 step: 909, loss is 0.002835493301972747\n",
      "epoch: 24 step: 910, loss is 0.020636573433876038\n",
      "epoch: 24 step: 911, loss is 0.00047114823246374726\n",
      "epoch: 24 step: 912, loss is 0.008150574751198292\n",
      "epoch: 24 step: 913, loss is 0.020804785192012787\n",
      "epoch: 24 step: 914, loss is 0.00806979089975357\n",
      "epoch: 24 step: 915, loss is 0.015043295919895172\n",
      "epoch: 24 step: 916, loss is 0.00016461353516206145\n",
      "epoch: 24 step: 917, loss is 0.02954290248453617\n",
      "epoch: 24 step: 918, loss is 0.018302692100405693\n",
      "epoch: 24 step: 919, loss is 0.030795050784945488\n",
      "epoch: 24 step: 920, loss is 0.0006447454798035324\n",
      "epoch: 24 step: 921, loss is 0.13143837451934814\n",
      "epoch: 24 step: 922, loss is 0.007780090440064669\n",
      "epoch: 24 step: 923, loss is 0.00655320193618536\n",
      "epoch: 24 step: 924, loss is 0.02163117751479149\n",
      "epoch: 24 step: 925, loss is 9.636630420573056e-05\n",
      "epoch: 24 step: 926, loss is 0.020017322152853012\n",
      "epoch: 24 step: 927, loss is 0.09801699966192245\n",
      "epoch: 24 step: 928, loss is 0.023599211126565933\n",
      "epoch: 24 step: 929, loss is 0.00043141210335306823\n",
      "epoch: 24 step: 930, loss is 0.03284300118684769\n",
      "epoch: 24 step: 931, loss is 0.0034864256158471107\n",
      "epoch: 24 step: 932, loss is 0.005723342299461365\n",
      "epoch: 24 step: 933, loss is 0.0012380806729197502\n",
      "epoch: 24 step: 934, loss is 0.007343360222876072\n",
      "epoch: 24 step: 935, loss is 1.8760481907520443e-05\n",
      "epoch: 24 step: 936, loss is 0.003733030753210187\n",
      "epoch: 24 step: 937, loss is 0.028506742790341377\n",
      "epoch: 25 step: 1, loss is 0.004182044882327318\n",
      "epoch: 25 step: 2, loss is 0.004942742176353931\n",
      "epoch: 25 step: 3, loss is 0.0013359501026570797\n",
      "epoch: 25 step: 4, loss is 0.00500307884067297\n",
      "epoch: 25 step: 5, loss is 0.009361384436488152\n",
      "epoch: 25 step: 6, loss is 0.009513437747955322\n",
      "epoch: 25 step: 7, loss is 0.000888333423063159\n",
      "epoch: 25 step: 8, loss is 0.0073552667163312435\n",
      "epoch: 25 step: 9, loss is 0.00046915828716009855\n",
      "epoch: 25 step: 10, loss is 0.008206615224480629\n",
      "epoch: 25 step: 11, loss is 0.00037444967892952263\n",
      "epoch: 25 step: 12, loss is 0.0021508170757442713\n",
      "epoch: 25 step: 13, loss is 0.05079399421811104\n",
      "epoch: 25 step: 14, loss is 0.012579564936459064\n",
      "epoch: 25 step: 15, loss is 0.0006672398303635418\n",
      "epoch: 25 step: 16, loss is 0.0003260841185692698\n",
      "epoch: 25 step: 17, loss is 0.0031498989555984735\n",
      "epoch: 25 step: 18, loss is 0.0012242207303643227\n",
      "epoch: 25 step: 19, loss is 0.006038477644324303\n",
      "epoch: 25 step: 20, loss is 0.0027010515332221985\n",
      "epoch: 25 step: 21, loss is 0.09779103845357895\n",
      "epoch: 25 step: 22, loss is 0.0014957990497350693\n",
      "epoch: 25 step: 23, loss is 0.0019718303810805082\n",
      "epoch: 25 step: 24, loss is 0.001208101399242878\n",
      "epoch: 25 step: 25, loss is 0.007860320620238781\n",
      "epoch: 25 step: 26, loss is 0.01135340891778469\n",
      "epoch: 25 step: 27, loss is 0.038918279111385345\n",
      "epoch: 25 step: 28, loss is 0.0031007425859570503\n",
      "epoch: 25 step: 29, loss is 0.021899515762925148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 30, loss is 0.0010352220851927996\n",
      "epoch: 25 step: 31, loss is 0.00020354957086965442\n",
      "epoch: 25 step: 32, loss is 0.000565080379601568\n",
      "epoch: 25 step: 33, loss is 0.006329337600618601\n",
      "epoch: 25 step: 34, loss is 0.006933989934623241\n",
      "epoch: 25 step: 35, loss is 0.007758437190204859\n",
      "epoch: 25 step: 36, loss is 0.0004468863771762699\n",
      "epoch: 25 step: 37, loss is 0.0009200174827128649\n",
      "epoch: 25 step: 38, loss is 0.0071584805846214294\n",
      "epoch: 25 step: 39, loss is 0.17318764328956604\n",
      "epoch: 25 step: 40, loss is 0.02857988141477108\n",
      "epoch: 25 step: 41, loss is 0.0010973603930324316\n",
      "epoch: 25 step: 42, loss is 0.057233043015003204\n",
      "epoch: 25 step: 43, loss is 0.001753446296788752\n",
      "epoch: 25 step: 44, loss is 0.003495684126392007\n",
      "epoch: 25 step: 45, loss is 0.00031488490640185773\n",
      "epoch: 25 step: 46, loss is 0.00011084381549153477\n",
      "epoch: 25 step: 47, loss is 0.04219549521803856\n",
      "epoch: 25 step: 48, loss is 0.033956293016672134\n",
      "epoch: 25 step: 49, loss is 0.09871865808963776\n",
      "epoch: 25 step: 50, loss is 0.04063162952661514\n",
      "epoch: 25 step: 51, loss is 0.0024735203478485346\n",
      "epoch: 25 step: 52, loss is 0.01030502375215292\n",
      "epoch: 25 step: 53, loss is 0.00010057547478936613\n",
      "epoch: 25 step: 54, loss is 0.011909380555152893\n",
      "epoch: 25 step: 55, loss is 0.0017904118867591023\n",
      "epoch: 25 step: 56, loss is 0.000862313318066299\n",
      "epoch: 25 step: 57, loss is 0.011305461637675762\n",
      "epoch: 25 step: 58, loss is 0.02350648120045662\n",
      "epoch: 25 step: 59, loss is 0.0011357152834534645\n",
      "epoch: 25 step: 60, loss is 0.0004942096420563757\n",
      "epoch: 25 step: 61, loss is 0.004824796225875616\n",
      "epoch: 25 step: 62, loss is 0.030847396701574326\n",
      "epoch: 25 step: 63, loss is 0.011682182550430298\n",
      "epoch: 25 step: 64, loss is 0.0032225907780230045\n",
      "epoch: 25 step: 65, loss is 0.0006464447360485792\n",
      "epoch: 25 step: 66, loss is 0.0011741055641323328\n",
      "epoch: 25 step: 67, loss is 0.005342539865523577\n",
      "epoch: 25 step: 68, loss is 0.002986124251037836\n",
      "epoch: 25 step: 69, loss is 0.0007557879434898496\n",
      "epoch: 25 step: 70, loss is 0.0025598329957574606\n",
      "epoch: 25 step: 71, loss is 0.00029155495576560497\n",
      "epoch: 25 step: 72, loss is 0.016741804778575897\n",
      "epoch: 25 step: 73, loss is 0.006656633224338293\n",
      "epoch: 25 step: 74, loss is 0.005606214050203562\n",
      "epoch: 25 step: 75, loss is 0.0036264618393033743\n",
      "epoch: 25 step: 76, loss is 0.0033676906023174524\n",
      "epoch: 25 step: 77, loss is 0.002607905538752675\n",
      "epoch: 25 step: 78, loss is 0.00017072544142138213\n",
      "epoch: 25 step: 79, loss is 0.004233933053910732\n",
      "epoch: 25 step: 80, loss is 0.0014841199154034257\n",
      "epoch: 25 step: 81, loss is 0.001144331181421876\n",
      "epoch: 25 step: 82, loss is 5.085642260382883e-05\n",
      "epoch: 25 step: 83, loss is 0.0008038947707973421\n",
      "epoch: 25 step: 84, loss is 0.0038917078636586666\n",
      "epoch: 25 step: 85, loss is 0.015483921393752098\n",
      "epoch: 25 step: 86, loss is 0.03006856143474579\n",
      "epoch: 25 step: 87, loss is 0.00038133756606839597\n",
      "epoch: 25 step: 88, loss is 0.0002260852634208277\n",
      "epoch: 25 step: 89, loss is 0.0005047303857281804\n",
      "epoch: 25 step: 90, loss is 0.0013619314413517714\n",
      "epoch: 25 step: 91, loss is 0.0018629607511684299\n",
      "epoch: 25 step: 92, loss is 0.0014348409604281187\n",
      "epoch: 25 step: 93, loss is 0.0002784712123684585\n",
      "epoch: 25 step: 94, loss is 0.0016071221325546503\n",
      "epoch: 25 step: 95, loss is 0.027044419199228287\n",
      "epoch: 25 step: 96, loss is 0.05947050824761391\n",
      "epoch: 25 step: 97, loss is 0.014504064805805683\n",
      "epoch: 25 step: 98, loss is 0.0024279654026031494\n",
      "epoch: 25 step: 99, loss is 0.012352756224572659\n",
      "epoch: 25 step: 100, loss is 0.03273215889930725\n",
      "epoch: 25 step: 101, loss is 0.004083964042365551\n",
      "epoch: 25 step: 102, loss is 0.003067132318392396\n",
      "epoch: 25 step: 103, loss is 0.04676702246069908\n",
      "epoch: 25 step: 104, loss is 0.00095319066895172\n",
      "epoch: 25 step: 105, loss is 0.008304143324494362\n",
      "epoch: 25 step: 106, loss is 0.03217899426817894\n",
      "epoch: 25 step: 107, loss is 7.341621676459908e-05\n",
      "epoch: 25 step: 108, loss is 0.0001213557698065415\n",
      "epoch: 25 step: 109, loss is 0.007260648533701897\n",
      "epoch: 25 step: 110, loss is 0.0006220101495273411\n",
      "epoch: 25 step: 111, loss is 0.019297804683446884\n",
      "epoch: 25 step: 112, loss is 0.006181077565997839\n",
      "epoch: 25 step: 113, loss is 0.001132652978412807\n",
      "epoch: 25 step: 114, loss is 0.0035298243165016174\n",
      "epoch: 25 step: 115, loss is 0.00099962018430233\n",
      "epoch: 25 step: 116, loss is 0.00011800717038568109\n",
      "epoch: 25 step: 117, loss is 0.0015496467240154743\n",
      "epoch: 25 step: 118, loss is 0.0005825062980875373\n",
      "epoch: 25 step: 119, loss is 0.009758627973496914\n",
      "epoch: 25 step: 120, loss is 0.012019570916891098\n",
      "epoch: 25 step: 121, loss is 0.0484115332365036\n",
      "epoch: 25 step: 122, loss is 0.009151669219136238\n",
      "epoch: 25 step: 123, loss is 0.06241880729794502\n",
      "epoch: 25 step: 124, loss is 0.031695738434791565\n",
      "epoch: 25 step: 125, loss is 0.00309811276383698\n",
      "epoch: 25 step: 126, loss is 0.003848315216600895\n",
      "epoch: 25 step: 127, loss is 0.0004827760567422956\n",
      "epoch: 25 step: 128, loss is 0.0004135675844736397\n",
      "epoch: 25 step: 129, loss is 0.0020980879198759794\n",
      "epoch: 25 step: 130, loss is 0.0022564553655683994\n",
      "epoch: 25 step: 131, loss is 0.016232499852776527\n",
      "epoch: 25 step: 132, loss is 0.0002790004655253142\n",
      "epoch: 25 step: 133, loss is 0.001984270056709647\n",
      "epoch: 25 step: 134, loss is 0.02592376619577408\n",
      "epoch: 25 step: 135, loss is 0.004002446308732033\n",
      "epoch: 25 step: 136, loss is 0.00018259526405017823\n",
      "epoch: 25 step: 137, loss is 0.007332354784011841\n",
      "epoch: 25 step: 138, loss is 0.0008420532685704529\n",
      "epoch: 25 step: 139, loss is 4.030397758469917e-05\n",
      "epoch: 25 step: 140, loss is 0.0004460055788513273\n",
      "epoch: 25 step: 141, loss is 0.004726829472929239\n",
      "epoch: 25 step: 142, loss is 0.00868559256196022\n",
      "epoch: 25 step: 143, loss is 0.000747875077649951\n",
      "epoch: 25 step: 144, loss is 0.0002992120571434498\n",
      "epoch: 25 step: 145, loss is 0.0009302475373260677\n",
      "epoch: 25 step: 146, loss is 0.021381506696343422\n",
      "epoch: 25 step: 147, loss is 0.011267868801951408\n",
      "epoch: 25 step: 148, loss is 0.0003330327454023063\n",
      "epoch: 25 step: 149, loss is 0.024662597104907036\n",
      "epoch: 25 step: 150, loss is 0.0034983702935278416\n",
      "epoch: 25 step: 151, loss is 0.0004801562463399023\n",
      "epoch: 25 step: 152, loss is 0.0762237012386322\n",
      "epoch: 25 step: 153, loss is 0.003356447210535407\n",
      "epoch: 25 step: 154, loss is 0.001616456895135343\n",
      "epoch: 25 step: 155, loss is 0.005530194845050573\n",
      "epoch: 25 step: 156, loss is 0.00011406010889913887\n",
      "epoch: 25 step: 157, loss is 0.007644966244697571\n",
      "epoch: 25 step: 158, loss is 0.0049043516628444195\n",
      "epoch: 25 step: 159, loss is 0.002293309895321727\n",
      "epoch: 25 step: 160, loss is 0.058990851044654846\n",
      "epoch: 25 step: 161, loss is 8.820760558592156e-05\n",
      "epoch: 25 step: 162, loss is 0.0011138677364215255\n",
      "epoch: 25 step: 163, loss is 0.0018870134372264147\n",
      "epoch: 25 step: 164, loss is 0.019901007413864136\n",
      "epoch: 25 step: 165, loss is 0.0008293903665617108\n",
      "epoch: 25 step: 166, loss is 0.006596455350518227\n",
      "epoch: 25 step: 167, loss is 0.0197453536093235\n",
      "epoch: 25 step: 168, loss is 0.00048540293937548995\n",
      "epoch: 25 step: 169, loss is 0.006606974173337221\n",
      "epoch: 25 step: 170, loss is 0.0009037081617861986\n",
      "epoch: 25 step: 171, loss is 0.008833684027194977\n",
      "epoch: 25 step: 172, loss is 0.000302053609630093\n",
      "epoch: 25 step: 173, loss is 0.003707267576828599\n",
      "epoch: 25 step: 174, loss is 0.0035447829868644476\n",
      "epoch: 25 step: 175, loss is 0.0008966554305516183\n",
      "epoch: 25 step: 176, loss is 0.0034599031787365675\n",
      "epoch: 25 step: 177, loss is 0.014892345294356346\n",
      "epoch: 25 step: 178, loss is 0.0007016803720034659\n",
      "epoch: 25 step: 179, loss is 0.012444393709301949\n",
      "epoch: 25 step: 180, loss is 0.0017529749311506748\n",
      "epoch: 25 step: 181, loss is 0.00021996402938384563\n",
      "epoch: 25 step: 182, loss is 0.07450105249881744\n",
      "epoch: 25 step: 183, loss is 0.0006393326912075281\n",
      "epoch: 25 step: 184, loss is 0.009019162505865097\n",
      "epoch: 25 step: 185, loss is 4.023403744213283e-05\n",
      "epoch: 25 step: 186, loss is 0.00025173331960104406\n",
      "epoch: 25 step: 187, loss is 0.0029035289771854877\n",
      "epoch: 25 step: 188, loss is 0.00048236094880849123\n",
      "epoch: 25 step: 189, loss is 0.0009649922722019255\n",
      "epoch: 25 step: 190, loss is 0.004372475668787956\n",
      "epoch: 25 step: 191, loss is 0.00587397301569581\n",
      "epoch: 25 step: 192, loss is 0.02084537036716938\n",
      "epoch: 25 step: 193, loss is 0.000739207083825022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 194, loss is 0.01721241883933544\n",
      "epoch: 25 step: 195, loss is 0.0007421916234306991\n",
      "epoch: 25 step: 196, loss is 0.00016787750064395368\n",
      "epoch: 25 step: 197, loss is 0.002189317485317588\n",
      "epoch: 25 step: 198, loss is 0.0011466057039797306\n",
      "epoch: 25 step: 199, loss is 0.0024258990306407213\n",
      "epoch: 25 step: 200, loss is 0.00018553566769696772\n",
      "epoch: 25 step: 201, loss is 0.0010117869824171066\n",
      "epoch: 25 step: 202, loss is 0.0021103082690387964\n",
      "epoch: 25 step: 203, loss is 0.0007930822903290391\n",
      "epoch: 25 step: 204, loss is 8.886989235179499e-05\n",
      "epoch: 25 step: 205, loss is 0.00039819348603487015\n",
      "epoch: 25 step: 206, loss is 0.026364127174019814\n",
      "epoch: 25 step: 207, loss is 0.00023155202507041395\n",
      "epoch: 25 step: 208, loss is 0.09339113533496857\n",
      "epoch: 25 step: 209, loss is 0.05492781102657318\n",
      "epoch: 25 step: 210, loss is 0.004435665439814329\n",
      "epoch: 25 step: 211, loss is 0.020556112751364708\n",
      "epoch: 25 step: 212, loss is 0.00410980312153697\n",
      "epoch: 25 step: 213, loss is 0.00022710463963449\n",
      "epoch: 25 step: 214, loss is 0.005052422638982534\n",
      "epoch: 25 step: 215, loss is 0.0017916392534971237\n",
      "epoch: 25 step: 216, loss is 0.02786238305270672\n",
      "epoch: 25 step: 217, loss is 0.008811505511403084\n",
      "epoch: 25 step: 218, loss is 0.01032476406544447\n",
      "epoch: 25 step: 219, loss is 0.0008814631728455424\n",
      "epoch: 25 step: 220, loss is 0.00013876758748665452\n",
      "epoch: 25 step: 221, loss is 0.01110265962779522\n",
      "epoch: 25 step: 222, loss is 0.03970826044678688\n",
      "epoch: 25 step: 223, loss is 0.013176195323467255\n",
      "epoch: 25 step: 224, loss is 0.04321357607841492\n",
      "epoch: 25 step: 225, loss is 0.11722192168235779\n",
      "epoch: 25 step: 226, loss is 0.0008223560871556401\n",
      "epoch: 25 step: 227, loss is 0.022980261594057083\n",
      "epoch: 25 step: 228, loss is 0.0019415558781474829\n",
      "epoch: 25 step: 229, loss is 0.022043226286768913\n",
      "epoch: 25 step: 230, loss is 0.05820680409669876\n",
      "epoch: 25 step: 231, loss is 0.0003696184721775353\n",
      "epoch: 25 step: 232, loss is 0.0017513579223304987\n",
      "epoch: 25 step: 233, loss is 0.0016841944307088852\n",
      "epoch: 25 step: 234, loss is 0.00013541872613132\n",
      "epoch: 25 step: 235, loss is 0.0041553908959031105\n",
      "epoch: 25 step: 236, loss is 0.00546996109187603\n",
      "epoch: 25 step: 237, loss is 0.006210252642631531\n",
      "epoch: 25 step: 238, loss is 0.13435588777065277\n",
      "epoch: 25 step: 239, loss is 0.00034014220000244677\n",
      "epoch: 25 step: 240, loss is 0.0007609420572407544\n",
      "epoch: 25 step: 241, loss is 0.058445874601602554\n",
      "epoch: 25 step: 242, loss is 0.02661610022187233\n",
      "epoch: 25 step: 243, loss is 0.09669289737939835\n",
      "epoch: 25 step: 244, loss is 0.04995512589812279\n",
      "epoch: 25 step: 245, loss is 0.004915640223771334\n",
      "epoch: 25 step: 246, loss is 0.0006671278970316052\n",
      "epoch: 25 step: 247, loss is 0.0014518523821607232\n",
      "epoch: 25 step: 248, loss is 0.043451327830553055\n",
      "epoch: 25 step: 249, loss is 0.010756073519587517\n",
      "epoch: 25 step: 250, loss is 0.005403931252658367\n",
      "epoch: 25 step: 251, loss is 0.0005537458346225321\n",
      "epoch: 25 step: 252, loss is 0.012954536825418472\n",
      "epoch: 25 step: 253, loss is 1.6292304280796088e-05\n",
      "epoch: 25 step: 254, loss is 0.006199963390827179\n",
      "epoch: 25 step: 255, loss is 0.0016243034042418003\n",
      "epoch: 25 step: 256, loss is 0.001010434003546834\n",
      "epoch: 25 step: 257, loss is 0.008298547007143497\n",
      "epoch: 25 step: 258, loss is 0.007578379008919001\n",
      "epoch: 25 step: 259, loss is 0.055026717483997345\n",
      "epoch: 25 step: 260, loss is 0.00041984140989370644\n",
      "epoch: 25 step: 261, loss is 0.0014587228652089834\n",
      "epoch: 25 step: 262, loss is 0.00041564268758520484\n",
      "epoch: 25 step: 263, loss is 0.0010272458894178271\n",
      "epoch: 25 step: 264, loss is 0.006598970852792263\n",
      "epoch: 25 step: 265, loss is 0.0175654087215662\n",
      "epoch: 25 step: 266, loss is 0.00034990423591807485\n",
      "epoch: 25 step: 267, loss is 0.002638240810483694\n",
      "epoch: 25 step: 268, loss is 0.0022590775042772293\n",
      "epoch: 25 step: 269, loss is 0.00022300757700577378\n",
      "epoch: 25 step: 270, loss is 0.0012375114019960165\n",
      "epoch: 25 step: 271, loss is 0.06355505436658859\n",
      "epoch: 25 step: 272, loss is 0.0015482406597584486\n",
      "epoch: 25 step: 273, loss is 0.030039340257644653\n",
      "epoch: 25 step: 274, loss is 0.010803413577377796\n",
      "epoch: 25 step: 275, loss is 0.013581973500549793\n",
      "epoch: 25 step: 276, loss is 0.0006305251154117286\n",
      "epoch: 25 step: 277, loss is 0.0035942071117460728\n",
      "epoch: 25 step: 278, loss is 0.013454204425215721\n",
      "epoch: 25 step: 279, loss is 0.0014266014331951737\n",
      "epoch: 25 step: 280, loss is 0.05121398717164993\n",
      "epoch: 25 step: 281, loss is 0.005050148349255323\n",
      "epoch: 25 step: 282, loss is 0.0010055588791146874\n",
      "epoch: 25 step: 283, loss is 0.0014639778528362513\n",
      "epoch: 25 step: 284, loss is 0.0011743551585823298\n",
      "epoch: 25 step: 285, loss is 0.00591172743588686\n",
      "epoch: 25 step: 286, loss is 0.000995500711724162\n",
      "epoch: 25 step: 287, loss is 0.007047805469483137\n",
      "epoch: 25 step: 288, loss is 0.002476348541676998\n",
      "epoch: 25 step: 289, loss is 0.0022754105739295483\n",
      "epoch: 25 step: 290, loss is 0.004583403002470732\n",
      "epoch: 25 step: 291, loss is 0.007550370879471302\n",
      "epoch: 25 step: 292, loss is 0.0012675964972004294\n",
      "epoch: 25 step: 293, loss is 0.04268605634570122\n",
      "epoch: 25 step: 294, loss is 0.013661209493875504\n",
      "epoch: 25 step: 295, loss is 4.794904089067131e-05\n",
      "epoch: 25 step: 296, loss is 0.0020601823925971985\n",
      "epoch: 25 step: 297, loss is 0.0369320884346962\n",
      "epoch: 25 step: 298, loss is 0.012463095597922802\n",
      "epoch: 25 step: 299, loss is 0.0008920311229303479\n",
      "epoch: 25 step: 300, loss is 0.008950750343501568\n",
      "epoch: 25 step: 301, loss is 0.0006629626150242984\n",
      "epoch: 25 step: 302, loss is 0.00013392047549132258\n",
      "epoch: 25 step: 303, loss is 0.003919901791960001\n",
      "epoch: 25 step: 304, loss is 0.0011383317178115249\n",
      "epoch: 25 step: 305, loss is 0.019546473398804665\n",
      "epoch: 25 step: 306, loss is 2.53149864875013e-05\n",
      "epoch: 25 step: 307, loss is 0.0004152431502006948\n",
      "epoch: 25 step: 308, loss is 0.0001581159740453586\n",
      "epoch: 25 step: 309, loss is 5.4717911552870646e-05\n",
      "epoch: 25 step: 310, loss is 0.0009944933699443936\n",
      "epoch: 25 step: 311, loss is 0.05942259356379509\n",
      "epoch: 25 step: 312, loss is 0.02574964240193367\n",
      "epoch: 25 step: 313, loss is 0.00021259771892800927\n",
      "epoch: 25 step: 314, loss is 0.011512698605656624\n",
      "epoch: 25 step: 315, loss is 0.010359046049416065\n",
      "epoch: 25 step: 316, loss is 0.11143014580011368\n",
      "epoch: 25 step: 317, loss is 0.019277801737189293\n",
      "epoch: 25 step: 318, loss is 0.0001114143306040205\n",
      "epoch: 25 step: 319, loss is 0.0015947665087878704\n",
      "epoch: 25 step: 320, loss is 8.4281200543046e-06\n",
      "epoch: 25 step: 321, loss is 0.0006165396189317107\n",
      "epoch: 25 step: 322, loss is 0.018090905621647835\n",
      "epoch: 25 step: 323, loss is 0.0009879691060632467\n",
      "epoch: 25 step: 324, loss is 0.004015656653791666\n",
      "epoch: 25 step: 325, loss is 0.018559863790869713\n",
      "epoch: 25 step: 326, loss is 0.0810994952917099\n",
      "epoch: 25 step: 327, loss is 0.1913720667362213\n",
      "epoch: 25 step: 328, loss is 0.022777797654271126\n",
      "epoch: 25 step: 329, loss is 0.019607113674283028\n",
      "epoch: 25 step: 330, loss is 0.009769056923687458\n",
      "epoch: 25 step: 331, loss is 0.001851003267802298\n",
      "epoch: 25 step: 332, loss is 4.934128446620889e-05\n",
      "epoch: 25 step: 333, loss is 6.300321547314525e-05\n",
      "epoch: 25 step: 334, loss is 0.05698879808187485\n",
      "epoch: 25 step: 335, loss is 0.046700619161129\n",
      "epoch: 25 step: 336, loss is 0.004014855250716209\n",
      "epoch: 25 step: 337, loss is 0.0639229491353035\n",
      "epoch: 25 step: 338, loss is 0.005609145388007164\n",
      "epoch: 25 step: 339, loss is 0.023885607719421387\n",
      "epoch: 25 step: 340, loss is 0.007824641652405262\n",
      "epoch: 25 step: 341, loss is 5.66714552405756e-05\n",
      "epoch: 25 step: 342, loss is 0.013123075477778912\n",
      "epoch: 25 step: 343, loss is 0.0004743295139633119\n",
      "epoch: 25 step: 344, loss is 0.0015587408561259508\n",
      "epoch: 25 step: 345, loss is 0.0009886280167847872\n",
      "epoch: 25 step: 346, loss is 0.0005895380745641887\n",
      "epoch: 25 step: 347, loss is 0.04799145460128784\n",
      "epoch: 25 step: 348, loss is 0.001456945901736617\n",
      "epoch: 25 step: 349, loss is 0.006796589586883783\n",
      "epoch: 25 step: 350, loss is 0.008247848600149155\n",
      "epoch: 25 step: 351, loss is 0.053033825010061264\n",
      "epoch: 25 step: 352, loss is 0.0020037596113979816\n",
      "epoch: 25 step: 353, loss is 0.008739727549254894\n",
      "epoch: 25 step: 354, loss is 0.03154706954956055\n",
      "epoch: 25 step: 355, loss is 0.0006992594571784139\n",
      "epoch: 25 step: 356, loss is 0.028519580140709877\n",
      "epoch: 25 step: 357, loss is 0.17395639419555664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 358, loss is 0.00040809655911289155\n",
      "epoch: 25 step: 359, loss is 0.0042998287826776505\n",
      "epoch: 25 step: 360, loss is 0.001997543964534998\n",
      "epoch: 25 step: 361, loss is 0.013788506388664246\n",
      "epoch: 25 step: 362, loss is 0.013080714270472527\n",
      "epoch: 25 step: 363, loss is 0.00013484244118444622\n",
      "epoch: 25 step: 364, loss is 0.0158908199518919\n",
      "epoch: 25 step: 365, loss is 0.02969694323837757\n",
      "epoch: 25 step: 366, loss is 0.010888759978115559\n",
      "epoch: 25 step: 367, loss is 0.0021573228295892477\n",
      "epoch: 25 step: 368, loss is 0.0219996590167284\n",
      "epoch: 25 step: 369, loss is 0.016487956047058105\n",
      "epoch: 25 step: 370, loss is 0.00047498283674940467\n",
      "epoch: 25 step: 371, loss is 0.0008399192010983825\n",
      "epoch: 25 step: 372, loss is 0.016455916687846184\n",
      "epoch: 25 step: 373, loss is 0.0004996418138034642\n",
      "epoch: 25 step: 374, loss is 0.0028584350366145372\n",
      "epoch: 25 step: 375, loss is 0.00011055971117457375\n",
      "epoch: 25 step: 376, loss is 0.0002613876131363213\n",
      "epoch: 25 step: 377, loss is 0.024167919531464577\n",
      "epoch: 25 step: 378, loss is 0.0006636597099713981\n",
      "epoch: 25 step: 379, loss is 0.0026781554333865643\n",
      "epoch: 25 step: 380, loss is 0.022663725540041924\n",
      "epoch: 25 step: 381, loss is 0.06572102010250092\n",
      "epoch: 25 step: 382, loss is 0.0015235156752169132\n",
      "epoch: 25 step: 383, loss is 0.10996603220701218\n",
      "epoch: 25 step: 384, loss is 0.0017470180755481124\n",
      "epoch: 25 step: 385, loss is 0.0004308599454816431\n",
      "epoch: 25 step: 386, loss is 0.001288541010580957\n",
      "epoch: 25 step: 387, loss is 0.0063073099590837955\n",
      "epoch: 25 step: 388, loss is 0.02710040844976902\n",
      "epoch: 25 step: 389, loss is 0.00038723571924492717\n",
      "epoch: 25 step: 390, loss is 0.02073781006038189\n",
      "epoch: 25 step: 391, loss is 0.0007825667271390557\n",
      "epoch: 25 step: 392, loss is 0.001825052429921925\n",
      "epoch: 25 step: 393, loss is 0.0610409751534462\n",
      "epoch: 25 step: 394, loss is 0.001379815163090825\n",
      "epoch: 25 step: 395, loss is 0.004293920937925577\n",
      "epoch: 25 step: 396, loss is 0.00707126734778285\n",
      "epoch: 25 step: 397, loss is 0.0011359708150848746\n",
      "epoch: 25 step: 398, loss is 0.0019188045989722013\n",
      "epoch: 25 step: 399, loss is 0.00013478571781888604\n",
      "epoch: 25 step: 400, loss is 0.02580326236784458\n",
      "epoch: 25 step: 401, loss is 0.0007206974551081657\n",
      "epoch: 25 step: 402, loss is 0.025122445076704025\n",
      "epoch: 25 step: 403, loss is 0.0545937642455101\n",
      "epoch: 25 step: 404, loss is 0.023381298407912254\n",
      "epoch: 25 step: 405, loss is 0.004298060201108456\n",
      "epoch: 25 step: 406, loss is 0.0022639180533587933\n",
      "epoch: 25 step: 407, loss is 0.0003124963550362736\n",
      "epoch: 25 step: 408, loss is 0.0001907257828861475\n",
      "epoch: 25 step: 409, loss is 0.0006642976659350097\n",
      "epoch: 25 step: 410, loss is 0.008707861416041851\n",
      "epoch: 25 step: 411, loss is 0.00021480469149537385\n",
      "epoch: 25 step: 412, loss is 0.0002831064339261502\n",
      "epoch: 25 step: 413, loss is 0.007787168957293034\n",
      "epoch: 25 step: 414, loss is 0.015958331525325775\n",
      "epoch: 25 step: 415, loss is 0.07585254311561584\n",
      "epoch: 25 step: 416, loss is 0.018207497894763947\n",
      "epoch: 25 step: 417, loss is 0.00015977202565409243\n",
      "epoch: 25 step: 418, loss is 0.01375537272542715\n",
      "epoch: 25 step: 419, loss is 0.0198742114007473\n",
      "epoch: 25 step: 420, loss is 0.01228873711079359\n",
      "epoch: 25 step: 421, loss is 0.005414689891040325\n",
      "epoch: 25 step: 422, loss is 0.11263155937194824\n",
      "epoch: 25 step: 423, loss is 0.005789425689727068\n",
      "epoch: 25 step: 424, loss is 0.0005143074668012559\n",
      "epoch: 25 step: 425, loss is 0.026980657130479813\n",
      "epoch: 25 step: 426, loss is 0.0004383579653222114\n",
      "epoch: 25 step: 427, loss is 0.0004894955200143158\n",
      "epoch: 25 step: 428, loss is 0.006024566944688559\n",
      "epoch: 25 step: 429, loss is 0.00032038509380072355\n",
      "epoch: 25 step: 430, loss is 0.0004068555135745555\n",
      "epoch: 25 step: 431, loss is 0.01038010697811842\n",
      "epoch: 25 step: 432, loss is 0.12657497823238373\n",
      "epoch: 25 step: 433, loss is 0.001839736825786531\n",
      "epoch: 25 step: 434, loss is 0.00011759966582758352\n",
      "epoch: 25 step: 435, loss is 0.023908395320177078\n",
      "epoch: 25 step: 436, loss is 0.0018077977001667023\n",
      "epoch: 25 step: 437, loss is 3.821800055447966e-05\n",
      "epoch: 25 step: 438, loss is 0.00553465262055397\n",
      "epoch: 25 step: 439, loss is 0.0007107165874913335\n",
      "epoch: 25 step: 440, loss is 0.033544059842824936\n",
      "epoch: 25 step: 441, loss is 0.04200156033039093\n",
      "epoch: 25 step: 442, loss is 0.00793018564581871\n",
      "epoch: 25 step: 443, loss is 0.004016112070530653\n",
      "epoch: 25 step: 444, loss is 0.06542766839265823\n",
      "epoch: 25 step: 445, loss is 0.002842782298102975\n",
      "epoch: 25 step: 446, loss is 0.04348858818411827\n",
      "epoch: 25 step: 447, loss is 0.0015217256732285023\n",
      "epoch: 25 step: 448, loss is 0.00011279802129138261\n",
      "epoch: 25 step: 449, loss is 0.01591433398425579\n",
      "epoch: 25 step: 450, loss is 0.0003486481145955622\n",
      "epoch: 25 step: 451, loss is 0.00024405692238360643\n",
      "epoch: 25 step: 452, loss is 0.0006880676373839378\n",
      "epoch: 25 step: 453, loss is 0.044582612812519073\n",
      "epoch: 25 step: 454, loss is 0.004576531238853931\n",
      "epoch: 25 step: 455, loss is 2.5537123292451724e-05\n",
      "epoch: 25 step: 456, loss is 0.010303005576133728\n",
      "epoch: 25 step: 457, loss is 0.002349920105189085\n",
      "epoch: 25 step: 458, loss is 0.002066331449896097\n",
      "epoch: 25 step: 459, loss is 0.007718319538980722\n",
      "epoch: 25 step: 460, loss is 0.0030244626104831696\n",
      "epoch: 25 step: 461, loss is 0.004136905539780855\n",
      "epoch: 25 step: 462, loss is 0.0015771810431033373\n",
      "epoch: 25 step: 463, loss is 0.001861098106019199\n",
      "epoch: 25 step: 464, loss is 0.008739395067095757\n",
      "epoch: 25 step: 465, loss is 0.0008287604432553053\n",
      "epoch: 25 step: 466, loss is 0.0009920848533511162\n",
      "epoch: 25 step: 467, loss is 0.07060771435499191\n",
      "epoch: 25 step: 468, loss is 0.00018765842833090574\n",
      "epoch: 25 step: 469, loss is 0.0010220133699476719\n",
      "epoch: 25 step: 470, loss is 0.00016116767073981464\n",
      "epoch: 25 step: 471, loss is 0.0005728472606278956\n",
      "epoch: 25 step: 472, loss is 0.003588396590203047\n",
      "epoch: 25 step: 473, loss is 0.0008469978347420692\n",
      "epoch: 25 step: 474, loss is 0.0002489697653800249\n",
      "epoch: 25 step: 475, loss is 0.03214818611741066\n",
      "epoch: 25 step: 476, loss is 0.0007214806391857564\n",
      "epoch: 25 step: 477, loss is 0.000682048499584198\n",
      "epoch: 25 step: 478, loss is 0.004100439138710499\n",
      "epoch: 25 step: 479, loss is 0.0007777641294524074\n",
      "epoch: 25 step: 480, loss is 0.006217342801392078\n",
      "epoch: 25 step: 481, loss is 9.437213520868681e-06\n",
      "epoch: 25 step: 482, loss is 0.0004306505434215069\n",
      "epoch: 25 step: 483, loss is 0.00044129701564088464\n",
      "epoch: 25 step: 484, loss is 0.006905837915837765\n",
      "epoch: 25 step: 485, loss is 0.0003976162988692522\n",
      "epoch: 25 step: 486, loss is 0.00012996874283999205\n",
      "epoch: 25 step: 487, loss is 0.011691750027239323\n",
      "epoch: 25 step: 488, loss is 0.045732636004686356\n",
      "epoch: 25 step: 489, loss is 0.0010579106165096164\n",
      "epoch: 25 step: 490, loss is 0.0010901169152930379\n",
      "epoch: 25 step: 491, loss is 0.0062368218787014484\n",
      "epoch: 25 step: 492, loss is 0.018963603302836418\n",
      "epoch: 25 step: 493, loss is 0.002576668281108141\n",
      "epoch: 25 step: 494, loss is 0.050672970712184906\n",
      "epoch: 25 step: 495, loss is 0.0018738287035375834\n",
      "epoch: 25 step: 496, loss is 0.007286649663001299\n",
      "epoch: 25 step: 497, loss is 0.02963930554687977\n",
      "epoch: 25 step: 498, loss is 0.0003620187344495207\n",
      "epoch: 25 step: 499, loss is 0.0006438518175855279\n",
      "epoch: 25 step: 500, loss is 0.0201072096824646\n",
      "epoch: 25 step: 501, loss is 0.11456035077571869\n",
      "epoch: 25 step: 502, loss is 2.196287277911324e-05\n",
      "epoch: 25 step: 503, loss is 0.07929947227239609\n",
      "epoch: 25 step: 504, loss is 0.002317841397598386\n",
      "epoch: 25 step: 505, loss is 0.0008811544976197183\n",
      "epoch: 25 step: 506, loss is 0.0005868406151421368\n",
      "epoch: 25 step: 507, loss is 0.031052621081471443\n",
      "epoch: 25 step: 508, loss is 0.0009480976732447743\n",
      "epoch: 25 step: 509, loss is 0.002928735688328743\n",
      "epoch: 25 step: 510, loss is 0.01631328836083412\n",
      "epoch: 25 step: 511, loss is 0.035227008163928986\n",
      "epoch: 25 step: 512, loss is 0.02087303064763546\n",
      "epoch: 25 step: 513, loss is 0.0018869687337428331\n",
      "epoch: 25 step: 514, loss is 0.0010966230183839798\n",
      "epoch: 25 step: 515, loss is 0.010835102759301662\n",
      "epoch: 25 step: 516, loss is 0.0005675393040291965\n",
      "epoch: 25 step: 517, loss is 0.0008258194429799914\n",
      "epoch: 25 step: 518, loss is 0.004735896829515696\n",
      "epoch: 25 step: 519, loss is 0.0007856885204091668\n",
      "epoch: 25 step: 520, loss is 0.00018397040548734367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 521, loss is 0.010841053910553455\n",
      "epoch: 25 step: 522, loss is 0.00018410407938063145\n",
      "epoch: 25 step: 523, loss is 0.0001338901638519019\n",
      "epoch: 25 step: 524, loss is 0.0035573600325733423\n",
      "epoch: 25 step: 525, loss is 0.04023334011435509\n",
      "epoch: 25 step: 526, loss is 7.055346941342577e-05\n",
      "epoch: 25 step: 527, loss is 0.001123534282669425\n",
      "epoch: 25 step: 528, loss is 0.0024521760642528534\n",
      "epoch: 25 step: 529, loss is 0.0013064945815131068\n",
      "epoch: 25 step: 530, loss is 0.001047189929522574\n",
      "epoch: 25 step: 531, loss is 0.005000498611479998\n",
      "epoch: 25 step: 532, loss is 0.08361877501010895\n",
      "epoch: 25 step: 533, loss is 0.05304589122533798\n",
      "epoch: 25 step: 534, loss is 0.01156520564109087\n",
      "epoch: 25 step: 535, loss is 0.018627576529979706\n",
      "epoch: 25 step: 536, loss is 0.004853676538914442\n",
      "epoch: 25 step: 537, loss is 0.0002028434828389436\n",
      "epoch: 25 step: 538, loss is 0.000766027660574764\n",
      "epoch: 25 step: 539, loss is 0.005638518370687962\n",
      "epoch: 25 step: 540, loss is 0.0017752170097082853\n",
      "epoch: 25 step: 541, loss is 0.0019881201442331076\n",
      "epoch: 25 step: 542, loss is 0.0032293854746967554\n",
      "epoch: 25 step: 543, loss is 0.00748109258711338\n",
      "epoch: 25 step: 544, loss is 0.05195429548621178\n",
      "epoch: 25 step: 545, loss is 0.002452458953484893\n",
      "epoch: 25 step: 546, loss is 0.002108428394421935\n",
      "epoch: 25 step: 547, loss is 0.00017124443547800183\n",
      "epoch: 25 step: 548, loss is 0.0009468310163356364\n",
      "epoch: 25 step: 549, loss is 0.0025512618012726307\n",
      "epoch: 25 step: 550, loss is 0.03743499889969826\n",
      "epoch: 25 step: 551, loss is 0.0006748210289515555\n",
      "epoch: 25 step: 552, loss is 0.011971811763942242\n",
      "epoch: 25 step: 553, loss is 0.09039610624313354\n",
      "epoch: 25 step: 554, loss is 0.009269746020436287\n",
      "epoch: 25 step: 555, loss is 0.0014561843127012253\n",
      "epoch: 25 step: 556, loss is 0.057964425534009933\n",
      "epoch: 25 step: 557, loss is 0.0006864452734589577\n",
      "epoch: 25 step: 558, loss is 0.0011560118291527033\n",
      "epoch: 25 step: 559, loss is 0.001149751478806138\n",
      "epoch: 25 step: 560, loss is 0.010503269731998444\n",
      "epoch: 25 step: 561, loss is 0.08671757578849792\n",
      "epoch: 25 step: 562, loss is 0.0003202611696906388\n",
      "epoch: 25 step: 563, loss is 0.00803497713059187\n",
      "epoch: 25 step: 564, loss is 0.002939948346465826\n",
      "epoch: 25 step: 565, loss is 0.0005119117558933794\n",
      "epoch: 25 step: 566, loss is 0.008183996193110943\n",
      "epoch: 25 step: 567, loss is 0.012255356647074223\n",
      "epoch: 25 step: 568, loss is 0.0023095509968698025\n",
      "epoch: 25 step: 569, loss is 0.0023455738555639982\n",
      "epoch: 25 step: 570, loss is 0.08096026629209518\n",
      "epoch: 25 step: 571, loss is 0.004336219280958176\n",
      "epoch: 25 step: 572, loss is 0.0006835529929958284\n",
      "epoch: 25 step: 573, loss is 0.024251356720924377\n",
      "epoch: 25 step: 574, loss is 0.005002589430660009\n",
      "epoch: 25 step: 575, loss is 0.003996845800429583\n",
      "epoch: 25 step: 576, loss is 0.0002912910422310233\n",
      "epoch: 25 step: 577, loss is 0.005004882346838713\n",
      "epoch: 25 step: 578, loss is 0.003231725422665477\n",
      "epoch: 25 step: 579, loss is 3.233472307329066e-05\n",
      "epoch: 25 step: 580, loss is 0.0007060422794893384\n",
      "epoch: 25 step: 581, loss is 0.004049079027026892\n",
      "epoch: 25 step: 582, loss is 0.009308350272476673\n",
      "epoch: 25 step: 583, loss is 0.003444741014391184\n",
      "epoch: 25 step: 584, loss is 0.0033526821061968803\n",
      "epoch: 25 step: 585, loss is 0.004256917629390955\n",
      "epoch: 25 step: 586, loss is 0.0010343760950490832\n",
      "epoch: 25 step: 587, loss is 0.029311394318938255\n",
      "epoch: 25 step: 588, loss is 0.0005354483146220446\n",
      "epoch: 25 step: 589, loss is 0.00854353979229927\n",
      "epoch: 25 step: 590, loss is 0.01763414405286312\n",
      "epoch: 25 step: 591, loss is 0.006185435689985752\n",
      "epoch: 25 step: 592, loss is 0.00017414688772987574\n",
      "epoch: 25 step: 593, loss is 8.619669824838638e-05\n",
      "epoch: 25 step: 594, loss is 0.0017365755047649145\n",
      "epoch: 25 step: 595, loss is 0.002294543432071805\n",
      "epoch: 25 step: 596, loss is 0.010903005488216877\n",
      "epoch: 25 step: 597, loss is 0.0026400848291814327\n",
      "epoch: 25 step: 598, loss is 0.0003383113071322441\n",
      "epoch: 25 step: 599, loss is 0.0014922608388587832\n",
      "epoch: 25 step: 600, loss is 0.00017650474910624325\n",
      "epoch: 25 step: 601, loss is 4.717025512945838e-05\n",
      "epoch: 25 step: 602, loss is 0.0006710469024255872\n",
      "epoch: 25 step: 603, loss is 0.0003662034869194031\n",
      "epoch: 25 step: 604, loss is 0.0013224523281678557\n",
      "epoch: 25 step: 605, loss is 0.006629980634897947\n",
      "epoch: 25 step: 606, loss is 0.00016041364870034158\n",
      "epoch: 25 step: 607, loss is 0.07282152026891708\n",
      "epoch: 25 step: 608, loss is 0.00473700650036335\n",
      "epoch: 25 step: 609, loss is 0.00442791823297739\n",
      "epoch: 25 step: 610, loss is 0.00851857103407383\n",
      "epoch: 25 step: 611, loss is 0.003922839183360338\n",
      "epoch: 25 step: 612, loss is 0.025169482454657555\n",
      "epoch: 25 step: 613, loss is 0.0006809399928897619\n",
      "epoch: 25 step: 614, loss is 0.0002675331779755652\n",
      "epoch: 25 step: 615, loss is 0.0017849261639639735\n",
      "epoch: 25 step: 616, loss is 0.0007386081852018833\n",
      "epoch: 25 step: 617, loss is 0.013928123749792576\n",
      "epoch: 25 step: 618, loss is 0.019585179165005684\n",
      "epoch: 25 step: 619, loss is 0.005880299489945173\n",
      "epoch: 25 step: 620, loss is 0.0003614986489992589\n",
      "epoch: 25 step: 621, loss is 0.0012561168987303972\n",
      "epoch: 25 step: 622, loss is 0.00013768626376986504\n",
      "epoch: 25 step: 623, loss is 0.06108897551894188\n",
      "epoch: 25 step: 624, loss is 0.017386900261044502\n",
      "epoch: 25 step: 625, loss is 0.000133729656226933\n",
      "epoch: 25 step: 626, loss is 7.04170306562446e-05\n",
      "epoch: 25 step: 627, loss is 0.0032453672029078007\n",
      "epoch: 25 step: 628, loss is 0.11067341268062592\n",
      "epoch: 25 step: 629, loss is 0.0013487701071426272\n",
      "epoch: 25 step: 630, loss is 0.002458564005792141\n",
      "epoch: 25 step: 631, loss is 0.01185840368270874\n",
      "epoch: 25 step: 632, loss is 0.012527058832347393\n",
      "epoch: 25 step: 633, loss is 0.0007980685913935304\n",
      "epoch: 25 step: 634, loss is 0.0017270814860239625\n",
      "epoch: 25 step: 635, loss is 0.03944040834903717\n",
      "epoch: 25 step: 636, loss is 0.005149594508111477\n",
      "epoch: 25 step: 637, loss is 0.061091672629117966\n",
      "epoch: 25 step: 638, loss is 0.0030667618848383427\n",
      "epoch: 25 step: 639, loss is 0.0017496268264949322\n",
      "epoch: 25 step: 640, loss is 0.012060788460075855\n",
      "epoch: 25 step: 641, loss is 0.00016194992349483073\n",
      "epoch: 25 step: 642, loss is 0.17285025119781494\n",
      "epoch: 25 step: 643, loss is 0.010565854609012604\n",
      "epoch: 25 step: 644, loss is 0.10212785005569458\n",
      "epoch: 25 step: 645, loss is 0.0005306858220137656\n",
      "epoch: 25 step: 646, loss is 0.004566127434372902\n",
      "epoch: 25 step: 647, loss is 0.0015906174667179585\n",
      "epoch: 25 step: 648, loss is 0.001279684598557651\n",
      "epoch: 25 step: 649, loss is 0.004028727300465107\n",
      "epoch: 25 step: 650, loss is 0.0008047645096667111\n",
      "epoch: 25 step: 651, loss is 0.0071081118658185005\n",
      "epoch: 25 step: 652, loss is 0.0015928049106150866\n",
      "epoch: 25 step: 653, loss is 0.0020669070072472095\n",
      "epoch: 25 step: 654, loss is 0.0008206454804167151\n",
      "epoch: 25 step: 655, loss is 0.009516393765807152\n",
      "epoch: 25 step: 656, loss is 0.005617071408778429\n",
      "epoch: 25 step: 657, loss is 0.014322223141789436\n",
      "epoch: 25 step: 658, loss is 0.002944876905530691\n",
      "epoch: 25 step: 659, loss is 0.007374480366706848\n",
      "epoch: 25 step: 660, loss is 0.0011309104738757014\n",
      "epoch: 25 step: 661, loss is 0.000335994380293414\n",
      "epoch: 25 step: 662, loss is 0.004191522020846605\n",
      "epoch: 25 step: 663, loss is 0.006884108763188124\n",
      "epoch: 25 step: 664, loss is 0.005139164160937071\n",
      "epoch: 25 step: 665, loss is 0.002230301033705473\n",
      "epoch: 25 step: 666, loss is 0.00265697855502367\n",
      "epoch: 25 step: 667, loss is 0.0493316687643528\n",
      "epoch: 25 step: 668, loss is 0.00011111877392977476\n",
      "epoch: 25 step: 669, loss is 0.0014721110928803682\n",
      "epoch: 25 step: 670, loss is 0.002920227823778987\n",
      "epoch: 25 step: 671, loss is 0.000456223584478721\n",
      "epoch: 25 step: 672, loss is 0.00523633835837245\n",
      "epoch: 25 step: 673, loss is 0.001520149060525\n",
      "epoch: 25 step: 674, loss is 0.00044988308218307793\n",
      "epoch: 25 step: 675, loss is 0.0009558489546179771\n",
      "epoch: 25 step: 676, loss is 0.007674267049878836\n",
      "epoch: 25 step: 677, loss is 8.10590572655201e-05\n",
      "epoch: 25 step: 678, loss is 0.008999524638056755\n",
      "epoch: 25 step: 679, loss is 0.019167019054293633\n",
      "epoch: 25 step: 680, loss is 0.008449849672615528\n",
      "epoch: 25 step: 681, loss is 0.01919417269527912\n",
      "epoch: 25 step: 682, loss is 0.004370077513158321\n",
      "epoch: 25 step: 683, loss is 0.004540824797004461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 684, loss is 0.0465637743473053\n",
      "epoch: 25 step: 685, loss is 0.0564391165971756\n",
      "epoch: 25 step: 686, loss is 0.002177741378545761\n",
      "epoch: 25 step: 687, loss is 0.0009156409068964422\n",
      "epoch: 25 step: 688, loss is 0.0016303949523717165\n",
      "epoch: 25 step: 689, loss is 0.0532282218337059\n",
      "epoch: 25 step: 690, loss is 0.0022803498432040215\n",
      "epoch: 25 step: 691, loss is 0.0021793139167129993\n",
      "epoch: 25 step: 692, loss is 0.0018912047380581498\n",
      "epoch: 25 step: 693, loss is 0.0005220974562689662\n",
      "epoch: 25 step: 694, loss is 0.09350228309631348\n",
      "epoch: 25 step: 695, loss is 0.05920709669589996\n",
      "epoch: 25 step: 696, loss is 0.14447486400604248\n",
      "epoch: 25 step: 697, loss is 0.01409325934946537\n",
      "epoch: 25 step: 698, loss is 0.0005401722737587988\n",
      "epoch: 25 step: 699, loss is 0.010327788069844246\n",
      "epoch: 25 step: 700, loss is 0.0016041573835536838\n",
      "epoch: 25 step: 701, loss is 0.007585511542856693\n",
      "epoch: 25 step: 702, loss is 0.0038523490075021982\n",
      "epoch: 25 step: 703, loss is 0.00875640194863081\n",
      "epoch: 25 step: 704, loss is 0.03228461369872093\n",
      "epoch: 25 step: 705, loss is 0.0011440792586654425\n",
      "epoch: 25 step: 706, loss is 0.007248840294778347\n",
      "epoch: 25 step: 707, loss is 0.0015121465548872948\n",
      "epoch: 25 step: 708, loss is 0.0018591166008263826\n",
      "epoch: 25 step: 709, loss is 0.006630477029830217\n",
      "epoch: 25 step: 710, loss is 0.004400582518428564\n",
      "epoch: 25 step: 711, loss is 0.009821547195315361\n",
      "epoch: 25 step: 712, loss is 0.024551358073949814\n",
      "epoch: 25 step: 713, loss is 0.0009335275972262025\n",
      "epoch: 25 step: 714, loss is 0.0027387202717363834\n",
      "epoch: 25 step: 715, loss is 0.1587408483028412\n",
      "epoch: 25 step: 716, loss is 0.023609735071659088\n",
      "epoch: 25 step: 717, loss is 0.009396271780133247\n",
      "epoch: 25 step: 718, loss is 0.021843483671545982\n",
      "epoch: 25 step: 719, loss is 0.002654214622452855\n",
      "epoch: 25 step: 720, loss is 0.0033113115932792425\n",
      "epoch: 25 step: 721, loss is 0.011026083491742611\n",
      "epoch: 25 step: 722, loss is 0.08316097408533096\n",
      "epoch: 25 step: 723, loss is 0.001535114599391818\n",
      "epoch: 25 step: 724, loss is 0.0009700212976895273\n",
      "epoch: 25 step: 725, loss is 0.0298143420368433\n",
      "epoch: 25 step: 726, loss is 0.0042011006735265255\n",
      "epoch: 25 step: 727, loss is 0.0011167734628543258\n",
      "epoch: 25 step: 728, loss is 0.006611245218664408\n",
      "epoch: 25 step: 729, loss is 0.004375176969915628\n",
      "epoch: 25 step: 730, loss is 0.047274962067604065\n",
      "epoch: 25 step: 731, loss is 0.049002256244421005\n",
      "epoch: 25 step: 732, loss is 0.0005901744589209557\n",
      "epoch: 25 step: 733, loss is 0.0001253614464076236\n",
      "epoch: 25 step: 734, loss is 0.020014040172100067\n",
      "epoch: 25 step: 735, loss is 0.00019551876175682992\n",
      "epoch: 25 step: 736, loss is 0.018751127645373344\n",
      "epoch: 25 step: 737, loss is 0.006501934956759214\n",
      "epoch: 25 step: 738, loss is 0.0041288649663329124\n",
      "epoch: 25 step: 739, loss is 0.01579015515744686\n",
      "epoch: 25 step: 740, loss is 0.0007219438557513058\n",
      "epoch: 25 step: 741, loss is 0.0012379701947793365\n",
      "epoch: 25 step: 742, loss is 0.0006135646253824234\n",
      "epoch: 25 step: 743, loss is 0.0024259977508336306\n",
      "epoch: 25 step: 744, loss is 0.07439855486154556\n",
      "epoch: 25 step: 745, loss is 0.04255928099155426\n",
      "epoch: 25 step: 746, loss is 0.0001225863234139979\n",
      "epoch: 25 step: 747, loss is 0.04136844351887703\n",
      "epoch: 25 step: 748, loss is 0.0002757157781161368\n",
      "epoch: 25 step: 749, loss is 9.119041351368651e-05\n",
      "epoch: 25 step: 750, loss is 0.0010317668784409761\n",
      "epoch: 25 step: 751, loss is 0.01644846238195896\n",
      "epoch: 25 step: 752, loss is 0.005809321999549866\n",
      "epoch: 25 step: 753, loss is 0.003325145225971937\n",
      "epoch: 25 step: 754, loss is 0.05302514135837555\n",
      "epoch: 25 step: 755, loss is 0.011309920810163021\n",
      "epoch: 25 step: 756, loss is 3.6616500437958166e-05\n",
      "epoch: 25 step: 757, loss is 0.00014725020446348935\n",
      "epoch: 25 step: 758, loss is 0.0032961000688374043\n",
      "epoch: 25 step: 759, loss is 0.00047060157521627843\n",
      "epoch: 25 step: 760, loss is 0.004926241934299469\n",
      "epoch: 25 step: 761, loss is 0.00040465977508574724\n",
      "epoch: 25 step: 762, loss is 0.021928565576672554\n",
      "epoch: 25 step: 763, loss is 0.21719001233577728\n",
      "epoch: 25 step: 764, loss is 0.002288034651428461\n",
      "epoch: 25 step: 765, loss is 0.00039115670369938016\n",
      "epoch: 25 step: 766, loss is 0.0008131889044307172\n",
      "epoch: 25 step: 767, loss is 0.013250515796244144\n",
      "epoch: 25 step: 768, loss is 0.008322327397763729\n",
      "epoch: 25 step: 769, loss is 7.618800009367988e-05\n",
      "epoch: 25 step: 770, loss is 0.002143201418220997\n",
      "epoch: 25 step: 771, loss is 0.004968986380845308\n",
      "epoch: 25 step: 772, loss is 0.03747100383043289\n",
      "epoch: 25 step: 773, loss is 0.0001539974909974262\n",
      "epoch: 25 step: 774, loss is 0.032936789095401764\n",
      "epoch: 25 step: 775, loss is 0.0002299464395036921\n",
      "epoch: 25 step: 776, loss is 0.04874300956726074\n",
      "epoch: 25 step: 777, loss is 0.014681560918688774\n",
      "epoch: 25 step: 778, loss is 0.0015125470235943794\n",
      "epoch: 25 step: 779, loss is 0.000340100668836385\n",
      "epoch: 25 step: 780, loss is 0.04776875674724579\n",
      "epoch: 25 step: 781, loss is 0.001742602325975895\n",
      "epoch: 25 step: 782, loss is 0.013654546812176704\n",
      "epoch: 25 step: 783, loss is 0.0015082170721143484\n",
      "epoch: 25 step: 784, loss is 0.05428396165370941\n",
      "epoch: 25 step: 785, loss is 2.7669988412526436e-05\n",
      "epoch: 25 step: 786, loss is 0.002350042574107647\n",
      "epoch: 25 step: 787, loss is 0.027577238157391548\n",
      "epoch: 25 step: 788, loss is 0.01850091852247715\n",
      "epoch: 25 step: 789, loss is 0.0007373291300609708\n",
      "epoch: 25 step: 790, loss is 0.0023181638680398464\n",
      "epoch: 25 step: 791, loss is 0.012207579798996449\n",
      "epoch: 25 step: 792, loss is 0.00041136954678222537\n",
      "epoch: 25 step: 793, loss is 0.0002770270511973649\n",
      "epoch: 25 step: 794, loss is 0.00115744408685714\n",
      "epoch: 25 step: 795, loss is 0.0008849120931699872\n",
      "epoch: 25 step: 796, loss is 0.05827099829912186\n",
      "epoch: 25 step: 797, loss is 0.001190014649182558\n",
      "epoch: 25 step: 798, loss is 0.002579102525487542\n",
      "epoch: 25 step: 799, loss is 0.0009850047063082457\n",
      "epoch: 25 step: 800, loss is 0.002002040157094598\n",
      "epoch: 25 step: 801, loss is 0.04370277747511864\n",
      "epoch: 25 step: 802, loss is 0.0023366096429526806\n",
      "epoch: 25 step: 803, loss is 0.0026817258913069963\n",
      "epoch: 25 step: 804, loss is 0.004712365567684174\n",
      "epoch: 25 step: 805, loss is 0.031555384397506714\n",
      "epoch: 25 step: 806, loss is 0.0009320463286712766\n",
      "epoch: 25 step: 807, loss is 0.00024097420100588351\n",
      "epoch: 25 step: 808, loss is 0.024549610912799835\n",
      "epoch: 25 step: 809, loss is 0.09829916805028915\n",
      "epoch: 25 step: 810, loss is 0.011192109435796738\n",
      "epoch: 25 step: 811, loss is 0.0006579402834177017\n",
      "epoch: 25 step: 812, loss is 0.00980521459132433\n",
      "epoch: 25 step: 813, loss is 0.0034330510534346104\n",
      "epoch: 25 step: 814, loss is 0.00032779385219328105\n",
      "epoch: 25 step: 815, loss is 0.0012318765511736274\n",
      "epoch: 25 step: 816, loss is 0.002275060163810849\n",
      "epoch: 25 step: 817, loss is 0.022379301488399506\n",
      "epoch: 25 step: 818, loss is 0.001195231918245554\n",
      "epoch: 25 step: 819, loss is 0.0007118764915503561\n",
      "epoch: 25 step: 820, loss is 0.00013151831808499992\n",
      "epoch: 25 step: 821, loss is 0.021823156625032425\n",
      "epoch: 25 step: 822, loss is 0.003724318463355303\n",
      "epoch: 25 step: 823, loss is 0.0359145924448967\n",
      "epoch: 25 step: 824, loss is 0.021291134878993034\n",
      "epoch: 25 step: 825, loss is 0.0007459858315996826\n",
      "epoch: 25 step: 826, loss is 0.0011977897956967354\n",
      "epoch: 25 step: 827, loss is 0.002367516281083226\n",
      "epoch: 25 step: 828, loss is 0.0014667310751974583\n",
      "epoch: 25 step: 829, loss is 0.000529280980117619\n",
      "epoch: 25 step: 830, loss is 4.705461469711736e-05\n",
      "epoch: 25 step: 831, loss is 0.00460977153852582\n",
      "epoch: 25 step: 832, loss is 0.005918758921325207\n",
      "epoch: 25 step: 833, loss is 0.000487975514261052\n",
      "epoch: 25 step: 834, loss is 0.025355856865644455\n",
      "epoch: 25 step: 835, loss is 0.00435335049405694\n",
      "epoch: 25 step: 836, loss is 0.0034238421358168125\n",
      "epoch: 25 step: 837, loss is 0.002221655333414674\n",
      "epoch: 25 step: 838, loss is 0.0014803119702264667\n",
      "epoch: 25 step: 839, loss is 0.010782254859805107\n",
      "epoch: 25 step: 840, loss is 0.0024761429522186518\n",
      "epoch: 25 step: 841, loss is 0.010671825148165226\n",
      "epoch: 25 step: 842, loss is 0.0029293789993971586\n",
      "epoch: 25 step: 843, loss is 0.04669902101159096\n",
      "epoch: 25 step: 844, loss is 0.006617195904254913\n",
      "epoch: 25 step: 845, loss is 0.0017624438041821122\n",
      "epoch: 25 step: 846, loss is 0.000168399783433415\n",
      "epoch: 25 step: 847, loss is 0.0029620968271046877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 848, loss is 0.0008855668129399419\n",
      "epoch: 25 step: 849, loss is 0.00412166491150856\n",
      "epoch: 25 step: 850, loss is 0.034283846616744995\n",
      "epoch: 25 step: 851, loss is 0.008450083434581757\n",
      "epoch: 25 step: 852, loss is 0.0009563056519255042\n",
      "epoch: 25 step: 853, loss is 0.010702104307711124\n",
      "epoch: 25 step: 854, loss is 0.004482065327465534\n",
      "epoch: 25 step: 855, loss is 0.04529160261154175\n",
      "epoch: 25 step: 856, loss is 0.0007946625119075179\n",
      "epoch: 25 step: 857, loss is 0.06380299478769302\n",
      "epoch: 25 step: 858, loss is 0.1152978166937828\n",
      "epoch: 25 step: 859, loss is 0.00020461101667024195\n",
      "epoch: 25 step: 860, loss is 0.002236110158264637\n",
      "epoch: 25 step: 861, loss is 0.01629611849784851\n",
      "epoch: 25 step: 862, loss is 0.03879285603761673\n",
      "epoch: 25 step: 863, loss is 0.0006623034714721143\n",
      "epoch: 25 step: 864, loss is 0.0007654789369553328\n",
      "epoch: 25 step: 865, loss is 0.006260447204113007\n",
      "epoch: 25 step: 866, loss is 7.405709038721398e-05\n",
      "epoch: 25 step: 867, loss is 0.013289534486830235\n",
      "epoch: 25 step: 868, loss is 0.03392760083079338\n",
      "epoch: 25 step: 869, loss is 0.05474764481186867\n",
      "epoch: 25 step: 870, loss is 0.0010856213048100471\n",
      "epoch: 25 step: 871, loss is 8.548209734726697e-05\n",
      "epoch: 25 step: 872, loss is 0.12006356567144394\n",
      "epoch: 25 step: 873, loss is 0.0001343389303656295\n",
      "epoch: 25 step: 874, loss is 0.004712939728051424\n",
      "epoch: 25 step: 875, loss is 0.0006783644785173237\n",
      "epoch: 25 step: 876, loss is 0.000580918975174427\n",
      "epoch: 25 step: 877, loss is 0.07534940540790558\n",
      "epoch: 25 step: 878, loss is 8.129370689857751e-05\n",
      "epoch: 25 step: 879, loss is 0.006480569485574961\n",
      "epoch: 25 step: 880, loss is 7.012419519014657e-05\n",
      "epoch: 25 step: 881, loss is 0.00029161691782064736\n",
      "epoch: 25 step: 882, loss is 0.0008840052178129554\n",
      "epoch: 25 step: 883, loss is 0.003919838462024927\n",
      "epoch: 25 step: 884, loss is 0.0058573330752551556\n",
      "epoch: 25 step: 885, loss is 0.002120192861184478\n",
      "epoch: 25 step: 886, loss is 0.0013844073982909322\n",
      "epoch: 25 step: 887, loss is 0.013475624844431877\n",
      "epoch: 25 step: 888, loss is 0.007619278039783239\n",
      "epoch: 25 step: 889, loss is 8.990135938802268e-06\n",
      "epoch: 25 step: 890, loss is 0.055247243493795395\n",
      "epoch: 25 step: 891, loss is 0.005199797451496124\n",
      "epoch: 25 step: 892, loss is 0.00017632693925406784\n",
      "epoch: 25 step: 893, loss is 0.0004551774763967842\n",
      "epoch: 25 step: 894, loss is 0.004575611092150211\n",
      "epoch: 25 step: 895, loss is 0.015073119662702084\n",
      "epoch: 25 step: 896, loss is 0.002576121361926198\n",
      "epoch: 25 step: 897, loss is 0.03039899840950966\n",
      "epoch: 25 step: 898, loss is 0.002333833137527108\n",
      "epoch: 25 step: 899, loss is 0.0006698147626593709\n",
      "epoch: 25 step: 900, loss is 0.15213324129581451\n",
      "epoch: 25 step: 901, loss is 0.00032410529092885554\n",
      "epoch: 25 step: 902, loss is 0.003499970305711031\n",
      "epoch: 25 step: 903, loss is 0.02596477046608925\n",
      "epoch: 25 step: 904, loss is 0.09210546314716339\n",
      "epoch: 25 step: 905, loss is 0.07568357139825821\n",
      "epoch: 25 step: 906, loss is 0.0009489024523645639\n",
      "epoch: 25 step: 907, loss is 0.07667219638824463\n",
      "epoch: 25 step: 908, loss is 0.0018796751974150538\n",
      "epoch: 25 step: 909, loss is 0.047887157648801804\n",
      "epoch: 25 step: 910, loss is 0.07797727733850479\n",
      "epoch: 25 step: 911, loss is 0.15362180769443512\n",
      "epoch: 25 step: 912, loss is 0.10761316120624542\n",
      "epoch: 25 step: 913, loss is 0.15801917016506195\n",
      "epoch: 25 step: 914, loss is 0.13425426185131073\n",
      "epoch: 25 step: 915, loss is 0.06538064032793045\n",
      "epoch: 25 step: 916, loss is 0.0002637324796523899\n",
      "epoch: 25 step: 917, loss is 0.02587449736893177\n",
      "epoch: 25 step: 918, loss is 3.5114433558192104e-05\n",
      "epoch: 25 step: 919, loss is 0.03648762032389641\n",
      "epoch: 25 step: 920, loss is 0.0010303054004907608\n",
      "epoch: 25 step: 921, loss is 0.013888556510210037\n",
      "epoch: 25 step: 922, loss is 0.01431396696716547\n",
      "epoch: 25 step: 923, loss is 0.0029554935172200203\n",
      "epoch: 25 step: 924, loss is 0.0018133244011551142\n",
      "epoch: 25 step: 925, loss is 0.0040144589729607105\n",
      "epoch: 25 step: 926, loss is 0.06888039410114288\n",
      "epoch: 25 step: 927, loss is 0.0027975691482424736\n",
      "epoch: 25 step: 928, loss is 0.0008292261627502739\n",
      "epoch: 25 step: 929, loss is 0.055110864341259\n",
      "epoch: 25 step: 930, loss is 0.05553886666893959\n",
      "epoch: 25 step: 931, loss is 0.00838801171630621\n",
      "epoch: 25 step: 932, loss is 0.0019269032636657357\n",
      "epoch: 25 step: 933, loss is 0.20575399696826935\n",
      "epoch: 25 step: 934, loss is 0.06014026328921318\n",
      "epoch: 25 step: 935, loss is 0.006105520762503147\n",
      "epoch: 25 step: 936, loss is 0.029854387044906616\n",
      "epoch: 25 step: 937, loss is 0.0064870440401136875\n",
      "epoch: 26 step: 1, loss is 0.010690600611269474\n",
      "epoch: 26 step: 2, loss is 0.024667764082551003\n",
      "epoch: 26 step: 3, loss is 0.006359115242958069\n",
      "epoch: 26 step: 4, loss is 0.012750678695738316\n",
      "epoch: 26 step: 5, loss is 0.0001162547487183474\n",
      "epoch: 26 step: 6, loss is 0.0003690858429763466\n",
      "epoch: 26 step: 7, loss is 0.06593723595142365\n",
      "epoch: 26 step: 8, loss is 0.0005081166164018214\n",
      "epoch: 26 step: 9, loss is 0.06098709627985954\n",
      "epoch: 26 step: 10, loss is 0.004854900296777487\n",
      "epoch: 26 step: 11, loss is 0.0003044277254957706\n",
      "epoch: 26 step: 12, loss is 0.00028314098017290235\n",
      "epoch: 26 step: 13, loss is 0.016801461577415466\n",
      "epoch: 26 step: 14, loss is 0.017282210290431976\n",
      "epoch: 26 step: 15, loss is 0.002884618006646633\n",
      "epoch: 26 step: 16, loss is 0.10750041902065277\n",
      "epoch: 26 step: 17, loss is 0.004621179774403572\n",
      "epoch: 26 step: 18, loss is 0.002279224107041955\n",
      "epoch: 26 step: 19, loss is 0.0002332320436835289\n",
      "epoch: 26 step: 20, loss is 0.010060111060738564\n",
      "epoch: 26 step: 21, loss is 0.008704591542482376\n",
      "epoch: 26 step: 22, loss is 0.0005594821996055543\n",
      "epoch: 26 step: 23, loss is 0.006391721311956644\n",
      "epoch: 26 step: 24, loss is 0.01289597898721695\n",
      "epoch: 26 step: 25, loss is 0.02564956061542034\n",
      "epoch: 26 step: 26, loss is 0.004799825605005026\n",
      "epoch: 26 step: 27, loss is 0.0036706209648400545\n",
      "epoch: 26 step: 28, loss is 0.08435271680355072\n",
      "epoch: 26 step: 29, loss is 0.009168559685349464\n",
      "epoch: 26 step: 30, loss is 0.004196603782474995\n",
      "epoch: 26 step: 31, loss is 0.01087375171482563\n",
      "epoch: 26 step: 32, loss is 0.00044150446774438024\n",
      "epoch: 26 step: 33, loss is 0.002529017860069871\n",
      "epoch: 26 step: 34, loss is 0.0006226013647392392\n",
      "epoch: 26 step: 35, loss is 0.00010604212002363056\n",
      "epoch: 26 step: 36, loss is 0.00408752029761672\n",
      "epoch: 26 step: 37, loss is 0.028293004259467125\n",
      "epoch: 26 step: 38, loss is 0.004989985376596451\n",
      "epoch: 26 step: 39, loss is 0.0008852475439198315\n",
      "epoch: 26 step: 40, loss is 0.005471468903124332\n",
      "epoch: 26 step: 41, loss is 1.8767399524222128e-05\n",
      "epoch: 26 step: 42, loss is 0.005423964466899633\n",
      "epoch: 26 step: 43, loss is 0.0022235072683542967\n",
      "epoch: 26 step: 44, loss is 0.0048929238691926\n",
      "epoch: 26 step: 45, loss is 0.018832504749298096\n",
      "epoch: 26 step: 46, loss is 0.051907043904066086\n",
      "epoch: 26 step: 47, loss is 0.0004139176453463733\n",
      "epoch: 26 step: 48, loss is 0.0007804789929650724\n",
      "epoch: 26 step: 49, loss is 0.005652790889143944\n",
      "epoch: 26 step: 50, loss is 0.029575230553746223\n",
      "epoch: 26 step: 51, loss is 0.035576336085796356\n",
      "epoch: 26 step: 52, loss is 0.0004327520728111267\n",
      "epoch: 26 step: 53, loss is 0.0016272488282993436\n",
      "epoch: 26 step: 54, loss is 0.0007703918381594121\n",
      "epoch: 26 step: 55, loss is 0.04447811096906662\n",
      "epoch: 26 step: 56, loss is 0.000396410672692582\n",
      "epoch: 26 step: 57, loss is 0.001867671380750835\n",
      "epoch: 26 step: 58, loss is 0.03999777510762215\n",
      "epoch: 26 step: 59, loss is 0.0057227835059165955\n",
      "epoch: 26 step: 60, loss is 0.009335723705589771\n",
      "epoch: 26 step: 61, loss is 0.009284834377467632\n",
      "epoch: 26 step: 62, loss is 0.00021164769714232534\n",
      "epoch: 26 step: 63, loss is 0.005895479116588831\n",
      "epoch: 26 step: 64, loss is 0.0029661846347153187\n",
      "epoch: 26 step: 65, loss is 0.0004941304214298725\n",
      "epoch: 26 step: 66, loss is 0.04766736179590225\n",
      "epoch: 26 step: 67, loss is 0.00511797284707427\n",
      "epoch: 26 step: 68, loss is 0.0013612167676910758\n",
      "epoch: 26 step: 69, loss is 0.04532410576939583\n",
      "epoch: 26 step: 70, loss is 0.006611499935388565\n",
      "epoch: 26 step: 71, loss is 0.004235715139657259\n",
      "epoch: 26 step: 72, loss is 0.001179804326966405\n",
      "epoch: 26 step: 73, loss is 0.004457573406398296\n",
      "epoch: 26 step: 74, loss is 0.0019468446262180805\n",
      "epoch: 26 step: 75, loss is 0.0003628087288234383\n",
      "epoch: 26 step: 76, loss is 0.018473627045750618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 77, loss is 0.0012163306819275022\n",
      "epoch: 26 step: 78, loss is 0.004898711107671261\n",
      "epoch: 26 step: 79, loss is 0.06078346073627472\n",
      "epoch: 26 step: 80, loss is 0.021230224519968033\n",
      "epoch: 26 step: 81, loss is 0.0645071491599083\n",
      "epoch: 26 step: 82, loss is 0.0011647639330476522\n",
      "epoch: 26 step: 83, loss is 0.0016950486460700631\n",
      "epoch: 26 step: 84, loss is 0.011237997561693192\n",
      "epoch: 26 step: 85, loss is 0.001789941219612956\n",
      "epoch: 26 step: 86, loss is 0.0017586869653314352\n",
      "epoch: 26 step: 87, loss is 2.7045554816140793e-05\n",
      "epoch: 26 step: 88, loss is 0.008774449117481709\n",
      "epoch: 26 step: 89, loss is 0.002547035226598382\n",
      "epoch: 26 step: 90, loss is 0.02659287303686142\n",
      "epoch: 26 step: 91, loss is 0.00010661721898941323\n",
      "epoch: 26 step: 92, loss is 0.04479653760790825\n",
      "epoch: 26 step: 93, loss is 0.0022712924983352423\n",
      "epoch: 26 step: 94, loss is 0.0014451700262725353\n",
      "epoch: 26 step: 95, loss is 0.002124475548043847\n",
      "epoch: 26 step: 96, loss is 0.00022015075956005603\n",
      "epoch: 26 step: 97, loss is 0.00013603994739241898\n",
      "epoch: 26 step: 98, loss is 0.08223625272512436\n",
      "epoch: 26 step: 99, loss is 7.909753912827e-05\n",
      "epoch: 26 step: 100, loss is 0.001153519144281745\n",
      "epoch: 26 step: 101, loss is 0.006869541481137276\n",
      "epoch: 26 step: 102, loss is 0.00024451446370221674\n",
      "epoch: 26 step: 103, loss is 0.001038955757394433\n",
      "epoch: 26 step: 104, loss is 0.008550438098609447\n",
      "epoch: 26 step: 105, loss is 0.061852846294641495\n",
      "epoch: 26 step: 106, loss is 0.0019797373097389936\n",
      "epoch: 26 step: 107, loss is 0.00011332310532452539\n",
      "epoch: 26 step: 108, loss is 0.011574167758226395\n",
      "epoch: 26 step: 109, loss is 6.639469938818365e-05\n",
      "epoch: 26 step: 110, loss is 0.00024366180878132582\n",
      "epoch: 26 step: 111, loss is 0.0002304913941770792\n",
      "epoch: 26 step: 112, loss is 0.0007031155400909483\n",
      "epoch: 26 step: 113, loss is 0.04311007261276245\n",
      "epoch: 26 step: 114, loss is 0.0147389005869627\n",
      "epoch: 26 step: 115, loss is 0.009771525859832764\n",
      "epoch: 26 step: 116, loss is 0.019810792058706284\n",
      "epoch: 26 step: 117, loss is 0.06070027872920036\n",
      "epoch: 26 step: 118, loss is 0.0036823067348450422\n",
      "epoch: 26 step: 119, loss is 0.00034684056299738586\n",
      "epoch: 26 step: 120, loss is 0.0010308560449630022\n",
      "epoch: 26 step: 121, loss is 0.017969386652112007\n",
      "epoch: 26 step: 122, loss is 0.0017164942109957337\n",
      "epoch: 26 step: 123, loss is 0.0074282255955040455\n",
      "epoch: 26 step: 124, loss is 0.0007880498887971044\n",
      "epoch: 26 step: 125, loss is 0.0026605818420648575\n",
      "epoch: 26 step: 126, loss is 0.058822620660066605\n",
      "epoch: 26 step: 127, loss is 0.0016465875087305903\n",
      "epoch: 26 step: 128, loss is 0.014993177726864815\n",
      "epoch: 26 step: 129, loss is 0.0013752070954069495\n",
      "epoch: 26 step: 130, loss is 0.0005890081520192325\n",
      "epoch: 26 step: 131, loss is 0.011160844005644321\n",
      "epoch: 26 step: 132, loss is 0.0012277807109057903\n",
      "epoch: 26 step: 133, loss is 0.005249093286693096\n",
      "epoch: 26 step: 134, loss is 0.008541787043213844\n",
      "epoch: 26 step: 135, loss is 0.0009026348707266152\n",
      "epoch: 26 step: 136, loss is 0.017161382362246513\n",
      "epoch: 26 step: 137, loss is 0.002346697263419628\n",
      "epoch: 26 step: 138, loss is 0.00042006763396784663\n",
      "epoch: 26 step: 139, loss is 0.0012446825858205557\n",
      "epoch: 26 step: 140, loss is 0.002158774295821786\n",
      "epoch: 26 step: 141, loss is 0.0005332109285518527\n",
      "epoch: 26 step: 142, loss is 0.024327274411916733\n",
      "epoch: 26 step: 143, loss is 0.001372052589431405\n",
      "epoch: 26 step: 144, loss is 0.0009158077300526202\n",
      "epoch: 26 step: 145, loss is 0.006375968921929598\n",
      "epoch: 26 step: 146, loss is 0.001407952280715108\n",
      "epoch: 26 step: 147, loss is 0.0032765818759799004\n",
      "epoch: 26 step: 148, loss is 0.0009912512032315135\n",
      "epoch: 26 step: 149, loss is 0.00036070571513846517\n",
      "epoch: 26 step: 150, loss is 0.001953063067048788\n",
      "epoch: 26 step: 151, loss is 0.03371712192893028\n",
      "epoch: 26 step: 152, loss is 0.00245108176022768\n",
      "epoch: 26 step: 153, loss is 3.705676135723479e-05\n",
      "epoch: 26 step: 154, loss is 0.047786418348550797\n",
      "epoch: 26 step: 155, loss is 0.006673410069197416\n",
      "epoch: 26 step: 156, loss is 0.0014177787816151977\n",
      "epoch: 26 step: 157, loss is 0.003954457584768534\n",
      "epoch: 26 step: 158, loss is 0.0020550526678562164\n",
      "epoch: 26 step: 159, loss is 0.007763618137687445\n",
      "epoch: 26 step: 160, loss is 0.007498515769839287\n",
      "epoch: 26 step: 161, loss is 0.022609734907746315\n",
      "epoch: 26 step: 162, loss is 0.0038573830388486385\n",
      "epoch: 26 step: 163, loss is 0.0009744134149514139\n",
      "epoch: 26 step: 164, loss is 0.00016482948558405042\n",
      "epoch: 26 step: 165, loss is 0.0032022728119045496\n",
      "epoch: 26 step: 166, loss is 0.013540410436689854\n",
      "epoch: 26 step: 167, loss is 0.0003917716967407614\n",
      "epoch: 26 step: 168, loss is 0.002841425593942404\n",
      "epoch: 26 step: 169, loss is 0.02242644876241684\n",
      "epoch: 26 step: 170, loss is 0.000516512431204319\n",
      "epoch: 26 step: 171, loss is 0.0046209366992115974\n",
      "epoch: 26 step: 172, loss is 0.00016212021000683308\n",
      "epoch: 26 step: 173, loss is 0.0005439622909761965\n",
      "epoch: 26 step: 174, loss is 0.04748283326625824\n",
      "epoch: 26 step: 175, loss is 0.0009769314201548696\n",
      "epoch: 26 step: 176, loss is 0.003922918811440468\n",
      "epoch: 26 step: 177, loss is 0.0017588972114026546\n",
      "epoch: 26 step: 178, loss is 0.00022945168893784285\n",
      "epoch: 26 step: 179, loss is 0.0016543134115636349\n",
      "epoch: 26 step: 180, loss is 0.017804229632019997\n",
      "epoch: 26 step: 181, loss is 0.0026446187403053045\n",
      "epoch: 26 step: 182, loss is 0.0008557135006412864\n",
      "epoch: 26 step: 183, loss is 9.95406589936465e-05\n",
      "epoch: 26 step: 184, loss is 0.0011941117700189352\n",
      "epoch: 26 step: 185, loss is 0.007751148659735918\n",
      "epoch: 26 step: 186, loss is 0.0004070242866873741\n",
      "epoch: 26 step: 187, loss is 0.0021542825270444155\n",
      "epoch: 26 step: 188, loss is 0.0873926505446434\n",
      "epoch: 26 step: 189, loss is 0.009693142957985401\n",
      "epoch: 26 step: 190, loss is 0.0002618221042212099\n",
      "epoch: 26 step: 191, loss is 0.011980535462498665\n",
      "epoch: 26 step: 192, loss is 0.001377365319058299\n",
      "epoch: 26 step: 193, loss is 0.013523806817829609\n",
      "epoch: 26 step: 194, loss is 9.293080074712634e-05\n",
      "epoch: 26 step: 195, loss is 0.0018466481706127524\n",
      "epoch: 26 step: 196, loss is 0.00037397409323602915\n",
      "epoch: 26 step: 197, loss is 0.0021930609364062548\n",
      "epoch: 26 step: 198, loss is 0.006147280801087618\n",
      "epoch: 26 step: 199, loss is 0.004569315817207098\n",
      "epoch: 26 step: 200, loss is 0.0017621383303776383\n",
      "epoch: 26 step: 201, loss is 0.018690383061766624\n",
      "epoch: 26 step: 202, loss is 0.00029451074078679085\n",
      "epoch: 26 step: 203, loss is 4.805238131666556e-05\n",
      "epoch: 26 step: 204, loss is 0.004682511556893587\n",
      "epoch: 26 step: 205, loss is 0.07277873903512955\n",
      "epoch: 26 step: 206, loss is 4.1582767153158784e-05\n",
      "epoch: 26 step: 207, loss is 0.0003298110968898982\n",
      "epoch: 26 step: 208, loss is 0.0027032149955630302\n",
      "epoch: 26 step: 209, loss is 0.0012374750804156065\n",
      "epoch: 26 step: 210, loss is 0.0007923325174488127\n",
      "epoch: 26 step: 211, loss is 0.0005519423284567893\n",
      "epoch: 26 step: 212, loss is 0.01371302455663681\n",
      "epoch: 26 step: 213, loss is 0.0005807512206956744\n",
      "epoch: 26 step: 214, loss is 0.000135144277010113\n",
      "epoch: 26 step: 215, loss is 0.0014525754377245903\n",
      "epoch: 26 step: 216, loss is 8.253169653471559e-05\n",
      "epoch: 26 step: 217, loss is 0.0029185772873461246\n",
      "epoch: 26 step: 218, loss is 7.049229316180572e-05\n",
      "epoch: 26 step: 219, loss is 0.028997372835874557\n",
      "epoch: 26 step: 220, loss is 0.0010267336620017886\n",
      "epoch: 26 step: 221, loss is 0.0006252771709114313\n",
      "epoch: 26 step: 222, loss is 0.0022229026071727276\n",
      "epoch: 26 step: 223, loss is 0.0002204232441727072\n",
      "epoch: 26 step: 224, loss is 0.016940616071224213\n",
      "epoch: 26 step: 225, loss is 0.000707964354660362\n",
      "epoch: 26 step: 226, loss is 0.0031001311726868153\n",
      "epoch: 26 step: 227, loss is 0.008408136665821075\n",
      "epoch: 26 step: 228, loss is 0.00016418693121522665\n",
      "epoch: 26 step: 229, loss is 0.00010258269321639091\n",
      "epoch: 26 step: 230, loss is 0.020422890782356262\n",
      "epoch: 26 step: 231, loss is 0.0007468099938705564\n",
      "epoch: 26 step: 232, loss is 0.000393764756154269\n",
      "epoch: 26 step: 233, loss is 0.00047429517144337296\n",
      "epoch: 26 step: 234, loss is 0.000987589475698769\n",
      "epoch: 26 step: 235, loss is 0.0008186521008610725\n",
      "epoch: 26 step: 236, loss is 0.023989733308553696\n",
      "epoch: 26 step: 237, loss is 0.003134641330689192\n",
      "epoch: 26 step: 238, loss is 0.010700486600399017\n",
      "epoch: 26 step: 239, loss is 0.0596419982612133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 240, loss is 0.02037675306200981\n",
      "epoch: 26 step: 241, loss is 0.03338131681084633\n",
      "epoch: 26 step: 242, loss is 0.0317140631377697\n",
      "epoch: 26 step: 243, loss is 0.00047178706154227257\n",
      "epoch: 26 step: 244, loss is 0.001338260481134057\n",
      "epoch: 26 step: 245, loss is 5.087023964733817e-06\n",
      "epoch: 26 step: 246, loss is 0.07615149021148682\n",
      "epoch: 26 step: 247, loss is 0.038418252021074295\n",
      "epoch: 26 step: 248, loss is 0.012466822750866413\n",
      "epoch: 26 step: 249, loss is 0.005655039567500353\n",
      "epoch: 26 step: 250, loss is 0.0009683547541499138\n",
      "epoch: 26 step: 251, loss is 0.00011779071792261675\n",
      "epoch: 26 step: 252, loss is 0.004115585703402758\n",
      "epoch: 26 step: 253, loss is 0.03281170502305031\n",
      "epoch: 26 step: 254, loss is 0.0051573109813034534\n",
      "epoch: 26 step: 255, loss is 0.005844463594257832\n",
      "epoch: 26 step: 256, loss is 0.012489529326558113\n",
      "epoch: 26 step: 257, loss is 0.0002899093960877508\n",
      "epoch: 26 step: 258, loss is 0.0013840190367773175\n",
      "epoch: 26 step: 259, loss is 0.007449198979884386\n",
      "epoch: 26 step: 260, loss is 0.027697039768099785\n",
      "epoch: 26 step: 261, loss is 0.010766053572297096\n",
      "epoch: 26 step: 262, loss is 2.3571006749989465e-05\n",
      "epoch: 26 step: 263, loss is 3.343731805216521e-05\n",
      "epoch: 26 step: 264, loss is 7.265943713719025e-05\n",
      "epoch: 26 step: 265, loss is 0.0016417652368545532\n",
      "epoch: 26 step: 266, loss is 7.526716217398643e-05\n",
      "epoch: 26 step: 267, loss is 0.00012510920350905508\n",
      "epoch: 26 step: 268, loss is 0.03455134853720665\n",
      "epoch: 26 step: 269, loss is 0.0001643611612962559\n",
      "epoch: 26 step: 270, loss is 0.0035352979321032763\n",
      "epoch: 26 step: 271, loss is 9.094943379750475e-05\n",
      "epoch: 26 step: 272, loss is 0.11981084942817688\n",
      "epoch: 26 step: 273, loss is 0.07332718372344971\n",
      "epoch: 26 step: 274, loss is 0.0009011438814923167\n",
      "epoch: 26 step: 275, loss is 0.04734665900468826\n",
      "epoch: 26 step: 276, loss is 0.0003962620976381004\n",
      "epoch: 26 step: 277, loss is 0.012187924236059189\n",
      "epoch: 26 step: 278, loss is 0.06179949641227722\n",
      "epoch: 26 step: 279, loss is 0.020979570224881172\n",
      "epoch: 26 step: 280, loss is 0.001647427212446928\n",
      "epoch: 26 step: 281, loss is 0.012238586321473122\n",
      "epoch: 26 step: 282, loss is 0.0011058889795094728\n",
      "epoch: 26 step: 283, loss is 0.002666357671841979\n",
      "epoch: 26 step: 284, loss is 0.00021945142361801118\n",
      "epoch: 26 step: 285, loss is 0.001687298878096044\n",
      "epoch: 26 step: 286, loss is 0.004262489732354879\n",
      "epoch: 26 step: 287, loss is 0.0249937791377306\n",
      "epoch: 26 step: 288, loss is 0.00013541834778152406\n",
      "epoch: 26 step: 289, loss is 0.012333190068602562\n",
      "epoch: 26 step: 290, loss is 0.000605227833148092\n",
      "epoch: 26 step: 291, loss is 0.0013732828665524721\n",
      "epoch: 26 step: 292, loss is 0.0003220026846975088\n",
      "epoch: 26 step: 293, loss is 0.00032415773603133857\n",
      "epoch: 26 step: 294, loss is 0.0021008048206567764\n",
      "epoch: 26 step: 295, loss is 0.00034659041557461023\n",
      "epoch: 26 step: 296, loss is 0.019607219845056534\n",
      "epoch: 26 step: 297, loss is 0.00333873531781137\n",
      "epoch: 26 step: 298, loss is 0.012026866897940636\n",
      "epoch: 26 step: 299, loss is 0.0011251945979893208\n",
      "epoch: 26 step: 300, loss is 0.0039615267887711525\n",
      "epoch: 26 step: 301, loss is 0.0008068865281529725\n",
      "epoch: 26 step: 302, loss is 0.0011575402459129691\n",
      "epoch: 26 step: 303, loss is 0.0002599529398139566\n",
      "epoch: 26 step: 304, loss is 0.0005563021986745298\n",
      "epoch: 26 step: 305, loss is 0.030176013708114624\n",
      "epoch: 26 step: 306, loss is 0.007152762729674578\n",
      "epoch: 26 step: 307, loss is 0.0005579658900387585\n",
      "epoch: 26 step: 308, loss is 0.029449181631207466\n",
      "epoch: 26 step: 309, loss is 0.0011104578152298927\n",
      "epoch: 26 step: 310, loss is 0.004974090028554201\n",
      "epoch: 26 step: 311, loss is 0.00046619950444437563\n",
      "epoch: 26 step: 312, loss is 0.01787589304149151\n",
      "epoch: 26 step: 313, loss is 4.886008900939487e-05\n",
      "epoch: 26 step: 314, loss is 0.0008683228516019881\n",
      "epoch: 26 step: 315, loss is 0.012488867156207561\n",
      "epoch: 26 step: 316, loss is 0.0006874784012325108\n",
      "epoch: 26 step: 317, loss is 0.0037277592346072197\n",
      "epoch: 26 step: 318, loss is 0.00704319030046463\n",
      "epoch: 26 step: 319, loss is 0.01040914561599493\n",
      "epoch: 26 step: 320, loss is 0.007484266068786383\n",
      "epoch: 26 step: 321, loss is 0.03734353557229042\n",
      "epoch: 26 step: 322, loss is 0.007848341949284077\n",
      "epoch: 26 step: 323, loss is 0.08958890289068222\n",
      "epoch: 26 step: 324, loss is 0.00536132138222456\n",
      "epoch: 26 step: 325, loss is 0.003944720141589642\n",
      "epoch: 26 step: 326, loss is 0.0006903479807078838\n",
      "epoch: 26 step: 327, loss is 0.0007401311886496842\n",
      "epoch: 26 step: 328, loss is 0.002124846214428544\n",
      "epoch: 26 step: 329, loss is 0.00125687918625772\n",
      "epoch: 26 step: 330, loss is 0.0007624980644322932\n",
      "epoch: 26 step: 331, loss is 0.01277143694460392\n",
      "epoch: 26 step: 332, loss is 0.015797525644302368\n",
      "epoch: 26 step: 333, loss is 0.018960127606987953\n",
      "epoch: 26 step: 334, loss is 0.00018848356558009982\n",
      "epoch: 26 step: 335, loss is 0.002445496851578355\n",
      "epoch: 26 step: 336, loss is 0.0007812808034941554\n",
      "epoch: 26 step: 337, loss is 4.145708226133138e-05\n",
      "epoch: 26 step: 338, loss is 0.0029260776937007904\n",
      "epoch: 26 step: 339, loss is 0.03943409398198128\n",
      "epoch: 26 step: 340, loss is 0.05595894902944565\n",
      "epoch: 26 step: 341, loss is 0.0001870978157967329\n",
      "epoch: 26 step: 342, loss is 0.004717462230473757\n",
      "epoch: 26 step: 343, loss is 0.008684800937771797\n",
      "epoch: 26 step: 344, loss is 0.0030600465834140778\n",
      "epoch: 26 step: 345, loss is 0.001894147600978613\n",
      "epoch: 26 step: 346, loss is 0.0032687641214579344\n",
      "epoch: 26 step: 347, loss is 0.000343984313076362\n",
      "epoch: 26 step: 348, loss is 0.00043355298112146556\n",
      "epoch: 26 step: 349, loss is 0.0019206181168556213\n",
      "epoch: 26 step: 350, loss is 0.002305885311216116\n",
      "epoch: 26 step: 351, loss is 0.004461427219212055\n",
      "epoch: 26 step: 352, loss is 0.0019215475767850876\n",
      "epoch: 26 step: 353, loss is 0.00044106977293267846\n",
      "epoch: 26 step: 354, loss is 0.0691956952214241\n",
      "epoch: 26 step: 355, loss is 0.024950925260782242\n",
      "epoch: 26 step: 356, loss is 0.0046014683321118355\n",
      "epoch: 26 step: 357, loss is 0.00010209731408394873\n",
      "epoch: 26 step: 358, loss is 0.0014819998759776354\n",
      "epoch: 26 step: 359, loss is 0.03328905254602432\n",
      "epoch: 26 step: 360, loss is 0.005793410819023848\n",
      "epoch: 26 step: 361, loss is 0.000240529072470963\n",
      "epoch: 26 step: 362, loss is 0.017627080902457237\n",
      "epoch: 26 step: 363, loss is 0.01032137218862772\n",
      "epoch: 26 step: 364, loss is 0.001964996801689267\n",
      "epoch: 26 step: 365, loss is 0.07633181661367416\n",
      "epoch: 26 step: 366, loss is 0.00015357010124716908\n",
      "epoch: 26 step: 367, loss is 0.027717575430870056\n",
      "epoch: 26 step: 368, loss is 0.0010269673075526953\n",
      "epoch: 26 step: 369, loss is 0.00310510559938848\n",
      "epoch: 26 step: 370, loss is 0.02944313921034336\n",
      "epoch: 26 step: 371, loss is 0.01441645622253418\n",
      "epoch: 26 step: 372, loss is 0.0015833675861358643\n",
      "epoch: 26 step: 373, loss is 0.07442571967840195\n",
      "epoch: 26 step: 374, loss is 0.00014045133139006793\n",
      "epoch: 26 step: 375, loss is 0.002896481193602085\n",
      "epoch: 26 step: 376, loss is 0.0009471732191741467\n",
      "epoch: 26 step: 377, loss is 0.09898849576711655\n",
      "epoch: 26 step: 378, loss is 0.022112717851996422\n",
      "epoch: 26 step: 379, loss is 0.009792798198759556\n",
      "epoch: 26 step: 380, loss is 0.002199266105890274\n",
      "epoch: 26 step: 381, loss is 0.011639254167675972\n",
      "epoch: 26 step: 382, loss is 0.0007709925994277\n",
      "epoch: 26 step: 383, loss is 0.00018322354299016297\n",
      "epoch: 26 step: 384, loss is 0.02539217285811901\n",
      "epoch: 26 step: 385, loss is 0.0047667790204286575\n",
      "epoch: 26 step: 386, loss is 0.010483047924935818\n",
      "epoch: 26 step: 387, loss is 0.001077371765859425\n",
      "epoch: 26 step: 388, loss is 0.038020018488168716\n",
      "epoch: 26 step: 389, loss is 0.011735200881958008\n",
      "epoch: 26 step: 390, loss is 0.0006545068463310599\n",
      "epoch: 26 step: 391, loss is 0.009787838906049728\n",
      "epoch: 26 step: 392, loss is 0.0011776962783187628\n",
      "epoch: 26 step: 393, loss is 0.005799357779324055\n",
      "epoch: 26 step: 394, loss is 0.04594007506966591\n",
      "epoch: 26 step: 395, loss is 0.0008068480528891087\n",
      "epoch: 26 step: 396, loss is 0.0015375673538073897\n",
      "epoch: 26 step: 397, loss is 0.007320609409362078\n",
      "epoch: 26 step: 398, loss is 0.018223227933049202\n",
      "epoch: 26 step: 399, loss is 0.002695818431675434\n",
      "epoch: 26 step: 400, loss is 0.001908097299747169\n",
      "epoch: 26 step: 401, loss is 0.006009517703205347\n",
      "epoch: 26 step: 402, loss is 0.0010617482475936413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 403, loss is 0.0068838223814964294\n",
      "epoch: 26 step: 404, loss is 0.0025560418143868446\n",
      "epoch: 26 step: 405, loss is 0.013447455130517483\n",
      "epoch: 26 step: 406, loss is 0.018750157207250595\n",
      "epoch: 26 step: 407, loss is 0.0009019832359626889\n",
      "epoch: 26 step: 408, loss is 0.007855155505239964\n",
      "epoch: 26 step: 409, loss is 0.013164172880351543\n",
      "epoch: 26 step: 410, loss is 0.005422219168394804\n",
      "epoch: 26 step: 411, loss is 0.0033158655278384686\n",
      "epoch: 26 step: 412, loss is 0.0013005421496927738\n",
      "epoch: 26 step: 413, loss is 0.04149790108203888\n",
      "epoch: 26 step: 414, loss is 0.0005934928776696324\n",
      "epoch: 26 step: 415, loss is 0.000488409074023366\n",
      "epoch: 26 step: 416, loss is 0.00040903600165620446\n",
      "epoch: 26 step: 417, loss is 0.014881647191941738\n",
      "epoch: 26 step: 418, loss is 0.0013327364576980472\n",
      "epoch: 26 step: 419, loss is 0.00015018445265013725\n",
      "epoch: 26 step: 420, loss is 0.00034465634962543845\n",
      "epoch: 26 step: 421, loss is 0.028953345492482185\n",
      "epoch: 26 step: 422, loss is 0.003387683304026723\n",
      "epoch: 26 step: 423, loss is 0.0009678257629275322\n",
      "epoch: 26 step: 424, loss is 0.0007190283504314721\n",
      "epoch: 26 step: 425, loss is 0.0007103869575075805\n",
      "epoch: 26 step: 426, loss is 3.3246611565118656e-05\n",
      "epoch: 26 step: 427, loss is 0.00026843350497074425\n",
      "epoch: 26 step: 428, loss is 0.007134052459150553\n",
      "epoch: 26 step: 429, loss is 0.0022214907221496105\n",
      "epoch: 26 step: 430, loss is 0.007072293199598789\n",
      "epoch: 26 step: 431, loss is 0.01425668690353632\n",
      "epoch: 26 step: 432, loss is 0.017142293974757195\n",
      "epoch: 26 step: 433, loss is 0.000106066967418883\n",
      "epoch: 26 step: 434, loss is 0.024052105844020844\n",
      "epoch: 26 step: 435, loss is 0.07094408571720123\n",
      "epoch: 26 step: 436, loss is 0.0008176661795005202\n",
      "epoch: 26 step: 437, loss is 0.0018741197418421507\n",
      "epoch: 26 step: 438, loss is 0.003861422883346677\n",
      "epoch: 26 step: 439, loss is 0.0008467384614050388\n",
      "epoch: 26 step: 440, loss is 0.00020495569333434105\n",
      "epoch: 26 step: 441, loss is 0.0014953266363590956\n",
      "epoch: 26 step: 442, loss is 0.007230888120830059\n",
      "epoch: 26 step: 443, loss is 0.07194331288337708\n",
      "epoch: 26 step: 444, loss is 0.012131416238844395\n",
      "epoch: 26 step: 445, loss is 4.611497206497006e-05\n",
      "epoch: 26 step: 446, loss is 0.002563276793807745\n",
      "epoch: 26 step: 447, loss is 0.01418227981775999\n",
      "epoch: 26 step: 448, loss is 0.008168318308889866\n",
      "epoch: 26 step: 449, loss is 0.0009707141434773803\n",
      "epoch: 26 step: 450, loss is 0.05303677171468735\n",
      "epoch: 26 step: 451, loss is 0.008813189342617989\n",
      "epoch: 26 step: 452, loss is 0.0002445568679831922\n",
      "epoch: 26 step: 453, loss is 0.0008503832505084574\n",
      "epoch: 26 step: 454, loss is 0.0001739386934787035\n",
      "epoch: 26 step: 455, loss is 0.02031140774488449\n",
      "epoch: 26 step: 456, loss is 0.0005525641026906669\n",
      "epoch: 26 step: 457, loss is 0.00039513292722404003\n",
      "epoch: 26 step: 458, loss is 0.017729980871081352\n",
      "epoch: 26 step: 459, loss is 0.005761217325925827\n",
      "epoch: 26 step: 460, loss is 8.117184188449755e-05\n",
      "epoch: 26 step: 461, loss is 0.006122041027992964\n",
      "epoch: 26 step: 462, loss is 0.012249315157532692\n",
      "epoch: 26 step: 463, loss is 0.011501319706439972\n",
      "epoch: 26 step: 464, loss is 0.03318032622337341\n",
      "epoch: 26 step: 465, loss is 0.0012685003457590938\n",
      "epoch: 26 step: 466, loss is 0.00549588305875659\n",
      "epoch: 26 step: 467, loss is 0.00044955057092010975\n",
      "epoch: 26 step: 468, loss is 0.00033993914257735014\n",
      "epoch: 26 step: 469, loss is 0.03783261030912399\n",
      "epoch: 26 step: 470, loss is 0.003267717082053423\n",
      "epoch: 26 step: 471, loss is 0.0004995176568627357\n",
      "epoch: 26 step: 472, loss is 0.0005263789789751172\n",
      "epoch: 26 step: 473, loss is 0.0010438147000968456\n",
      "epoch: 26 step: 474, loss is 0.00014484896382782608\n",
      "epoch: 26 step: 475, loss is 0.0005387315759435296\n",
      "epoch: 26 step: 476, loss is 0.013827478513121605\n",
      "epoch: 26 step: 477, loss is 8.963380969362333e-05\n",
      "epoch: 26 step: 478, loss is 0.0007544633699581027\n",
      "epoch: 26 step: 479, loss is 0.00017332147399429232\n",
      "epoch: 26 step: 480, loss is 0.0028338448610156775\n",
      "epoch: 26 step: 481, loss is 0.00012773166236001998\n",
      "epoch: 26 step: 482, loss is 0.0063246628269553185\n",
      "epoch: 26 step: 483, loss is 0.0003249883884564042\n",
      "epoch: 26 step: 484, loss is 0.007422581780701876\n",
      "epoch: 26 step: 485, loss is 0.0015962397446855903\n",
      "epoch: 26 step: 486, loss is 0.001954467734321952\n",
      "epoch: 26 step: 487, loss is 0.0025975205935537815\n",
      "epoch: 26 step: 488, loss is 0.012755216099321842\n",
      "epoch: 26 step: 489, loss is 0.00458380114287138\n",
      "epoch: 26 step: 490, loss is 0.00022943693329580128\n",
      "epoch: 26 step: 491, loss is 0.0003374121442902833\n",
      "epoch: 26 step: 492, loss is 0.0010668347822502255\n",
      "epoch: 26 step: 493, loss is 8.219984010793269e-05\n",
      "epoch: 26 step: 494, loss is 0.00047799391904845834\n",
      "epoch: 26 step: 495, loss is 0.002606157446280122\n",
      "epoch: 26 step: 496, loss is 5.7393390306970105e-05\n",
      "epoch: 26 step: 497, loss is 0.029399795457720757\n",
      "epoch: 26 step: 498, loss is 8.912166958907619e-05\n",
      "epoch: 26 step: 499, loss is 4.250036727171391e-05\n",
      "epoch: 26 step: 500, loss is 7.182086847024038e-05\n",
      "epoch: 26 step: 501, loss is 0.0020747839007526636\n",
      "epoch: 26 step: 502, loss is 0.0001935961190611124\n",
      "epoch: 26 step: 503, loss is 0.0018921273294836283\n",
      "epoch: 26 step: 504, loss is 0.04332304745912552\n",
      "epoch: 26 step: 505, loss is 0.00047195274964906275\n",
      "epoch: 26 step: 506, loss is 0.014287803322076797\n",
      "epoch: 26 step: 507, loss is 0.0008221677853725851\n",
      "epoch: 26 step: 508, loss is 0.1073116734623909\n",
      "epoch: 26 step: 509, loss is 0.001449136296287179\n",
      "epoch: 26 step: 510, loss is 9.144150681095198e-06\n",
      "epoch: 26 step: 511, loss is 4.7619749238947406e-05\n",
      "epoch: 26 step: 512, loss is 7.233758515212685e-05\n",
      "epoch: 26 step: 513, loss is 0.0049327765591442585\n",
      "epoch: 26 step: 514, loss is 0.00044180304394103587\n",
      "epoch: 26 step: 515, loss is 0.003745949361473322\n",
      "epoch: 26 step: 516, loss is 0.00020153794321231544\n",
      "epoch: 26 step: 517, loss is 0.018916308879852295\n",
      "epoch: 26 step: 518, loss is 0.023774735629558563\n",
      "epoch: 26 step: 519, loss is 0.01583722233772278\n",
      "epoch: 26 step: 520, loss is 0.009847941808402538\n",
      "epoch: 26 step: 521, loss is 0.00041857638279907405\n",
      "epoch: 26 step: 522, loss is 0.0021833342034369707\n",
      "epoch: 26 step: 523, loss is 0.0004683814768213779\n",
      "epoch: 26 step: 524, loss is 0.009673971682786942\n",
      "epoch: 26 step: 525, loss is 0.0017371855210512877\n",
      "epoch: 26 step: 526, loss is 0.0004935787292197347\n",
      "epoch: 26 step: 527, loss is 0.021449433639645576\n",
      "epoch: 26 step: 528, loss is 0.004820224829018116\n",
      "epoch: 26 step: 529, loss is 0.0037174089811742306\n",
      "epoch: 26 step: 530, loss is 7.810247916495427e-05\n",
      "epoch: 26 step: 531, loss is 0.0021163837518543005\n",
      "epoch: 26 step: 532, loss is 0.00021124884369783103\n",
      "epoch: 26 step: 533, loss is 0.02177433669567108\n",
      "epoch: 26 step: 534, loss is 0.0012353904312476516\n",
      "epoch: 26 step: 535, loss is 0.01070893369615078\n",
      "epoch: 26 step: 536, loss is 0.00020106727606616914\n",
      "epoch: 26 step: 537, loss is 0.03142797574400902\n",
      "epoch: 26 step: 538, loss is 0.005111732520163059\n",
      "epoch: 26 step: 539, loss is 0.0001907016703626141\n",
      "epoch: 26 step: 540, loss is 0.0007017620373517275\n",
      "epoch: 26 step: 541, loss is 0.0013325029285624623\n",
      "epoch: 26 step: 542, loss is 0.00249302526935935\n",
      "epoch: 26 step: 543, loss is 0.0001314365945290774\n",
      "epoch: 26 step: 544, loss is 0.0037665306590497494\n",
      "epoch: 26 step: 545, loss is 0.004889913834631443\n",
      "epoch: 26 step: 546, loss is 0.0006721959798596799\n",
      "epoch: 26 step: 547, loss is 0.028634225949645042\n",
      "epoch: 26 step: 548, loss is 0.011733694933354855\n",
      "epoch: 26 step: 549, loss is 0.0022906921803951263\n",
      "epoch: 26 step: 550, loss is 0.0004516409244388342\n",
      "epoch: 26 step: 551, loss is 0.00011982965224888176\n",
      "epoch: 26 step: 552, loss is 0.004050761461257935\n",
      "epoch: 26 step: 553, loss is 0.042844705283641815\n",
      "epoch: 26 step: 554, loss is 0.0032454850152134895\n",
      "epoch: 26 step: 555, loss is 0.0017601101426407695\n",
      "epoch: 26 step: 556, loss is 0.0006285149720497429\n",
      "epoch: 26 step: 557, loss is 0.0001795572752598673\n",
      "epoch: 26 step: 558, loss is 0.00048315973253920674\n",
      "epoch: 26 step: 559, loss is 0.00048061792040243745\n",
      "epoch: 26 step: 560, loss is 0.001189226983115077\n",
      "epoch: 26 step: 561, loss is 0.018912341445684433\n",
      "epoch: 26 step: 562, loss is 0.003473638789728284\n",
      "epoch: 26 step: 563, loss is 0.0051407464779913425\n",
      "epoch: 26 step: 564, loss is 0.0011574218515306711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 565, loss is 0.008727572858333588\n",
      "epoch: 26 step: 566, loss is 0.10162904113531113\n",
      "epoch: 26 step: 567, loss is 0.0050330194644629955\n",
      "epoch: 26 step: 568, loss is 0.0006169067928567529\n",
      "epoch: 26 step: 569, loss is 0.0019246921874582767\n",
      "epoch: 26 step: 570, loss is 0.011095769703388214\n",
      "epoch: 26 step: 571, loss is 0.00035348464734852314\n",
      "epoch: 26 step: 572, loss is 7.186271977843717e-05\n",
      "epoch: 26 step: 573, loss is 0.00582093233242631\n",
      "epoch: 26 step: 574, loss is 0.0031351689249277115\n",
      "epoch: 26 step: 575, loss is 0.0625600665807724\n",
      "epoch: 26 step: 576, loss is 0.004038234241306782\n",
      "epoch: 26 step: 577, loss is 0.0006523891934193671\n",
      "epoch: 26 step: 578, loss is 0.013891476206481457\n",
      "epoch: 26 step: 579, loss is 0.026298830285668373\n",
      "epoch: 26 step: 580, loss is 4.6101296902634203e-05\n",
      "epoch: 26 step: 581, loss is 0.003451244905591011\n",
      "epoch: 26 step: 582, loss is 0.006492016837000847\n",
      "epoch: 26 step: 583, loss is 0.011952877044677734\n",
      "epoch: 26 step: 584, loss is 0.0006531031103804708\n",
      "epoch: 26 step: 585, loss is 0.01714368537068367\n",
      "epoch: 26 step: 586, loss is 0.022356703877449036\n",
      "epoch: 26 step: 587, loss is 0.01394455973058939\n",
      "epoch: 26 step: 588, loss is 0.000394955714000389\n",
      "epoch: 26 step: 589, loss is 0.016690663993358612\n",
      "epoch: 26 step: 590, loss is 0.011661676689982414\n",
      "epoch: 26 step: 591, loss is 0.0015551978722214699\n",
      "epoch: 26 step: 592, loss is 0.0038814444560557604\n",
      "epoch: 26 step: 593, loss is 0.00010852449486264959\n",
      "epoch: 26 step: 594, loss is 0.045674730092287064\n",
      "epoch: 26 step: 595, loss is 0.050860170274972916\n",
      "epoch: 26 step: 596, loss is 0.0032273235265165567\n",
      "epoch: 26 step: 597, loss is 0.0027295968029648066\n",
      "epoch: 26 step: 598, loss is 0.0008220217423513532\n",
      "epoch: 26 step: 599, loss is 4.197216185275465e-05\n",
      "epoch: 26 step: 600, loss is 0.014040800742805004\n",
      "epoch: 26 step: 601, loss is 0.012883699499070644\n",
      "epoch: 26 step: 602, loss is 0.000218298751860857\n",
      "epoch: 26 step: 603, loss is 4.89761951030232e-05\n",
      "epoch: 26 step: 604, loss is 0.0011708259116858244\n",
      "epoch: 26 step: 605, loss is 8.897812222130597e-05\n",
      "epoch: 26 step: 606, loss is 6.192467117216438e-05\n",
      "epoch: 26 step: 607, loss is 0.027937578037381172\n",
      "epoch: 26 step: 608, loss is 0.0013269264018163085\n",
      "epoch: 26 step: 609, loss is 0.006289523094892502\n",
      "epoch: 26 step: 610, loss is 0.002244133735075593\n",
      "epoch: 26 step: 611, loss is 0.00028204245609231293\n",
      "epoch: 26 step: 612, loss is 0.0013515312457457185\n",
      "epoch: 26 step: 613, loss is 0.0006496204878203571\n",
      "epoch: 26 step: 614, loss is 0.00035891844891011715\n",
      "epoch: 26 step: 615, loss is 0.005822089966386557\n",
      "epoch: 26 step: 616, loss is 0.000148757389979437\n",
      "epoch: 26 step: 617, loss is 0.0017869067378342152\n",
      "epoch: 26 step: 618, loss is 0.0024874769151210785\n",
      "epoch: 26 step: 619, loss is 0.00012231161235831678\n",
      "epoch: 26 step: 620, loss is 0.01929626241326332\n",
      "epoch: 26 step: 621, loss is 0.017561137676239014\n",
      "epoch: 26 step: 622, loss is 0.01976926065981388\n",
      "epoch: 26 step: 623, loss is 0.013262920081615448\n",
      "epoch: 26 step: 624, loss is 0.00039016283699311316\n",
      "epoch: 26 step: 625, loss is 0.013980301097035408\n",
      "epoch: 26 step: 626, loss is 0.0001985457492992282\n",
      "epoch: 26 step: 627, loss is 0.0002576285623945296\n",
      "epoch: 26 step: 628, loss is 0.005024249199777842\n",
      "epoch: 26 step: 629, loss is 2.028378912655171e-05\n",
      "epoch: 26 step: 630, loss is 2.814324943756219e-05\n",
      "epoch: 26 step: 631, loss is 0.02223196066915989\n",
      "epoch: 26 step: 632, loss is 0.019126491621136665\n",
      "epoch: 26 step: 633, loss is 0.004412772133946419\n",
      "epoch: 26 step: 634, loss is 0.007266644388437271\n",
      "epoch: 26 step: 635, loss is 0.014583316631615162\n",
      "epoch: 26 step: 636, loss is 0.0003008731291629374\n",
      "epoch: 26 step: 637, loss is 7.935093162814155e-05\n",
      "epoch: 26 step: 638, loss is 0.0003630576829891652\n",
      "epoch: 26 step: 639, loss is 6.604777445318177e-05\n",
      "epoch: 26 step: 640, loss is 0.00302070751786232\n",
      "epoch: 26 step: 641, loss is 0.003364796284586191\n",
      "epoch: 26 step: 642, loss is 5.038297604187392e-05\n",
      "epoch: 26 step: 643, loss is 0.061682600528001785\n",
      "epoch: 26 step: 644, loss is 0.0025786729529500008\n",
      "epoch: 26 step: 645, loss is 0.0019183994736522436\n",
      "epoch: 26 step: 646, loss is 0.0002694292925298214\n",
      "epoch: 26 step: 647, loss is 0.016862571239471436\n",
      "epoch: 26 step: 648, loss is 0.005579590331763029\n",
      "epoch: 26 step: 649, loss is 0.002715293550863862\n",
      "epoch: 26 step: 650, loss is 1.600729774509091e-05\n",
      "epoch: 26 step: 651, loss is 0.00998543668538332\n",
      "epoch: 26 step: 652, loss is 0.0005745947128161788\n",
      "epoch: 26 step: 653, loss is 0.0001728316565277055\n",
      "epoch: 26 step: 654, loss is 0.0022740622516721487\n",
      "epoch: 26 step: 655, loss is 0.0004900281201116741\n",
      "epoch: 26 step: 656, loss is 0.00880783423781395\n",
      "epoch: 26 step: 657, loss is 0.0018108332296833396\n",
      "epoch: 26 step: 658, loss is 0.0032583274878561497\n",
      "epoch: 26 step: 659, loss is 0.00028299837140366435\n",
      "epoch: 26 step: 660, loss is 0.0013754250248894095\n",
      "epoch: 26 step: 661, loss is 0.012098723091185093\n",
      "epoch: 26 step: 662, loss is 0.004153411369770765\n",
      "epoch: 26 step: 663, loss is 0.000484403979498893\n",
      "epoch: 26 step: 664, loss is 0.009651062078773975\n",
      "epoch: 26 step: 665, loss is 0.00014408805873245\n",
      "epoch: 26 step: 666, loss is 0.005844452418386936\n",
      "epoch: 26 step: 667, loss is 0.0006232297164388001\n",
      "epoch: 26 step: 668, loss is 0.00017908001609612256\n",
      "epoch: 26 step: 669, loss is 0.00013795755512546748\n",
      "epoch: 26 step: 670, loss is 8.997839904623106e-05\n",
      "epoch: 26 step: 671, loss is 0.00209945160895586\n",
      "epoch: 26 step: 672, loss is 0.002182748867198825\n",
      "epoch: 26 step: 673, loss is 0.009424571879208088\n",
      "epoch: 26 step: 674, loss is 0.0006699218647554517\n",
      "epoch: 26 step: 675, loss is 0.007671228609979153\n",
      "epoch: 26 step: 676, loss is 0.00025489667314104736\n",
      "epoch: 26 step: 677, loss is 0.060977693647146225\n",
      "epoch: 26 step: 678, loss is 0.0026022614911198616\n",
      "epoch: 26 step: 679, loss is 0.00013957814371678978\n",
      "epoch: 26 step: 680, loss is 0.007268261164426804\n",
      "epoch: 26 step: 681, loss is 0.0007102533709257841\n",
      "epoch: 26 step: 682, loss is 0.01006320770829916\n",
      "epoch: 26 step: 683, loss is 0.00721999816596508\n",
      "epoch: 26 step: 684, loss is 0.002011797856539488\n",
      "epoch: 26 step: 685, loss is 0.0005275856237858534\n",
      "epoch: 26 step: 686, loss is 0.00020716006110887975\n",
      "epoch: 26 step: 687, loss is 0.00029301512404344976\n",
      "epoch: 26 step: 688, loss is 0.07758155465126038\n",
      "epoch: 26 step: 689, loss is 0.00033364989212714136\n",
      "epoch: 26 step: 690, loss is 0.0032481527887284756\n",
      "epoch: 26 step: 691, loss is 0.07426627725362778\n",
      "epoch: 26 step: 692, loss is 0.0006434620008803904\n",
      "epoch: 26 step: 693, loss is 0.0001411881239619106\n",
      "epoch: 26 step: 694, loss is 0.0002436502109048888\n",
      "epoch: 26 step: 695, loss is 0.0009390363120473921\n",
      "epoch: 26 step: 696, loss is 0.00162152957636863\n",
      "epoch: 26 step: 697, loss is 0.0028930087573826313\n",
      "epoch: 26 step: 698, loss is 0.054766349494457245\n",
      "epoch: 26 step: 699, loss is 0.013523350469768047\n",
      "epoch: 26 step: 700, loss is 0.0011674329871311784\n",
      "epoch: 26 step: 701, loss is 0.05574682727456093\n",
      "epoch: 26 step: 702, loss is 0.002415870316326618\n",
      "epoch: 26 step: 703, loss is 0.00021506186749320477\n",
      "epoch: 26 step: 704, loss is 0.008014150895178318\n",
      "epoch: 26 step: 705, loss is 0.00032134121283888817\n",
      "epoch: 26 step: 706, loss is 0.001276589697226882\n",
      "epoch: 26 step: 707, loss is 0.0007040645577944815\n",
      "epoch: 26 step: 708, loss is 0.00013660319382324815\n",
      "epoch: 26 step: 709, loss is 1.3573460819316097e-05\n",
      "epoch: 26 step: 710, loss is 0.003336401656270027\n",
      "epoch: 26 step: 711, loss is 0.01577586680650711\n",
      "epoch: 26 step: 712, loss is 0.06686363369226456\n",
      "epoch: 26 step: 713, loss is 0.006381060462445021\n",
      "epoch: 26 step: 714, loss is 0.0016986735863611102\n",
      "epoch: 26 step: 715, loss is 0.004815045744180679\n",
      "epoch: 26 step: 716, loss is 0.004953327123075724\n",
      "epoch: 26 step: 717, loss is 0.005086872261017561\n",
      "epoch: 26 step: 718, loss is 0.000359002617187798\n",
      "epoch: 26 step: 719, loss is 0.036352820694446564\n",
      "epoch: 26 step: 720, loss is 0.013657830655574799\n",
      "epoch: 26 step: 721, loss is 0.0011056209914386272\n",
      "epoch: 26 step: 722, loss is 0.0018659137422218919\n",
      "epoch: 26 step: 723, loss is 0.008779632858932018\n",
      "epoch: 26 step: 724, loss is 0.0003663457464426756\n",
      "epoch: 26 step: 725, loss is 0.0008763225632719696\n",
      "epoch: 26 step: 726, loss is 0.004814131185412407\n",
      "epoch: 26 step: 727, loss is 0.0012826725142076612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 728, loss is 0.005330553278326988\n",
      "epoch: 26 step: 729, loss is 0.007783523295074701\n",
      "epoch: 26 step: 730, loss is 0.0005628681392408907\n",
      "epoch: 26 step: 731, loss is 0.009646308608353138\n",
      "epoch: 26 step: 732, loss is 0.005635963752865791\n",
      "epoch: 26 step: 733, loss is 0.0020704104099422693\n",
      "epoch: 26 step: 734, loss is 0.08869170397520065\n",
      "epoch: 26 step: 735, loss is 0.0447273924946785\n",
      "epoch: 26 step: 736, loss is 0.00017369320266880095\n",
      "epoch: 26 step: 737, loss is 0.08758369088172913\n",
      "epoch: 26 step: 738, loss is 0.00015608844114467502\n",
      "epoch: 26 step: 739, loss is 0.001664143754169345\n",
      "epoch: 26 step: 740, loss is 0.0012145143700763583\n",
      "epoch: 26 step: 741, loss is 0.008140438236296177\n",
      "epoch: 26 step: 742, loss is 0.00833137333393097\n",
      "epoch: 26 step: 743, loss is 0.0010482920333743095\n",
      "epoch: 26 step: 744, loss is 0.011438493616878986\n",
      "epoch: 26 step: 745, loss is 0.002675877884030342\n",
      "epoch: 26 step: 746, loss is 0.010583953000605106\n",
      "epoch: 26 step: 747, loss is 0.0017506489530205727\n",
      "epoch: 26 step: 748, loss is 0.006763616111129522\n",
      "epoch: 26 step: 749, loss is 0.00034411944216117263\n",
      "epoch: 26 step: 750, loss is 0.03337360545992851\n",
      "epoch: 26 step: 751, loss is 0.007680961396545172\n",
      "epoch: 26 step: 752, loss is 0.024655628949403763\n",
      "epoch: 26 step: 753, loss is 0.004518670495599508\n",
      "epoch: 26 step: 754, loss is 0.0018540738383308053\n",
      "epoch: 26 step: 755, loss is 0.013904435560107231\n",
      "epoch: 26 step: 756, loss is 0.06100554019212723\n",
      "epoch: 26 step: 757, loss is 0.0012443484738469124\n",
      "epoch: 26 step: 758, loss is 0.043455906212329865\n",
      "epoch: 26 step: 759, loss is 0.00027368025621399283\n",
      "epoch: 26 step: 760, loss is 0.0049085188657045364\n",
      "epoch: 26 step: 761, loss is 0.011144785210490227\n",
      "epoch: 26 step: 762, loss is 0.0024659731425344944\n",
      "epoch: 26 step: 763, loss is 0.005527691449970007\n",
      "epoch: 26 step: 764, loss is 0.0002092168142553419\n",
      "epoch: 26 step: 765, loss is 0.0047315750271081924\n",
      "epoch: 26 step: 766, loss is 0.00119839480612427\n",
      "epoch: 26 step: 767, loss is 0.0003010711807291955\n",
      "epoch: 26 step: 768, loss is 0.013749031350016594\n",
      "epoch: 26 step: 769, loss is 0.02033683843910694\n",
      "epoch: 26 step: 770, loss is 0.0016881530173122883\n",
      "epoch: 26 step: 771, loss is 0.0034175156615674496\n",
      "epoch: 26 step: 772, loss is 0.0002860217937268317\n",
      "epoch: 26 step: 773, loss is 0.0015286665875464678\n",
      "epoch: 26 step: 774, loss is 0.0002899449609685689\n",
      "epoch: 26 step: 775, loss is 0.0018498924328014255\n",
      "epoch: 26 step: 776, loss is 0.0008661752217449248\n",
      "epoch: 26 step: 777, loss is 0.00027184549253433943\n",
      "epoch: 26 step: 778, loss is 0.001517818309366703\n",
      "epoch: 26 step: 779, loss is 0.0005027849110774696\n",
      "epoch: 26 step: 780, loss is 0.00030577037250623107\n",
      "epoch: 26 step: 781, loss is 0.0001475712633691728\n",
      "epoch: 26 step: 782, loss is 0.0005148565396666527\n",
      "epoch: 26 step: 783, loss is 0.0027382965199649334\n",
      "epoch: 26 step: 784, loss is 0.03161075711250305\n",
      "epoch: 26 step: 785, loss is 0.0015401961281895638\n",
      "epoch: 26 step: 786, loss is 0.0007634059875272214\n",
      "epoch: 26 step: 787, loss is 0.0022763924207538366\n",
      "epoch: 26 step: 788, loss is 0.012672492302954197\n",
      "epoch: 26 step: 789, loss is 0.0003857721167150885\n",
      "epoch: 26 step: 790, loss is 0.0010338797001168132\n",
      "epoch: 26 step: 791, loss is 0.0003274942864663899\n",
      "epoch: 26 step: 792, loss is 0.0006556488224305212\n",
      "epoch: 26 step: 793, loss is 0.005263527389615774\n",
      "epoch: 26 step: 794, loss is 0.0019496792228892446\n",
      "epoch: 26 step: 795, loss is 0.01562945358455181\n",
      "epoch: 26 step: 796, loss is 0.002473043277859688\n",
      "epoch: 26 step: 797, loss is 0.009480642154812813\n",
      "epoch: 26 step: 798, loss is 0.008233887143433094\n",
      "epoch: 26 step: 799, loss is 0.007527685258537531\n",
      "epoch: 26 step: 800, loss is 0.00011939895921386778\n",
      "epoch: 26 step: 801, loss is 0.02080686204135418\n",
      "epoch: 26 step: 802, loss is 0.0007443035719916224\n",
      "epoch: 26 step: 803, loss is 0.00204530730843544\n",
      "epoch: 26 step: 804, loss is 0.004913244396448135\n",
      "epoch: 26 step: 805, loss is 0.008467834442853928\n",
      "epoch: 26 step: 806, loss is 0.009399756789207458\n",
      "epoch: 26 step: 807, loss is 0.00017735370784066617\n",
      "epoch: 26 step: 808, loss is 0.0007782932370901108\n",
      "epoch: 26 step: 809, loss is 0.004025784321129322\n",
      "epoch: 26 step: 810, loss is 0.0014116832753643394\n",
      "epoch: 26 step: 811, loss is 0.047454364597797394\n",
      "epoch: 26 step: 812, loss is 0.002367922570556402\n",
      "epoch: 26 step: 813, loss is 0.0008306600502692163\n",
      "epoch: 26 step: 814, loss is 0.016277575865387917\n",
      "epoch: 26 step: 815, loss is 0.0031442369800060987\n",
      "epoch: 26 step: 816, loss is 0.0006507138023152947\n",
      "epoch: 26 step: 817, loss is 0.00047705392353236675\n",
      "epoch: 26 step: 818, loss is 0.0003646325203590095\n",
      "epoch: 26 step: 819, loss is 0.0007774829282425344\n",
      "epoch: 26 step: 820, loss is 0.009785710833966732\n",
      "epoch: 26 step: 821, loss is 2.8551910872920416e-05\n",
      "epoch: 26 step: 822, loss is 0.0009253810276277363\n",
      "epoch: 26 step: 823, loss is 0.005890801548957825\n",
      "epoch: 26 step: 824, loss is 9.807344758883119e-05\n",
      "epoch: 26 step: 825, loss is 0.001638782094232738\n",
      "epoch: 26 step: 826, loss is 0.0018100776942446828\n",
      "epoch: 26 step: 827, loss is 0.003347074845805764\n",
      "epoch: 26 step: 828, loss is 0.006553927436470985\n",
      "epoch: 26 step: 829, loss is 0.06385330110788345\n",
      "epoch: 26 step: 830, loss is 0.002629093127325177\n",
      "epoch: 26 step: 831, loss is 0.00014900248788762838\n",
      "epoch: 26 step: 832, loss is 0.00029962544795125723\n",
      "epoch: 26 step: 833, loss is 0.00019237272499594837\n",
      "epoch: 26 step: 834, loss is 0.0011822448577731848\n",
      "epoch: 26 step: 835, loss is 2.5140778234344907e-05\n",
      "epoch: 26 step: 836, loss is 0.0010353652760386467\n",
      "epoch: 26 step: 837, loss is 0.00014944816939532757\n",
      "epoch: 26 step: 838, loss is 0.030384641140699387\n",
      "epoch: 26 step: 839, loss is 0.0004810811660718173\n",
      "epoch: 26 step: 840, loss is 0.003155752085149288\n",
      "epoch: 26 step: 841, loss is 0.00870902743190527\n",
      "epoch: 26 step: 842, loss is 9.632521687308326e-05\n",
      "epoch: 26 step: 843, loss is 0.00042794476030394435\n",
      "epoch: 26 step: 844, loss is 0.002176088048145175\n",
      "epoch: 26 step: 845, loss is 0.00977433193475008\n",
      "epoch: 26 step: 846, loss is 0.00023189101193565875\n",
      "epoch: 26 step: 847, loss is 0.00048330012941733\n",
      "epoch: 26 step: 848, loss is 0.015470386482775211\n",
      "epoch: 26 step: 849, loss is 0.0005651473184116185\n",
      "epoch: 26 step: 850, loss is 0.00132468668743968\n",
      "epoch: 26 step: 851, loss is 0.04127462953329086\n",
      "epoch: 26 step: 852, loss is 0.0001281414006371051\n",
      "epoch: 26 step: 853, loss is 0.0019455031724646688\n",
      "epoch: 26 step: 854, loss is 0.0010628653690218925\n",
      "epoch: 26 step: 855, loss is 4.50613115390297e-05\n",
      "epoch: 26 step: 856, loss is 0.007440018001943827\n",
      "epoch: 26 step: 857, loss is 0.0002200194721808657\n",
      "epoch: 26 step: 858, loss is 0.002869064686819911\n",
      "epoch: 26 step: 859, loss is 0.0009790363255888224\n",
      "epoch: 26 step: 860, loss is 0.0178243238478899\n",
      "epoch: 26 step: 861, loss is 0.010189186781644821\n",
      "epoch: 26 step: 862, loss is 0.011246628127992153\n",
      "epoch: 26 step: 863, loss is 0.00035048372228629887\n",
      "epoch: 26 step: 864, loss is 0.0009305557468906045\n",
      "epoch: 26 step: 865, loss is 0.004565752111375332\n",
      "epoch: 26 step: 866, loss is 0.013571714051067829\n",
      "epoch: 26 step: 867, loss is 6.813492655055597e-05\n",
      "epoch: 26 step: 868, loss is 0.00012795859947800636\n",
      "epoch: 26 step: 869, loss is 2.0996196326450445e-05\n",
      "epoch: 26 step: 870, loss is 0.019784193485975266\n",
      "epoch: 26 step: 871, loss is 0.000505781383253634\n",
      "epoch: 26 step: 872, loss is 0.006294064689427614\n",
      "epoch: 26 step: 873, loss is 0.0005477817612700164\n",
      "epoch: 26 step: 874, loss is 0.013286907225847244\n",
      "epoch: 26 step: 875, loss is 2.5969347916543484e-05\n",
      "epoch: 26 step: 876, loss is 0.0004337129066698253\n",
      "epoch: 26 step: 877, loss is 0.08517836034297943\n",
      "epoch: 26 step: 878, loss is 0.013619361445307732\n",
      "epoch: 26 step: 879, loss is 0.029859447851777077\n",
      "epoch: 26 step: 880, loss is 0.00023133230570238084\n",
      "epoch: 26 step: 881, loss is 6.405031308531761e-05\n",
      "epoch: 26 step: 882, loss is 0.0002207222714787349\n",
      "epoch: 26 step: 883, loss is 0.16550053656101227\n",
      "epoch: 26 step: 884, loss is 0.001096545485779643\n",
      "epoch: 26 step: 885, loss is 0.005132952239364386\n",
      "epoch: 26 step: 886, loss is 0.0003184349334333092\n",
      "epoch: 26 step: 887, loss is 0.0009259668295271695\n",
      "epoch: 26 step: 888, loss is 0.0007907784311100841\n",
      "epoch: 26 step: 889, loss is 0.01730017550289631\n",
      "epoch: 26 step: 890, loss is 0.008094231598079205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 891, loss is 0.00796594563871622\n",
      "epoch: 26 step: 892, loss is 0.004718117415904999\n",
      "epoch: 26 step: 893, loss is 0.07814329117536545\n",
      "epoch: 26 step: 894, loss is 0.03360908851027489\n",
      "epoch: 26 step: 895, loss is 0.0019541848450899124\n",
      "epoch: 26 step: 896, loss is 6.354927609208971e-05\n",
      "epoch: 26 step: 897, loss is 0.004483312368392944\n",
      "epoch: 26 step: 898, loss is 0.005432839505374432\n",
      "epoch: 26 step: 899, loss is 0.006664958316832781\n",
      "epoch: 26 step: 900, loss is 0.01407881174236536\n",
      "epoch: 26 step: 901, loss is 0.0032717508729547262\n",
      "epoch: 26 step: 902, loss is 0.0347699373960495\n",
      "epoch: 26 step: 903, loss is 0.002112559974193573\n",
      "epoch: 26 step: 904, loss is 0.0019265076844021678\n",
      "epoch: 26 step: 905, loss is 0.01680494286119938\n",
      "epoch: 26 step: 906, loss is 0.01672298088669777\n",
      "epoch: 26 step: 907, loss is 0.01009364239871502\n",
      "epoch: 26 step: 908, loss is 0.0021539481822401285\n",
      "epoch: 26 step: 909, loss is 0.004093655850738287\n",
      "epoch: 26 step: 910, loss is 0.0007005049264989793\n",
      "epoch: 26 step: 911, loss is 0.010073418729007244\n",
      "epoch: 26 step: 912, loss is 0.06813957542181015\n",
      "epoch: 26 step: 913, loss is 0.026876626536250114\n",
      "epoch: 26 step: 914, loss is 0.0011370384600013494\n",
      "epoch: 26 step: 915, loss is 0.0001770067901816219\n",
      "epoch: 26 step: 916, loss is 0.01396802719682455\n",
      "epoch: 26 step: 917, loss is 0.0024907621555030346\n",
      "epoch: 26 step: 918, loss is 0.09590204805135727\n",
      "epoch: 26 step: 919, loss is 0.0032482300885021687\n",
      "epoch: 26 step: 920, loss is 0.0013055232120677829\n",
      "epoch: 26 step: 921, loss is 0.002850478980690241\n",
      "epoch: 26 step: 922, loss is 0.0004936428740620613\n",
      "epoch: 26 step: 923, loss is 0.0032380418851971626\n",
      "epoch: 26 step: 924, loss is 0.00041862850775942206\n",
      "epoch: 26 step: 925, loss is 0.000160608600708656\n",
      "epoch: 26 step: 926, loss is 0.01448381133377552\n",
      "epoch: 26 step: 927, loss is 0.07198620587587357\n",
      "epoch: 26 step: 928, loss is 0.0006330459727905691\n",
      "epoch: 26 step: 929, loss is 0.020485853776335716\n",
      "epoch: 26 step: 930, loss is 0.005366602912545204\n",
      "epoch: 26 step: 931, loss is 0.007181374356150627\n",
      "epoch: 26 step: 932, loss is 0.002192386658862233\n",
      "epoch: 26 step: 933, loss is 0.006888707168400288\n",
      "epoch: 26 step: 934, loss is 0.0003649150603450835\n",
      "epoch: 26 step: 935, loss is 0.027115382254123688\n",
      "epoch: 26 step: 936, loss is 0.00316016748547554\n",
      "epoch: 26 step: 937, loss is 0.007475807331502438\n",
      "epoch: 27 step: 1, loss is 0.00098127918317914\n",
      "epoch: 27 step: 2, loss is 0.0004590529133565724\n",
      "epoch: 27 step: 3, loss is 0.008245132863521576\n",
      "epoch: 27 step: 4, loss is 0.00021716507035307586\n",
      "epoch: 27 step: 5, loss is 0.008246379904448986\n",
      "epoch: 27 step: 6, loss is 0.00046358347753994167\n",
      "epoch: 27 step: 7, loss is 0.000339568592607975\n",
      "epoch: 27 step: 8, loss is 0.0013903117505833507\n",
      "epoch: 27 step: 9, loss is 0.061733439564704895\n",
      "epoch: 27 step: 10, loss is 0.0038145058788359165\n",
      "epoch: 27 step: 11, loss is 0.00028231291798874736\n",
      "epoch: 27 step: 12, loss is 0.0008599223801866174\n",
      "epoch: 27 step: 13, loss is 0.0003361019480507821\n",
      "epoch: 27 step: 14, loss is 0.0029837917536497116\n",
      "epoch: 27 step: 15, loss is 0.0005082205170765519\n",
      "epoch: 27 step: 16, loss is 0.008995408192276955\n",
      "epoch: 27 step: 17, loss is 0.020807020366191864\n",
      "epoch: 27 step: 18, loss is 0.014579959213733673\n",
      "epoch: 27 step: 19, loss is 0.009472398087382317\n",
      "epoch: 27 step: 20, loss is 0.01321486197412014\n",
      "epoch: 27 step: 21, loss is 0.0011651241220533848\n",
      "epoch: 27 step: 22, loss is 0.014353561215102673\n",
      "epoch: 27 step: 23, loss is 0.022696007043123245\n",
      "epoch: 27 step: 24, loss is 0.0017347242683172226\n",
      "epoch: 27 step: 25, loss is 0.0004957113997079432\n",
      "epoch: 27 step: 26, loss is 0.001612756634131074\n",
      "epoch: 27 step: 27, loss is 0.0032400693744421005\n",
      "epoch: 27 step: 28, loss is 0.0029470003210008144\n",
      "epoch: 27 step: 29, loss is 0.0005470809992402792\n",
      "epoch: 27 step: 30, loss is 0.041284821927547455\n",
      "epoch: 27 step: 31, loss is 0.0008274888969026506\n",
      "epoch: 27 step: 32, loss is 7.908465340733528e-05\n",
      "epoch: 27 step: 33, loss is 0.04317706078290939\n",
      "epoch: 27 step: 34, loss is 0.00041750859236344695\n",
      "epoch: 27 step: 35, loss is 0.0013461234048008919\n",
      "epoch: 27 step: 36, loss is 0.004121787380427122\n",
      "epoch: 27 step: 37, loss is 0.0007890444830991328\n",
      "epoch: 27 step: 38, loss is 0.011954644694924355\n",
      "epoch: 27 step: 39, loss is 0.0014153290539979935\n",
      "epoch: 27 step: 40, loss is 8.315854938700795e-05\n",
      "epoch: 27 step: 41, loss is 0.034647922962903976\n",
      "epoch: 27 step: 42, loss is 0.002244239440187812\n",
      "epoch: 27 step: 43, loss is 0.004713117610663176\n",
      "epoch: 27 step: 44, loss is 0.022362641990184784\n",
      "epoch: 27 step: 45, loss is 0.0004561717214528471\n",
      "epoch: 27 step: 46, loss is 0.002033029915764928\n",
      "epoch: 27 step: 47, loss is 8.461796096526086e-05\n",
      "epoch: 27 step: 48, loss is 0.01615213043987751\n",
      "epoch: 27 step: 49, loss is 0.0001678483240539208\n",
      "epoch: 27 step: 50, loss is 0.00021639834449160844\n",
      "epoch: 27 step: 51, loss is 0.019919399172067642\n",
      "epoch: 27 step: 52, loss is 0.0025360286235809326\n",
      "epoch: 27 step: 53, loss is 0.002287359908223152\n",
      "epoch: 27 step: 54, loss is 0.10844727605581284\n",
      "epoch: 27 step: 55, loss is 0.005514892749488354\n",
      "epoch: 27 step: 56, loss is 0.0009430168429389596\n",
      "epoch: 27 step: 57, loss is 0.001061506220139563\n",
      "epoch: 27 step: 58, loss is 0.0057237944565713406\n",
      "epoch: 27 step: 59, loss is 2.809025681926869e-05\n",
      "epoch: 27 step: 60, loss is 0.00024071898951660842\n",
      "epoch: 27 step: 61, loss is 0.00025831410312093794\n",
      "epoch: 27 step: 62, loss is 0.0035831681452691555\n",
      "epoch: 27 step: 63, loss is 0.0007901574717834592\n",
      "epoch: 27 step: 64, loss is 0.002167705213651061\n",
      "epoch: 27 step: 65, loss is 0.0006907968781888485\n",
      "epoch: 27 step: 66, loss is 0.001070165541023016\n",
      "epoch: 27 step: 67, loss is 0.00021071289665997028\n",
      "epoch: 27 step: 68, loss is 0.005839857272803783\n",
      "epoch: 27 step: 69, loss is 0.0010405241046100855\n",
      "epoch: 27 step: 70, loss is 0.0004286370240151882\n",
      "epoch: 27 step: 71, loss is 0.0002916888624895364\n",
      "epoch: 27 step: 72, loss is 0.00012538977898657322\n",
      "epoch: 27 step: 73, loss is 0.0010026453528553247\n",
      "epoch: 27 step: 74, loss is 0.0002482800919096917\n",
      "epoch: 27 step: 75, loss is 0.029100731015205383\n",
      "epoch: 27 step: 76, loss is 0.005884409416466951\n",
      "epoch: 27 step: 77, loss is 0.001645746291615069\n",
      "epoch: 27 step: 78, loss is 0.0021242592483758926\n",
      "epoch: 27 step: 79, loss is 0.025807291269302368\n",
      "epoch: 27 step: 80, loss is 0.007786588743329048\n",
      "epoch: 27 step: 81, loss is 2.1089159417897463e-05\n",
      "epoch: 27 step: 82, loss is 0.0004914456512778997\n",
      "epoch: 27 step: 83, loss is 0.0060379295609891415\n",
      "epoch: 27 step: 84, loss is 0.012367041781544685\n",
      "epoch: 27 step: 85, loss is 0.0002123348822351545\n",
      "epoch: 27 step: 86, loss is 0.0005094465450383723\n",
      "epoch: 27 step: 87, loss is 0.000469379621790722\n",
      "epoch: 27 step: 88, loss is 0.0015752814942970872\n",
      "epoch: 27 step: 89, loss is 0.001942186732776463\n",
      "epoch: 27 step: 90, loss is 0.0026768187526613474\n",
      "epoch: 27 step: 91, loss is 0.00017822376685217023\n",
      "epoch: 27 step: 92, loss is 0.001322488416917622\n",
      "epoch: 27 step: 93, loss is 0.008815353736281395\n",
      "epoch: 27 step: 94, loss is 0.0007416550652123988\n",
      "epoch: 27 step: 95, loss is 5.177119601285085e-05\n",
      "epoch: 27 step: 96, loss is 0.0003070975071750581\n",
      "epoch: 27 step: 97, loss is 0.00028571992879733443\n",
      "epoch: 27 step: 98, loss is 0.012265021912753582\n",
      "epoch: 27 step: 99, loss is 0.0012583464849740267\n",
      "epoch: 27 step: 100, loss is 0.0035855306778103113\n",
      "epoch: 27 step: 101, loss is 0.0006319162785075605\n",
      "epoch: 27 step: 102, loss is 0.008313458412885666\n",
      "epoch: 27 step: 103, loss is 0.0048194993287324905\n",
      "epoch: 27 step: 104, loss is 0.0019436359871178865\n",
      "epoch: 27 step: 105, loss is 0.0001687433396000415\n",
      "epoch: 27 step: 106, loss is 0.020665965974330902\n",
      "epoch: 27 step: 107, loss is 0.00026807960239239037\n",
      "epoch: 27 step: 108, loss is 0.00010542431118665263\n",
      "epoch: 27 step: 109, loss is 2.500995651644189e-05\n",
      "epoch: 27 step: 110, loss is 0.0003298998635727912\n",
      "epoch: 27 step: 111, loss is 0.00018047318735625595\n",
      "epoch: 27 step: 112, loss is 0.0002759338531177491\n",
      "epoch: 27 step: 113, loss is 0.0005872626788914204\n",
      "epoch: 27 step: 114, loss is 4.879909465671517e-05\n",
      "epoch: 27 step: 115, loss is 0.004026527516543865\n",
      "epoch: 27 step: 116, loss is 0.000505059608258307\n",
      "epoch: 27 step: 117, loss is 0.005676536820828915\n",
      "epoch: 27 step: 118, loss is 6.0423546528909355e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 119, loss is 0.009331559762358665\n",
      "epoch: 27 step: 120, loss is 0.00014144218584988266\n",
      "epoch: 27 step: 121, loss is 0.001143922097980976\n",
      "epoch: 27 step: 122, loss is 0.00021150038810446858\n",
      "epoch: 27 step: 123, loss is 0.004597542807459831\n",
      "epoch: 27 step: 124, loss is 0.0016109324060380459\n",
      "epoch: 27 step: 125, loss is 0.0006770684849470854\n",
      "epoch: 27 step: 126, loss is 8.474038622807711e-05\n",
      "epoch: 27 step: 127, loss is 0.00012497328862082213\n",
      "epoch: 27 step: 128, loss is 0.0006189711857587099\n",
      "epoch: 27 step: 129, loss is 0.0007829996175132692\n",
      "epoch: 27 step: 130, loss is 2.5079745682887733e-05\n",
      "epoch: 27 step: 131, loss is 0.0008086799061857164\n",
      "epoch: 27 step: 132, loss is 0.0013440409675240517\n",
      "epoch: 27 step: 133, loss is 0.0009672527085058391\n",
      "epoch: 27 step: 134, loss is 0.0016143375542014837\n",
      "epoch: 27 step: 135, loss is 0.002065730281174183\n",
      "epoch: 27 step: 136, loss is 0.0006201338255777955\n",
      "epoch: 27 step: 137, loss is 0.007319100666791201\n",
      "epoch: 27 step: 138, loss is 0.0004999303491786122\n",
      "epoch: 27 step: 139, loss is 0.0011137896217405796\n",
      "epoch: 27 step: 140, loss is 0.003400900401175022\n",
      "epoch: 27 step: 141, loss is 0.01544906571507454\n",
      "epoch: 27 step: 142, loss is 0.0004239717382006347\n",
      "epoch: 27 step: 143, loss is 0.0023426201660186052\n",
      "epoch: 27 step: 144, loss is 0.002048333641141653\n",
      "epoch: 27 step: 145, loss is 0.0014591272920370102\n",
      "epoch: 27 step: 146, loss is 0.004734513349831104\n",
      "epoch: 27 step: 147, loss is 0.01225351169705391\n",
      "epoch: 27 step: 148, loss is 0.0016617353539913893\n",
      "epoch: 27 step: 149, loss is 0.0008129766210913658\n",
      "epoch: 27 step: 150, loss is 0.001210541813634336\n",
      "epoch: 27 step: 151, loss is 0.000517211970873177\n",
      "epoch: 27 step: 152, loss is 0.00023182826407719404\n",
      "epoch: 27 step: 153, loss is 0.004506112542003393\n",
      "epoch: 27 step: 154, loss is 0.00016903328651096672\n",
      "epoch: 27 step: 155, loss is 0.000834882608614862\n",
      "epoch: 27 step: 156, loss is 1.858576615632046e-05\n",
      "epoch: 27 step: 157, loss is 0.000665913918055594\n",
      "epoch: 27 step: 158, loss is 8.451200119452551e-05\n",
      "epoch: 27 step: 159, loss is 0.00857704970985651\n",
      "epoch: 27 step: 160, loss is 0.0009861703729256988\n",
      "epoch: 27 step: 161, loss is 6.391999340848997e-05\n",
      "epoch: 27 step: 162, loss is 0.0004345907655078918\n",
      "epoch: 27 step: 163, loss is 0.11821409314870834\n",
      "epoch: 27 step: 164, loss is 0.0002109854540321976\n",
      "epoch: 27 step: 165, loss is 0.0010130160953849554\n",
      "epoch: 27 step: 166, loss is 0.0005306866369210184\n",
      "epoch: 27 step: 167, loss is 0.00024668913101777434\n",
      "epoch: 27 step: 168, loss is 4.2207626393064857e-05\n",
      "epoch: 27 step: 169, loss is 0.0014612916857004166\n",
      "epoch: 27 step: 170, loss is 0.02872004546225071\n",
      "epoch: 27 step: 171, loss is 0.012224450707435608\n",
      "epoch: 27 step: 172, loss is 7.791112693666946e-06\n",
      "epoch: 27 step: 173, loss is 0.0017441101372241974\n",
      "epoch: 27 step: 174, loss is 5.873335976502858e-05\n",
      "epoch: 27 step: 175, loss is 0.034828443080186844\n",
      "epoch: 27 step: 176, loss is 0.00020833872258663177\n",
      "epoch: 27 step: 177, loss is 0.017217809334397316\n",
      "epoch: 27 step: 178, loss is 0.006861310452222824\n",
      "epoch: 27 step: 179, loss is 0.0003627778496593237\n",
      "epoch: 27 step: 180, loss is 7.570327397843357e-06\n",
      "epoch: 27 step: 181, loss is 0.0021843360736966133\n",
      "epoch: 27 step: 182, loss is 0.0006801225827075541\n",
      "epoch: 27 step: 183, loss is 0.009315980598330498\n",
      "epoch: 27 step: 184, loss is 0.0017682977486401796\n",
      "epoch: 27 step: 185, loss is 0.005510769784450531\n",
      "epoch: 27 step: 186, loss is 0.004931786097586155\n",
      "epoch: 27 step: 187, loss is 0.00308744958601892\n",
      "epoch: 27 step: 188, loss is 2.226035576313734e-05\n",
      "epoch: 27 step: 189, loss is 0.009328294545412064\n",
      "epoch: 27 step: 190, loss is 0.0012238299241289496\n",
      "epoch: 27 step: 191, loss is 0.0005089602782391012\n",
      "epoch: 27 step: 192, loss is 0.007615320384502411\n",
      "epoch: 27 step: 193, loss is 3.9405746065312997e-05\n",
      "epoch: 27 step: 194, loss is 2.9404354791040532e-05\n",
      "epoch: 27 step: 195, loss is 0.0006658621714450419\n",
      "epoch: 27 step: 196, loss is 0.0008378439815714955\n",
      "epoch: 27 step: 197, loss is 0.004795025568455458\n",
      "epoch: 27 step: 198, loss is 0.0017624564934521914\n",
      "epoch: 27 step: 199, loss is 0.0018192376010119915\n",
      "epoch: 27 step: 200, loss is 0.0011457548243924975\n",
      "epoch: 27 step: 201, loss is 0.003855827497318387\n",
      "epoch: 27 step: 202, loss is 0.001034479239024222\n",
      "epoch: 27 step: 203, loss is 0.00032137177186086774\n",
      "epoch: 27 step: 204, loss is 0.0007285326137207448\n",
      "epoch: 27 step: 205, loss is 0.007541419938206673\n",
      "epoch: 27 step: 206, loss is 0.002200554823502898\n",
      "epoch: 27 step: 207, loss is 2.1580210159299895e-05\n",
      "epoch: 27 step: 208, loss is 0.0001825629879022017\n",
      "epoch: 27 step: 209, loss is 0.004395935218781233\n",
      "epoch: 27 step: 210, loss is 0.0001915428729262203\n",
      "epoch: 27 step: 211, loss is 0.0036858844105154276\n",
      "epoch: 27 step: 212, loss is 0.00017533196660224348\n",
      "epoch: 27 step: 213, loss is 0.00030308053828775883\n",
      "epoch: 27 step: 214, loss is 0.005893897730857134\n",
      "epoch: 27 step: 215, loss is 0.0021601058542728424\n",
      "epoch: 27 step: 216, loss is 0.008549896068871021\n",
      "epoch: 27 step: 217, loss is 0.0008662747568450868\n",
      "epoch: 27 step: 218, loss is 0.059749115258455276\n",
      "epoch: 27 step: 219, loss is 6.393418971128995e-06\n",
      "epoch: 27 step: 220, loss is 0.00048044725554063916\n",
      "epoch: 27 step: 221, loss is 0.0017346169333904982\n",
      "epoch: 27 step: 222, loss is 0.0044714659452438354\n",
      "epoch: 27 step: 223, loss is 0.015474632382392883\n",
      "epoch: 27 step: 224, loss is 0.00012392191274557263\n",
      "epoch: 27 step: 225, loss is 0.010562904179096222\n",
      "epoch: 27 step: 226, loss is 0.00016385300841648132\n",
      "epoch: 27 step: 227, loss is 0.0004074954485986382\n",
      "epoch: 27 step: 228, loss is 0.011311177164316177\n",
      "epoch: 27 step: 229, loss is 0.0028048711828887463\n",
      "epoch: 27 step: 230, loss is 0.0005048265447840095\n",
      "epoch: 27 step: 231, loss is 0.00016649455938022584\n",
      "epoch: 27 step: 232, loss is 0.0002789809077512473\n",
      "epoch: 27 step: 233, loss is 0.0018295838963240385\n",
      "epoch: 27 step: 234, loss is 0.0030596263241022825\n",
      "epoch: 27 step: 235, loss is 0.07038366049528122\n",
      "epoch: 27 step: 236, loss is 0.0016024959040805697\n",
      "epoch: 27 step: 237, loss is 0.0005520051927305758\n",
      "epoch: 27 step: 238, loss is 0.00027058611158281565\n",
      "epoch: 27 step: 239, loss is 0.02506350353360176\n",
      "epoch: 27 step: 240, loss is 0.013575962744653225\n",
      "epoch: 27 step: 241, loss is 0.0001192111085401848\n",
      "epoch: 27 step: 242, loss is 0.06170045956969261\n",
      "epoch: 27 step: 243, loss is 0.0005200852174311876\n",
      "epoch: 27 step: 244, loss is 0.0003043904434889555\n",
      "epoch: 27 step: 245, loss is 0.003957293462008238\n",
      "epoch: 27 step: 246, loss is 0.003369815181940794\n",
      "epoch: 27 step: 247, loss is 0.0015244417591020465\n",
      "epoch: 27 step: 248, loss is 0.0001240074197994545\n",
      "epoch: 27 step: 249, loss is 0.00048329454148188233\n",
      "epoch: 27 step: 250, loss is 0.006239608861505985\n",
      "epoch: 27 step: 251, loss is 0.09931884706020355\n",
      "epoch: 27 step: 252, loss is 0.08067400008440018\n",
      "epoch: 27 step: 253, loss is 0.008721538819372654\n",
      "epoch: 27 step: 254, loss is 0.0030007530003786087\n",
      "epoch: 27 step: 255, loss is 6.861342262709513e-05\n",
      "epoch: 27 step: 256, loss is 0.036882489919662476\n",
      "epoch: 27 step: 257, loss is 0.003828437300398946\n",
      "epoch: 27 step: 258, loss is 0.0025426005013287067\n",
      "epoch: 27 step: 259, loss is 0.004862215835601091\n",
      "epoch: 27 step: 260, loss is 0.033242907375097275\n",
      "epoch: 27 step: 261, loss is 0.00395531952381134\n",
      "epoch: 27 step: 262, loss is 0.0010805693455040455\n",
      "epoch: 27 step: 263, loss is 0.00038491582381539047\n",
      "epoch: 27 step: 264, loss is 0.03640041500329971\n",
      "epoch: 27 step: 265, loss is 0.0570809580385685\n",
      "epoch: 27 step: 266, loss is 0.006673146970570087\n",
      "epoch: 27 step: 267, loss is 0.02735498733818531\n",
      "epoch: 27 step: 268, loss is 0.0012326408177614212\n",
      "epoch: 27 step: 269, loss is 0.000859029358252883\n",
      "epoch: 27 step: 270, loss is 0.0032166545279324055\n",
      "epoch: 27 step: 271, loss is 0.13606694340705872\n",
      "epoch: 27 step: 272, loss is 0.001178456237539649\n",
      "epoch: 27 step: 273, loss is 0.07449572533369064\n",
      "epoch: 27 step: 274, loss is 0.00034091254929080606\n",
      "epoch: 27 step: 275, loss is 0.007525842636823654\n",
      "epoch: 27 step: 276, loss is 0.0017106352606788278\n",
      "epoch: 27 step: 277, loss is 0.037179429084062576\n",
      "epoch: 27 step: 278, loss is 0.0002169711224269122\n",
      "epoch: 27 step: 279, loss is 0.00502057746052742\n",
      "epoch: 27 step: 280, loss is 0.0005538495606742799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 281, loss is 0.006531212013214827\n",
      "epoch: 27 step: 282, loss is 0.00036797707434743643\n",
      "epoch: 27 step: 283, loss is 0.0017124006990343332\n",
      "epoch: 27 step: 284, loss is 0.00024201894120778888\n",
      "epoch: 27 step: 285, loss is 0.04022136330604553\n",
      "epoch: 27 step: 286, loss is 0.004603971727192402\n",
      "epoch: 27 step: 287, loss is 0.0005252972478047013\n",
      "epoch: 27 step: 288, loss is 0.04075520485639572\n",
      "epoch: 27 step: 289, loss is 0.000188163248822093\n",
      "epoch: 27 step: 290, loss is 0.008992213755846024\n",
      "epoch: 27 step: 291, loss is 0.0005189465591683984\n",
      "epoch: 27 step: 292, loss is 0.046239856630563736\n",
      "epoch: 27 step: 293, loss is 0.0012612082064151764\n",
      "epoch: 27 step: 294, loss is 0.05149228870868683\n",
      "epoch: 27 step: 295, loss is 0.0009573361603543162\n",
      "epoch: 27 step: 296, loss is 8.904043352231383e-05\n",
      "epoch: 27 step: 297, loss is 0.00256005534902215\n",
      "epoch: 27 step: 298, loss is 0.013713708147406578\n",
      "epoch: 27 step: 299, loss is 0.0014359774067997932\n",
      "epoch: 27 step: 300, loss is 0.01998288184404373\n",
      "epoch: 27 step: 301, loss is 0.015459191985428333\n",
      "epoch: 27 step: 302, loss is 0.00012011719081783667\n",
      "epoch: 27 step: 303, loss is 0.0002634298871271312\n",
      "epoch: 27 step: 304, loss is 0.010186603292822838\n",
      "epoch: 27 step: 305, loss is 7.016737799858674e-05\n",
      "epoch: 27 step: 306, loss is 0.03608616068959236\n",
      "epoch: 27 step: 307, loss is 0.0027388904709368944\n",
      "epoch: 27 step: 308, loss is 0.003568544052541256\n",
      "epoch: 27 step: 309, loss is 0.0005333656445145607\n",
      "epoch: 27 step: 310, loss is 2.7830514227389358e-05\n",
      "epoch: 27 step: 311, loss is 0.0018491885857656598\n",
      "epoch: 27 step: 312, loss is 0.009956859983503819\n",
      "epoch: 27 step: 313, loss is 0.0021080742590129375\n",
      "epoch: 27 step: 314, loss is 0.09337446838617325\n",
      "epoch: 27 step: 315, loss is 0.001485453569330275\n",
      "epoch: 27 step: 316, loss is 0.07762033492326736\n",
      "epoch: 27 step: 317, loss is 0.022243132814764977\n",
      "epoch: 27 step: 318, loss is 0.0001082945236703381\n",
      "epoch: 27 step: 319, loss is 0.006384397856891155\n",
      "epoch: 27 step: 320, loss is 0.011146189644932747\n",
      "epoch: 27 step: 321, loss is 0.0017353965668007731\n",
      "epoch: 27 step: 322, loss is 0.009330845437943935\n",
      "epoch: 27 step: 323, loss is 0.001240878482349217\n",
      "epoch: 27 step: 324, loss is 0.00013520766515284777\n",
      "epoch: 27 step: 325, loss is 0.013505993410944939\n",
      "epoch: 27 step: 326, loss is 2.0367830074974336e-05\n",
      "epoch: 27 step: 327, loss is 0.0015611969865858555\n",
      "epoch: 27 step: 328, loss is 0.006684491410851479\n",
      "epoch: 27 step: 329, loss is 0.0031279781833291054\n",
      "epoch: 27 step: 330, loss is 0.0007516922196373343\n",
      "epoch: 27 step: 331, loss is 0.0008846159325912595\n",
      "epoch: 27 step: 332, loss is 0.003455315250903368\n",
      "epoch: 27 step: 333, loss is 0.0005206126952543855\n",
      "epoch: 27 step: 334, loss is 8.319677363033406e-06\n",
      "epoch: 27 step: 335, loss is 0.09104888141155243\n",
      "epoch: 27 step: 336, loss is 0.006474494934082031\n",
      "epoch: 27 step: 337, loss is 0.0003791250055655837\n",
      "epoch: 27 step: 338, loss is 0.0007014909060671926\n",
      "epoch: 27 step: 339, loss is 0.0010144177358597517\n",
      "epoch: 27 step: 340, loss is 5.617730857920833e-05\n",
      "epoch: 27 step: 341, loss is 3.890919833793305e-05\n",
      "epoch: 27 step: 342, loss is 0.018872348591685295\n",
      "epoch: 27 step: 343, loss is 0.0004645402659662068\n",
      "epoch: 27 step: 344, loss is 0.010323469527065754\n",
      "epoch: 27 step: 345, loss is 0.0008133613737300038\n",
      "epoch: 27 step: 346, loss is 0.0007189496536739171\n",
      "epoch: 27 step: 347, loss is 0.0007013996946625412\n",
      "epoch: 27 step: 348, loss is 0.0012353588826954365\n",
      "epoch: 27 step: 349, loss is 6.914031837368384e-05\n",
      "epoch: 27 step: 350, loss is 9.064781625056639e-05\n",
      "epoch: 27 step: 351, loss is 0.0005486577865667641\n",
      "epoch: 27 step: 352, loss is 0.01851039007306099\n",
      "epoch: 27 step: 353, loss is 0.000274780613835901\n",
      "epoch: 27 step: 354, loss is 0.004093223717063665\n",
      "epoch: 27 step: 355, loss is 0.0016827634535729885\n",
      "epoch: 27 step: 356, loss is 0.0009246225235983729\n",
      "epoch: 27 step: 357, loss is 0.0009948457591235638\n",
      "epoch: 27 step: 358, loss is 0.016385354101657867\n",
      "epoch: 27 step: 359, loss is 0.026052454486489296\n",
      "epoch: 27 step: 360, loss is 0.0013907431857660413\n",
      "epoch: 27 step: 361, loss is 0.00031609542202204466\n",
      "epoch: 27 step: 362, loss is 1.0924972229986452e-05\n",
      "epoch: 27 step: 363, loss is 0.0004256228858139366\n",
      "epoch: 27 step: 364, loss is 0.001246195170097053\n",
      "epoch: 27 step: 365, loss is 0.00010352995013818145\n",
      "epoch: 27 step: 366, loss is 0.0003084857598878443\n",
      "epoch: 27 step: 367, loss is 0.0010868634562939405\n",
      "epoch: 27 step: 368, loss is 0.00817615445703268\n",
      "epoch: 27 step: 369, loss is 0.012390323914587498\n",
      "epoch: 27 step: 370, loss is 0.010364880785346031\n",
      "epoch: 27 step: 371, loss is 0.017567824572324753\n",
      "epoch: 27 step: 372, loss is 0.0140781095251441\n",
      "epoch: 27 step: 373, loss is 0.00832394603639841\n",
      "epoch: 27 step: 374, loss is 0.0002755540481302887\n",
      "epoch: 27 step: 375, loss is 0.000988418236374855\n",
      "epoch: 27 step: 376, loss is 0.001102906884625554\n",
      "epoch: 27 step: 377, loss is 0.004511152394115925\n",
      "epoch: 27 step: 378, loss is 0.0417536161839962\n",
      "epoch: 27 step: 379, loss is 0.040072035044431686\n",
      "epoch: 27 step: 380, loss is 0.0010099858045578003\n",
      "epoch: 27 step: 381, loss is 0.002163012744858861\n",
      "epoch: 27 step: 382, loss is 0.0017985701560974121\n",
      "epoch: 27 step: 383, loss is 0.003249483648687601\n",
      "epoch: 27 step: 384, loss is 0.014226040802896023\n",
      "epoch: 27 step: 385, loss is 0.0071119461208581924\n",
      "epoch: 27 step: 386, loss is 0.0008835309999994934\n",
      "epoch: 27 step: 387, loss is 0.007152671925723553\n",
      "epoch: 27 step: 388, loss is 0.002930824877694249\n",
      "epoch: 27 step: 389, loss is 0.0006983166676945984\n",
      "epoch: 27 step: 390, loss is 0.0023670741356909275\n",
      "epoch: 27 step: 391, loss is 0.0005460503743961453\n",
      "epoch: 27 step: 392, loss is 0.0011961503187194467\n",
      "epoch: 27 step: 393, loss is 0.0019603597465902567\n",
      "epoch: 27 step: 394, loss is 0.0006749270833097398\n",
      "epoch: 27 step: 395, loss is 0.006292920093983412\n",
      "epoch: 27 step: 396, loss is 0.025064809247851372\n",
      "epoch: 27 step: 397, loss is 0.0001612319319974631\n",
      "epoch: 27 step: 398, loss is 0.10093338042497635\n",
      "epoch: 27 step: 399, loss is 0.05568681284785271\n",
      "epoch: 27 step: 400, loss is 0.004209183156490326\n",
      "epoch: 27 step: 401, loss is 0.003903948003426194\n",
      "epoch: 27 step: 402, loss is 0.0012211203575134277\n",
      "epoch: 27 step: 403, loss is 9.698444046080112e-05\n",
      "epoch: 27 step: 404, loss is 0.0157114639878273\n",
      "epoch: 27 step: 405, loss is 0.00317664397880435\n",
      "epoch: 27 step: 406, loss is 0.004326581954956055\n",
      "epoch: 27 step: 407, loss is 0.005816350225359201\n",
      "epoch: 27 step: 408, loss is 0.015434877946972847\n",
      "epoch: 27 step: 409, loss is 6.700785888824612e-05\n",
      "epoch: 27 step: 410, loss is 0.08032389730215073\n",
      "epoch: 27 step: 411, loss is 0.08147059381008148\n",
      "epoch: 27 step: 412, loss is 0.004958553239703178\n",
      "epoch: 27 step: 413, loss is 0.001826571999117732\n",
      "epoch: 27 step: 414, loss is 0.00013331940863281488\n",
      "epoch: 27 step: 415, loss is 0.0009294529445469379\n",
      "epoch: 27 step: 416, loss is 0.008679306134581566\n",
      "epoch: 27 step: 417, loss is 8.796449401415884e-05\n",
      "epoch: 27 step: 418, loss is 3.179159830324352e-05\n",
      "epoch: 27 step: 419, loss is 3.719034430105239e-05\n",
      "epoch: 27 step: 420, loss is 0.018758650869131088\n",
      "epoch: 27 step: 421, loss is 0.036745231598615646\n",
      "epoch: 27 step: 422, loss is 3.4813663205568446e-06\n",
      "epoch: 27 step: 423, loss is 0.02780192345380783\n",
      "epoch: 27 step: 424, loss is 0.035695526748895645\n",
      "epoch: 27 step: 425, loss is 0.00011078640818595886\n",
      "epoch: 27 step: 426, loss is 0.005914904642850161\n",
      "epoch: 27 step: 427, loss is 0.0008602068410255015\n",
      "epoch: 27 step: 428, loss is 0.01101831067353487\n",
      "epoch: 27 step: 429, loss is 0.011581492610275745\n",
      "epoch: 27 step: 430, loss is 0.0017943683778867126\n",
      "epoch: 27 step: 431, loss is 0.0007740557193756104\n",
      "epoch: 27 step: 432, loss is 0.013648195192217827\n",
      "epoch: 27 step: 433, loss is 0.007327535655349493\n",
      "epoch: 27 step: 434, loss is 0.10773587226867676\n",
      "epoch: 27 step: 435, loss is 0.00545282568782568\n",
      "epoch: 27 step: 436, loss is 0.004560306202620268\n",
      "epoch: 27 step: 437, loss is 0.002311217598617077\n",
      "epoch: 27 step: 438, loss is 0.018107648938894272\n",
      "epoch: 27 step: 439, loss is 0.04056620970368385\n",
      "epoch: 27 step: 440, loss is 5.43060086783953e-05\n",
      "epoch: 27 step: 441, loss is 0.08426747471094131\n",
      "epoch: 27 step: 442, loss is 0.0005210894742049277\n",
      "epoch: 27 step: 443, loss is 0.007725996896624565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 444, loss is 0.16048218309879303\n",
      "epoch: 27 step: 445, loss is 0.0002931208291556686\n",
      "epoch: 27 step: 446, loss is 0.003669378347694874\n",
      "epoch: 27 step: 447, loss is 0.00037421780871227384\n",
      "epoch: 27 step: 448, loss is 0.021262826398015022\n",
      "epoch: 27 step: 449, loss is 0.0011168147902935743\n",
      "epoch: 27 step: 450, loss is 0.004559458699077368\n",
      "epoch: 27 step: 451, loss is 0.018324244767427444\n",
      "epoch: 27 step: 452, loss is 0.00808155070990324\n",
      "epoch: 27 step: 453, loss is 0.1558578908443451\n",
      "epoch: 27 step: 454, loss is 0.02083556540310383\n",
      "epoch: 27 step: 455, loss is 0.0028886902146041393\n",
      "epoch: 27 step: 456, loss is 0.029293512925505638\n",
      "epoch: 27 step: 457, loss is 0.01879969611763954\n",
      "epoch: 27 step: 458, loss is 0.11881123483181\n",
      "epoch: 27 step: 459, loss is 0.05834212526679039\n",
      "epoch: 27 step: 460, loss is 0.023918280377984047\n",
      "epoch: 27 step: 461, loss is 0.05213051661849022\n",
      "epoch: 27 step: 462, loss is 0.0011622788151726127\n",
      "epoch: 27 step: 463, loss is 0.0054304227232933044\n",
      "epoch: 27 step: 464, loss is 0.03537706285715103\n",
      "epoch: 27 step: 465, loss is 0.009219789877533913\n",
      "epoch: 27 step: 466, loss is 0.00540524534881115\n",
      "epoch: 27 step: 467, loss is 0.010022034868597984\n",
      "epoch: 27 step: 468, loss is 4.549485311144963e-05\n",
      "epoch: 27 step: 469, loss is 0.0002765200042631477\n",
      "epoch: 27 step: 470, loss is 0.006175674498081207\n",
      "epoch: 27 step: 471, loss is 0.4098895192146301\n",
      "epoch: 27 step: 472, loss is 8.667899965075776e-05\n",
      "epoch: 27 step: 473, loss is 0.0025019582826644182\n",
      "epoch: 27 step: 474, loss is 0.002430527936667204\n",
      "epoch: 27 step: 475, loss is 0.01776537485420704\n",
      "epoch: 27 step: 476, loss is 0.04767761379480362\n",
      "epoch: 27 step: 477, loss is 0.09897386282682419\n",
      "epoch: 27 step: 478, loss is 0.044740229845047\n",
      "epoch: 27 step: 479, loss is 0.0008537362446077168\n",
      "epoch: 27 step: 480, loss is 0.017534999176859856\n",
      "epoch: 27 step: 481, loss is 0.02816440723836422\n",
      "epoch: 27 step: 482, loss is 0.005195483099669218\n",
      "epoch: 27 step: 483, loss is 0.002223969902843237\n",
      "epoch: 27 step: 484, loss is 0.03908523544669151\n",
      "epoch: 27 step: 485, loss is 0.06121879816055298\n",
      "epoch: 27 step: 486, loss is 0.01060414221137762\n",
      "epoch: 27 step: 487, loss is 0.003568833228200674\n",
      "epoch: 27 step: 488, loss is 0.002722043776884675\n",
      "epoch: 27 step: 489, loss is 0.01573501154780388\n",
      "epoch: 27 step: 490, loss is 0.021083464846014977\n",
      "epoch: 27 step: 491, loss is 0.00377417029812932\n",
      "epoch: 27 step: 492, loss is 0.06044725701212883\n",
      "epoch: 27 step: 493, loss is 0.005610390100628138\n",
      "epoch: 27 step: 494, loss is 0.0025328616611659527\n",
      "epoch: 27 step: 495, loss is 0.00037886746576987207\n",
      "epoch: 27 step: 496, loss is 0.025280453264713287\n",
      "epoch: 27 step: 497, loss is 0.07865642756223679\n",
      "epoch: 27 step: 498, loss is 0.001216223230585456\n",
      "epoch: 27 step: 499, loss is 0.08748023211956024\n",
      "epoch: 27 step: 500, loss is 0.020293980836868286\n",
      "epoch: 27 step: 501, loss is 0.02355787716805935\n",
      "epoch: 27 step: 502, loss is 0.006693188101053238\n",
      "epoch: 27 step: 503, loss is 0.029551196843385696\n",
      "epoch: 27 step: 504, loss is 0.005494188517332077\n",
      "epoch: 27 step: 505, loss is 0.00047295165131799877\n",
      "epoch: 27 step: 506, loss is 0.028789808973670006\n",
      "epoch: 27 step: 507, loss is 0.004387104418128729\n",
      "epoch: 27 step: 508, loss is 0.004861969035118818\n",
      "epoch: 27 step: 509, loss is 0.02964750863611698\n",
      "epoch: 27 step: 510, loss is 0.0010123179526999593\n",
      "epoch: 27 step: 511, loss is 0.00015162970521487296\n",
      "epoch: 27 step: 512, loss is 0.028571709990501404\n",
      "epoch: 27 step: 513, loss is 6.687540007987991e-06\n",
      "epoch: 27 step: 514, loss is 0.0015363580314442515\n",
      "epoch: 27 step: 515, loss is 0.0025890874676406384\n",
      "epoch: 27 step: 516, loss is 0.032429251819849014\n",
      "epoch: 27 step: 517, loss is 0.009608335793018341\n",
      "epoch: 27 step: 518, loss is 0.002130538458004594\n",
      "epoch: 27 step: 519, loss is 0.009106437675654888\n",
      "epoch: 27 step: 520, loss is 0.037421178072690964\n",
      "epoch: 27 step: 521, loss is 3.5372031561564654e-05\n",
      "epoch: 27 step: 522, loss is 0.1006663516163826\n",
      "epoch: 27 step: 523, loss is 0.004128230269998312\n",
      "epoch: 27 step: 524, loss is 0.0041887820698320866\n",
      "epoch: 27 step: 525, loss is 0.006868540309369564\n",
      "epoch: 27 step: 526, loss is 0.0017255034763365984\n",
      "epoch: 27 step: 527, loss is 0.035541195422410965\n",
      "epoch: 27 step: 528, loss is 0.00023887418501544744\n",
      "epoch: 27 step: 529, loss is 0.0008985387976281345\n",
      "epoch: 27 step: 530, loss is 0.006312704179435968\n",
      "epoch: 27 step: 531, loss is 0.002005122136324644\n",
      "epoch: 27 step: 532, loss is 0.004892366472631693\n",
      "epoch: 27 step: 533, loss is 0.03156149759888649\n",
      "epoch: 27 step: 534, loss is 0.024266723543405533\n",
      "epoch: 27 step: 535, loss is 0.01989336498081684\n",
      "epoch: 27 step: 536, loss is 0.0003060442104469985\n",
      "epoch: 27 step: 537, loss is 0.033118557184934616\n",
      "epoch: 27 step: 538, loss is 0.07006200402975082\n",
      "epoch: 27 step: 539, loss is 0.00545909907668829\n",
      "epoch: 27 step: 540, loss is 0.07541590929031372\n",
      "epoch: 27 step: 541, loss is 0.014010039158165455\n",
      "epoch: 27 step: 542, loss is 0.017139112576842308\n",
      "epoch: 27 step: 543, loss is 0.0065576075576245785\n",
      "epoch: 27 step: 544, loss is 0.0385078489780426\n",
      "epoch: 27 step: 545, loss is 0.005845591425895691\n",
      "epoch: 27 step: 546, loss is 0.019861895591020584\n",
      "epoch: 27 step: 547, loss is 0.005672124680131674\n",
      "epoch: 27 step: 548, loss is 0.012802882120013237\n",
      "epoch: 27 step: 549, loss is 0.0249683428555727\n",
      "epoch: 27 step: 550, loss is 0.006082803010940552\n",
      "epoch: 27 step: 551, loss is 0.19115854799747467\n",
      "epoch: 27 step: 552, loss is 0.014110803604125977\n",
      "epoch: 27 step: 553, loss is 0.004250048194080591\n",
      "epoch: 27 step: 554, loss is 0.0025932437274605036\n",
      "epoch: 27 step: 555, loss is 0.006000746041536331\n",
      "epoch: 27 step: 556, loss is 0.0015982050681486726\n",
      "epoch: 27 step: 557, loss is 0.004126420244574547\n",
      "epoch: 27 step: 558, loss is 0.01068209484219551\n",
      "epoch: 27 step: 559, loss is 0.0008296449086628854\n",
      "epoch: 27 step: 560, loss is 0.0012341218534857035\n",
      "epoch: 27 step: 561, loss is 0.006266927812248468\n",
      "epoch: 27 step: 562, loss is 0.1003672406077385\n",
      "epoch: 27 step: 563, loss is 0.037608034908771515\n",
      "epoch: 27 step: 564, loss is 0.0004915735917165875\n",
      "epoch: 27 step: 565, loss is 0.0016562273958697915\n",
      "epoch: 27 step: 566, loss is 0.00409880792722106\n",
      "epoch: 27 step: 567, loss is 0.005818784702569246\n",
      "epoch: 27 step: 568, loss is 0.025129832327365875\n",
      "epoch: 27 step: 569, loss is 0.007342790253460407\n",
      "epoch: 27 step: 570, loss is 0.009208904579281807\n",
      "epoch: 27 step: 571, loss is 0.010767465457320213\n",
      "epoch: 27 step: 572, loss is 0.005409109406173229\n",
      "epoch: 27 step: 573, loss is 0.015499935485422611\n",
      "epoch: 27 step: 574, loss is 0.021381547674536705\n",
      "epoch: 27 step: 575, loss is 0.0023745261132717133\n",
      "epoch: 27 step: 576, loss is 0.001070078113116324\n",
      "epoch: 27 step: 577, loss is 4.820526373805478e-05\n",
      "epoch: 27 step: 578, loss is 0.004903796128928661\n",
      "epoch: 27 step: 579, loss is 0.03249528259038925\n",
      "epoch: 27 step: 580, loss is 0.0008451942703686655\n",
      "epoch: 27 step: 581, loss is 0.00015531148528680205\n",
      "epoch: 27 step: 582, loss is 0.000610589690040797\n",
      "epoch: 27 step: 583, loss is 0.00083821831503883\n",
      "epoch: 27 step: 584, loss is 0.0009206813992932439\n",
      "epoch: 27 step: 585, loss is 0.004164953716099262\n",
      "epoch: 27 step: 586, loss is 0.00601397268474102\n",
      "epoch: 27 step: 587, loss is 0.00010329701035516337\n",
      "epoch: 27 step: 588, loss is 0.0017613498494029045\n",
      "epoch: 27 step: 589, loss is 0.00246951961889863\n",
      "epoch: 27 step: 590, loss is 0.14697743952274323\n",
      "epoch: 27 step: 591, loss is 0.005405465606600046\n",
      "epoch: 27 step: 592, loss is 0.09831177443265915\n",
      "epoch: 27 step: 593, loss is 0.017375728115439415\n",
      "epoch: 27 step: 594, loss is 0.002269140211865306\n",
      "epoch: 27 step: 595, loss is 0.0014779339544475079\n",
      "epoch: 27 step: 596, loss is 0.0003566300147213042\n",
      "epoch: 27 step: 597, loss is 1.980533306777943e-05\n",
      "epoch: 27 step: 598, loss is 0.0008198174182325602\n",
      "epoch: 27 step: 599, loss is 0.0015075288247317076\n",
      "epoch: 27 step: 600, loss is 0.03372262418270111\n",
      "epoch: 27 step: 601, loss is 0.02378671243786812\n",
      "epoch: 27 step: 602, loss is 0.0004452041466720402\n",
      "epoch: 27 step: 603, loss is 0.01078050211071968\n",
      "epoch: 27 step: 604, loss is 0.0024238319601863623\n",
      "epoch: 27 step: 605, loss is 0.005903436336666346\n",
      "epoch: 27 step: 606, loss is 0.0008302876958623528\n",
      "epoch: 27 step: 607, loss is 0.012814832851290703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 608, loss is 0.04298684373497963\n",
      "epoch: 27 step: 609, loss is 0.059500183910131454\n",
      "epoch: 27 step: 610, loss is 0.000723930774256587\n",
      "epoch: 27 step: 611, loss is 0.015078006312251091\n",
      "epoch: 27 step: 612, loss is 0.00479261064901948\n",
      "epoch: 27 step: 613, loss is 0.000635778356809169\n",
      "epoch: 27 step: 614, loss is 0.005146801006048918\n",
      "epoch: 27 step: 615, loss is 0.00011267387890256941\n",
      "epoch: 27 step: 616, loss is 0.026712380349636078\n",
      "epoch: 27 step: 617, loss is 0.013394877314567566\n",
      "epoch: 27 step: 618, loss is 0.0006249626167118549\n",
      "epoch: 27 step: 619, loss is 3.010147156601306e-05\n",
      "epoch: 27 step: 620, loss is 0.002356108045205474\n",
      "epoch: 27 step: 621, loss is 0.0024095589760690928\n",
      "epoch: 27 step: 622, loss is 0.0004413889837451279\n",
      "epoch: 27 step: 623, loss is 0.04290522634983063\n",
      "epoch: 27 step: 624, loss is 0.0076675103046\n",
      "epoch: 27 step: 625, loss is 0.00017204383038915694\n",
      "epoch: 27 step: 626, loss is 0.02260633185505867\n",
      "epoch: 27 step: 627, loss is 0.01629510708153248\n",
      "epoch: 27 step: 628, loss is 0.00019954655726905912\n",
      "epoch: 27 step: 629, loss is 0.00023954360221978277\n",
      "epoch: 27 step: 630, loss is 0.006374799646437168\n",
      "epoch: 27 step: 631, loss is 0.0023783512879163027\n",
      "epoch: 27 step: 632, loss is 0.004909994080662727\n",
      "epoch: 27 step: 633, loss is 0.036647021770477295\n",
      "epoch: 27 step: 634, loss is 0.0038695314433425665\n",
      "epoch: 27 step: 635, loss is 0.0022799414582550526\n",
      "epoch: 27 step: 636, loss is 0.0007963268435560167\n",
      "epoch: 27 step: 637, loss is 0.06933221966028214\n",
      "epoch: 27 step: 638, loss is 0.014004516415297985\n",
      "epoch: 27 step: 639, loss is 0.03136473149061203\n",
      "epoch: 27 step: 640, loss is 0.004192564636468887\n",
      "epoch: 27 step: 641, loss is 0.07069171220064163\n",
      "epoch: 27 step: 642, loss is 0.053828440606594086\n",
      "epoch: 27 step: 643, loss is 0.002527636708691716\n",
      "epoch: 27 step: 644, loss is 0.037529461085796356\n",
      "epoch: 27 step: 645, loss is 7.390934479190037e-05\n",
      "epoch: 27 step: 646, loss is 0.0031261546537280083\n",
      "epoch: 27 step: 647, loss is 0.018590839579701424\n",
      "epoch: 27 step: 648, loss is 0.0007795015699230134\n",
      "epoch: 27 step: 649, loss is 0.009195935912430286\n",
      "epoch: 27 step: 650, loss is 0.07587076723575592\n",
      "epoch: 27 step: 651, loss is 0.021454405039548874\n",
      "epoch: 27 step: 652, loss is 0.02310756780207157\n",
      "epoch: 27 step: 653, loss is 0.00752228731289506\n",
      "epoch: 27 step: 654, loss is 0.028337670490145683\n",
      "epoch: 27 step: 655, loss is 1.7832453522714786e-05\n",
      "epoch: 27 step: 656, loss is 0.0004208969185128808\n",
      "epoch: 27 step: 657, loss is 0.06073978915810585\n",
      "epoch: 27 step: 658, loss is 0.00023071022587828338\n",
      "epoch: 27 step: 659, loss is 6.389825284713879e-05\n",
      "epoch: 27 step: 660, loss is 0.022024104371666908\n",
      "epoch: 27 step: 661, loss is 0.053995732218027115\n",
      "epoch: 27 step: 662, loss is 0.06648024171590805\n",
      "epoch: 27 step: 663, loss is 0.0019181536044925451\n",
      "epoch: 27 step: 664, loss is 0.017229735851287842\n",
      "epoch: 27 step: 665, loss is 0.00037474758573807776\n",
      "epoch: 27 step: 666, loss is 0.0009160077897831798\n",
      "epoch: 27 step: 667, loss is 0.014093625359237194\n",
      "epoch: 27 step: 668, loss is 0.0022427088115364313\n",
      "epoch: 27 step: 669, loss is 0.0005834404146298766\n",
      "epoch: 27 step: 670, loss is 0.004639552906155586\n",
      "epoch: 27 step: 671, loss is 0.007384089287370443\n",
      "epoch: 27 step: 672, loss is 0.01668465882539749\n",
      "epoch: 27 step: 673, loss is 0.0005320201744325459\n",
      "epoch: 27 step: 674, loss is 0.0007766407215967774\n",
      "epoch: 27 step: 675, loss is 0.010525689460337162\n",
      "epoch: 27 step: 676, loss is 0.010451752692461014\n",
      "epoch: 27 step: 677, loss is 0.00949848908931017\n",
      "epoch: 27 step: 678, loss is 0.0008608811185695231\n",
      "epoch: 27 step: 679, loss is 0.032074593007564545\n",
      "epoch: 27 step: 680, loss is 0.0004016840539406985\n",
      "epoch: 27 step: 681, loss is 0.009159782901406288\n",
      "epoch: 27 step: 682, loss is 0.0005970768397673965\n",
      "epoch: 27 step: 683, loss is 0.023871125653386116\n",
      "epoch: 27 step: 684, loss is 0.006384789943695068\n",
      "epoch: 27 step: 685, loss is 3.701103196362965e-05\n",
      "epoch: 27 step: 686, loss is 0.015733065083622932\n",
      "epoch: 27 step: 687, loss is 0.16838297247886658\n",
      "epoch: 27 step: 688, loss is 0.0019415495917201042\n",
      "epoch: 27 step: 689, loss is 0.0006856039981357753\n",
      "epoch: 27 step: 690, loss is 0.00020179607963655144\n",
      "epoch: 27 step: 691, loss is 0.07234475761651993\n",
      "epoch: 27 step: 692, loss is 0.0011807907139882445\n",
      "epoch: 27 step: 693, loss is 0.01057659462094307\n",
      "epoch: 27 step: 694, loss is 0.0009779748506844044\n",
      "epoch: 27 step: 695, loss is 0.006737327203154564\n",
      "epoch: 27 step: 696, loss is 0.0019785682670772076\n",
      "epoch: 27 step: 697, loss is 0.005612839013338089\n",
      "epoch: 27 step: 698, loss is 0.0024010834749788046\n",
      "epoch: 27 step: 699, loss is 0.054469846189022064\n",
      "epoch: 27 step: 700, loss is 0.0002806728589348495\n",
      "epoch: 27 step: 701, loss is 0.001210579532198608\n",
      "epoch: 27 step: 702, loss is 0.011559032835066319\n",
      "epoch: 27 step: 703, loss is 0.01626410149037838\n",
      "epoch: 27 step: 704, loss is 0.02171633206307888\n",
      "epoch: 27 step: 705, loss is 0.0006331607000902295\n",
      "epoch: 27 step: 706, loss is 0.005180722568184137\n",
      "epoch: 27 step: 707, loss is 0.004472965840250254\n",
      "epoch: 27 step: 708, loss is 0.06077409163117409\n",
      "epoch: 27 step: 709, loss is 7.758595165796578e-05\n",
      "epoch: 27 step: 710, loss is 0.023819008842110634\n",
      "epoch: 27 step: 711, loss is 0.01279575563967228\n",
      "epoch: 27 step: 712, loss is 0.008904330432415009\n",
      "epoch: 27 step: 713, loss is 0.01865425892174244\n",
      "epoch: 27 step: 714, loss is 0.0017981475684791803\n",
      "epoch: 27 step: 715, loss is 0.009869970381259918\n",
      "epoch: 27 step: 716, loss is 0.05785493552684784\n",
      "epoch: 27 step: 717, loss is 0.0030162865296006203\n",
      "epoch: 27 step: 718, loss is 0.027931861579418182\n",
      "epoch: 27 step: 719, loss is 0.0024091957602649927\n",
      "epoch: 27 step: 720, loss is 0.22090882062911987\n",
      "epoch: 27 step: 721, loss is 0.016255713999271393\n",
      "epoch: 27 step: 722, loss is 0.03697889670729637\n",
      "epoch: 27 step: 723, loss is 0.0008510446641594172\n",
      "epoch: 27 step: 724, loss is 0.009741319343447685\n",
      "epoch: 27 step: 725, loss is 0.0008894656784832478\n",
      "epoch: 27 step: 726, loss is 0.011977218091487885\n",
      "epoch: 27 step: 727, loss is 0.13224124908447266\n",
      "epoch: 27 step: 728, loss is 0.0016869818791747093\n",
      "epoch: 27 step: 729, loss is 0.031955037266016006\n",
      "epoch: 27 step: 730, loss is 0.01645113155245781\n",
      "epoch: 27 step: 731, loss is 0.015408576466143131\n",
      "epoch: 27 step: 732, loss is 0.003470224794000387\n",
      "epoch: 27 step: 733, loss is 0.031388554722070694\n",
      "epoch: 27 step: 734, loss is 0.006047058384865522\n",
      "epoch: 27 step: 735, loss is 7.549097063019872e-06\n",
      "epoch: 27 step: 736, loss is 0.017374742776155472\n",
      "epoch: 27 step: 737, loss is 0.0007289584027603269\n",
      "epoch: 27 step: 738, loss is 0.003317735157907009\n",
      "epoch: 27 step: 739, loss is 0.0011035145726054907\n",
      "epoch: 27 step: 740, loss is 0.07073698192834854\n",
      "epoch: 27 step: 741, loss is 0.4452415406703949\n",
      "epoch: 27 step: 742, loss is 0.00037812648224644363\n",
      "epoch: 27 step: 743, loss is 0.003480322193354368\n",
      "epoch: 27 step: 744, loss is 0.028201313689351082\n",
      "epoch: 27 step: 745, loss is 0.004508676938712597\n",
      "epoch: 27 step: 746, loss is 0.007211592514067888\n",
      "epoch: 27 step: 747, loss is 0.013925161212682724\n",
      "epoch: 27 step: 748, loss is 0.04772353172302246\n",
      "epoch: 27 step: 749, loss is 0.00217711110599339\n",
      "epoch: 27 step: 750, loss is 0.001666889525949955\n",
      "epoch: 27 step: 751, loss is 8.08790500741452e-05\n",
      "epoch: 27 step: 752, loss is 0.000357620301656425\n",
      "epoch: 27 step: 753, loss is 0.0002743360528256744\n",
      "epoch: 27 step: 754, loss is 0.03106134571135044\n",
      "epoch: 27 step: 755, loss is 0.1563921570777893\n",
      "epoch: 27 step: 756, loss is 0.0011278352467343211\n",
      "epoch: 27 step: 757, loss is 0.03827979788184166\n",
      "epoch: 27 step: 758, loss is 0.0005372727173380554\n",
      "epoch: 27 step: 759, loss is 0.043786294758319855\n",
      "epoch: 27 step: 760, loss is 0.0006168006802909076\n",
      "epoch: 27 step: 761, loss is 0.003644205629825592\n",
      "epoch: 27 step: 762, loss is 0.0013968277489766479\n",
      "epoch: 27 step: 763, loss is 0.0014547125902026892\n",
      "epoch: 27 step: 764, loss is 4.533335959422402e-05\n",
      "epoch: 27 step: 765, loss is 0.0029205894097685814\n",
      "epoch: 27 step: 766, loss is 4.967076893080957e-05\n",
      "epoch: 27 step: 767, loss is 0.06601017713546753\n",
      "epoch: 27 step: 768, loss is 0.019130950793623924\n",
      "epoch: 27 step: 769, loss is 0.007046837359666824\n",
      "epoch: 27 step: 770, loss is 0.005837523844093084\n",
      "epoch: 27 step: 771, loss is 0.024952951818704605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 772, loss is 0.001696653664112091\n",
      "epoch: 27 step: 773, loss is 0.008921252563595772\n",
      "epoch: 27 step: 774, loss is 0.0025105951353907585\n",
      "epoch: 27 step: 775, loss is 0.07540072500705719\n",
      "epoch: 27 step: 776, loss is 0.0007821584586054087\n",
      "epoch: 27 step: 777, loss is 0.029018742963671684\n",
      "epoch: 27 step: 778, loss is 0.0012637288309633732\n",
      "epoch: 27 step: 779, loss is 0.09189552813768387\n",
      "epoch: 27 step: 780, loss is 0.12339631468057632\n",
      "epoch: 27 step: 781, loss is 0.00766507163643837\n",
      "epoch: 27 step: 782, loss is 0.004893574398010969\n",
      "epoch: 27 step: 783, loss is 0.050523385405540466\n",
      "epoch: 27 step: 784, loss is 0.08190744370222092\n",
      "epoch: 27 step: 785, loss is 0.0005889825988560915\n",
      "epoch: 27 step: 786, loss is 0.0005761192878708243\n",
      "epoch: 27 step: 787, loss is 0.1635359525680542\n",
      "epoch: 27 step: 788, loss is 0.0001512847957201302\n",
      "epoch: 27 step: 789, loss is 0.0013486829120665789\n",
      "epoch: 27 step: 790, loss is 0.010251647792756557\n",
      "epoch: 27 step: 791, loss is 0.017140857875347137\n",
      "epoch: 27 step: 792, loss is 0.004177879076451063\n",
      "epoch: 27 step: 793, loss is 0.005010226275771856\n",
      "epoch: 27 step: 794, loss is 0.05016673728823662\n",
      "epoch: 27 step: 795, loss is 0.013447780162096024\n",
      "epoch: 27 step: 796, loss is 8.927109593059868e-05\n",
      "epoch: 27 step: 797, loss is 0.10561173409223557\n",
      "epoch: 27 step: 798, loss is 0.017221184447407722\n",
      "epoch: 27 step: 799, loss is 0.05452815070748329\n",
      "epoch: 27 step: 800, loss is 0.17385201156139374\n",
      "epoch: 27 step: 801, loss is 0.004383386578410864\n",
      "epoch: 27 step: 802, loss is 0.08659137040376663\n",
      "epoch: 27 step: 803, loss is 0.02095036581158638\n",
      "epoch: 27 step: 804, loss is 0.12942326068878174\n",
      "epoch: 27 step: 805, loss is 0.02981017343699932\n",
      "epoch: 27 step: 806, loss is 0.003589413594454527\n",
      "epoch: 27 step: 807, loss is 0.0051909820176661015\n",
      "epoch: 27 step: 808, loss is 0.0691559910774231\n",
      "epoch: 27 step: 809, loss is 0.0004184506251476705\n",
      "epoch: 27 step: 810, loss is 0.021164460107684135\n",
      "epoch: 27 step: 811, loss is 0.205936998128891\n",
      "epoch: 27 step: 812, loss is 0.014761185273528099\n",
      "epoch: 27 step: 813, loss is 0.051658689975738525\n",
      "epoch: 27 step: 814, loss is 0.25561589002609253\n",
      "epoch: 27 step: 815, loss is 0.002715610433369875\n",
      "epoch: 27 step: 816, loss is 0.015955384820699692\n",
      "epoch: 27 step: 817, loss is 0.02172304503619671\n",
      "epoch: 27 step: 818, loss is 0.011551281437277794\n",
      "epoch: 27 step: 819, loss is 0.0360865518450737\n",
      "epoch: 27 step: 820, loss is 0.00017092571943067014\n",
      "epoch: 27 step: 821, loss is 0.0018437522230669856\n",
      "epoch: 27 step: 822, loss is 0.004286732990294695\n",
      "epoch: 27 step: 823, loss is 0.006294257938861847\n",
      "epoch: 27 step: 824, loss is 0.000558310654014349\n",
      "epoch: 27 step: 825, loss is 0.00941086933016777\n",
      "epoch: 27 step: 826, loss is 0.00029429601272568107\n",
      "epoch: 27 step: 827, loss is 0.0002317190810572356\n",
      "epoch: 27 step: 828, loss is 0.042516935616731644\n",
      "epoch: 27 step: 829, loss is 0.007230186834931374\n",
      "epoch: 27 step: 830, loss is 0.10545441508293152\n",
      "epoch: 27 step: 831, loss is 0.043662212789058685\n",
      "epoch: 27 step: 832, loss is 0.002223295858129859\n",
      "epoch: 27 step: 833, loss is 0.006672308314591646\n",
      "epoch: 27 step: 834, loss is 0.010449770838022232\n",
      "epoch: 27 step: 835, loss is 0.0016493885777890682\n",
      "epoch: 27 step: 836, loss is 0.005103282630443573\n",
      "epoch: 27 step: 837, loss is 0.002479336690157652\n",
      "epoch: 27 step: 838, loss is 0.05331135541200638\n",
      "epoch: 27 step: 839, loss is 0.07079333811998367\n",
      "epoch: 27 step: 840, loss is 0.007564026862382889\n",
      "epoch: 27 step: 841, loss is 0.016553441062569618\n",
      "epoch: 27 step: 842, loss is 0.06094876676797867\n",
      "epoch: 27 step: 843, loss is 0.0002618178550619632\n",
      "epoch: 27 step: 844, loss is 0.000828166666906327\n",
      "epoch: 27 step: 845, loss is 0.008041095919907093\n",
      "epoch: 27 step: 846, loss is 0.0037248372100293636\n",
      "epoch: 27 step: 847, loss is 0.010427421890199184\n",
      "epoch: 27 step: 848, loss is 0.0339941680431366\n",
      "epoch: 27 step: 849, loss is 0.0017460943199694157\n",
      "epoch: 27 step: 850, loss is 0.14359547197818756\n",
      "epoch: 27 step: 851, loss is 0.015538954176008701\n",
      "epoch: 27 step: 852, loss is 0.00019898635218851268\n",
      "epoch: 27 step: 853, loss is 0.07510758936405182\n",
      "epoch: 27 step: 854, loss is 0.13665489852428436\n",
      "epoch: 27 step: 855, loss is 0.056487757712602615\n",
      "epoch: 27 step: 856, loss is 0.0009988828096538782\n",
      "epoch: 27 step: 857, loss is 0.0697309598326683\n",
      "epoch: 27 step: 858, loss is 0.006686603184789419\n",
      "epoch: 27 step: 859, loss is 0.0012249493738636374\n",
      "epoch: 27 step: 860, loss is 0.0027296245098114014\n",
      "epoch: 27 step: 861, loss is 0.020262859761714935\n",
      "epoch: 27 step: 862, loss is 0.0025410086382180452\n",
      "epoch: 27 step: 863, loss is 0.003978456370532513\n",
      "epoch: 27 step: 864, loss is 0.0001862593780970201\n",
      "epoch: 27 step: 865, loss is 0.027285834774374962\n",
      "epoch: 27 step: 866, loss is 0.010678534395992756\n",
      "epoch: 27 step: 867, loss is 0.11334405839443207\n",
      "epoch: 27 step: 868, loss is 0.02672860026359558\n",
      "epoch: 27 step: 869, loss is 0.020239274948835373\n",
      "epoch: 27 step: 870, loss is 0.04093816876411438\n",
      "epoch: 27 step: 871, loss is 0.02503177896142006\n",
      "epoch: 27 step: 872, loss is 0.04691074416041374\n",
      "epoch: 27 step: 873, loss is 8.481951954308897e-05\n",
      "epoch: 27 step: 874, loss is 0.0029041755478829145\n",
      "epoch: 27 step: 875, loss is 0.00923231989145279\n",
      "epoch: 27 step: 876, loss is 0.013521740213036537\n",
      "epoch: 27 step: 877, loss is 0.014909888617694378\n",
      "epoch: 27 step: 878, loss is 0.06298231333494186\n",
      "epoch: 27 step: 879, loss is 0.0008404699619859457\n",
      "epoch: 27 step: 880, loss is 0.022784488275647163\n",
      "epoch: 27 step: 881, loss is 0.03272741660475731\n",
      "epoch: 27 step: 882, loss is 0.01128905639052391\n",
      "epoch: 27 step: 883, loss is 0.0004398644086904824\n",
      "epoch: 27 step: 884, loss is 0.0019746595062315464\n",
      "epoch: 27 step: 885, loss is 0.001636199769563973\n",
      "epoch: 27 step: 886, loss is 0.03974959999322891\n",
      "epoch: 27 step: 887, loss is 0.004146779887378216\n",
      "epoch: 27 step: 888, loss is 0.04788219928741455\n",
      "epoch: 27 step: 889, loss is 0.01352384127676487\n",
      "epoch: 27 step: 890, loss is 0.0033718834165483713\n",
      "epoch: 27 step: 891, loss is 0.00014001497766003013\n",
      "epoch: 27 step: 892, loss is 0.010092519223690033\n",
      "epoch: 27 step: 893, loss is 0.0008940465049818158\n",
      "epoch: 27 step: 894, loss is 0.06747487932443619\n",
      "epoch: 27 step: 895, loss is 0.0008895039791241288\n",
      "epoch: 27 step: 896, loss is 0.03541596606373787\n",
      "epoch: 27 step: 897, loss is 0.003130694618448615\n",
      "epoch: 27 step: 898, loss is 0.0012351790210232139\n",
      "epoch: 27 step: 899, loss is 0.06085348129272461\n",
      "epoch: 27 step: 900, loss is 0.016221700236201286\n",
      "epoch: 27 step: 901, loss is 0.10821215808391571\n",
      "epoch: 27 step: 902, loss is 0.022051146253943443\n",
      "epoch: 27 step: 903, loss is 0.012815198861062527\n",
      "epoch: 27 step: 904, loss is 0.0162894856184721\n",
      "epoch: 27 step: 905, loss is 0.031337834894657135\n",
      "epoch: 27 step: 906, loss is 0.003169643459841609\n",
      "epoch: 27 step: 907, loss is 0.008008734323084354\n",
      "epoch: 27 step: 908, loss is 0.001174216391518712\n",
      "epoch: 27 step: 909, loss is 0.03368893638253212\n",
      "epoch: 27 step: 910, loss is 0.005661088973283768\n",
      "epoch: 27 step: 911, loss is 0.01361625362187624\n",
      "epoch: 27 step: 912, loss is 0.0011270316317677498\n",
      "epoch: 27 step: 913, loss is 0.0024829800240695477\n",
      "epoch: 27 step: 914, loss is 0.007113478146493435\n",
      "epoch: 27 step: 915, loss is 0.05991462990641594\n",
      "epoch: 27 step: 916, loss is 0.001802854472771287\n",
      "epoch: 27 step: 917, loss is 0.001544525264762342\n",
      "epoch: 27 step: 918, loss is 0.0010840746108442545\n",
      "epoch: 27 step: 919, loss is 0.006859700195491314\n",
      "epoch: 27 step: 920, loss is 0.014664198271930218\n",
      "epoch: 27 step: 921, loss is 0.005917734000831842\n",
      "epoch: 27 step: 922, loss is 0.015265853144228458\n",
      "epoch: 27 step: 923, loss is 0.0025546399410814047\n",
      "epoch: 27 step: 924, loss is 0.005038515664637089\n",
      "epoch: 27 step: 925, loss is 0.0030317294877022505\n",
      "epoch: 27 step: 926, loss is 0.00023510484606958926\n",
      "epoch: 27 step: 927, loss is 0.012714243493974209\n",
      "epoch: 27 step: 928, loss is 0.024732407182455063\n",
      "epoch: 27 step: 929, loss is 0.04931940883398056\n",
      "epoch: 27 step: 930, loss is 0.002169880084693432\n",
      "epoch: 27 step: 931, loss is 0.0012378800893202424\n",
      "epoch: 27 step: 932, loss is 0.0002185357006965205\n",
      "epoch: 27 step: 933, loss is 0.04428935423493385\n",
      "epoch: 27 step: 934, loss is 0.010600784793496132\n",
      "epoch: 27 step: 935, loss is 0.049752458930015564\n",
      "epoch: 27 step: 936, loss is 0.052068281918764114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 937, loss is 0.19111144542694092\n",
      "epoch: 28 step: 1, loss is 0.0025715527590364218\n",
      "epoch: 28 step: 2, loss is 0.004332168959081173\n",
      "epoch: 28 step: 3, loss is 0.07186321169137955\n",
      "epoch: 28 step: 4, loss is 0.0019130005966871977\n",
      "epoch: 28 step: 5, loss is 0.004499965813010931\n",
      "epoch: 28 step: 6, loss is 0.0011261340696364641\n",
      "epoch: 28 step: 7, loss is 0.0014809680869802833\n",
      "epoch: 28 step: 8, loss is 0.0017228811047971249\n",
      "epoch: 28 step: 9, loss is 0.0035785790532827377\n",
      "epoch: 28 step: 10, loss is 0.003950991202145815\n",
      "epoch: 28 step: 11, loss is 0.0001305858459090814\n",
      "epoch: 28 step: 12, loss is 0.00025184109108522534\n",
      "epoch: 28 step: 13, loss is 0.0011288791429251432\n",
      "epoch: 28 step: 14, loss is 0.009940932504832745\n",
      "epoch: 28 step: 15, loss is 0.012496249750256538\n",
      "epoch: 28 step: 16, loss is 0.1580924540758133\n",
      "epoch: 28 step: 17, loss is 0.07141200453042984\n",
      "epoch: 28 step: 18, loss is 0.15295253694057465\n",
      "epoch: 28 step: 19, loss is 0.005915336310863495\n",
      "epoch: 28 step: 20, loss is 0.03011482022702694\n",
      "epoch: 28 step: 21, loss is 0.07380479574203491\n",
      "epoch: 28 step: 22, loss is 0.01562457624822855\n",
      "epoch: 28 step: 23, loss is 0.023269465193152428\n",
      "epoch: 28 step: 24, loss is 0.03730813413858414\n",
      "epoch: 28 step: 25, loss is 0.22073520720005035\n",
      "epoch: 28 step: 26, loss is 0.002112276153638959\n",
      "epoch: 28 step: 27, loss is 0.0028680923860520124\n",
      "epoch: 28 step: 28, loss is 0.010244514793157578\n",
      "epoch: 28 step: 29, loss is 0.017884107306599617\n",
      "epoch: 28 step: 30, loss is 0.0031058387830853462\n",
      "epoch: 28 step: 31, loss is 0.0007942469674162567\n",
      "epoch: 28 step: 32, loss is 0.0006730047753080726\n",
      "epoch: 28 step: 33, loss is 0.0009997240267693996\n",
      "epoch: 28 step: 34, loss is 0.00039424782153218985\n",
      "epoch: 28 step: 35, loss is 0.08319790661334991\n",
      "epoch: 28 step: 36, loss is 0.002180584240704775\n",
      "epoch: 28 step: 37, loss is 0.004976620897650719\n",
      "epoch: 28 step: 38, loss is 0.03019355982542038\n",
      "epoch: 28 step: 39, loss is 0.03499935194849968\n",
      "epoch: 28 step: 40, loss is 0.03487294912338257\n",
      "epoch: 28 step: 41, loss is 0.012674838304519653\n",
      "epoch: 28 step: 42, loss is 0.0012556592701002955\n",
      "epoch: 28 step: 43, loss is 0.008734175004065037\n",
      "epoch: 28 step: 44, loss is 0.03439776971936226\n",
      "epoch: 28 step: 45, loss is 0.00823855958878994\n",
      "epoch: 28 step: 46, loss is 0.01816103234887123\n",
      "epoch: 28 step: 47, loss is 0.0015313357580453157\n",
      "epoch: 28 step: 48, loss is 0.0039208740927278996\n",
      "epoch: 28 step: 49, loss is 0.01606639474630356\n",
      "epoch: 28 step: 50, loss is 5.691662590834312e-05\n",
      "epoch: 28 step: 51, loss is 0.009925290942192078\n",
      "epoch: 28 step: 52, loss is 0.0004998042713850737\n",
      "epoch: 28 step: 53, loss is 0.08049085736274719\n",
      "epoch: 28 step: 54, loss is 0.0009419634006917477\n",
      "epoch: 28 step: 55, loss is 0.04296744242310524\n",
      "epoch: 28 step: 56, loss is 0.0018869641935452819\n",
      "epoch: 28 step: 57, loss is 0.0016330332728102803\n",
      "epoch: 28 step: 58, loss is 0.0006402008002623916\n",
      "epoch: 28 step: 59, loss is 0.015526202507317066\n",
      "epoch: 28 step: 60, loss is 0.01442642230540514\n",
      "epoch: 28 step: 61, loss is 0.00887721125036478\n",
      "epoch: 28 step: 62, loss is 0.0012812360655516386\n",
      "epoch: 28 step: 63, loss is 0.07893107086420059\n",
      "epoch: 28 step: 64, loss is 0.005116364452987909\n",
      "epoch: 28 step: 65, loss is 0.0024803767446428537\n",
      "epoch: 28 step: 66, loss is 0.014331704005599022\n",
      "epoch: 28 step: 67, loss is 0.00011997019464615732\n",
      "epoch: 28 step: 68, loss is 0.0032698214054107666\n",
      "epoch: 28 step: 69, loss is 0.0015478244749829173\n",
      "epoch: 28 step: 70, loss is 0.006734125316143036\n",
      "epoch: 28 step: 71, loss is 0.0024062772281467915\n",
      "epoch: 28 step: 72, loss is 0.01312244962900877\n",
      "epoch: 28 step: 73, loss is 0.002530594589188695\n",
      "epoch: 28 step: 74, loss is 0.003725785994902253\n",
      "epoch: 28 step: 75, loss is 0.009777306579053402\n",
      "epoch: 28 step: 76, loss is 0.007684182841330767\n",
      "epoch: 28 step: 77, loss is 0.00030019209953024983\n",
      "epoch: 28 step: 78, loss is 0.003084377618506551\n",
      "epoch: 28 step: 79, loss is 0.00011518569226609543\n",
      "epoch: 28 step: 80, loss is 0.00039065018063411117\n",
      "epoch: 28 step: 81, loss is 0.0019165028352290392\n",
      "epoch: 28 step: 82, loss is 0.003942030481994152\n",
      "epoch: 28 step: 83, loss is 0.001138548948802054\n",
      "epoch: 28 step: 84, loss is 0.002144417492672801\n",
      "epoch: 28 step: 85, loss is 0.0004335207340773195\n",
      "epoch: 28 step: 86, loss is 0.00036883368738926947\n",
      "epoch: 28 step: 87, loss is 0.0020338979084044695\n",
      "epoch: 28 step: 88, loss is 0.00036544608883559704\n",
      "epoch: 28 step: 89, loss is 0.0001585834688739851\n",
      "epoch: 28 step: 90, loss is 0.017148219048976898\n",
      "epoch: 28 step: 91, loss is 0.003972959704697132\n",
      "epoch: 28 step: 92, loss is 0.1173778846859932\n",
      "epoch: 28 step: 93, loss is 0.0035107568837702274\n",
      "epoch: 28 step: 94, loss is 1.9601679014158435e-05\n",
      "epoch: 28 step: 95, loss is 0.014207967557013035\n",
      "epoch: 28 step: 96, loss is 0.025628026574850082\n",
      "epoch: 28 step: 97, loss is 0.0009747829753905535\n",
      "epoch: 28 step: 98, loss is 0.00047825148794800043\n",
      "epoch: 28 step: 99, loss is 0.0020110176410526037\n",
      "epoch: 28 step: 100, loss is 0.07073132693767548\n",
      "epoch: 28 step: 101, loss is 0.0029273212421685457\n",
      "epoch: 28 step: 102, loss is 0.008792351000010967\n",
      "epoch: 28 step: 103, loss is 0.00620598578825593\n",
      "epoch: 28 step: 104, loss is 0.0007809117087163031\n",
      "epoch: 28 step: 105, loss is 0.0017100939294323325\n",
      "epoch: 28 step: 106, loss is 0.00043655611807480454\n",
      "epoch: 28 step: 107, loss is 0.0031404446344822645\n",
      "epoch: 28 step: 108, loss is 0.01191319152712822\n",
      "epoch: 28 step: 109, loss is 0.03363823890686035\n",
      "epoch: 28 step: 110, loss is 0.0011220404412597418\n",
      "epoch: 28 step: 111, loss is 0.06976185739040375\n",
      "epoch: 28 step: 112, loss is 0.0058243898674845695\n",
      "epoch: 28 step: 113, loss is 0.0003306295839138329\n",
      "epoch: 28 step: 114, loss is 0.008376033045351505\n",
      "epoch: 28 step: 115, loss is 0.000554658705368638\n",
      "epoch: 28 step: 116, loss is 1.2808841347577982e-05\n",
      "epoch: 28 step: 117, loss is 0.03350186347961426\n",
      "epoch: 28 step: 118, loss is 0.013716349378228188\n",
      "epoch: 28 step: 119, loss is 0.0921218991279602\n",
      "epoch: 28 step: 120, loss is 0.050862230360507965\n",
      "epoch: 28 step: 121, loss is 0.043411072343587875\n",
      "epoch: 28 step: 122, loss is 0.001078097615391016\n",
      "epoch: 28 step: 123, loss is 0.0021251116413623095\n",
      "epoch: 28 step: 124, loss is 0.0003227749839425087\n",
      "epoch: 28 step: 125, loss is 0.011401272378861904\n",
      "epoch: 28 step: 126, loss is 0.0002459032984916121\n",
      "epoch: 28 step: 127, loss is 0.001149535644799471\n",
      "epoch: 28 step: 128, loss is 0.0034140951465815306\n",
      "epoch: 28 step: 129, loss is 0.011796880513429642\n",
      "epoch: 28 step: 130, loss is 0.01163980457931757\n",
      "epoch: 28 step: 131, loss is 0.011274519376456738\n",
      "epoch: 28 step: 132, loss is 0.003415151033550501\n",
      "epoch: 28 step: 133, loss is 0.16068649291992188\n",
      "epoch: 28 step: 134, loss is 0.01249756384640932\n",
      "epoch: 28 step: 135, loss is 0.0009067064383998513\n",
      "epoch: 28 step: 136, loss is 0.0024022115394473076\n",
      "epoch: 28 step: 137, loss is 0.009799541905522346\n",
      "epoch: 28 step: 138, loss is 0.026357406750321388\n",
      "epoch: 28 step: 139, loss is 0.012228415347635746\n",
      "epoch: 28 step: 140, loss is 0.008666005916893482\n",
      "epoch: 28 step: 141, loss is 0.06411547213792801\n",
      "epoch: 28 step: 142, loss is 0.01680987887084484\n",
      "epoch: 28 step: 143, loss is 0.0015617695171386003\n",
      "epoch: 28 step: 144, loss is 0.005458427127450705\n",
      "epoch: 28 step: 145, loss is 0.007824623957276344\n",
      "epoch: 28 step: 146, loss is 0.022918572649359703\n",
      "epoch: 28 step: 147, loss is 0.0007428056560456753\n",
      "epoch: 28 step: 148, loss is 0.00534816924482584\n",
      "epoch: 28 step: 149, loss is 0.10128264129161835\n",
      "epoch: 28 step: 150, loss is 0.0003088224621023983\n",
      "epoch: 28 step: 151, loss is 0.007788701914250851\n",
      "epoch: 28 step: 152, loss is 0.039858780801296234\n",
      "epoch: 28 step: 153, loss is 0.044592857360839844\n",
      "epoch: 28 step: 154, loss is 0.00023841619258746505\n",
      "epoch: 28 step: 155, loss is 0.0008399366633966565\n",
      "epoch: 28 step: 156, loss is 0.0014967937022447586\n",
      "epoch: 28 step: 157, loss is 0.006476612761616707\n",
      "epoch: 28 step: 158, loss is 9.12193936528638e-05\n",
      "epoch: 28 step: 159, loss is 0.06646937131881714\n",
      "epoch: 28 step: 160, loss is 0.006265767849981785\n",
      "epoch: 28 step: 161, loss is 0.1772211194038391\n",
      "epoch: 28 step: 162, loss is 0.0006647309055551887\n",
      "epoch: 28 step: 163, loss is 0.0001812219270505011\n",
      "epoch: 28 step: 164, loss is 0.009311791509389877\n",
      "epoch: 28 step: 165, loss is 0.05253750458359718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 166, loss is 0.036838289350271225\n",
      "epoch: 28 step: 167, loss is 0.002647633897140622\n",
      "epoch: 28 step: 168, loss is 0.0031538333278149366\n",
      "epoch: 28 step: 169, loss is 0.004733161069452763\n",
      "epoch: 28 step: 170, loss is 0.00012365948350634426\n",
      "epoch: 28 step: 171, loss is 0.10818469524383545\n",
      "epoch: 28 step: 172, loss is 0.00039030940388329327\n",
      "epoch: 28 step: 173, loss is 0.00015439855633303523\n",
      "epoch: 28 step: 174, loss is 0.0009478338761255145\n",
      "epoch: 28 step: 175, loss is 0.002025329042226076\n",
      "epoch: 28 step: 176, loss is 0.031101446598768234\n",
      "epoch: 28 step: 177, loss is 0.012919665314257145\n",
      "epoch: 28 step: 178, loss is 0.00042634247802197933\n",
      "epoch: 28 step: 179, loss is 0.013822141103446484\n",
      "epoch: 28 step: 180, loss is 0.0005202590837143362\n",
      "epoch: 28 step: 181, loss is 0.017834624275565147\n",
      "epoch: 28 step: 182, loss is 0.0035195411182940006\n",
      "epoch: 28 step: 183, loss is 0.0008105147280730307\n",
      "epoch: 28 step: 184, loss is 0.00022590692969970405\n",
      "epoch: 28 step: 185, loss is 0.0010758916614577174\n",
      "epoch: 28 step: 186, loss is 0.011918161995708942\n",
      "epoch: 28 step: 187, loss is 0.0022973830346018076\n",
      "epoch: 28 step: 188, loss is 0.015818782150745392\n",
      "epoch: 28 step: 189, loss is 0.00046145913074724376\n",
      "epoch: 28 step: 190, loss is 0.0187589880079031\n",
      "epoch: 28 step: 191, loss is 0.0311852116137743\n",
      "epoch: 28 step: 192, loss is 0.07494651526212692\n",
      "epoch: 28 step: 193, loss is 0.00503606116399169\n",
      "epoch: 28 step: 194, loss is 0.027894703671336174\n",
      "epoch: 28 step: 195, loss is 0.0014821894001215696\n",
      "epoch: 28 step: 196, loss is 0.0007066904217936099\n",
      "epoch: 28 step: 197, loss is 0.0013039105106145144\n",
      "epoch: 28 step: 198, loss is 0.00666886055842042\n",
      "epoch: 28 step: 199, loss is 0.030004430562257767\n",
      "epoch: 28 step: 200, loss is 0.0014119987608864903\n",
      "epoch: 28 step: 201, loss is 0.0022558083292096853\n",
      "epoch: 28 step: 202, loss is 0.0017955326475203037\n",
      "epoch: 28 step: 203, loss is 0.00032930393354035914\n",
      "epoch: 28 step: 204, loss is 0.0010475276503711939\n",
      "epoch: 28 step: 205, loss is 0.002955223200842738\n",
      "epoch: 28 step: 206, loss is 0.0011203192407265306\n",
      "epoch: 28 step: 207, loss is 0.0012213466688990593\n",
      "epoch: 28 step: 208, loss is 0.001165271271020174\n",
      "epoch: 28 step: 209, loss is 0.04476652294397354\n",
      "epoch: 28 step: 210, loss is 0.016509654000401497\n",
      "epoch: 28 step: 211, loss is 0.01313813403248787\n",
      "epoch: 28 step: 212, loss is 0.015730978921055794\n",
      "epoch: 28 step: 213, loss is 0.004840731155127287\n",
      "epoch: 28 step: 214, loss is 0.03711036220192909\n",
      "epoch: 28 step: 215, loss is 0.0005981361609883606\n",
      "epoch: 28 step: 216, loss is 0.0215194970369339\n",
      "epoch: 28 step: 217, loss is 0.0011743954382836819\n",
      "epoch: 28 step: 218, loss is 0.0007631509797647595\n",
      "epoch: 28 step: 219, loss is 0.03260118141770363\n",
      "epoch: 28 step: 220, loss is 0.012293488718569279\n",
      "epoch: 28 step: 221, loss is 0.003009380307048559\n",
      "epoch: 28 step: 222, loss is 0.0009692170424386859\n",
      "epoch: 28 step: 223, loss is 0.053521472960710526\n",
      "epoch: 28 step: 224, loss is 0.0011219691950827837\n",
      "epoch: 28 step: 225, loss is 0.00038830458652228117\n",
      "epoch: 28 step: 226, loss is 0.008030884899199009\n",
      "epoch: 28 step: 227, loss is 0.0075386399403214455\n",
      "epoch: 28 step: 228, loss is 0.09799958020448685\n",
      "epoch: 28 step: 229, loss is 0.00021891540382057428\n",
      "epoch: 28 step: 230, loss is 0.01505284383893013\n",
      "epoch: 28 step: 231, loss is 0.019328327849507332\n",
      "epoch: 28 step: 232, loss is 0.024461369961500168\n",
      "epoch: 28 step: 233, loss is 0.004928207024931908\n",
      "epoch: 28 step: 234, loss is 0.0056508323177695274\n",
      "epoch: 28 step: 235, loss is 0.02526004984974861\n",
      "epoch: 28 step: 236, loss is 0.008318756707012653\n",
      "epoch: 28 step: 237, loss is 0.0022837871219962835\n",
      "epoch: 28 step: 238, loss is 0.0007447793614119291\n",
      "epoch: 28 step: 239, loss is 0.0029425015673041344\n",
      "epoch: 28 step: 240, loss is 0.0008735061855986714\n",
      "epoch: 28 step: 241, loss is 0.002112360903993249\n",
      "epoch: 28 step: 242, loss is 0.003082259325310588\n",
      "epoch: 28 step: 243, loss is 0.003988808486610651\n",
      "epoch: 28 step: 244, loss is 0.0029725031927227974\n",
      "epoch: 28 step: 245, loss is 0.0010263249278068542\n",
      "epoch: 28 step: 246, loss is 0.0008510567713528872\n",
      "epoch: 28 step: 247, loss is 0.04666706547141075\n",
      "epoch: 28 step: 248, loss is 0.05606497824192047\n",
      "epoch: 28 step: 249, loss is 0.0011231194948777556\n",
      "epoch: 28 step: 250, loss is 0.05992293730378151\n",
      "epoch: 28 step: 251, loss is 0.08939683437347412\n",
      "epoch: 28 step: 252, loss is 0.010474322363734245\n",
      "epoch: 28 step: 253, loss is 0.03781565651297569\n",
      "epoch: 28 step: 254, loss is 0.06000060588121414\n",
      "epoch: 28 step: 255, loss is 0.00015962448378559202\n",
      "epoch: 28 step: 256, loss is 0.008454782888293266\n",
      "epoch: 28 step: 257, loss is 0.0003865826001856476\n",
      "epoch: 28 step: 258, loss is 0.015062361024320126\n",
      "epoch: 28 step: 259, loss is 0.0003711886238306761\n",
      "epoch: 28 step: 260, loss is 0.014463616535067558\n",
      "epoch: 28 step: 261, loss is 0.0009904982289299369\n",
      "epoch: 28 step: 262, loss is 0.0015258226776495576\n",
      "epoch: 28 step: 263, loss is 0.0006555130239576101\n",
      "epoch: 28 step: 264, loss is 0.007557468954473734\n",
      "epoch: 28 step: 265, loss is 0.0021644169464707375\n",
      "epoch: 28 step: 266, loss is 0.0025016283616423607\n",
      "epoch: 28 step: 267, loss is 0.06629982590675354\n",
      "epoch: 28 step: 268, loss is 0.037791527807712555\n",
      "epoch: 28 step: 269, loss is 0.004287331365048885\n",
      "epoch: 28 step: 270, loss is 0.0017090264009311795\n",
      "epoch: 28 step: 271, loss is 0.06379204988479614\n",
      "epoch: 28 step: 272, loss is 0.018696879968047142\n",
      "epoch: 28 step: 273, loss is 0.008750027976930141\n",
      "epoch: 28 step: 274, loss is 0.008357240818440914\n",
      "epoch: 28 step: 275, loss is 0.0019379797158762813\n",
      "epoch: 28 step: 276, loss is 0.027366146445274353\n",
      "epoch: 28 step: 277, loss is 0.002713038818910718\n",
      "epoch: 28 step: 278, loss is 0.012549137696623802\n",
      "epoch: 28 step: 279, loss is 0.004602422006428242\n",
      "epoch: 28 step: 280, loss is 0.0005533790099434555\n",
      "epoch: 28 step: 281, loss is 0.0010417996672913432\n",
      "epoch: 28 step: 282, loss is 0.00015936057025101036\n",
      "epoch: 28 step: 283, loss is 0.05860046669840813\n",
      "epoch: 28 step: 284, loss is 0.07404246181249619\n",
      "epoch: 28 step: 285, loss is 0.0008553177467547357\n",
      "epoch: 28 step: 286, loss is 0.002941478742286563\n",
      "epoch: 28 step: 287, loss is 0.03604930639266968\n",
      "epoch: 28 step: 288, loss is 0.0023208518978208303\n",
      "epoch: 28 step: 289, loss is 0.0006649038405157626\n",
      "epoch: 28 step: 290, loss is 0.0018778134835883975\n",
      "epoch: 28 step: 291, loss is 0.09994244575500488\n",
      "epoch: 28 step: 292, loss is 0.0005126363830640912\n",
      "epoch: 28 step: 293, loss is 0.00440461840480566\n",
      "epoch: 28 step: 294, loss is 0.002756282454356551\n",
      "epoch: 28 step: 295, loss is 0.0014792003203183413\n",
      "epoch: 28 step: 296, loss is 0.04851403459906578\n",
      "epoch: 28 step: 297, loss is 0.001821118756197393\n",
      "epoch: 28 step: 298, loss is 0.002378973178565502\n",
      "epoch: 28 step: 299, loss is 0.001922529423609376\n",
      "epoch: 28 step: 300, loss is 0.0072966995649039745\n",
      "epoch: 28 step: 301, loss is 0.002762451535090804\n",
      "epoch: 28 step: 302, loss is 0.0008249914390034974\n",
      "epoch: 28 step: 303, loss is 0.0010137702338397503\n",
      "epoch: 28 step: 304, loss is 0.010545743629336357\n",
      "epoch: 28 step: 305, loss is 0.0034526041708886623\n",
      "epoch: 28 step: 306, loss is 0.010435272939503193\n",
      "epoch: 28 step: 307, loss is 0.0029793798457831144\n",
      "epoch: 28 step: 308, loss is 0.032563284039497375\n",
      "epoch: 28 step: 309, loss is 0.025684881955385208\n",
      "epoch: 28 step: 310, loss is 0.0007717397529631853\n",
      "epoch: 28 step: 311, loss is 0.008219379000365734\n",
      "epoch: 28 step: 312, loss is 0.0019643374253064394\n",
      "epoch: 28 step: 313, loss is 0.0028716777451336384\n",
      "epoch: 28 step: 314, loss is 0.0015269073192030191\n",
      "epoch: 28 step: 315, loss is 0.00019080789934378117\n",
      "epoch: 28 step: 316, loss is 0.0007902310462668538\n",
      "epoch: 28 step: 317, loss is 0.0017818453488871455\n",
      "epoch: 28 step: 318, loss is 0.05015904828906059\n",
      "epoch: 28 step: 319, loss is 0.0031832619570195675\n",
      "epoch: 28 step: 320, loss is 0.001734405755996704\n",
      "epoch: 28 step: 321, loss is 0.026968207210302353\n",
      "epoch: 28 step: 322, loss is 0.056809838861227036\n",
      "epoch: 28 step: 323, loss is 0.015109198167920113\n",
      "epoch: 28 step: 324, loss is 0.0057941945269703865\n",
      "epoch: 28 step: 325, loss is 0.0007928081904537976\n",
      "epoch: 28 step: 326, loss is 0.0015033786185085773\n",
      "epoch: 28 step: 327, loss is 0.0032439534552395344\n",
      "epoch: 28 step: 328, loss is 0.05167010426521301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 329, loss is 0.0004195994697511196\n",
      "epoch: 28 step: 330, loss is 1.7912458133650944e-05\n",
      "epoch: 28 step: 331, loss is 0.04218411445617676\n",
      "epoch: 28 step: 332, loss is 0.0036628686357289553\n",
      "epoch: 28 step: 333, loss is 0.007292259018868208\n",
      "epoch: 28 step: 334, loss is 0.0028317973483353853\n",
      "epoch: 28 step: 335, loss is 0.00015459868882317096\n",
      "epoch: 28 step: 336, loss is 0.0008707986562512815\n",
      "epoch: 28 step: 337, loss is 0.008913232013583183\n",
      "epoch: 28 step: 338, loss is 0.0005348210106603801\n",
      "epoch: 28 step: 339, loss is 0.1338367909193039\n",
      "epoch: 28 step: 340, loss is 0.013956040143966675\n",
      "epoch: 28 step: 341, loss is 0.0008496309164911509\n",
      "epoch: 28 step: 342, loss is 4.6440214646281675e-05\n",
      "epoch: 28 step: 343, loss is 0.004256695508956909\n",
      "epoch: 28 step: 344, loss is 0.000693121284712106\n",
      "epoch: 28 step: 345, loss is 0.010197659954428673\n",
      "epoch: 28 step: 346, loss is 0.05638091638684273\n",
      "epoch: 28 step: 347, loss is 0.0033212616108357906\n",
      "epoch: 28 step: 348, loss is 0.0532112792134285\n",
      "epoch: 28 step: 349, loss is 0.002411206951364875\n",
      "epoch: 28 step: 350, loss is 0.00013022460916545242\n",
      "epoch: 28 step: 351, loss is 0.001491181435994804\n",
      "epoch: 28 step: 352, loss is 7.497284968849272e-05\n",
      "epoch: 28 step: 353, loss is 0.004020851105451584\n",
      "epoch: 28 step: 354, loss is 0.0045044803991913795\n",
      "epoch: 28 step: 355, loss is 4.282880399841815e-05\n",
      "epoch: 28 step: 356, loss is 0.003667109180241823\n",
      "epoch: 28 step: 357, loss is 0.0008228365331888199\n",
      "epoch: 28 step: 358, loss is 0.0021097897551953793\n",
      "epoch: 28 step: 359, loss is 2.3292242985917255e-05\n",
      "epoch: 28 step: 360, loss is 0.002791858511045575\n",
      "epoch: 28 step: 361, loss is 0.03808944299817085\n",
      "epoch: 28 step: 362, loss is 0.0024809124879539013\n",
      "epoch: 28 step: 363, loss is 0.09118898957967758\n",
      "epoch: 28 step: 364, loss is 0.008028960786759853\n",
      "epoch: 28 step: 365, loss is 0.0019889650866389275\n",
      "epoch: 28 step: 366, loss is 0.0011904819402843714\n",
      "epoch: 28 step: 367, loss is 0.0028685673605650663\n",
      "epoch: 28 step: 368, loss is 0.042296141386032104\n",
      "epoch: 28 step: 369, loss is 0.025056898593902588\n",
      "epoch: 28 step: 370, loss is 0.011813298799097538\n",
      "epoch: 28 step: 371, loss is 0.0005229477537795901\n",
      "epoch: 28 step: 372, loss is 8.556991815567017e-05\n",
      "epoch: 28 step: 373, loss is 0.019767124205827713\n",
      "epoch: 28 step: 374, loss is 0.005961972754448652\n",
      "epoch: 28 step: 375, loss is 0.009505974128842354\n",
      "epoch: 28 step: 376, loss is 0.0030028794426470995\n",
      "epoch: 28 step: 377, loss is 0.00016769679496064782\n",
      "epoch: 28 step: 378, loss is 0.002045299159362912\n",
      "epoch: 28 step: 379, loss is 0.0011255060089752078\n",
      "epoch: 28 step: 380, loss is 0.0020262771286070347\n",
      "epoch: 28 step: 381, loss is 0.0061103045009076595\n",
      "epoch: 28 step: 382, loss is 0.0005673909327015281\n",
      "epoch: 28 step: 383, loss is 0.027372948825359344\n",
      "epoch: 28 step: 384, loss is 0.0008478667587041855\n",
      "epoch: 28 step: 385, loss is 0.0097446758300066\n",
      "epoch: 28 step: 386, loss is 0.010415058583021164\n",
      "epoch: 28 step: 387, loss is 0.00512413214892149\n",
      "epoch: 28 step: 388, loss is 0.000359253870556131\n",
      "epoch: 28 step: 389, loss is 4.06714498240035e-05\n",
      "epoch: 28 step: 390, loss is 0.031071795150637627\n",
      "epoch: 28 step: 391, loss is 0.0006934660486876965\n",
      "epoch: 28 step: 392, loss is 0.0028455867432057858\n",
      "epoch: 28 step: 393, loss is 0.03475804999470711\n",
      "epoch: 28 step: 394, loss is 4.7196623199852183e-05\n",
      "epoch: 28 step: 395, loss is 0.08013739436864853\n",
      "epoch: 28 step: 396, loss is 0.014458470046520233\n",
      "epoch: 28 step: 397, loss is 1.8353988707531244e-05\n",
      "epoch: 28 step: 398, loss is 0.01600368693470955\n",
      "epoch: 28 step: 399, loss is 0.058714136481285095\n",
      "epoch: 28 step: 400, loss is 0.003782935906201601\n",
      "epoch: 28 step: 401, loss is 2.8007805667584762e-05\n",
      "epoch: 28 step: 402, loss is 0.0008235746645368636\n",
      "epoch: 28 step: 403, loss is 0.0031098343897610903\n",
      "epoch: 28 step: 404, loss is 0.0007236128440126777\n",
      "epoch: 28 step: 405, loss is 0.016695929691195488\n",
      "epoch: 28 step: 406, loss is 0.016903115436434746\n",
      "epoch: 28 step: 407, loss is 0.007782545872032642\n",
      "epoch: 28 step: 408, loss is 0.009177758358418941\n",
      "epoch: 28 step: 409, loss is 0.011608833447098732\n",
      "epoch: 28 step: 410, loss is 0.03265578672289848\n",
      "epoch: 28 step: 411, loss is 0.0009289279114454985\n",
      "epoch: 28 step: 412, loss is 0.0608551912009716\n",
      "epoch: 28 step: 413, loss is 0.0005547687178477645\n",
      "epoch: 28 step: 414, loss is 0.0005414250772446394\n",
      "epoch: 28 step: 415, loss is 0.0003502473991829902\n",
      "epoch: 28 step: 416, loss is 0.007864417508244514\n",
      "epoch: 28 step: 417, loss is 0.023973971605300903\n",
      "epoch: 28 step: 418, loss is 0.007167240604758263\n",
      "epoch: 28 step: 419, loss is 0.012295858934521675\n",
      "epoch: 28 step: 420, loss is 0.00030017667450010777\n",
      "epoch: 28 step: 421, loss is 0.0024220955092459917\n",
      "epoch: 28 step: 422, loss is 0.00817247573286295\n",
      "epoch: 28 step: 423, loss is 0.0010872051352635026\n",
      "epoch: 28 step: 424, loss is 0.0017733316635712981\n",
      "epoch: 28 step: 425, loss is 0.0027724874671548605\n",
      "epoch: 28 step: 426, loss is 0.058385755866765976\n",
      "epoch: 28 step: 427, loss is 0.008680992759764194\n",
      "epoch: 28 step: 428, loss is 0.004949609749019146\n",
      "epoch: 28 step: 429, loss is 0.00021654492593370378\n",
      "epoch: 28 step: 430, loss is 0.02893177606165409\n",
      "epoch: 28 step: 431, loss is 0.006549586541950703\n",
      "epoch: 28 step: 432, loss is 0.006385316606611013\n",
      "epoch: 28 step: 433, loss is 0.022017376497387886\n",
      "epoch: 28 step: 434, loss is 0.005230372305959463\n",
      "epoch: 28 step: 435, loss is 0.03659200668334961\n",
      "epoch: 28 step: 436, loss is 0.038080211728811264\n",
      "epoch: 28 step: 437, loss is 7.37808077246882e-05\n",
      "epoch: 28 step: 438, loss is 0.004742600955069065\n",
      "epoch: 28 step: 439, loss is 0.0005392109742388129\n",
      "epoch: 28 step: 440, loss is 0.00176412018481642\n",
      "epoch: 28 step: 441, loss is 0.005935643799602985\n",
      "epoch: 28 step: 442, loss is 0.0012728996807709336\n",
      "epoch: 28 step: 443, loss is 0.004226174205541611\n",
      "epoch: 28 step: 444, loss is 0.003977847285568714\n",
      "epoch: 28 step: 445, loss is 0.007438838481903076\n",
      "epoch: 28 step: 446, loss is 0.027383018285036087\n",
      "epoch: 28 step: 447, loss is 0.0036932353395968676\n",
      "epoch: 28 step: 448, loss is 0.0001792358816601336\n",
      "epoch: 28 step: 449, loss is 0.021469535306096077\n",
      "epoch: 28 step: 450, loss is 0.11113061010837555\n",
      "epoch: 28 step: 451, loss is 0.008684891276061535\n",
      "epoch: 28 step: 452, loss is 0.0012940161395817995\n",
      "epoch: 28 step: 453, loss is 0.009465760551393032\n",
      "epoch: 28 step: 454, loss is 0.00019417155999690294\n",
      "epoch: 28 step: 455, loss is 0.00416964665055275\n",
      "epoch: 28 step: 456, loss is 0.010181177407503128\n",
      "epoch: 28 step: 457, loss is 0.001387252239510417\n",
      "epoch: 28 step: 458, loss is 0.00019761521252803504\n",
      "epoch: 28 step: 459, loss is 0.045267704874277115\n",
      "epoch: 28 step: 460, loss is 0.00024685179232619703\n",
      "epoch: 28 step: 461, loss is 0.0031204044353216887\n",
      "epoch: 28 step: 462, loss is 0.00016458865138702095\n",
      "epoch: 28 step: 463, loss is 8.088798495009542e-05\n",
      "epoch: 28 step: 464, loss is 0.004831630736589432\n",
      "epoch: 28 step: 465, loss is 0.0016795647097751498\n",
      "epoch: 28 step: 466, loss is 0.001558953314088285\n",
      "epoch: 28 step: 467, loss is 0.0014715506695210934\n",
      "epoch: 28 step: 468, loss is 0.009265432134270668\n",
      "epoch: 28 step: 469, loss is 0.0015215259045362473\n",
      "epoch: 28 step: 470, loss is 0.0001992212637560442\n",
      "epoch: 28 step: 471, loss is 0.03116949461400509\n",
      "epoch: 28 step: 472, loss is 0.00037670083111152053\n",
      "epoch: 28 step: 473, loss is 3.202908192179166e-05\n",
      "epoch: 28 step: 474, loss is 0.003994143567979336\n",
      "epoch: 28 step: 475, loss is 0.0017664179904386401\n",
      "epoch: 28 step: 476, loss is 0.010060467757284641\n",
      "epoch: 28 step: 477, loss is 0.0007865765364840627\n",
      "epoch: 28 step: 478, loss is 0.0009035641560330987\n",
      "epoch: 28 step: 479, loss is 0.0002688413951545954\n",
      "epoch: 28 step: 480, loss is 0.0024662078358232975\n",
      "epoch: 28 step: 481, loss is 6.0401056543923914e-05\n",
      "epoch: 28 step: 482, loss is 0.016852255910634995\n",
      "epoch: 28 step: 483, loss is 0.036212269216775894\n",
      "epoch: 28 step: 484, loss is 0.000750056526158005\n",
      "epoch: 28 step: 485, loss is 0.001029002945870161\n",
      "epoch: 28 step: 486, loss is 0.00402340292930603\n",
      "epoch: 28 step: 487, loss is 0.0013224930735304952\n",
      "epoch: 28 step: 488, loss is 0.0047575561329722404\n",
      "epoch: 28 step: 489, loss is 0.026739856228232384\n",
      "epoch: 28 step: 490, loss is 0.01100710965692997\n",
      "epoch: 28 step: 491, loss is 0.0024619416799396276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 492, loss is 0.010460651479661465\n",
      "epoch: 28 step: 493, loss is 0.024565428495407104\n",
      "epoch: 28 step: 494, loss is 0.0014490779722109437\n",
      "epoch: 28 step: 495, loss is 0.016583936288952827\n",
      "epoch: 28 step: 496, loss is 0.02050076238811016\n",
      "epoch: 28 step: 497, loss is 0.005428708158433437\n",
      "epoch: 28 step: 498, loss is 0.03978199139237404\n",
      "epoch: 28 step: 499, loss is 0.027643200010061264\n",
      "epoch: 28 step: 500, loss is 0.02180906943976879\n",
      "epoch: 28 step: 501, loss is 0.0005629921797662973\n",
      "epoch: 28 step: 502, loss is 0.04711359739303589\n",
      "epoch: 28 step: 503, loss is 0.00039604349876753986\n",
      "epoch: 28 step: 504, loss is 0.0019237747183069587\n",
      "epoch: 28 step: 505, loss is 0.010463048703968525\n",
      "epoch: 28 step: 506, loss is 0.001081689610145986\n",
      "epoch: 28 step: 507, loss is 0.000732491142116487\n",
      "epoch: 28 step: 508, loss is 0.01402110606431961\n",
      "epoch: 28 step: 509, loss is 0.0030497675761580467\n",
      "epoch: 28 step: 510, loss is 0.005995884537696838\n",
      "epoch: 28 step: 511, loss is 0.0009412154904566705\n",
      "epoch: 28 step: 512, loss is 0.06401533633470535\n",
      "epoch: 28 step: 513, loss is 0.0004036213213112205\n",
      "epoch: 28 step: 514, loss is 0.0038250740617513657\n",
      "epoch: 28 step: 515, loss is 1.657965185586363e-05\n",
      "epoch: 28 step: 516, loss is 0.00034946034429594874\n",
      "epoch: 28 step: 517, loss is 0.015256310813128948\n",
      "epoch: 28 step: 518, loss is 0.05092570185661316\n",
      "epoch: 28 step: 519, loss is 0.0014862281968817115\n",
      "epoch: 28 step: 520, loss is 0.0036112009547650814\n",
      "epoch: 28 step: 521, loss is 0.0001709657080937177\n",
      "epoch: 28 step: 522, loss is 0.1560724675655365\n",
      "epoch: 28 step: 523, loss is 0.0023412532173097134\n",
      "epoch: 28 step: 524, loss is 0.0009792540222406387\n",
      "epoch: 28 step: 525, loss is 0.006775648798793554\n",
      "epoch: 28 step: 526, loss is 0.00400518486276269\n",
      "epoch: 28 step: 527, loss is 0.00016453131684102118\n",
      "epoch: 28 step: 528, loss is 0.0027638222090899944\n",
      "epoch: 28 step: 529, loss is 0.0070165712386369705\n",
      "epoch: 28 step: 530, loss is 0.044821783900260925\n",
      "epoch: 28 step: 531, loss is 0.00017655486590228975\n",
      "epoch: 28 step: 532, loss is 0.0004535339248832315\n",
      "epoch: 28 step: 533, loss is 0.021558886393904686\n",
      "epoch: 28 step: 534, loss is 0.0007014977163635194\n",
      "epoch: 28 step: 535, loss is 0.0060466923750936985\n",
      "epoch: 28 step: 536, loss is 0.002204461023211479\n",
      "epoch: 28 step: 537, loss is 0.0015471155056729913\n",
      "epoch: 28 step: 538, loss is 0.053740743547677994\n",
      "epoch: 28 step: 539, loss is 5.153495294507593e-05\n",
      "epoch: 28 step: 540, loss is 0.001978493994101882\n",
      "epoch: 28 step: 541, loss is 0.006600011605769396\n",
      "epoch: 28 step: 542, loss is 0.0012940533924847841\n",
      "epoch: 28 step: 543, loss is 0.004337575286626816\n",
      "epoch: 28 step: 544, loss is 0.018154533579945564\n",
      "epoch: 28 step: 545, loss is 0.014097770676016808\n",
      "epoch: 28 step: 546, loss is 0.0035157669335603714\n",
      "epoch: 28 step: 547, loss is 0.0012447589542716742\n",
      "epoch: 28 step: 548, loss is 0.0016566287958994508\n",
      "epoch: 28 step: 549, loss is 0.015043560415506363\n",
      "epoch: 28 step: 550, loss is 0.0007771316450089216\n",
      "epoch: 28 step: 551, loss is 0.005366698373109102\n",
      "epoch: 28 step: 552, loss is 0.0001395009021507576\n",
      "epoch: 28 step: 553, loss is 0.0164696853607893\n",
      "epoch: 28 step: 554, loss is 0.010781710967421532\n",
      "epoch: 28 step: 555, loss is 0.038705382496118546\n",
      "epoch: 28 step: 556, loss is 0.018882445991039276\n",
      "epoch: 28 step: 557, loss is 0.06537255644798279\n",
      "epoch: 28 step: 558, loss is 0.0005925080622546375\n",
      "epoch: 28 step: 559, loss is 0.003027450991794467\n",
      "epoch: 28 step: 560, loss is 0.014017892070114613\n",
      "epoch: 28 step: 561, loss is 0.01674126647412777\n",
      "epoch: 28 step: 562, loss is 0.00011639401054708287\n",
      "epoch: 28 step: 563, loss is 0.017347615212202072\n",
      "epoch: 28 step: 564, loss is 0.002518641296774149\n",
      "epoch: 28 step: 565, loss is 0.0019443756900727749\n",
      "epoch: 28 step: 566, loss is 0.0012130403192713857\n",
      "epoch: 28 step: 567, loss is 0.061511825770139694\n",
      "epoch: 28 step: 568, loss is 0.0035970963072031736\n",
      "epoch: 28 step: 569, loss is 0.0008565300959162414\n",
      "epoch: 28 step: 570, loss is 0.031725939363241196\n",
      "epoch: 28 step: 571, loss is 0.056257374584674835\n",
      "epoch: 28 step: 572, loss is 0.01633353717625141\n",
      "epoch: 28 step: 573, loss is 0.026963340118527412\n",
      "epoch: 28 step: 574, loss is 0.1853594183921814\n",
      "epoch: 28 step: 575, loss is 0.01610867865383625\n",
      "epoch: 28 step: 576, loss is 0.001566014951094985\n",
      "epoch: 28 step: 577, loss is 0.021976515650749207\n",
      "epoch: 28 step: 578, loss is 0.03849438577890396\n",
      "epoch: 28 step: 579, loss is 0.010064278729259968\n",
      "epoch: 28 step: 580, loss is 0.033942751586437225\n",
      "epoch: 28 step: 581, loss is 0.007253522053360939\n",
      "epoch: 28 step: 582, loss is 6.893603858770803e-05\n",
      "epoch: 28 step: 583, loss is 0.00011887358414242044\n",
      "epoch: 28 step: 584, loss is 0.0261948574334383\n",
      "epoch: 28 step: 585, loss is 0.00010099697828991339\n",
      "epoch: 28 step: 586, loss is 0.00024268946435768157\n",
      "epoch: 28 step: 587, loss is 0.0017235185950994492\n",
      "epoch: 28 step: 588, loss is 0.00011124951561214402\n",
      "epoch: 28 step: 589, loss is 0.06592991948127747\n",
      "epoch: 28 step: 590, loss is 0.0006754273781552911\n",
      "epoch: 28 step: 591, loss is 0.002972604241222143\n",
      "epoch: 28 step: 592, loss is 0.0009414189262315631\n",
      "epoch: 28 step: 593, loss is 0.004077178426086903\n",
      "epoch: 28 step: 594, loss is 0.0055979653261601925\n",
      "epoch: 28 step: 595, loss is 0.005956338718533516\n",
      "epoch: 28 step: 596, loss is 0.0015013013035058975\n",
      "epoch: 28 step: 597, loss is 0.0026022924575954676\n",
      "epoch: 28 step: 598, loss is 0.0014456312637776136\n",
      "epoch: 28 step: 599, loss is 0.008232040330767632\n",
      "epoch: 28 step: 600, loss is 0.00507067097350955\n",
      "epoch: 28 step: 601, loss is 0.0027305532712489367\n",
      "epoch: 28 step: 602, loss is 0.02319631725549698\n",
      "epoch: 28 step: 603, loss is 0.0014672827674075961\n",
      "epoch: 28 step: 604, loss is 0.001388621050864458\n",
      "epoch: 28 step: 605, loss is 0.002576235681772232\n",
      "epoch: 28 step: 606, loss is 0.002586852991953492\n",
      "epoch: 28 step: 607, loss is 0.0022991823498159647\n",
      "epoch: 28 step: 608, loss is 0.0024741170927882195\n",
      "epoch: 28 step: 609, loss is 0.000960255041718483\n",
      "epoch: 28 step: 610, loss is 0.02162625454366207\n",
      "epoch: 28 step: 611, loss is 0.04704667627811432\n",
      "epoch: 28 step: 612, loss is 0.0018785942811518908\n",
      "epoch: 28 step: 613, loss is 0.0006221070070751011\n",
      "epoch: 28 step: 614, loss is 0.00035324774216860533\n",
      "epoch: 28 step: 615, loss is 0.014533033594489098\n",
      "epoch: 28 step: 616, loss is 8.244017226388678e-05\n",
      "epoch: 28 step: 617, loss is 0.000643489183858037\n",
      "epoch: 28 step: 618, loss is 0.020269760861992836\n",
      "epoch: 28 step: 619, loss is 0.02209467627108097\n",
      "epoch: 28 step: 620, loss is 0.005794834811240435\n",
      "epoch: 28 step: 621, loss is 0.007831444963812828\n",
      "epoch: 28 step: 622, loss is 0.00243968702852726\n",
      "epoch: 28 step: 623, loss is 0.0011624835897237062\n",
      "epoch: 28 step: 624, loss is 0.0014036648208275437\n",
      "epoch: 28 step: 625, loss is 0.0033014598302543163\n",
      "epoch: 28 step: 626, loss is 3.898265276802704e-05\n",
      "epoch: 28 step: 627, loss is 0.00150893977843225\n",
      "epoch: 28 step: 628, loss is 0.013228336349129677\n",
      "epoch: 28 step: 629, loss is 0.0046922299079597\n",
      "epoch: 28 step: 630, loss is 0.0002575449470896274\n",
      "epoch: 28 step: 631, loss is 0.0007443120121024549\n",
      "epoch: 28 step: 632, loss is 0.001024518278427422\n",
      "epoch: 28 step: 633, loss is 0.0005770364077761769\n",
      "epoch: 28 step: 634, loss is 0.0004072924784850329\n",
      "epoch: 28 step: 635, loss is 0.00016914754814933985\n",
      "epoch: 28 step: 636, loss is 0.002418069401755929\n",
      "epoch: 28 step: 637, loss is 0.00040513786370866\n",
      "epoch: 28 step: 638, loss is 0.00032221339643001556\n",
      "epoch: 28 step: 639, loss is 0.00022919684124644846\n",
      "epoch: 28 step: 640, loss is 0.0005785347311757505\n",
      "epoch: 28 step: 641, loss is 0.0010191788896918297\n",
      "epoch: 28 step: 642, loss is 0.0003915966080967337\n",
      "epoch: 28 step: 643, loss is 0.00014365058450493962\n",
      "epoch: 28 step: 644, loss is 0.0013243912253528833\n",
      "epoch: 28 step: 645, loss is 0.0029102240223437548\n",
      "epoch: 28 step: 646, loss is 0.034966450184583664\n",
      "epoch: 28 step: 647, loss is 0.00019316877296660095\n",
      "epoch: 28 step: 648, loss is 0.00026940766838379204\n",
      "epoch: 28 step: 649, loss is 0.0023472534958273172\n",
      "epoch: 28 step: 650, loss is 0.015444910153746605\n",
      "epoch: 28 step: 651, loss is 0.03690950199961662\n",
      "epoch: 28 step: 652, loss is 0.00605069799348712\n",
      "epoch: 28 step: 653, loss is 0.00032193565857596695\n",
      "epoch: 28 step: 654, loss is 0.010377645492553711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 655, loss is 0.00048454751959070563\n",
      "epoch: 28 step: 656, loss is 0.0010001726914197206\n",
      "epoch: 28 step: 657, loss is 0.0023903108667582273\n",
      "epoch: 28 step: 658, loss is 0.00040612692828290164\n",
      "epoch: 28 step: 659, loss is 0.004759017378091812\n",
      "epoch: 28 step: 660, loss is 0.009553235955536366\n",
      "epoch: 28 step: 661, loss is 0.0008098629186861217\n",
      "epoch: 28 step: 662, loss is 0.0005641324096359313\n",
      "epoch: 28 step: 663, loss is 0.013580580241978168\n",
      "epoch: 28 step: 664, loss is 0.017328601330518723\n",
      "epoch: 28 step: 665, loss is 0.01694285310804844\n",
      "epoch: 28 step: 666, loss is 0.0005939074326306581\n",
      "epoch: 28 step: 667, loss is 0.0030571382958441973\n",
      "epoch: 28 step: 668, loss is 6.881502486066893e-05\n",
      "epoch: 28 step: 669, loss is 0.0015633093426004052\n",
      "epoch: 28 step: 670, loss is 0.007896475493907928\n",
      "epoch: 28 step: 671, loss is 0.00014094531070441008\n",
      "epoch: 28 step: 672, loss is 0.024520570412278175\n",
      "epoch: 28 step: 673, loss is 0.014176986180245876\n",
      "epoch: 28 step: 674, loss is 0.0001436683814972639\n",
      "epoch: 28 step: 675, loss is 0.0008401504601351917\n",
      "epoch: 28 step: 676, loss is 0.04174981638789177\n",
      "epoch: 28 step: 677, loss is 0.01179445255547762\n",
      "epoch: 28 step: 678, loss is 0.0014839660143479705\n",
      "epoch: 28 step: 679, loss is 0.001121819601394236\n",
      "epoch: 28 step: 680, loss is 0.002794556552544236\n",
      "epoch: 28 step: 681, loss is 0.002456856658682227\n",
      "epoch: 28 step: 682, loss is 4.2317707993788645e-05\n",
      "epoch: 28 step: 683, loss is 0.004725689999759197\n",
      "epoch: 28 step: 684, loss is 0.0001561894896440208\n",
      "epoch: 28 step: 685, loss is 0.001530323177576065\n",
      "epoch: 28 step: 686, loss is 0.0011852376628667116\n",
      "epoch: 28 step: 687, loss is 0.0005260012694634497\n",
      "epoch: 28 step: 688, loss is 0.0004914902965538204\n",
      "epoch: 28 step: 689, loss is 0.00016037389286793768\n",
      "epoch: 28 step: 690, loss is 0.001133063924498856\n",
      "epoch: 28 step: 691, loss is 0.0004408642998896539\n",
      "epoch: 28 step: 692, loss is 0.0005738336476497352\n",
      "epoch: 28 step: 693, loss is 0.06290649622678757\n",
      "epoch: 28 step: 694, loss is 0.010135628283023834\n",
      "epoch: 28 step: 695, loss is 0.0426323227584362\n",
      "epoch: 28 step: 696, loss is 3.494103657430969e-05\n",
      "epoch: 28 step: 697, loss is 0.00869278609752655\n",
      "epoch: 28 step: 698, loss is 0.008857408538460732\n",
      "epoch: 28 step: 699, loss is 0.0008174069807864726\n",
      "epoch: 28 step: 700, loss is 0.0037138285115361214\n",
      "epoch: 28 step: 701, loss is 0.02004259265959263\n",
      "epoch: 28 step: 702, loss is 0.0006882060552015901\n",
      "epoch: 28 step: 703, loss is 0.0006526861106976867\n",
      "epoch: 28 step: 704, loss is 0.0002416721690678969\n",
      "epoch: 28 step: 705, loss is 0.00047031763824634254\n",
      "epoch: 28 step: 706, loss is 0.007233678828924894\n",
      "epoch: 28 step: 707, loss is 5.0114696932723746e-05\n",
      "epoch: 28 step: 708, loss is 0.0009667010745033622\n",
      "epoch: 28 step: 709, loss is 0.009767310693860054\n",
      "epoch: 28 step: 710, loss is 3.0476889151032083e-05\n",
      "epoch: 28 step: 711, loss is 4.37511844211258e-05\n",
      "epoch: 28 step: 712, loss is 0.017261851578950882\n",
      "epoch: 28 step: 713, loss is 0.0014210096560418606\n",
      "epoch: 28 step: 714, loss is 0.06282776594161987\n",
      "epoch: 28 step: 715, loss is 0.008662226609885693\n",
      "epoch: 28 step: 716, loss is 0.0037247384898364544\n",
      "epoch: 28 step: 717, loss is 0.000867107359226793\n",
      "epoch: 28 step: 718, loss is 0.0010257468093186617\n",
      "epoch: 28 step: 719, loss is 0.0034991824068129063\n",
      "epoch: 28 step: 720, loss is 0.0006331829936243594\n",
      "epoch: 28 step: 721, loss is 0.001717640203423798\n",
      "epoch: 28 step: 722, loss is 0.0012580042239278555\n",
      "epoch: 28 step: 723, loss is 0.0049220803193748\n",
      "epoch: 28 step: 724, loss is 4.3612926674541086e-05\n",
      "epoch: 28 step: 725, loss is 0.0019187867874279618\n",
      "epoch: 28 step: 726, loss is 0.06266052275896072\n",
      "epoch: 28 step: 727, loss is 0.00851537100970745\n",
      "epoch: 28 step: 728, loss is 0.0021929575596004725\n",
      "epoch: 28 step: 729, loss is 0.0003124628565274179\n",
      "epoch: 28 step: 730, loss is 3.582740100682713e-05\n",
      "epoch: 28 step: 731, loss is 0.0008183801546692848\n",
      "epoch: 28 step: 732, loss is 0.000159201052156277\n",
      "epoch: 28 step: 733, loss is 0.00015345239080488682\n",
      "epoch: 28 step: 734, loss is 0.01291689369827509\n",
      "epoch: 28 step: 735, loss is 0.007972546853125095\n",
      "epoch: 28 step: 736, loss is 0.03567768633365631\n",
      "epoch: 28 step: 737, loss is 0.0010719854617491364\n",
      "epoch: 28 step: 738, loss is 0.0012679551728069782\n",
      "epoch: 28 step: 739, loss is 0.017650390043854713\n",
      "epoch: 28 step: 740, loss is 0.021364109590649605\n",
      "epoch: 28 step: 741, loss is 0.0008482277626171708\n",
      "epoch: 28 step: 742, loss is 0.004666302353143692\n",
      "epoch: 28 step: 743, loss is 0.013812432996928692\n",
      "epoch: 28 step: 744, loss is 0.0005273791612125933\n",
      "epoch: 28 step: 745, loss is 0.0008170745568349957\n",
      "epoch: 28 step: 746, loss is 0.0012492944952100515\n",
      "epoch: 28 step: 747, loss is 0.03553864732384682\n",
      "epoch: 28 step: 748, loss is 0.0019806544296443462\n",
      "epoch: 28 step: 749, loss is 0.0015117921866476536\n",
      "epoch: 28 step: 750, loss is 0.0012972616823390126\n",
      "epoch: 28 step: 751, loss is 0.010202532634139061\n",
      "epoch: 28 step: 752, loss is 0.06683552265167236\n",
      "epoch: 28 step: 753, loss is 0.09654541313648224\n",
      "epoch: 28 step: 754, loss is 0.009957664646208286\n",
      "epoch: 28 step: 755, loss is 0.0002181454619858414\n",
      "epoch: 28 step: 756, loss is 2.9125110813765787e-05\n",
      "epoch: 28 step: 757, loss is 0.00011989794438704848\n",
      "epoch: 28 step: 758, loss is 0.01845843903720379\n",
      "epoch: 28 step: 759, loss is 0.005043824203312397\n",
      "epoch: 28 step: 760, loss is 0.013096431270241737\n",
      "epoch: 28 step: 761, loss is 0.003457122016698122\n",
      "epoch: 28 step: 762, loss is 7.658457616344094e-05\n",
      "epoch: 28 step: 763, loss is 0.0005391158047132194\n",
      "epoch: 28 step: 764, loss is 0.004702073521912098\n",
      "epoch: 28 step: 765, loss is 0.00016859239258337766\n",
      "epoch: 28 step: 766, loss is 0.3401152193546295\n",
      "epoch: 28 step: 767, loss is 0.0009341686964035034\n",
      "epoch: 28 step: 768, loss is 0.00407874770462513\n",
      "epoch: 28 step: 769, loss is 1.8560836906544864e-05\n",
      "epoch: 28 step: 770, loss is 0.014439559541642666\n",
      "epoch: 28 step: 771, loss is 0.0031456593424081802\n",
      "epoch: 28 step: 772, loss is 9.98609684756957e-05\n",
      "epoch: 28 step: 773, loss is 0.006503036245703697\n",
      "epoch: 28 step: 774, loss is 0.0003169312549289316\n",
      "epoch: 28 step: 775, loss is 0.0007195560028776526\n",
      "epoch: 28 step: 776, loss is 0.0008517044479958713\n",
      "epoch: 28 step: 777, loss is 0.0027297278866171837\n",
      "epoch: 28 step: 778, loss is 0.0055047995410859585\n",
      "epoch: 28 step: 779, loss is 0.028257712721824646\n",
      "epoch: 28 step: 780, loss is 0.004281546920537949\n",
      "epoch: 28 step: 781, loss is 0.0010727033950388432\n",
      "epoch: 28 step: 782, loss is 0.016766786575317383\n",
      "epoch: 28 step: 783, loss is 0.0008961223065853119\n",
      "epoch: 28 step: 784, loss is 0.01523089874535799\n",
      "epoch: 28 step: 785, loss is 0.000633570714853704\n",
      "epoch: 28 step: 786, loss is 0.011833466589450836\n",
      "epoch: 28 step: 787, loss is 0.001045568729750812\n",
      "epoch: 28 step: 788, loss is 0.0012975889258086681\n",
      "epoch: 28 step: 789, loss is 0.002440415322780609\n",
      "epoch: 28 step: 790, loss is 0.0001493945310357958\n",
      "epoch: 28 step: 791, loss is 0.058883849531412125\n",
      "epoch: 28 step: 792, loss is 5.1422928663669154e-05\n",
      "epoch: 28 step: 793, loss is 0.0006493797991424799\n",
      "epoch: 28 step: 794, loss is 0.0018059170106425881\n",
      "epoch: 28 step: 795, loss is 0.00010364605986978859\n",
      "epoch: 28 step: 796, loss is 0.03975673392415047\n",
      "epoch: 28 step: 797, loss is 0.0034101800993084908\n",
      "epoch: 28 step: 798, loss is 0.013650731183588505\n",
      "epoch: 28 step: 799, loss is 0.008140475489199162\n",
      "epoch: 28 step: 800, loss is 0.003026652382686734\n",
      "epoch: 28 step: 801, loss is 0.0005113118677400053\n",
      "epoch: 28 step: 802, loss is 0.028943907469511032\n",
      "epoch: 28 step: 803, loss is 0.00487511046230793\n",
      "epoch: 28 step: 804, loss is 0.07810401916503906\n",
      "epoch: 28 step: 805, loss is 0.011834857054054737\n",
      "epoch: 28 step: 806, loss is 0.047415003180503845\n",
      "epoch: 28 step: 807, loss is 0.0014977359678596258\n",
      "epoch: 28 step: 808, loss is 0.0027828295715153217\n",
      "epoch: 28 step: 809, loss is 0.002601180924102664\n",
      "epoch: 28 step: 810, loss is 0.007129107601940632\n",
      "epoch: 28 step: 811, loss is 0.0006074349512346089\n",
      "epoch: 28 step: 812, loss is 0.005906681064516306\n",
      "epoch: 28 step: 813, loss is 0.04583350569009781\n",
      "epoch: 28 step: 814, loss is 7.886585808591917e-05\n",
      "epoch: 28 step: 815, loss is 0.0029384128283709288\n",
      "epoch: 28 step: 816, loss is 0.0007586883148178458\n",
      "epoch: 28 step: 817, loss is 0.0955546572804451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 818, loss is 0.00010422910418128595\n",
      "epoch: 28 step: 819, loss is 0.0014903000555932522\n",
      "epoch: 28 step: 820, loss is 0.00761434156447649\n",
      "epoch: 28 step: 821, loss is 0.06446215510368347\n",
      "epoch: 28 step: 822, loss is 0.00042538237175904214\n",
      "epoch: 28 step: 823, loss is 0.003370296210050583\n",
      "epoch: 28 step: 824, loss is 0.007246890105307102\n",
      "epoch: 28 step: 825, loss is 0.006155879702419043\n",
      "epoch: 28 step: 826, loss is 0.005961774382740259\n",
      "epoch: 28 step: 827, loss is 0.005349933169782162\n",
      "epoch: 28 step: 828, loss is 0.001955767860636115\n",
      "epoch: 28 step: 829, loss is 0.017295390367507935\n",
      "epoch: 28 step: 830, loss is 0.010606234893202782\n",
      "epoch: 28 step: 831, loss is 0.0004945589462295175\n",
      "epoch: 28 step: 832, loss is 0.0797313004732132\n",
      "epoch: 28 step: 833, loss is 0.00636897049844265\n",
      "epoch: 28 step: 834, loss is 0.03877196088433266\n",
      "epoch: 28 step: 835, loss is 0.00011638014984782785\n",
      "epoch: 28 step: 836, loss is 0.0003130961558781564\n",
      "epoch: 28 step: 837, loss is 0.00031054578721523285\n",
      "epoch: 28 step: 838, loss is 0.00249976827763021\n",
      "epoch: 28 step: 839, loss is 0.0033382584806531668\n",
      "epoch: 28 step: 840, loss is 0.0008907914743758738\n",
      "epoch: 28 step: 841, loss is 0.002063978696241975\n",
      "epoch: 28 step: 842, loss is 0.07811246812343597\n",
      "epoch: 28 step: 843, loss is 0.0013878305908292532\n",
      "epoch: 28 step: 844, loss is 0.0016645598225295544\n",
      "epoch: 28 step: 845, loss is 0.0006171567947603762\n",
      "epoch: 28 step: 846, loss is 0.05055364966392517\n",
      "epoch: 28 step: 847, loss is 0.00017884225235320628\n",
      "epoch: 28 step: 848, loss is 0.0020956899970769882\n",
      "epoch: 28 step: 849, loss is 0.0005062657292000949\n",
      "epoch: 28 step: 850, loss is 0.0011196453124284744\n",
      "epoch: 28 step: 851, loss is 0.0206418689340353\n",
      "epoch: 28 step: 852, loss is 0.0004608930030371994\n",
      "epoch: 28 step: 853, loss is 0.0009379837429150939\n",
      "epoch: 28 step: 854, loss is 0.01355371531099081\n",
      "epoch: 28 step: 855, loss is 0.016239700838923454\n",
      "epoch: 28 step: 856, loss is 0.039499491453170776\n",
      "epoch: 28 step: 857, loss is 0.0018661068752408028\n",
      "epoch: 28 step: 858, loss is 0.0067639523185789585\n",
      "epoch: 28 step: 859, loss is 0.00011965786688961089\n",
      "epoch: 28 step: 860, loss is 0.0064634946174919605\n",
      "epoch: 28 step: 861, loss is 0.0005107101751491427\n",
      "epoch: 28 step: 862, loss is 0.0004144915146753192\n",
      "epoch: 28 step: 863, loss is 0.00036670221015810966\n",
      "epoch: 28 step: 864, loss is 0.010152806527912617\n",
      "epoch: 28 step: 865, loss is 0.004740374628454447\n",
      "epoch: 28 step: 866, loss is 0.001274044276215136\n",
      "epoch: 28 step: 867, loss is 0.02033659815788269\n",
      "epoch: 28 step: 868, loss is 0.00013924473023507744\n",
      "epoch: 28 step: 869, loss is 0.0003250782610848546\n",
      "epoch: 28 step: 870, loss is 0.0023433465976268053\n",
      "epoch: 28 step: 871, loss is 0.013564866036176682\n",
      "epoch: 28 step: 872, loss is 0.0002939472906291485\n",
      "epoch: 28 step: 873, loss is 0.01258033700287342\n",
      "epoch: 28 step: 874, loss is 0.0006583895301446319\n",
      "epoch: 28 step: 875, loss is 0.007665436249226332\n",
      "epoch: 28 step: 876, loss is 0.0001343952608294785\n",
      "epoch: 28 step: 877, loss is 0.0006653440650552511\n",
      "epoch: 28 step: 878, loss is 0.006677679717540741\n",
      "epoch: 28 step: 879, loss is 0.004466831684112549\n",
      "epoch: 28 step: 880, loss is 0.004914112854748964\n",
      "epoch: 28 step: 881, loss is 0.0004129973240196705\n",
      "epoch: 28 step: 882, loss is 0.0003528335364535451\n",
      "epoch: 28 step: 883, loss is 0.0030611162073910236\n",
      "epoch: 28 step: 884, loss is 0.000587299931794405\n",
      "epoch: 28 step: 885, loss is 0.0002949899062514305\n",
      "epoch: 28 step: 886, loss is 0.000433148758020252\n",
      "epoch: 28 step: 887, loss is 0.003656350541859865\n",
      "epoch: 28 step: 888, loss is 0.0015170396072790027\n",
      "epoch: 28 step: 889, loss is 0.041228413581848145\n",
      "epoch: 28 step: 890, loss is 0.00031885781208984554\n",
      "epoch: 28 step: 891, loss is 0.009449195116758347\n",
      "epoch: 28 step: 892, loss is 0.0008559508714824915\n",
      "epoch: 28 step: 893, loss is 0.00014332802675198764\n",
      "epoch: 28 step: 894, loss is 0.0016888573300093412\n",
      "epoch: 28 step: 895, loss is 0.0003470971714705229\n",
      "epoch: 28 step: 896, loss is 0.0022579224314540625\n",
      "epoch: 28 step: 897, loss is 5.4967043979559094e-05\n",
      "epoch: 28 step: 898, loss is 0.0007121313828974962\n",
      "epoch: 28 step: 899, loss is 0.011940065771341324\n",
      "epoch: 28 step: 900, loss is 0.0014663598267361522\n",
      "epoch: 28 step: 901, loss is 0.00010903268412221223\n",
      "epoch: 28 step: 902, loss is 0.006496806629002094\n",
      "epoch: 28 step: 903, loss is 0.0008782474906183779\n",
      "epoch: 28 step: 904, loss is 0.03226768225431442\n",
      "epoch: 28 step: 905, loss is 0.006265571340918541\n",
      "epoch: 28 step: 906, loss is 0.0016223029233515263\n",
      "epoch: 28 step: 907, loss is 0.027660958468914032\n",
      "epoch: 28 step: 908, loss is 0.0024771278258413076\n",
      "epoch: 28 step: 909, loss is 0.07357914745807648\n",
      "epoch: 28 step: 910, loss is 3.802959690801799e-05\n",
      "epoch: 28 step: 911, loss is 0.09350553154945374\n",
      "epoch: 28 step: 912, loss is 0.000102807221992407\n",
      "epoch: 28 step: 913, loss is 0.0002896742953453213\n",
      "epoch: 28 step: 914, loss is 0.00019526825053617358\n",
      "epoch: 28 step: 915, loss is 0.008863850496709347\n",
      "epoch: 28 step: 916, loss is 0.0036329615395516157\n",
      "epoch: 28 step: 917, loss is 0.024732500314712524\n",
      "epoch: 28 step: 918, loss is 0.0031723929569125175\n",
      "epoch: 28 step: 919, loss is 0.002815702697262168\n",
      "epoch: 28 step: 920, loss is 0.0006477555143646896\n",
      "epoch: 28 step: 921, loss is 0.008915524929761887\n",
      "epoch: 28 step: 922, loss is 0.037379562854766846\n",
      "epoch: 28 step: 923, loss is 0.00885387510061264\n",
      "epoch: 28 step: 924, loss is 0.007187920622527599\n",
      "epoch: 28 step: 925, loss is 0.006122520659118891\n",
      "epoch: 28 step: 926, loss is 0.0006681400700472295\n",
      "epoch: 28 step: 927, loss is 0.0017873868346214294\n",
      "epoch: 28 step: 928, loss is 0.0018891909858211875\n",
      "epoch: 28 step: 929, loss is 0.015525253489613533\n",
      "epoch: 28 step: 930, loss is 0.0006376506644301116\n",
      "epoch: 28 step: 931, loss is 0.0002993892121594399\n",
      "epoch: 28 step: 932, loss is 7.144346454879269e-05\n",
      "epoch: 28 step: 933, loss is 0.020701806992292404\n",
      "epoch: 28 step: 934, loss is 0.0004154588677920401\n",
      "epoch: 28 step: 935, loss is 0.00034140440402552485\n",
      "epoch: 28 step: 936, loss is 0.014286954887211323\n",
      "epoch: 28 step: 937, loss is 0.0017631070222705603\n",
      "epoch: 29 step: 1, loss is 0.0009189145057462156\n",
      "epoch: 29 step: 2, loss is 0.00124837935436517\n",
      "epoch: 29 step: 3, loss is 0.001801484846509993\n",
      "epoch: 29 step: 4, loss is 0.004021053668111563\n",
      "epoch: 29 step: 5, loss is 0.0027530710212886333\n",
      "epoch: 29 step: 6, loss is 0.0026525885332375765\n",
      "epoch: 29 step: 7, loss is 0.002883553970605135\n",
      "epoch: 29 step: 8, loss is 0.0022405602503567934\n",
      "epoch: 29 step: 9, loss is 0.005787304136902094\n",
      "epoch: 29 step: 10, loss is 8.442366379313171e-05\n",
      "epoch: 29 step: 11, loss is 0.00023027995484881103\n",
      "epoch: 29 step: 12, loss is 0.001477791229262948\n",
      "epoch: 29 step: 13, loss is 0.0007668674807064235\n",
      "epoch: 29 step: 14, loss is 0.013841436244547367\n",
      "epoch: 29 step: 15, loss is 0.0027313048485666513\n",
      "epoch: 29 step: 16, loss is 0.0011284954380244017\n",
      "epoch: 29 step: 17, loss is 0.0007535417680628598\n",
      "epoch: 29 step: 18, loss is 0.00338915316388011\n",
      "epoch: 29 step: 19, loss is 0.0005329965497367084\n",
      "epoch: 29 step: 20, loss is 0.0006952597759664059\n",
      "epoch: 29 step: 21, loss is 0.0018499091966077685\n",
      "epoch: 29 step: 22, loss is 0.0008345044334419072\n",
      "epoch: 29 step: 23, loss is 0.0003531077818479389\n",
      "epoch: 29 step: 24, loss is 0.009807257913053036\n",
      "epoch: 29 step: 25, loss is 0.003032525535672903\n",
      "epoch: 29 step: 26, loss is 0.0017735125729814172\n",
      "epoch: 29 step: 27, loss is 0.01744214817881584\n",
      "epoch: 29 step: 28, loss is 0.00028432419640012085\n",
      "epoch: 29 step: 29, loss is 0.0009135945001617074\n",
      "epoch: 29 step: 30, loss is 0.001310960971750319\n",
      "epoch: 29 step: 31, loss is 9.797020902624354e-05\n",
      "epoch: 29 step: 32, loss is 6.62645252305083e-05\n",
      "epoch: 29 step: 33, loss is 0.0009880743455141783\n",
      "epoch: 29 step: 34, loss is 0.0010943356901407242\n",
      "epoch: 29 step: 35, loss is 0.0008223737822845578\n",
      "epoch: 29 step: 36, loss is 1.6236313967965543e-05\n",
      "epoch: 29 step: 37, loss is 0.0002122913720086217\n",
      "epoch: 29 step: 38, loss is 0.0021609461400657892\n",
      "epoch: 29 step: 39, loss is 4.132230969844386e-05\n",
      "epoch: 29 step: 40, loss is 0.00017286429647356272\n",
      "epoch: 29 step: 41, loss is 0.00041671813232824206\n",
      "epoch: 29 step: 42, loss is 0.036427438259124756\n",
      "epoch: 29 step: 43, loss is 0.0028869742527604103\n",
      "epoch: 29 step: 44, loss is 0.00026443699607625604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 45, loss is 0.0023534148931503296\n",
      "epoch: 29 step: 46, loss is 0.0022178234066814184\n",
      "epoch: 29 step: 47, loss is 0.0032466973643749952\n",
      "epoch: 29 step: 48, loss is 0.00363344163633883\n",
      "epoch: 29 step: 49, loss is 0.000356539327185601\n",
      "epoch: 29 step: 50, loss is 0.0015296365600079298\n",
      "epoch: 29 step: 51, loss is 0.0015904917381703854\n",
      "epoch: 29 step: 52, loss is 0.00026303998311050236\n",
      "epoch: 29 step: 53, loss is 0.0021878117695450783\n",
      "epoch: 29 step: 54, loss is 0.0442369319498539\n",
      "epoch: 29 step: 55, loss is 0.0006966539658606052\n",
      "epoch: 29 step: 56, loss is 0.0002707093663047999\n",
      "epoch: 29 step: 57, loss is 0.009881834499537945\n",
      "epoch: 29 step: 58, loss is 0.001473825890570879\n",
      "epoch: 29 step: 59, loss is 0.02832530438899994\n",
      "epoch: 29 step: 60, loss is 0.005963469389826059\n",
      "epoch: 29 step: 61, loss is 0.00017670050146989524\n",
      "epoch: 29 step: 62, loss is 0.0014682762557640672\n",
      "epoch: 29 step: 63, loss is 0.000947770953644067\n",
      "epoch: 29 step: 64, loss is 0.004064368084073067\n",
      "epoch: 29 step: 65, loss is 0.0001407260133419186\n",
      "epoch: 29 step: 66, loss is 0.004252089653164148\n",
      "epoch: 29 step: 67, loss is 0.003494376316666603\n",
      "epoch: 29 step: 68, loss is 0.0009477872517891228\n",
      "epoch: 29 step: 69, loss is 0.0004734366084448993\n",
      "epoch: 29 step: 70, loss is 0.007556365802884102\n",
      "epoch: 29 step: 71, loss is 1.7744403521646746e-05\n",
      "epoch: 29 step: 72, loss is 0.0005715098232030869\n",
      "epoch: 29 step: 73, loss is 0.006219611968845129\n",
      "epoch: 29 step: 74, loss is 0.0012675204779952765\n",
      "epoch: 29 step: 75, loss is 0.00046066430513747036\n",
      "epoch: 29 step: 76, loss is 0.0034058918245136738\n",
      "epoch: 29 step: 77, loss is 0.0013944163220003247\n",
      "epoch: 29 step: 78, loss is 0.00102331698872149\n",
      "epoch: 29 step: 79, loss is 0.0005016219220124185\n",
      "epoch: 29 step: 80, loss is 0.00236056512221694\n",
      "epoch: 29 step: 81, loss is 0.00016762739687692374\n",
      "epoch: 29 step: 82, loss is 0.0002776625333353877\n",
      "epoch: 29 step: 83, loss is 0.012976204976439476\n",
      "epoch: 29 step: 84, loss is 0.00015213601000141352\n",
      "epoch: 29 step: 85, loss is 0.08318200707435608\n",
      "epoch: 29 step: 86, loss is 1.5809497199370526e-05\n",
      "epoch: 29 step: 87, loss is 0.0005072433268651366\n",
      "epoch: 29 step: 88, loss is 0.00020768368267454207\n",
      "epoch: 29 step: 89, loss is 0.004055825527757406\n",
      "epoch: 29 step: 90, loss is 0.000127091770991683\n",
      "epoch: 29 step: 91, loss is 0.0019039761973544955\n",
      "epoch: 29 step: 92, loss is 0.015524253249168396\n",
      "epoch: 29 step: 93, loss is 0.013467422686517239\n",
      "epoch: 29 step: 94, loss is 0.0005014061462134123\n",
      "epoch: 29 step: 95, loss is 0.04115729406476021\n",
      "epoch: 29 step: 96, loss is 0.006774065084755421\n",
      "epoch: 29 step: 97, loss is 0.025774650275707245\n",
      "epoch: 29 step: 98, loss is 0.00024336528440471739\n",
      "epoch: 29 step: 99, loss is 0.030126236379146576\n",
      "epoch: 29 step: 100, loss is 0.0005812498275190592\n",
      "epoch: 29 step: 101, loss is 0.007907754741609097\n",
      "epoch: 29 step: 102, loss is 0.0022428485099226236\n",
      "epoch: 29 step: 103, loss is 0.00023736016009934247\n",
      "epoch: 29 step: 104, loss is 5.832502574776299e-05\n",
      "epoch: 29 step: 105, loss is 0.0009016287513077259\n",
      "epoch: 29 step: 106, loss is 0.0016276686219498515\n",
      "epoch: 29 step: 107, loss is 0.007616071961820126\n",
      "epoch: 29 step: 108, loss is 0.0008132350631058216\n",
      "epoch: 29 step: 109, loss is 0.0024122900795191526\n",
      "epoch: 29 step: 110, loss is 0.0028255966026335955\n",
      "epoch: 29 step: 111, loss is 0.0005888075684197247\n",
      "epoch: 29 step: 112, loss is 0.0011485597351565957\n",
      "epoch: 29 step: 113, loss is 0.00040064388304017484\n",
      "epoch: 29 step: 114, loss is 0.0006003084708936512\n",
      "epoch: 29 step: 115, loss is 0.05577690899372101\n",
      "epoch: 29 step: 116, loss is 0.004074488766491413\n",
      "epoch: 29 step: 117, loss is 0.020555108785629272\n",
      "epoch: 29 step: 118, loss is 0.00040221112431026995\n",
      "epoch: 29 step: 119, loss is 0.007250367663800716\n",
      "epoch: 29 step: 120, loss is 0.0009438256383873522\n",
      "epoch: 29 step: 121, loss is 0.027594493702054024\n",
      "epoch: 29 step: 122, loss is 0.043287742882966995\n",
      "epoch: 29 step: 123, loss is 0.01178434956818819\n",
      "epoch: 29 step: 124, loss is 0.0004050257266499102\n",
      "epoch: 29 step: 125, loss is 0.04099150002002716\n",
      "epoch: 29 step: 126, loss is 0.002573983743786812\n",
      "epoch: 29 step: 127, loss is 0.0034265355207026005\n",
      "epoch: 29 step: 128, loss is 0.0006034703692421317\n",
      "epoch: 29 step: 129, loss is 0.00143708277028054\n",
      "epoch: 29 step: 130, loss is 0.006535162217915058\n",
      "epoch: 29 step: 131, loss is 0.041650839149951935\n",
      "epoch: 29 step: 132, loss is 0.002647790126502514\n",
      "epoch: 29 step: 133, loss is 0.045508041977882385\n",
      "epoch: 29 step: 134, loss is 0.0002155795373255387\n",
      "epoch: 29 step: 135, loss is 0.0024992860853672028\n",
      "epoch: 29 step: 136, loss is 0.0003573325229808688\n",
      "epoch: 29 step: 137, loss is 0.00018219943740405142\n",
      "epoch: 29 step: 138, loss is 0.00316628348082304\n",
      "epoch: 29 step: 139, loss is 0.00018239232304040343\n",
      "epoch: 29 step: 140, loss is 0.0002224480704171583\n",
      "epoch: 29 step: 141, loss is 0.00012186551612103358\n",
      "epoch: 29 step: 142, loss is 0.032651063054800034\n",
      "epoch: 29 step: 143, loss is 0.00019181780226062983\n",
      "epoch: 29 step: 144, loss is 0.0012094989651814103\n",
      "epoch: 29 step: 145, loss is 0.0029808124527335167\n",
      "epoch: 29 step: 146, loss is 0.028920255601406097\n",
      "epoch: 29 step: 147, loss is 0.00037004720070399344\n",
      "epoch: 29 step: 148, loss is 0.004087198991328478\n",
      "epoch: 29 step: 149, loss is 0.0003761689004022628\n",
      "epoch: 29 step: 150, loss is 0.00043827499030157924\n",
      "epoch: 29 step: 151, loss is 0.002011863747611642\n",
      "epoch: 29 step: 152, loss is 0.0003720470704138279\n",
      "epoch: 29 step: 153, loss is 0.0011359323980286717\n",
      "epoch: 29 step: 154, loss is 0.026866421103477478\n",
      "epoch: 29 step: 155, loss is 0.027042152360081673\n",
      "epoch: 29 step: 156, loss is 6.216973270056769e-05\n",
      "epoch: 29 step: 157, loss is 0.0017819880740717053\n",
      "epoch: 29 step: 158, loss is 0.00042333820601925254\n",
      "epoch: 29 step: 159, loss is 0.08454052358865738\n",
      "epoch: 29 step: 160, loss is 0.004497801419347525\n",
      "epoch: 29 step: 161, loss is 1.3824241250404157e-05\n",
      "epoch: 29 step: 162, loss is 0.0001174369317595847\n",
      "epoch: 29 step: 163, loss is 0.00011282998457318172\n",
      "epoch: 29 step: 164, loss is 0.00019904074724763632\n",
      "epoch: 29 step: 165, loss is 0.012335377745330334\n",
      "epoch: 29 step: 166, loss is 0.000503717630635947\n",
      "epoch: 29 step: 167, loss is 0.0004966226406395435\n",
      "epoch: 29 step: 168, loss is 0.00034689463791437447\n",
      "epoch: 29 step: 169, loss is 0.09954894334077835\n",
      "epoch: 29 step: 170, loss is 0.0005426882999017835\n",
      "epoch: 29 step: 171, loss is 0.004698890261352062\n",
      "epoch: 29 step: 172, loss is 0.0011069823522120714\n",
      "epoch: 29 step: 173, loss is 0.028558271005749702\n",
      "epoch: 29 step: 174, loss is 0.00631166435778141\n",
      "epoch: 29 step: 175, loss is 0.016130000352859497\n",
      "epoch: 29 step: 176, loss is 0.00016389510710723698\n",
      "epoch: 29 step: 177, loss is 0.00036890379851683974\n",
      "epoch: 29 step: 178, loss is 0.0030664412770420313\n",
      "epoch: 29 step: 179, loss is 2.9326300136744976e-05\n",
      "epoch: 29 step: 180, loss is 0.0002506083983462304\n",
      "epoch: 29 step: 181, loss is 0.0015671768924221396\n",
      "epoch: 29 step: 182, loss is 0.009093048982322216\n",
      "epoch: 29 step: 183, loss is 0.002715382957831025\n",
      "epoch: 29 step: 184, loss is 0.010940369218587875\n",
      "epoch: 29 step: 185, loss is 0.0004965267144143581\n",
      "epoch: 29 step: 186, loss is 0.0004662249411921948\n",
      "epoch: 29 step: 187, loss is 0.02234603464603424\n",
      "epoch: 29 step: 188, loss is 0.0021679699420928955\n",
      "epoch: 29 step: 189, loss is 0.013963124714791775\n",
      "epoch: 29 step: 190, loss is 0.0005049320170655847\n",
      "epoch: 29 step: 191, loss is 0.0015133429551497102\n",
      "epoch: 29 step: 192, loss is 0.00015253247693181038\n",
      "epoch: 29 step: 193, loss is 0.0035866685211658478\n",
      "epoch: 29 step: 194, loss is 0.003027170430868864\n",
      "epoch: 29 step: 195, loss is 0.0009885600302368402\n",
      "epoch: 29 step: 196, loss is 0.005559079349040985\n",
      "epoch: 29 step: 197, loss is 0.027418093755841255\n",
      "epoch: 29 step: 198, loss is 0.0013096556067466736\n",
      "epoch: 29 step: 199, loss is 0.0004011907731182873\n",
      "epoch: 29 step: 200, loss is 1.5602017811033875e-05\n",
      "epoch: 29 step: 201, loss is 0.010946821421384811\n",
      "epoch: 29 step: 202, loss is 0.0005436051287688315\n",
      "epoch: 29 step: 203, loss is 0.0618232786655426\n",
      "epoch: 29 step: 204, loss is 0.00023009901633486152\n",
      "epoch: 29 step: 205, loss is 0.019342897459864616\n",
      "epoch: 29 step: 206, loss is 0.00013091049913782626\n",
      "epoch: 29 step: 207, loss is 0.018525946885347366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 208, loss is 0.00010999265941791236\n",
      "epoch: 29 step: 209, loss is 0.0002231287508038804\n",
      "epoch: 29 step: 210, loss is 0.0003653753665275872\n",
      "epoch: 29 step: 211, loss is 0.00025671126786619425\n",
      "epoch: 29 step: 212, loss is 0.0002831772726494819\n",
      "epoch: 29 step: 213, loss is 6.950322131160647e-05\n",
      "epoch: 29 step: 214, loss is 0.07473635673522949\n",
      "epoch: 29 step: 215, loss is 0.005247060675173998\n",
      "epoch: 29 step: 216, loss is 0.0019799217116087675\n",
      "epoch: 29 step: 217, loss is 0.001258362433873117\n",
      "epoch: 29 step: 218, loss is 0.0011286108056083322\n",
      "epoch: 29 step: 219, loss is 0.0016237485688179731\n",
      "epoch: 29 step: 220, loss is 0.0011702717747539282\n",
      "epoch: 29 step: 221, loss is 0.0006669969880022109\n",
      "epoch: 29 step: 222, loss is 0.0016248247120529413\n",
      "epoch: 29 step: 223, loss is 0.00030680408235639334\n",
      "epoch: 29 step: 224, loss is 0.0007245328743010759\n",
      "epoch: 29 step: 225, loss is 0.009646785445511341\n",
      "epoch: 29 step: 226, loss is 0.05299143120646477\n",
      "epoch: 29 step: 227, loss is 5.144458918948658e-05\n",
      "epoch: 29 step: 228, loss is 0.00015561020700260997\n",
      "epoch: 29 step: 229, loss is 0.0002515628875698894\n",
      "epoch: 29 step: 230, loss is 0.0008951927302405238\n",
      "epoch: 29 step: 231, loss is 5.1493476348696277e-05\n",
      "epoch: 29 step: 232, loss is 0.00296104047447443\n",
      "epoch: 29 step: 233, loss is 0.0034273313358426094\n",
      "epoch: 29 step: 234, loss is 0.0012506627244874835\n",
      "epoch: 29 step: 235, loss is 0.04407864809036255\n",
      "epoch: 29 step: 236, loss is 0.055610887706279755\n",
      "epoch: 29 step: 237, loss is 0.0018552046967670321\n",
      "epoch: 29 step: 238, loss is 0.00027061442960985005\n",
      "epoch: 29 step: 239, loss is 0.0913100466132164\n",
      "epoch: 29 step: 240, loss is 0.0003306743165012449\n",
      "epoch: 29 step: 241, loss is 0.001243688864633441\n",
      "epoch: 29 step: 242, loss is 0.002948173088952899\n",
      "epoch: 29 step: 243, loss is 0.04927409067749977\n",
      "epoch: 29 step: 244, loss is 0.009358132258057594\n",
      "epoch: 29 step: 245, loss is 0.0024462472647428513\n",
      "epoch: 29 step: 246, loss is 0.004957389086484909\n",
      "epoch: 29 step: 247, loss is 0.033610451966524124\n",
      "epoch: 29 step: 248, loss is 0.00011511870980029926\n",
      "epoch: 29 step: 249, loss is 0.0015050803776830435\n",
      "epoch: 29 step: 250, loss is 0.02549109049141407\n",
      "epoch: 29 step: 251, loss is 0.003184546949341893\n",
      "epoch: 29 step: 252, loss is 0.004831878002732992\n",
      "epoch: 29 step: 253, loss is 0.00019105325918644667\n",
      "epoch: 29 step: 254, loss is 0.0003911297826562077\n",
      "epoch: 29 step: 255, loss is 3.655023101600818e-05\n",
      "epoch: 29 step: 256, loss is 0.005451619159430265\n",
      "epoch: 29 step: 257, loss is 0.003131162142381072\n",
      "epoch: 29 step: 258, loss is 0.001214373973198235\n",
      "epoch: 29 step: 259, loss is 0.0004281923465896398\n",
      "epoch: 29 step: 260, loss is 0.09688913077116013\n",
      "epoch: 29 step: 261, loss is 0.007126545999199152\n",
      "epoch: 29 step: 262, loss is 0.0003783140564337373\n",
      "epoch: 29 step: 263, loss is 0.001099723856896162\n",
      "epoch: 29 step: 264, loss is 0.09101340919733047\n",
      "epoch: 29 step: 265, loss is 0.00022129935678094625\n",
      "epoch: 29 step: 266, loss is 0.00020116573432460427\n",
      "epoch: 29 step: 267, loss is 5.74749537918251e-05\n",
      "epoch: 29 step: 268, loss is 0.002841018373146653\n",
      "epoch: 29 step: 269, loss is 0.00036113811074756086\n",
      "epoch: 29 step: 270, loss is 0.00883145909756422\n",
      "epoch: 29 step: 271, loss is 0.0011190229561179876\n",
      "epoch: 29 step: 272, loss is 0.014201715588569641\n",
      "epoch: 29 step: 273, loss is 0.023159336298704147\n",
      "epoch: 29 step: 274, loss is 0.011615520343184471\n",
      "epoch: 29 step: 275, loss is 0.001912289415486157\n",
      "epoch: 29 step: 276, loss is 4.3832271330757067e-05\n",
      "epoch: 29 step: 277, loss is 0.0009166807867586613\n",
      "epoch: 29 step: 278, loss is 0.0007680691196583211\n",
      "epoch: 29 step: 279, loss is 0.0006291555473580956\n",
      "epoch: 29 step: 280, loss is 0.009968389756977558\n",
      "epoch: 29 step: 281, loss is 0.02102971263229847\n",
      "epoch: 29 step: 282, loss is 0.0007274226518347859\n",
      "epoch: 29 step: 283, loss is 0.004120617639273405\n",
      "epoch: 29 step: 284, loss is 0.0011873784242197871\n",
      "epoch: 29 step: 285, loss is 3.0722669180249795e-05\n",
      "epoch: 29 step: 286, loss is 0.0029450049623847008\n",
      "epoch: 29 step: 287, loss is 0.03389061614871025\n",
      "epoch: 29 step: 288, loss is 0.0027152891270816326\n",
      "epoch: 29 step: 289, loss is 0.0015995745779946446\n",
      "epoch: 29 step: 290, loss is 0.0005921133561059833\n",
      "epoch: 29 step: 291, loss is 0.008568128570914268\n",
      "epoch: 29 step: 292, loss is 0.00636328523978591\n",
      "epoch: 29 step: 293, loss is 0.0008392040617763996\n",
      "epoch: 29 step: 294, loss is 0.00031450705137103796\n",
      "epoch: 29 step: 295, loss is 0.0006600443157367408\n",
      "epoch: 29 step: 296, loss is 0.0004959424259141088\n",
      "epoch: 29 step: 297, loss is 0.00015408277977257967\n",
      "epoch: 29 step: 298, loss is 0.02173803746700287\n",
      "epoch: 29 step: 299, loss is 0.016936760395765305\n",
      "epoch: 29 step: 300, loss is 0.00024135567946359515\n",
      "epoch: 29 step: 301, loss is 0.00030965913902036846\n",
      "epoch: 29 step: 302, loss is 0.0015614614821970463\n",
      "epoch: 29 step: 303, loss is 0.009677411057054996\n",
      "epoch: 29 step: 304, loss is 0.000716693582944572\n",
      "epoch: 29 step: 305, loss is 0.002246670890599489\n",
      "epoch: 29 step: 306, loss is 0.0010693190852180123\n",
      "epoch: 29 step: 307, loss is 0.004740910138934851\n",
      "epoch: 29 step: 308, loss is 0.00038766584475524724\n",
      "epoch: 29 step: 309, loss is 0.001581821241416037\n",
      "epoch: 29 step: 310, loss is 0.00011651174281723797\n",
      "epoch: 29 step: 311, loss is 0.004129057750105858\n",
      "epoch: 29 step: 312, loss is 0.0019531631842255592\n",
      "epoch: 29 step: 313, loss is 0.004089079797267914\n",
      "epoch: 29 step: 314, loss is 4.4192533096065745e-05\n",
      "epoch: 29 step: 315, loss is 0.0001239642733708024\n",
      "epoch: 29 step: 316, loss is 0.00017236382700502872\n",
      "epoch: 29 step: 317, loss is 6.898659921716899e-05\n",
      "epoch: 29 step: 318, loss is 0.0009306119754910469\n",
      "epoch: 29 step: 319, loss is 6.42982340650633e-05\n",
      "epoch: 29 step: 320, loss is 0.0005332542932592332\n",
      "epoch: 29 step: 321, loss is 0.03205316141247749\n",
      "epoch: 29 step: 322, loss is 0.0008742113132029772\n",
      "epoch: 29 step: 323, loss is 0.00023690296802669764\n",
      "epoch: 29 step: 324, loss is 0.006951185874640942\n",
      "epoch: 29 step: 325, loss is 3.393600127310492e-05\n",
      "epoch: 29 step: 326, loss is 0.0017365732928737998\n",
      "epoch: 29 step: 327, loss is 0.00013951522123534232\n",
      "epoch: 29 step: 328, loss is 0.0016161456005647779\n",
      "epoch: 29 step: 329, loss is 0.009015247225761414\n",
      "epoch: 29 step: 330, loss is 0.05105109140276909\n",
      "epoch: 29 step: 331, loss is 0.0011382424272596836\n",
      "epoch: 29 step: 332, loss is 0.0003203628584742546\n",
      "epoch: 29 step: 333, loss is 3.2485281735716853e-06\n",
      "epoch: 29 step: 334, loss is 0.0010864605428650975\n",
      "epoch: 29 step: 335, loss is 3.530078538460657e-05\n",
      "epoch: 29 step: 336, loss is 0.0003594234585762024\n",
      "epoch: 29 step: 337, loss is 0.0007833325653336942\n",
      "epoch: 29 step: 338, loss is 0.0007402917835861444\n",
      "epoch: 29 step: 339, loss is 0.0007541811792179942\n",
      "epoch: 29 step: 340, loss is 3.979742541559972e-05\n",
      "epoch: 29 step: 341, loss is 0.0028778815176337957\n",
      "epoch: 29 step: 342, loss is 0.005705743562430143\n",
      "epoch: 29 step: 343, loss is 1.943716051755473e-05\n",
      "epoch: 29 step: 344, loss is 0.003322461387142539\n",
      "epoch: 29 step: 345, loss is 0.001954604871571064\n",
      "epoch: 29 step: 346, loss is 0.001954528270289302\n",
      "epoch: 29 step: 347, loss is 0.005724466405808926\n",
      "epoch: 29 step: 348, loss is 0.0003577105817385018\n",
      "epoch: 29 step: 349, loss is 0.0058991992846131325\n",
      "epoch: 29 step: 350, loss is 0.0002202592440880835\n",
      "epoch: 29 step: 351, loss is 0.0004795315326191485\n",
      "epoch: 29 step: 352, loss is 0.012808558531105518\n",
      "epoch: 29 step: 353, loss is 2.18026452785125e-05\n",
      "epoch: 29 step: 354, loss is 1.9987719497294165e-05\n",
      "epoch: 29 step: 355, loss is 0.010181206278502941\n",
      "epoch: 29 step: 356, loss is 0.00032502872636541724\n",
      "epoch: 29 step: 357, loss is 0.005847123451530933\n",
      "epoch: 29 step: 358, loss is 7.168137381086126e-05\n",
      "epoch: 29 step: 359, loss is 0.0010505726095288992\n",
      "epoch: 29 step: 360, loss is 0.021428680047392845\n",
      "epoch: 29 step: 361, loss is 0.006583642680197954\n",
      "epoch: 29 step: 362, loss is 0.0002175479894503951\n",
      "epoch: 29 step: 363, loss is 7.031168934190646e-05\n",
      "epoch: 29 step: 364, loss is 0.007435690611600876\n",
      "epoch: 29 step: 365, loss is 0.01735050417482853\n",
      "epoch: 29 step: 366, loss is 0.0016096082981675863\n",
      "epoch: 29 step: 367, loss is 0.035340648144483566\n",
      "epoch: 29 step: 368, loss is 0.03048347309231758\n",
      "epoch: 29 step: 369, loss is 0.0008279927424155176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 370, loss is 0.0023946743458509445\n",
      "epoch: 29 step: 371, loss is 0.006494034081697464\n",
      "epoch: 29 step: 372, loss is 0.008335611782968044\n",
      "epoch: 29 step: 373, loss is 8.449641609331593e-05\n",
      "epoch: 29 step: 374, loss is 3.95034730900079e-05\n",
      "epoch: 29 step: 375, loss is 0.0014094895450398326\n",
      "epoch: 29 step: 376, loss is 0.0065426151268184185\n",
      "epoch: 29 step: 377, loss is 0.006697241682559252\n",
      "epoch: 29 step: 378, loss is 0.006446368992328644\n",
      "epoch: 29 step: 379, loss is 4.9350885092280805e-05\n",
      "epoch: 29 step: 380, loss is 0.00037504167994484305\n",
      "epoch: 29 step: 381, loss is 0.0002953587973024696\n",
      "epoch: 29 step: 382, loss is 0.00010139150981558487\n",
      "epoch: 29 step: 383, loss is 0.0010628406889736652\n",
      "epoch: 29 step: 384, loss is 0.00037621651426889\n",
      "epoch: 29 step: 385, loss is 0.007731261663138866\n",
      "epoch: 29 step: 386, loss is 0.00030138593865558505\n",
      "epoch: 29 step: 387, loss is 0.0003473322722129524\n",
      "epoch: 29 step: 388, loss is 9.340985707240179e-05\n",
      "epoch: 29 step: 389, loss is 0.00010356713028158993\n",
      "epoch: 29 step: 390, loss is 0.07482042163610458\n",
      "epoch: 29 step: 391, loss is 0.0006613790174014866\n",
      "epoch: 29 step: 392, loss is 0.008076123893260956\n",
      "epoch: 29 step: 393, loss is 0.01174989528954029\n",
      "epoch: 29 step: 394, loss is 0.00024395006767008454\n",
      "epoch: 29 step: 395, loss is 0.07516936212778091\n",
      "epoch: 29 step: 396, loss is 0.00012463370512705296\n",
      "epoch: 29 step: 397, loss is 0.009192879311740398\n",
      "epoch: 29 step: 398, loss is 0.00963064469397068\n",
      "epoch: 29 step: 399, loss is 9.535733261145651e-05\n",
      "epoch: 29 step: 400, loss is 4.100260048289783e-05\n",
      "epoch: 29 step: 401, loss is 0.05778847262263298\n",
      "epoch: 29 step: 402, loss is 0.005761149805039167\n",
      "epoch: 29 step: 403, loss is 4.212589192320593e-05\n",
      "epoch: 29 step: 404, loss is 0.0004391740949358791\n",
      "epoch: 29 step: 405, loss is 0.0084139509126544\n",
      "epoch: 29 step: 406, loss is 0.052223075181245804\n",
      "epoch: 29 step: 407, loss is 0.00042165437480434775\n",
      "epoch: 29 step: 408, loss is 0.00033604740747250617\n",
      "epoch: 29 step: 409, loss is 0.0010450127301737666\n",
      "epoch: 29 step: 410, loss is 0.0175646860152483\n",
      "epoch: 29 step: 411, loss is 0.0004893093137070537\n",
      "epoch: 29 step: 412, loss is 0.019914310425519943\n",
      "epoch: 29 step: 413, loss is 0.0032001121435314417\n",
      "epoch: 29 step: 414, loss is 0.0006445702747441828\n",
      "epoch: 29 step: 415, loss is 0.0009927012724801898\n",
      "epoch: 29 step: 416, loss is 0.0005620471783913672\n",
      "epoch: 29 step: 417, loss is 0.002593160839751363\n",
      "epoch: 29 step: 418, loss is 0.008352765813469887\n",
      "epoch: 29 step: 419, loss is 0.04725747928023338\n",
      "epoch: 29 step: 420, loss is 0.053485602140426636\n",
      "epoch: 29 step: 421, loss is 0.00481342151761055\n",
      "epoch: 29 step: 422, loss is 0.0012132438132539392\n",
      "epoch: 29 step: 423, loss is 1.551768946228549e-05\n",
      "epoch: 29 step: 424, loss is 0.00353341456502676\n",
      "epoch: 29 step: 425, loss is 8.91954914550297e-06\n",
      "epoch: 29 step: 426, loss is 0.03858964517712593\n",
      "epoch: 29 step: 427, loss is 0.028484933078289032\n",
      "epoch: 29 step: 428, loss is 0.0011121247662231326\n",
      "epoch: 29 step: 429, loss is 0.0029916628263890743\n",
      "epoch: 29 step: 430, loss is 0.00028829654911533\n",
      "epoch: 29 step: 431, loss is 0.0015440720599144697\n",
      "epoch: 29 step: 432, loss is 0.008283440954983234\n",
      "epoch: 29 step: 433, loss is 0.0001585291320225224\n",
      "epoch: 29 step: 434, loss is 0.0017028568545356393\n",
      "epoch: 29 step: 435, loss is 0.0002699813339859247\n",
      "epoch: 29 step: 436, loss is 0.00021530373487621546\n",
      "epoch: 29 step: 437, loss is 0.0001464041997678578\n",
      "epoch: 29 step: 438, loss is 0.2073061168193817\n",
      "epoch: 29 step: 439, loss is 0.05922417715191841\n",
      "epoch: 29 step: 440, loss is 0.010783335193991661\n",
      "epoch: 29 step: 441, loss is 9.275253250962123e-05\n",
      "epoch: 29 step: 442, loss is 0.0008467709994874895\n",
      "epoch: 29 step: 443, loss is 0.0004761509189847857\n",
      "epoch: 29 step: 444, loss is 0.02221817523241043\n",
      "epoch: 29 step: 445, loss is 0.056390900164842606\n",
      "epoch: 29 step: 446, loss is 0.0719175711274147\n",
      "epoch: 29 step: 447, loss is 0.0006738615920767188\n",
      "epoch: 29 step: 448, loss is 0.0017295407596975565\n",
      "epoch: 29 step: 449, loss is 0.002764157485216856\n",
      "epoch: 29 step: 450, loss is 0.003343577729538083\n",
      "epoch: 29 step: 451, loss is 0.00037463169428519905\n",
      "epoch: 29 step: 452, loss is 0.032414551824331284\n",
      "epoch: 29 step: 453, loss is 0.011452103033661842\n",
      "epoch: 29 step: 454, loss is 0.00210499856621027\n",
      "epoch: 29 step: 455, loss is 0.02399841696023941\n",
      "epoch: 29 step: 456, loss is 0.04775848984718323\n",
      "epoch: 29 step: 457, loss is 0.00011988385085714981\n",
      "epoch: 29 step: 458, loss is 0.00015174661530181766\n",
      "epoch: 29 step: 459, loss is 0.00045853538904339075\n",
      "epoch: 29 step: 460, loss is 0.06374985724687576\n",
      "epoch: 29 step: 461, loss is 0.0010024962248280644\n",
      "epoch: 29 step: 462, loss is 0.0029433066956698895\n",
      "epoch: 29 step: 463, loss is 0.01594936102628708\n",
      "epoch: 29 step: 464, loss is 0.0009488476789556444\n",
      "epoch: 29 step: 465, loss is 0.0009201354114338756\n",
      "epoch: 29 step: 466, loss is 0.003202122636139393\n",
      "epoch: 29 step: 467, loss is 0.005150923505425453\n",
      "epoch: 29 step: 468, loss is 0.08896882832050323\n",
      "epoch: 29 step: 469, loss is 0.004279503598809242\n",
      "epoch: 29 step: 470, loss is 0.00114534143358469\n",
      "epoch: 29 step: 471, loss is 0.0004672757931984961\n",
      "epoch: 29 step: 472, loss is 0.0502241887152195\n",
      "epoch: 29 step: 473, loss is 9.880393918137997e-05\n",
      "epoch: 29 step: 474, loss is 0.009739609435200691\n",
      "epoch: 29 step: 475, loss is 0.000890183262526989\n",
      "epoch: 29 step: 476, loss is 0.000886102148797363\n",
      "epoch: 29 step: 477, loss is 0.04325374588370323\n",
      "epoch: 29 step: 478, loss is 0.001206416287459433\n",
      "epoch: 29 step: 479, loss is 0.003568276995792985\n",
      "epoch: 29 step: 480, loss is 7.507832924602553e-05\n",
      "epoch: 29 step: 481, loss is 0.002242029644548893\n",
      "epoch: 29 step: 482, loss is 0.012050372548401356\n",
      "epoch: 29 step: 483, loss is 0.0003691361052915454\n",
      "epoch: 29 step: 484, loss is 0.009301639162003994\n",
      "epoch: 29 step: 485, loss is 0.0008814534521661699\n",
      "epoch: 29 step: 486, loss is 0.0008930534240789711\n",
      "epoch: 29 step: 487, loss is 0.06585394591093063\n",
      "epoch: 29 step: 488, loss is 9.676795889390633e-05\n",
      "epoch: 29 step: 489, loss is 3.398971239221282e-05\n",
      "epoch: 29 step: 490, loss is 0.001133186393417418\n",
      "epoch: 29 step: 491, loss is 8.329213596880436e-05\n",
      "epoch: 29 step: 492, loss is 0.0050162966363132\n",
      "epoch: 29 step: 493, loss is 0.005826418753713369\n",
      "epoch: 29 step: 494, loss is 0.0016566477715969086\n",
      "epoch: 29 step: 495, loss is 0.012487643398344517\n",
      "epoch: 29 step: 496, loss is 0.004232638515532017\n",
      "epoch: 29 step: 497, loss is 0.06827224791049957\n",
      "epoch: 29 step: 498, loss is 0.002044234424829483\n",
      "epoch: 29 step: 499, loss is 0.0019685907755047083\n",
      "epoch: 29 step: 500, loss is 0.006860591471195221\n",
      "epoch: 29 step: 501, loss is 0.008095855824649334\n",
      "epoch: 29 step: 502, loss is 0.0011503573041409254\n",
      "epoch: 29 step: 503, loss is 0.048219773918390274\n",
      "epoch: 29 step: 504, loss is 0.042187076061964035\n",
      "epoch: 29 step: 505, loss is 0.08751767873764038\n",
      "epoch: 29 step: 506, loss is 0.00016854776185937226\n",
      "epoch: 29 step: 507, loss is 0.0007150759920477867\n",
      "epoch: 29 step: 508, loss is 0.0051764389500021935\n",
      "epoch: 29 step: 509, loss is 0.047022439539432526\n",
      "epoch: 29 step: 510, loss is 0.0001282205485040322\n",
      "epoch: 29 step: 511, loss is 0.023857060819864273\n",
      "epoch: 29 step: 512, loss is 0.006020775064826012\n",
      "epoch: 29 step: 513, loss is 0.00011192669626325369\n",
      "epoch: 29 step: 514, loss is 0.016561072319746017\n",
      "epoch: 29 step: 515, loss is 0.022845039144158363\n",
      "epoch: 29 step: 516, loss is 0.04738224670290947\n",
      "epoch: 29 step: 517, loss is 0.07449717074632645\n",
      "epoch: 29 step: 518, loss is 0.00019460254407022148\n",
      "epoch: 29 step: 519, loss is 0.00020970433251932263\n",
      "epoch: 29 step: 520, loss is 0.0006040775333531201\n",
      "epoch: 29 step: 521, loss is 0.0007693360093981028\n",
      "epoch: 29 step: 522, loss is 0.002546919509768486\n",
      "epoch: 29 step: 523, loss is 0.009602371603250504\n",
      "epoch: 29 step: 524, loss is 0.0017361100763082504\n",
      "epoch: 29 step: 525, loss is 0.001012027612887323\n",
      "epoch: 29 step: 526, loss is 2.8871390895801596e-06\n",
      "epoch: 29 step: 527, loss is 0.003743417328223586\n",
      "epoch: 29 step: 528, loss is 0.0009517378639429808\n",
      "epoch: 29 step: 529, loss is 0.000380056444555521\n",
      "epoch: 29 step: 530, loss is 0.0009887823835015297\n",
      "epoch: 29 step: 531, loss is 0.0030225368682295084\n",
      "epoch: 29 step: 532, loss is 0.0004958458594046533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 533, loss is 1.220060653395194e-06\n",
      "epoch: 29 step: 534, loss is 0.002996979746967554\n",
      "epoch: 29 step: 535, loss is 0.007713428698480129\n",
      "epoch: 29 step: 536, loss is 0.00028746359748765826\n",
      "epoch: 29 step: 537, loss is 0.0001425161026418209\n",
      "epoch: 29 step: 538, loss is 0.0016450409311801195\n",
      "epoch: 29 step: 539, loss is 0.07693933695554733\n",
      "epoch: 29 step: 540, loss is 6.490466330433264e-05\n",
      "epoch: 29 step: 541, loss is 0.0017132641514763236\n",
      "epoch: 29 step: 542, loss is 0.0003960046742577106\n",
      "epoch: 29 step: 543, loss is 0.011104706674814224\n",
      "epoch: 29 step: 544, loss is 0.02858705073595047\n",
      "epoch: 29 step: 545, loss is 0.0031789105851203203\n",
      "epoch: 29 step: 546, loss is 0.00743506895378232\n",
      "epoch: 29 step: 547, loss is 0.0008687683148309588\n",
      "epoch: 29 step: 548, loss is 0.02523922361433506\n",
      "epoch: 29 step: 549, loss is 0.004256608430296183\n",
      "epoch: 29 step: 550, loss is 0.00023602336295880377\n",
      "epoch: 29 step: 551, loss is 0.03346109762787819\n",
      "epoch: 29 step: 552, loss is 0.0007647542515769601\n",
      "epoch: 29 step: 553, loss is 0.10428176075220108\n",
      "epoch: 29 step: 554, loss is 0.014458317309617996\n",
      "epoch: 29 step: 555, loss is 0.00915189553052187\n",
      "epoch: 29 step: 556, loss is 0.0020007777493447065\n",
      "epoch: 29 step: 557, loss is 0.001744347857311368\n",
      "epoch: 29 step: 558, loss is 0.006398335099220276\n",
      "epoch: 29 step: 559, loss is 0.09211195260286331\n",
      "epoch: 29 step: 560, loss is 0.00037182081723585725\n",
      "epoch: 29 step: 561, loss is 0.009230208583176136\n",
      "epoch: 29 step: 562, loss is 0.01813480816781521\n",
      "epoch: 29 step: 563, loss is 0.009928754530847073\n",
      "epoch: 29 step: 564, loss is 0.006524957716464996\n",
      "epoch: 29 step: 565, loss is 0.002128500724211335\n",
      "epoch: 29 step: 566, loss is 0.005807992070913315\n",
      "epoch: 29 step: 567, loss is 0.018485449254512787\n",
      "epoch: 29 step: 568, loss is 0.00033083013840951025\n",
      "epoch: 29 step: 569, loss is 0.006057834718376398\n",
      "epoch: 29 step: 570, loss is 0.00045206848881207407\n",
      "epoch: 29 step: 571, loss is 0.03812858462333679\n",
      "epoch: 29 step: 572, loss is 0.07895825058221817\n",
      "epoch: 29 step: 573, loss is 0.14727634191513062\n",
      "epoch: 29 step: 574, loss is 0.000261573150055483\n",
      "epoch: 29 step: 575, loss is 0.004880466498434544\n",
      "epoch: 29 step: 576, loss is 0.0010363614419475198\n",
      "epoch: 29 step: 577, loss is 0.012248041108250618\n",
      "epoch: 29 step: 578, loss is 0.013975624926388264\n",
      "epoch: 29 step: 579, loss is 0.02007950469851494\n",
      "epoch: 29 step: 580, loss is 0.01483949925750494\n",
      "epoch: 29 step: 581, loss is 0.016477035358548164\n",
      "epoch: 29 step: 582, loss is 0.002075183903798461\n",
      "epoch: 29 step: 583, loss is 0.0016998160863295197\n",
      "epoch: 29 step: 584, loss is 0.003085603704676032\n",
      "epoch: 29 step: 585, loss is 0.0038063805550336838\n",
      "epoch: 29 step: 586, loss is 0.00010262719297315925\n",
      "epoch: 29 step: 587, loss is 0.0006497061694972217\n",
      "epoch: 29 step: 588, loss is 0.0065144761465489864\n",
      "epoch: 29 step: 589, loss is 0.034836143255233765\n",
      "epoch: 29 step: 590, loss is 2.651649629115127e-05\n",
      "epoch: 29 step: 591, loss is 0.00862941239029169\n",
      "epoch: 29 step: 592, loss is 0.00998685508966446\n",
      "epoch: 29 step: 593, loss is 0.0006340456893667579\n",
      "epoch: 29 step: 594, loss is 0.012914876453578472\n",
      "epoch: 29 step: 595, loss is 0.0006438114796765149\n",
      "epoch: 29 step: 596, loss is 0.00039976841071620584\n",
      "epoch: 29 step: 597, loss is 0.04610664024949074\n",
      "epoch: 29 step: 598, loss is 0.00711231492459774\n",
      "epoch: 29 step: 599, loss is 0.007491173688322306\n",
      "epoch: 29 step: 600, loss is 0.022847605869174004\n",
      "epoch: 29 step: 601, loss is 0.031152987852692604\n",
      "epoch: 29 step: 602, loss is 0.006639779545366764\n",
      "epoch: 29 step: 603, loss is 0.001211236696690321\n",
      "epoch: 29 step: 604, loss is 0.14034418761730194\n",
      "epoch: 29 step: 605, loss is 0.0003041742602363229\n",
      "epoch: 29 step: 606, loss is 4.068734051543288e-05\n",
      "epoch: 29 step: 607, loss is 0.004411269910633564\n",
      "epoch: 29 step: 608, loss is 0.06610699743032455\n",
      "epoch: 29 step: 609, loss is 0.005950103048235178\n",
      "epoch: 29 step: 610, loss is 0.024229655042290688\n",
      "epoch: 29 step: 611, loss is 0.05717037618160248\n",
      "epoch: 29 step: 612, loss is 0.00013888863031752408\n",
      "epoch: 29 step: 613, loss is 0.006239641457796097\n",
      "epoch: 29 step: 614, loss is 0.03919975832104683\n",
      "epoch: 29 step: 615, loss is 0.01370152086019516\n",
      "epoch: 29 step: 616, loss is 0.00027650807169266045\n",
      "epoch: 29 step: 617, loss is 0.0027007870376110077\n",
      "epoch: 29 step: 618, loss is 0.00857568345963955\n",
      "epoch: 29 step: 619, loss is 0.00017108191968873143\n",
      "epoch: 29 step: 620, loss is 0.012735546566545963\n",
      "epoch: 29 step: 621, loss is 0.005780553910881281\n",
      "epoch: 29 step: 622, loss is 0.0005282143247313797\n",
      "epoch: 29 step: 623, loss is 0.0032692556269466877\n",
      "epoch: 29 step: 624, loss is 0.020407168194651604\n",
      "epoch: 29 step: 625, loss is 0.0018694037571549416\n",
      "epoch: 29 step: 626, loss is 0.008984711952507496\n",
      "epoch: 29 step: 627, loss is 0.004271280951797962\n",
      "epoch: 29 step: 628, loss is 0.0031445736531168222\n",
      "epoch: 29 step: 629, loss is 0.0003033291141036898\n",
      "epoch: 29 step: 630, loss is 0.005840930622071028\n",
      "epoch: 29 step: 631, loss is 0.002817353466525674\n",
      "epoch: 29 step: 632, loss is 0.001945410156622529\n",
      "epoch: 29 step: 633, loss is 0.000778272224124521\n",
      "epoch: 29 step: 634, loss is 0.001163007807917893\n",
      "epoch: 29 step: 635, loss is 0.0010035054292529821\n",
      "epoch: 29 step: 636, loss is 0.026584109291434288\n",
      "epoch: 29 step: 637, loss is 0.003851371118798852\n",
      "epoch: 29 step: 638, loss is 0.005043564364314079\n",
      "epoch: 29 step: 639, loss is 5.883386984351091e-05\n",
      "epoch: 29 step: 640, loss is 0.0860249400138855\n",
      "epoch: 29 step: 641, loss is 0.03784556686878204\n",
      "epoch: 29 step: 642, loss is 0.009556116536259651\n",
      "epoch: 29 step: 643, loss is 4.483915472519584e-05\n",
      "epoch: 29 step: 644, loss is 0.0027444346342235804\n",
      "epoch: 29 step: 645, loss is 0.018656229600310326\n",
      "epoch: 29 step: 646, loss is 0.0002687135711312294\n",
      "epoch: 29 step: 647, loss is 0.01635141298174858\n",
      "epoch: 29 step: 648, loss is 0.0015455828979611397\n",
      "epoch: 29 step: 649, loss is 0.0016412504483014345\n",
      "epoch: 29 step: 650, loss is 0.004091976210474968\n",
      "epoch: 29 step: 651, loss is 0.0005520505947060883\n",
      "epoch: 29 step: 652, loss is 0.0005010471795685589\n",
      "epoch: 29 step: 653, loss is 0.001084515475668013\n",
      "epoch: 29 step: 654, loss is 0.0010502813383936882\n",
      "epoch: 29 step: 655, loss is 0.06293737888336182\n",
      "epoch: 29 step: 656, loss is 0.0008149638306349516\n",
      "epoch: 29 step: 657, loss is 0.0011008637957274914\n",
      "epoch: 29 step: 658, loss is 0.07742097973823547\n",
      "epoch: 29 step: 659, loss is 2.7959300496149808e-05\n",
      "epoch: 29 step: 660, loss is 0.00025816832203418016\n",
      "epoch: 29 step: 661, loss is 0.0006281719543039799\n",
      "epoch: 29 step: 662, loss is 0.005981388036161661\n",
      "epoch: 29 step: 663, loss is 0.00028861314058303833\n",
      "epoch: 29 step: 664, loss is 0.055894169956445694\n",
      "epoch: 29 step: 665, loss is 0.0736001506447792\n",
      "epoch: 29 step: 666, loss is 0.04989103972911835\n",
      "epoch: 29 step: 667, loss is 0.0006699557416141033\n",
      "epoch: 29 step: 668, loss is 0.005051833111792803\n",
      "epoch: 29 step: 669, loss is 2.377245073148515e-05\n",
      "epoch: 29 step: 670, loss is 0.001422425382770598\n",
      "epoch: 29 step: 671, loss is 0.002387619810178876\n",
      "epoch: 29 step: 672, loss is 0.003822921309620142\n",
      "epoch: 29 step: 673, loss is 0.00046329095494002104\n",
      "epoch: 29 step: 674, loss is 0.0014931345358490944\n",
      "epoch: 29 step: 675, loss is 0.0010383594781160355\n",
      "epoch: 29 step: 676, loss is 0.026895800605416298\n",
      "epoch: 29 step: 677, loss is 0.0025844061747193336\n",
      "epoch: 29 step: 678, loss is 3.381008718861267e-05\n",
      "epoch: 29 step: 679, loss is 0.0005107501056045294\n",
      "epoch: 29 step: 680, loss is 0.0011243635090067983\n",
      "epoch: 29 step: 681, loss is 0.003147588577121496\n",
      "epoch: 29 step: 682, loss is 0.010974971577525139\n",
      "epoch: 29 step: 683, loss is 0.0003738074447028339\n",
      "epoch: 29 step: 684, loss is 0.01565944217145443\n",
      "epoch: 29 step: 685, loss is 0.007829059846699238\n",
      "epoch: 29 step: 686, loss is 0.00274734478443861\n",
      "epoch: 29 step: 687, loss is 0.0015786250587552786\n",
      "epoch: 29 step: 688, loss is 0.03537828102707863\n",
      "epoch: 29 step: 689, loss is 0.00014115644444245845\n",
      "epoch: 29 step: 690, loss is 0.009392981417477131\n",
      "epoch: 29 step: 691, loss is 0.02847902663052082\n",
      "epoch: 29 step: 692, loss is 0.023696472868323326\n",
      "epoch: 29 step: 693, loss is 0.0012203947408124804\n",
      "epoch: 29 step: 694, loss is 0.0002707946696318686\n",
      "epoch: 29 step: 695, loss is 0.001496232463978231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 696, loss is 0.00021152534463908523\n",
      "epoch: 29 step: 697, loss is 0.0026581077836453915\n",
      "epoch: 29 step: 698, loss is 0.001225279993377626\n",
      "epoch: 29 step: 699, loss is 0.012647849507629871\n",
      "epoch: 29 step: 700, loss is 0.044612985104322433\n",
      "epoch: 29 step: 701, loss is 0.014223056845366955\n",
      "epoch: 29 step: 702, loss is 0.01083403080701828\n",
      "epoch: 29 step: 703, loss is 0.0030259976629167795\n",
      "epoch: 29 step: 704, loss is 0.0003891266824211925\n",
      "epoch: 29 step: 705, loss is 0.005717264022678137\n",
      "epoch: 29 step: 706, loss is 0.006416642107069492\n",
      "epoch: 29 step: 707, loss is 0.00020328426035121083\n",
      "epoch: 29 step: 708, loss is 0.0008597963023930788\n",
      "epoch: 29 step: 709, loss is 0.22565650939941406\n",
      "epoch: 29 step: 710, loss is 0.011222505941987038\n",
      "epoch: 29 step: 711, loss is 0.0008103089057840407\n",
      "epoch: 29 step: 712, loss is 0.005926789715886116\n",
      "epoch: 29 step: 713, loss is 0.0014369799755513668\n",
      "epoch: 29 step: 714, loss is 0.021297652274370193\n",
      "epoch: 29 step: 715, loss is 0.01119968667626381\n",
      "epoch: 29 step: 716, loss is 0.16994129121303558\n",
      "epoch: 29 step: 717, loss is 0.052162494510412216\n",
      "epoch: 29 step: 718, loss is 0.020172588527202606\n",
      "epoch: 29 step: 719, loss is 0.01710350066423416\n",
      "epoch: 29 step: 720, loss is 0.003196033649146557\n",
      "epoch: 29 step: 721, loss is 0.009898613207042217\n",
      "epoch: 29 step: 722, loss is 0.06507255136966705\n",
      "epoch: 29 step: 723, loss is 0.00018452602671459317\n",
      "epoch: 29 step: 724, loss is 0.0032909067813307047\n",
      "epoch: 29 step: 725, loss is 0.0024137557484209538\n",
      "epoch: 29 step: 726, loss is 0.0008423675317317247\n",
      "epoch: 29 step: 727, loss is 0.042254235595464706\n",
      "epoch: 29 step: 728, loss is 0.0005649938830174506\n",
      "epoch: 29 step: 729, loss is 0.06085146218538284\n",
      "epoch: 29 step: 730, loss is 0.0008152080699801445\n",
      "epoch: 29 step: 731, loss is 0.00473181065171957\n",
      "epoch: 29 step: 732, loss is 0.09915928542613983\n",
      "epoch: 29 step: 733, loss is 0.0002697376476135105\n",
      "epoch: 29 step: 734, loss is 0.10256270319223404\n",
      "epoch: 29 step: 735, loss is 0.011702757328748703\n",
      "epoch: 29 step: 736, loss is 0.007868791930377483\n",
      "epoch: 29 step: 737, loss is 0.014455466531217098\n",
      "epoch: 29 step: 738, loss is 0.009134741500020027\n",
      "epoch: 29 step: 739, loss is 0.011286379769444466\n",
      "epoch: 29 step: 740, loss is 0.002184620127081871\n",
      "epoch: 29 step: 741, loss is 0.004188180435448885\n",
      "epoch: 29 step: 742, loss is 0.005362064111977816\n",
      "epoch: 29 step: 743, loss is 0.008529161103069782\n",
      "epoch: 29 step: 744, loss is 0.007337387651205063\n",
      "epoch: 29 step: 745, loss is 0.0532134510576725\n",
      "epoch: 29 step: 746, loss is 0.0012264694087207317\n",
      "epoch: 29 step: 747, loss is 0.0009138454333879054\n",
      "epoch: 29 step: 748, loss is 7.368567457888275e-05\n",
      "epoch: 29 step: 749, loss is 0.005146767012774944\n",
      "epoch: 29 step: 750, loss is 0.09654180705547333\n",
      "epoch: 29 step: 751, loss is 0.0004920809878967702\n",
      "epoch: 29 step: 752, loss is 0.002906952053308487\n",
      "epoch: 29 step: 753, loss is 0.0032219323329627514\n",
      "epoch: 29 step: 754, loss is 0.026127154007554054\n",
      "epoch: 29 step: 755, loss is 0.023791274055838585\n",
      "epoch: 29 step: 756, loss is 0.0007694659871049225\n",
      "epoch: 29 step: 757, loss is 0.0005587201449088752\n",
      "epoch: 29 step: 758, loss is 0.0007623967831023037\n",
      "epoch: 29 step: 759, loss is 0.0005334618035703897\n",
      "epoch: 29 step: 760, loss is 0.03622506558895111\n",
      "epoch: 29 step: 761, loss is 0.0007074110326357186\n",
      "epoch: 29 step: 762, loss is 0.14829228818416595\n",
      "epoch: 29 step: 763, loss is 0.00147144659422338\n",
      "epoch: 29 step: 764, loss is 0.0002271200792165473\n",
      "epoch: 29 step: 765, loss is 0.00038415510789491236\n",
      "epoch: 29 step: 766, loss is 0.0036551805678755045\n",
      "epoch: 29 step: 767, loss is 0.006291582249104977\n",
      "epoch: 29 step: 768, loss is 0.1272142231464386\n",
      "epoch: 29 step: 769, loss is 0.00018693172023631632\n",
      "epoch: 29 step: 770, loss is 0.0024907542392611504\n",
      "epoch: 29 step: 771, loss is 0.008521654643118382\n",
      "epoch: 29 step: 772, loss is 0.0010646790033206344\n",
      "epoch: 29 step: 773, loss is 0.0015073977410793304\n",
      "epoch: 29 step: 774, loss is 0.0037270961329340935\n",
      "epoch: 29 step: 775, loss is 0.11271729320287704\n",
      "epoch: 29 step: 776, loss is 0.003309316700324416\n",
      "epoch: 29 step: 777, loss is 0.003714292775839567\n",
      "epoch: 29 step: 778, loss is 0.0022030796390026808\n",
      "epoch: 29 step: 779, loss is 0.002824339084327221\n",
      "epoch: 29 step: 780, loss is 0.0654950812458992\n",
      "epoch: 29 step: 781, loss is 2.5562643713783473e-05\n",
      "epoch: 29 step: 782, loss is 0.020399287343025208\n",
      "epoch: 29 step: 783, loss is 0.0025897687301039696\n",
      "epoch: 29 step: 784, loss is 0.06298323720693588\n",
      "epoch: 29 step: 785, loss is 0.006366024725139141\n",
      "epoch: 29 step: 786, loss is 0.0022247950546443462\n",
      "epoch: 29 step: 787, loss is 0.0003112754784524441\n",
      "epoch: 29 step: 788, loss is 9.039345604833215e-05\n",
      "epoch: 29 step: 789, loss is 0.030472876504063606\n",
      "epoch: 29 step: 790, loss is 0.04449921473860741\n",
      "epoch: 29 step: 791, loss is 0.0016629825113341212\n",
      "epoch: 29 step: 792, loss is 0.03105136565864086\n",
      "epoch: 29 step: 793, loss is 0.026174571365118027\n",
      "epoch: 29 step: 794, loss is 0.0029176142998039722\n",
      "epoch: 29 step: 795, loss is 0.0057728528045117855\n",
      "epoch: 29 step: 796, loss is 7.595513307023793e-05\n",
      "epoch: 29 step: 797, loss is 0.00098576454911381\n",
      "epoch: 29 step: 798, loss is 0.0004117557546123862\n",
      "epoch: 29 step: 799, loss is 0.0055901529267430305\n",
      "epoch: 29 step: 800, loss is 0.007377337664365768\n",
      "epoch: 29 step: 801, loss is 0.016857115551829338\n",
      "epoch: 29 step: 802, loss is 0.00018999951134901494\n",
      "epoch: 29 step: 803, loss is 0.001825410989113152\n",
      "epoch: 29 step: 804, loss is 0.0024310587905347347\n",
      "epoch: 29 step: 805, loss is 0.007698397152125835\n",
      "epoch: 29 step: 806, loss is 0.0038243781309574842\n",
      "epoch: 29 step: 807, loss is 0.004476794041693211\n",
      "epoch: 29 step: 808, loss is 0.0014848444843664765\n",
      "epoch: 29 step: 809, loss is 0.014631818979978561\n",
      "epoch: 29 step: 810, loss is 0.04933259263634682\n",
      "epoch: 29 step: 811, loss is 0.00018449824710842222\n",
      "epoch: 29 step: 812, loss is 0.0005057130474597216\n",
      "epoch: 29 step: 813, loss is 0.00012622428766917437\n",
      "epoch: 29 step: 814, loss is 0.0004757469578180462\n",
      "epoch: 29 step: 815, loss is 0.00018382706912234426\n",
      "epoch: 29 step: 816, loss is 0.0036504638846963644\n",
      "epoch: 29 step: 817, loss is 0.005746523849666119\n",
      "epoch: 29 step: 818, loss is 0.0036545763723552227\n",
      "epoch: 29 step: 819, loss is 0.002335804048925638\n",
      "epoch: 29 step: 820, loss is 0.0026949928142130375\n",
      "epoch: 29 step: 821, loss is 0.0027003828436136246\n",
      "epoch: 29 step: 822, loss is 0.03145300969481468\n",
      "epoch: 29 step: 823, loss is 0.009236883372068405\n",
      "epoch: 29 step: 824, loss is 0.0005406028940342367\n",
      "epoch: 29 step: 825, loss is 0.026383019983768463\n",
      "epoch: 29 step: 826, loss is 0.01033356785774231\n",
      "epoch: 29 step: 827, loss is 0.0634627416729927\n",
      "epoch: 29 step: 828, loss is 0.07761035114526749\n",
      "epoch: 29 step: 829, loss is 1.1661308235488832e-05\n",
      "epoch: 29 step: 830, loss is 0.0023854023311287165\n",
      "epoch: 29 step: 831, loss is 0.03921571373939514\n",
      "epoch: 29 step: 832, loss is 0.00018108308722730726\n",
      "epoch: 29 step: 833, loss is 0.0074842544272542\n",
      "epoch: 29 step: 834, loss is 0.0045011150650680065\n",
      "epoch: 29 step: 835, loss is 0.02747860737144947\n",
      "epoch: 29 step: 836, loss is 0.0005726930685341358\n",
      "epoch: 29 step: 837, loss is 0.10899392515420914\n",
      "epoch: 29 step: 838, loss is 0.002802866045385599\n",
      "epoch: 29 step: 839, loss is 0.06797300279140472\n",
      "epoch: 29 step: 840, loss is 0.0059084417298436165\n",
      "epoch: 29 step: 841, loss is 0.0076749492436647415\n",
      "epoch: 29 step: 842, loss is 0.0017444647382944822\n",
      "epoch: 29 step: 843, loss is 0.06304366886615753\n",
      "epoch: 29 step: 844, loss is 0.034072402864694595\n",
      "epoch: 29 step: 845, loss is 0.001466744695790112\n",
      "epoch: 29 step: 846, loss is 0.002024837303906679\n",
      "epoch: 29 step: 847, loss is 0.0070830825716257095\n",
      "epoch: 29 step: 848, loss is 0.011720484122633934\n",
      "epoch: 29 step: 849, loss is 0.03206271305680275\n",
      "epoch: 29 step: 850, loss is 0.07972444593906403\n",
      "epoch: 29 step: 851, loss is 0.05315962806344032\n",
      "epoch: 29 step: 852, loss is 0.0012903724564239383\n",
      "epoch: 29 step: 853, loss is 0.022407101467251778\n",
      "epoch: 29 step: 854, loss is 0.04111470282077789\n",
      "epoch: 29 step: 855, loss is 0.001411912264302373\n",
      "epoch: 29 step: 856, loss is 0.002784360432997346\n",
      "epoch: 29 step: 857, loss is 0.018483662977814674\n",
      "epoch: 29 step: 858, loss is 0.00035758980084210634\n",
      "epoch: 29 step: 859, loss is 0.007029196247458458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 860, loss is 0.0004608699819073081\n",
      "epoch: 29 step: 861, loss is 0.0015264090616255999\n",
      "epoch: 29 step: 862, loss is 0.0011734707513824105\n",
      "epoch: 29 step: 863, loss is 0.0035283504985272884\n",
      "epoch: 29 step: 864, loss is 0.020648662000894547\n",
      "epoch: 29 step: 865, loss is 0.0009343564161099494\n",
      "epoch: 29 step: 866, loss is 0.030596740543842316\n",
      "epoch: 29 step: 867, loss is 0.1713133454322815\n",
      "epoch: 29 step: 868, loss is 0.00018505958723835647\n",
      "epoch: 29 step: 869, loss is 0.04247079789638519\n",
      "epoch: 29 step: 870, loss is 0.009908050298690796\n",
      "epoch: 29 step: 871, loss is 0.048133108764886856\n",
      "epoch: 29 step: 872, loss is 0.009703888557851315\n",
      "epoch: 29 step: 873, loss is 0.0009137332672253251\n",
      "epoch: 29 step: 874, loss is 0.0037025094497948885\n",
      "epoch: 29 step: 875, loss is 0.0003052988031413406\n",
      "epoch: 29 step: 876, loss is 0.03141070902347565\n",
      "epoch: 29 step: 877, loss is 0.00960896722972393\n",
      "epoch: 29 step: 878, loss is 0.06765328347682953\n",
      "epoch: 29 step: 879, loss is 0.0053228046745061874\n",
      "epoch: 29 step: 880, loss is 0.005065765231847763\n",
      "epoch: 29 step: 881, loss is 0.000775556662119925\n",
      "epoch: 29 step: 882, loss is 0.0015267326962202787\n",
      "epoch: 29 step: 883, loss is 0.057160310447216034\n",
      "epoch: 29 step: 884, loss is 0.0019233021885156631\n",
      "epoch: 29 step: 885, loss is 0.00968768447637558\n",
      "epoch: 29 step: 886, loss is 0.02746572345495224\n",
      "epoch: 29 step: 887, loss is 0.031295064836740494\n",
      "epoch: 29 step: 888, loss is 0.0314418263733387\n",
      "epoch: 29 step: 889, loss is 0.06268627941608429\n",
      "epoch: 29 step: 890, loss is 0.00016574238543398678\n",
      "epoch: 29 step: 891, loss is 0.010603216476738453\n",
      "epoch: 29 step: 892, loss is 0.0008566016331315041\n",
      "epoch: 29 step: 893, loss is 0.008052785880863667\n",
      "epoch: 29 step: 894, loss is 0.015882061794400215\n",
      "epoch: 29 step: 895, loss is 0.02866940200328827\n",
      "epoch: 29 step: 896, loss is 0.0029001571238040924\n",
      "epoch: 29 step: 897, loss is 0.007004479877650738\n",
      "epoch: 29 step: 898, loss is 0.0021971811074763536\n",
      "epoch: 29 step: 899, loss is 0.00017934356583282351\n",
      "epoch: 29 step: 900, loss is 0.023607883602380753\n",
      "epoch: 29 step: 901, loss is 0.0047401729971170425\n",
      "epoch: 29 step: 902, loss is 0.0015858840197324753\n",
      "epoch: 29 step: 903, loss is 0.004076133016496897\n",
      "epoch: 29 step: 904, loss is 0.0007654587388969958\n",
      "epoch: 29 step: 905, loss is 0.00017175439279526472\n",
      "epoch: 29 step: 906, loss is 0.0007873579161241651\n",
      "epoch: 29 step: 907, loss is 0.015195285901427269\n",
      "epoch: 29 step: 908, loss is 0.0010666061425581574\n",
      "epoch: 29 step: 909, loss is 0.13188882172107697\n",
      "epoch: 29 step: 910, loss is 0.033575545996427536\n",
      "epoch: 29 step: 911, loss is 0.0017635225085541606\n",
      "epoch: 29 step: 912, loss is 0.00947597436606884\n",
      "epoch: 29 step: 913, loss is 0.009240330196917057\n",
      "epoch: 29 step: 914, loss is 0.0018260792130604386\n",
      "epoch: 29 step: 915, loss is 0.002356782089918852\n",
      "epoch: 29 step: 916, loss is 8.758583862800151e-05\n",
      "epoch: 29 step: 917, loss is 0.00016613387560937554\n",
      "epoch: 29 step: 918, loss is 0.028016187250614166\n",
      "epoch: 29 step: 919, loss is 0.01291460357606411\n",
      "epoch: 29 step: 920, loss is 0.004213772714138031\n",
      "epoch: 29 step: 921, loss is 0.007911494001746178\n",
      "epoch: 29 step: 922, loss is 0.0005789098795503378\n",
      "epoch: 29 step: 923, loss is 0.0019641986582428217\n",
      "epoch: 29 step: 924, loss is 0.0005117918481118977\n",
      "epoch: 29 step: 925, loss is 0.06972459703683853\n",
      "epoch: 29 step: 926, loss is 0.03540729358792305\n",
      "epoch: 29 step: 927, loss is 0.012960241176187992\n",
      "epoch: 29 step: 928, loss is 0.07702498137950897\n",
      "epoch: 29 step: 929, loss is 0.06544796377420425\n",
      "epoch: 29 step: 930, loss is 0.0025686342269182205\n",
      "epoch: 29 step: 931, loss is 0.007841972634196281\n",
      "epoch: 29 step: 932, loss is 0.03241647779941559\n",
      "epoch: 29 step: 933, loss is 0.015180066227912903\n",
      "epoch: 29 step: 934, loss is 0.02074611186981201\n",
      "epoch: 29 step: 935, loss is 0.10939378291368484\n",
      "epoch: 29 step: 936, loss is 0.014465642161667347\n",
      "epoch: 29 step: 937, loss is 0.0002129789936589077\n",
      "epoch: 30 step: 1, loss is 0.001825515879318118\n",
      "epoch: 30 step: 2, loss is 0.01716412417590618\n",
      "epoch: 30 step: 3, loss is 0.0013564596883952618\n",
      "epoch: 30 step: 4, loss is 0.0034692399203777313\n",
      "epoch: 30 step: 5, loss is 0.0454411655664444\n",
      "epoch: 30 step: 6, loss is 0.008278653025627136\n",
      "epoch: 30 step: 7, loss is 0.026970086619257927\n",
      "epoch: 30 step: 8, loss is 0.005002165213227272\n",
      "epoch: 30 step: 9, loss is 4.3833948438987136e-05\n",
      "epoch: 30 step: 10, loss is 0.00010079337516799569\n",
      "epoch: 30 step: 11, loss is 0.03265021741390228\n",
      "epoch: 30 step: 12, loss is 0.006458989344537258\n",
      "epoch: 30 step: 13, loss is 0.0007116625201888382\n",
      "epoch: 30 step: 14, loss is 0.0004548676952254027\n",
      "epoch: 30 step: 15, loss is 0.0016660952242091298\n",
      "epoch: 30 step: 16, loss is 0.05333298072218895\n",
      "epoch: 30 step: 17, loss is 0.06126130744814873\n",
      "epoch: 30 step: 18, loss is 0.002158988965675235\n",
      "epoch: 30 step: 19, loss is 0.007142748683691025\n",
      "epoch: 30 step: 20, loss is 0.018405379727482796\n",
      "epoch: 30 step: 21, loss is 0.010500244796276093\n",
      "epoch: 30 step: 22, loss is 0.0031927626114338636\n",
      "epoch: 30 step: 23, loss is 0.002781309885904193\n",
      "epoch: 30 step: 24, loss is 0.0002517998800612986\n",
      "epoch: 30 step: 25, loss is 0.0001012193679343909\n",
      "epoch: 30 step: 26, loss is 0.06971723586320877\n",
      "epoch: 30 step: 27, loss is 0.010318238288164139\n",
      "epoch: 30 step: 28, loss is 0.02405344322323799\n",
      "epoch: 30 step: 29, loss is 0.023022472858428955\n",
      "epoch: 30 step: 30, loss is 0.0023995081428438425\n",
      "epoch: 30 step: 31, loss is 0.07688380032777786\n",
      "epoch: 30 step: 32, loss is 0.0036142522003501654\n",
      "epoch: 30 step: 33, loss is 0.0006482837488874793\n",
      "epoch: 30 step: 34, loss is 0.013684924691915512\n",
      "epoch: 30 step: 35, loss is 0.05629194527864456\n",
      "epoch: 30 step: 36, loss is 0.006444247905164957\n",
      "epoch: 30 step: 37, loss is 0.007731109391897917\n",
      "epoch: 30 step: 38, loss is 0.0008731649140827358\n",
      "epoch: 30 step: 39, loss is 0.0007286070613190532\n",
      "epoch: 30 step: 40, loss is 0.009055658243596554\n",
      "epoch: 30 step: 41, loss is 0.0017972777131944895\n",
      "epoch: 30 step: 42, loss is 4.699785858974792e-05\n",
      "epoch: 30 step: 43, loss is 0.010880385525524616\n",
      "epoch: 30 step: 44, loss is 0.0008053545607253909\n",
      "epoch: 30 step: 45, loss is 0.0005363093223422766\n",
      "epoch: 30 step: 46, loss is 0.00020563464204315096\n",
      "epoch: 30 step: 47, loss is 0.003837875323370099\n",
      "epoch: 30 step: 48, loss is 0.0004638687241822481\n",
      "epoch: 30 step: 49, loss is 0.0013459298061206937\n",
      "epoch: 30 step: 50, loss is 0.00048629226512275636\n",
      "epoch: 30 step: 51, loss is 0.0011941696284338832\n",
      "epoch: 30 step: 52, loss is 0.0022898439783602953\n",
      "epoch: 30 step: 53, loss is 2.6281624741386622e-05\n",
      "epoch: 30 step: 54, loss is 0.0005129819037392735\n",
      "epoch: 30 step: 55, loss is 0.00015469163190573454\n",
      "epoch: 30 step: 56, loss is 0.001980943838134408\n",
      "epoch: 30 step: 57, loss is 0.0008124759187921882\n",
      "epoch: 30 step: 58, loss is 0.00023831386351957917\n",
      "epoch: 30 step: 59, loss is 0.10228221118450165\n",
      "epoch: 30 step: 60, loss is 0.005836439784616232\n",
      "epoch: 30 step: 61, loss is 0.0016883286880329251\n",
      "epoch: 30 step: 62, loss is 0.012206447310745716\n",
      "epoch: 30 step: 63, loss is 0.00024689562269486487\n",
      "epoch: 30 step: 64, loss is 0.019977126270532608\n",
      "epoch: 30 step: 65, loss is 0.0001858332980191335\n",
      "epoch: 30 step: 66, loss is 0.001781656639650464\n",
      "epoch: 30 step: 67, loss is 0.002220915164798498\n",
      "epoch: 30 step: 68, loss is 0.0003456475678831339\n",
      "epoch: 30 step: 69, loss is 0.00014202043530531228\n",
      "epoch: 30 step: 70, loss is 0.0010047093965113163\n",
      "epoch: 30 step: 71, loss is 0.0008319432381540537\n",
      "epoch: 30 step: 72, loss is 0.020809967070817947\n",
      "epoch: 30 step: 73, loss is 0.0005718733882531524\n",
      "epoch: 30 step: 74, loss is 0.023217955604195595\n",
      "epoch: 30 step: 75, loss is 0.034365180879831314\n",
      "epoch: 30 step: 76, loss is 0.00882843416184187\n",
      "epoch: 30 step: 77, loss is 0.02299279160797596\n",
      "epoch: 30 step: 78, loss is 0.01867450214922428\n",
      "epoch: 30 step: 79, loss is 0.0022940796334296465\n",
      "epoch: 30 step: 80, loss is 0.060294073075056076\n",
      "epoch: 30 step: 81, loss is 0.0007426580996252596\n",
      "epoch: 30 step: 82, loss is 0.006198987830430269\n",
      "epoch: 30 step: 83, loss is 0.0021590145770460367\n",
      "epoch: 30 step: 84, loss is 0.002716130344197154\n",
      "epoch: 30 step: 85, loss is 0.0019467280944809318\n",
      "epoch: 30 step: 86, loss is 0.021870655938982964\n",
      "epoch: 30 step: 87, loss is 0.23150794208049774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 88, loss is 0.044437140226364136\n",
      "epoch: 30 step: 89, loss is 0.00203501433134079\n",
      "epoch: 30 step: 90, loss is 0.021959280595183372\n",
      "epoch: 30 step: 91, loss is 0.00038313999539241195\n",
      "epoch: 30 step: 92, loss is 0.003266892395913601\n",
      "epoch: 30 step: 93, loss is 0.0003344894794281572\n",
      "epoch: 30 step: 94, loss is 5.576707189902663e-05\n",
      "epoch: 30 step: 95, loss is 0.0031489282846450806\n",
      "epoch: 30 step: 96, loss is 0.007054941263049841\n",
      "epoch: 30 step: 97, loss is 0.14350874722003937\n",
      "epoch: 30 step: 98, loss is 0.0003294007037766278\n",
      "epoch: 30 step: 99, loss is 0.0006037704879418015\n",
      "epoch: 30 step: 100, loss is 0.00037542800419032574\n",
      "epoch: 30 step: 101, loss is 0.008791200816631317\n",
      "epoch: 30 step: 102, loss is 0.0002739546471275389\n",
      "epoch: 30 step: 103, loss is 0.00659052561968565\n",
      "epoch: 30 step: 104, loss is 0.00033563977922312915\n",
      "epoch: 30 step: 105, loss is 0.08670959621667862\n",
      "epoch: 30 step: 106, loss is 0.0005587904015555978\n",
      "epoch: 30 step: 107, loss is 0.0033091381192207336\n",
      "epoch: 30 step: 108, loss is 0.06089240312576294\n",
      "epoch: 30 step: 109, loss is 0.0015368382446467876\n",
      "epoch: 30 step: 110, loss is 0.006026208866387606\n",
      "epoch: 30 step: 111, loss is 0.010875997133553028\n",
      "epoch: 30 step: 112, loss is 0.037942253053188324\n",
      "epoch: 30 step: 113, loss is 0.0003993731806986034\n",
      "epoch: 30 step: 114, loss is 0.0023057202342897654\n",
      "epoch: 30 step: 115, loss is 0.05618557333946228\n",
      "epoch: 30 step: 116, loss is 0.03948727622628212\n",
      "epoch: 30 step: 117, loss is 0.0021181879565119743\n",
      "epoch: 30 step: 118, loss is 0.00023264727497007698\n",
      "epoch: 30 step: 119, loss is 0.00044246960896998644\n",
      "epoch: 30 step: 120, loss is 0.00037795474054291844\n",
      "epoch: 30 step: 121, loss is 0.04493435472249985\n",
      "epoch: 30 step: 122, loss is 0.00013050646521151066\n",
      "epoch: 30 step: 123, loss is 0.0037452431861311197\n",
      "epoch: 30 step: 124, loss is 0.005377602763473988\n",
      "epoch: 30 step: 125, loss is 0.014644349925220013\n",
      "epoch: 30 step: 126, loss is 0.0014935885556042194\n",
      "epoch: 30 step: 127, loss is 0.012404471635818481\n",
      "epoch: 30 step: 128, loss is 5.4499516409123316e-05\n",
      "epoch: 30 step: 129, loss is 0.00020097718515899032\n",
      "epoch: 30 step: 130, loss is 0.004270784556865692\n",
      "epoch: 30 step: 131, loss is 0.0013348573120310903\n",
      "epoch: 30 step: 132, loss is 0.0030214195139706135\n",
      "epoch: 30 step: 133, loss is 0.0065139466896653175\n",
      "epoch: 30 step: 134, loss is 0.03310009837150574\n",
      "epoch: 30 step: 135, loss is 0.0014833976747468114\n",
      "epoch: 30 step: 136, loss is 0.0009668230195529759\n",
      "epoch: 30 step: 137, loss is 0.00567857688292861\n",
      "epoch: 30 step: 138, loss is 6.460347503889352e-05\n",
      "epoch: 30 step: 139, loss is 0.00010111724986927584\n",
      "epoch: 30 step: 140, loss is 0.024478456005454063\n",
      "epoch: 30 step: 141, loss is 0.0007989556179381907\n",
      "epoch: 30 step: 142, loss is 0.0006493700202554464\n",
      "epoch: 30 step: 143, loss is 0.005158592481166124\n",
      "epoch: 30 step: 144, loss is 0.0014295810833573341\n",
      "epoch: 30 step: 145, loss is 0.023358408361673355\n",
      "epoch: 30 step: 146, loss is 0.0010707982582971454\n",
      "epoch: 30 step: 147, loss is 0.000666705658659339\n",
      "epoch: 30 step: 148, loss is 0.001984919421374798\n",
      "epoch: 30 step: 149, loss is 0.0028250599279999733\n",
      "epoch: 30 step: 150, loss is 0.002943342784419656\n",
      "epoch: 30 step: 151, loss is 0.002993873553350568\n",
      "epoch: 30 step: 152, loss is 0.0001421604974893853\n",
      "epoch: 30 step: 153, loss is 0.044451773166656494\n",
      "epoch: 30 step: 154, loss is 0.017227094620466232\n",
      "epoch: 30 step: 155, loss is 8.829616126604378e-05\n",
      "epoch: 30 step: 156, loss is 0.0008416572818532586\n",
      "epoch: 30 step: 157, loss is 0.0005324397934600711\n",
      "epoch: 30 step: 158, loss is 0.021402735263109207\n",
      "epoch: 30 step: 159, loss is 2.868993215088267e-05\n",
      "epoch: 30 step: 160, loss is 8.19637716631405e-05\n",
      "epoch: 30 step: 161, loss is 0.00014711645781062543\n",
      "epoch: 30 step: 162, loss is 6.404587475117296e-05\n",
      "epoch: 30 step: 163, loss is 5.818379941047169e-05\n",
      "epoch: 30 step: 164, loss is 0.0002658843295648694\n",
      "epoch: 30 step: 165, loss is 0.012401102110743523\n",
      "epoch: 30 step: 166, loss is 0.001337873749434948\n",
      "epoch: 30 step: 167, loss is 0.009623795747756958\n",
      "epoch: 30 step: 168, loss is 0.00045631066313944757\n",
      "epoch: 30 step: 169, loss is 0.00015537365106865764\n",
      "epoch: 30 step: 170, loss is 0.020848145708441734\n",
      "epoch: 30 step: 171, loss is 0.0013055079616606236\n",
      "epoch: 30 step: 172, loss is 5.489547402248718e-05\n",
      "epoch: 30 step: 173, loss is 0.008442170917987823\n",
      "epoch: 30 step: 174, loss is 6.61026788293384e-05\n",
      "epoch: 30 step: 175, loss is 0.0008734124130569398\n",
      "epoch: 30 step: 176, loss is 0.0008445220883004367\n",
      "epoch: 30 step: 177, loss is 0.006010900251567364\n",
      "epoch: 30 step: 178, loss is 0.0025109881535172462\n",
      "epoch: 30 step: 179, loss is 3.616623143898323e-05\n",
      "epoch: 30 step: 180, loss is 8.338624866155442e-06\n",
      "epoch: 30 step: 181, loss is 0.03240211308002472\n",
      "epoch: 30 step: 182, loss is 0.00025694785290397704\n",
      "epoch: 30 step: 183, loss is 0.0003985586517956108\n",
      "epoch: 30 step: 184, loss is 0.023643286898732185\n",
      "epoch: 30 step: 185, loss is 9.94835136225447e-05\n",
      "epoch: 30 step: 186, loss is 0.0008206478669308126\n",
      "epoch: 30 step: 187, loss is 0.0008579267305321991\n",
      "epoch: 30 step: 188, loss is 0.005874142982065678\n",
      "epoch: 30 step: 189, loss is 0.00023416694602929056\n",
      "epoch: 30 step: 190, loss is 0.0009310909663327038\n",
      "epoch: 30 step: 191, loss is 0.0001270024513360113\n",
      "epoch: 30 step: 192, loss is 0.00010651569027686492\n",
      "epoch: 30 step: 193, loss is 0.00022405966592486948\n",
      "epoch: 30 step: 194, loss is 0.0005253762938082218\n",
      "epoch: 30 step: 195, loss is 0.007940027862787247\n",
      "epoch: 30 step: 196, loss is 4.471262218430638e-05\n",
      "epoch: 30 step: 197, loss is 0.04526689648628235\n",
      "epoch: 30 step: 198, loss is 0.0002590065123513341\n",
      "epoch: 30 step: 199, loss is 0.0021761618554592133\n",
      "epoch: 30 step: 200, loss is 0.0001404192007612437\n",
      "epoch: 30 step: 201, loss is 0.004151879344135523\n",
      "epoch: 30 step: 202, loss is 0.0041611990891397\n",
      "epoch: 30 step: 203, loss is 0.024481719359755516\n",
      "epoch: 30 step: 204, loss is 0.0001537200150778517\n",
      "epoch: 30 step: 205, loss is 0.003946848679333925\n",
      "epoch: 30 step: 206, loss is 0.00018624085350893438\n",
      "epoch: 30 step: 207, loss is 0.0005252029513940215\n",
      "epoch: 30 step: 208, loss is 0.0014175744727253914\n",
      "epoch: 30 step: 209, loss is 0.016206704080104828\n",
      "epoch: 30 step: 210, loss is 0.06861888617277145\n",
      "epoch: 30 step: 211, loss is 0.0012742088874801993\n",
      "epoch: 30 step: 212, loss is 5.367014091461897e-05\n",
      "epoch: 30 step: 213, loss is 0.0015074140392243862\n",
      "epoch: 30 step: 214, loss is 0.04089418798685074\n",
      "epoch: 30 step: 215, loss is 0.009465896524488926\n",
      "epoch: 30 step: 216, loss is 8.738117321627215e-05\n",
      "epoch: 30 step: 217, loss is 0.012083066627383232\n",
      "epoch: 30 step: 218, loss is 0.01256497111171484\n",
      "epoch: 30 step: 219, loss is 0.012089510448276997\n",
      "epoch: 30 step: 220, loss is 0.0008770729182288051\n",
      "epoch: 30 step: 221, loss is 0.00253827846609056\n",
      "epoch: 30 step: 222, loss is 0.0011869387235492468\n",
      "epoch: 30 step: 223, loss is 0.00010240732808597386\n",
      "epoch: 30 step: 224, loss is 0.015616098418831825\n",
      "epoch: 30 step: 225, loss is 0.007677994668483734\n",
      "epoch: 30 step: 226, loss is 0.007951409555971622\n",
      "epoch: 30 step: 227, loss is 0.013006654568016529\n",
      "epoch: 30 step: 228, loss is 0.0027998508885502815\n",
      "epoch: 30 step: 229, loss is 0.0011451002210378647\n",
      "epoch: 30 step: 230, loss is 0.00015328431618399918\n",
      "epoch: 30 step: 231, loss is 0.005295488517731428\n",
      "epoch: 30 step: 232, loss is 0.00014687106886412948\n",
      "epoch: 30 step: 233, loss is 0.004190885461866856\n",
      "epoch: 30 step: 234, loss is 0.000898198108188808\n",
      "epoch: 30 step: 235, loss is 0.002503375057131052\n",
      "epoch: 30 step: 236, loss is 0.012766053900122643\n",
      "epoch: 30 step: 237, loss is 0.0034706550650298595\n",
      "epoch: 30 step: 238, loss is 0.04761761426925659\n",
      "epoch: 30 step: 239, loss is 0.009843012318015099\n",
      "epoch: 30 step: 240, loss is 0.0006875380640849471\n",
      "epoch: 30 step: 241, loss is 0.001231711357831955\n",
      "epoch: 30 step: 242, loss is 0.00011171289224876091\n",
      "epoch: 30 step: 243, loss is 0.00040579080814495683\n",
      "epoch: 30 step: 244, loss is 0.0016044642543420196\n",
      "epoch: 30 step: 245, loss is 0.00454738549888134\n",
      "epoch: 30 step: 246, loss is 0.0013130492297932506\n",
      "epoch: 30 step: 247, loss is 0.0014350051060318947\n",
      "epoch: 30 step: 248, loss is 0.1528923511505127\n",
      "epoch: 30 step: 249, loss is 0.0002083855652017519\n",
      "epoch: 30 step: 250, loss is 0.00201913109049201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 251, loss is 0.0007885865052230656\n",
      "epoch: 30 step: 252, loss is 0.03660927340388298\n",
      "epoch: 30 step: 253, loss is 0.0009274156764149666\n",
      "epoch: 30 step: 254, loss is 0.00027667611720971763\n",
      "epoch: 30 step: 255, loss is 0.003212135285139084\n",
      "epoch: 30 step: 256, loss is 0.002041370142251253\n",
      "epoch: 30 step: 257, loss is 0.0008767656399868429\n",
      "epoch: 30 step: 258, loss is 0.008096928708255291\n",
      "epoch: 30 step: 259, loss is 0.00037258892552927136\n",
      "epoch: 30 step: 260, loss is 0.00858019944280386\n",
      "epoch: 30 step: 261, loss is 0.01771230809390545\n",
      "epoch: 30 step: 262, loss is 0.0006189304986037314\n",
      "epoch: 30 step: 263, loss is 0.00034427884384058416\n",
      "epoch: 30 step: 264, loss is 0.020460575819015503\n",
      "epoch: 30 step: 265, loss is 0.0014431413728743792\n",
      "epoch: 30 step: 266, loss is 0.05852309986948967\n",
      "epoch: 30 step: 267, loss is 0.0029993108473718166\n",
      "epoch: 30 step: 268, loss is 0.0007843631319701672\n",
      "epoch: 30 step: 269, loss is 0.016206059604883194\n",
      "epoch: 30 step: 270, loss is 0.0007061092765070498\n",
      "epoch: 30 step: 271, loss is 9.147223317995667e-05\n",
      "epoch: 30 step: 272, loss is 1.071281440090388e-05\n",
      "epoch: 30 step: 273, loss is 0.00046947167720645666\n",
      "epoch: 30 step: 274, loss is 0.005343456752598286\n",
      "epoch: 30 step: 275, loss is 0.0024323666002601385\n",
      "epoch: 30 step: 276, loss is 0.010477857664227486\n",
      "epoch: 30 step: 277, loss is 0.009867786429822445\n",
      "epoch: 30 step: 278, loss is 0.00028724627918563783\n",
      "epoch: 30 step: 279, loss is 0.00018126957002095878\n",
      "epoch: 30 step: 280, loss is 0.0004707565240096301\n",
      "epoch: 30 step: 281, loss is 1.8165810615755618e-05\n",
      "epoch: 30 step: 282, loss is 0.2214020937681198\n",
      "epoch: 30 step: 283, loss is 4.1240968130296096e-05\n",
      "epoch: 30 step: 284, loss is 0.0004071087168995291\n",
      "epoch: 30 step: 285, loss is 0.0030949425417929888\n",
      "epoch: 30 step: 286, loss is 3.1042636692291126e-05\n",
      "epoch: 30 step: 287, loss is 0.0009356890805065632\n",
      "epoch: 30 step: 288, loss is 0.01259610615670681\n",
      "epoch: 30 step: 289, loss is 0.0003686579002533108\n",
      "epoch: 30 step: 290, loss is 0.014079354703426361\n",
      "epoch: 30 step: 291, loss is 0.00756358727812767\n",
      "epoch: 30 step: 292, loss is 0.07866784185171127\n",
      "epoch: 30 step: 293, loss is 0.001329546794295311\n",
      "epoch: 30 step: 294, loss is 0.0035054015461355448\n",
      "epoch: 30 step: 295, loss is 5.8959569287253544e-05\n",
      "epoch: 30 step: 296, loss is 0.0002865976421162486\n",
      "epoch: 30 step: 297, loss is 0.001296979608014226\n",
      "epoch: 30 step: 298, loss is 0.001009313971735537\n",
      "epoch: 30 step: 299, loss is 0.007571077439934015\n",
      "epoch: 30 step: 300, loss is 0.07939701527357101\n",
      "epoch: 30 step: 301, loss is 0.0016631388571113348\n",
      "epoch: 30 step: 302, loss is 0.00018812992493622005\n",
      "epoch: 30 step: 303, loss is 0.0003871650551445782\n",
      "epoch: 30 step: 304, loss is 0.005299974232912064\n",
      "epoch: 30 step: 305, loss is 0.0022502040956169367\n",
      "epoch: 30 step: 306, loss is 0.0018014509696513414\n",
      "epoch: 30 step: 307, loss is 0.09050531685352325\n",
      "epoch: 30 step: 308, loss is 0.001497814548201859\n",
      "epoch: 30 step: 309, loss is 0.005834911949932575\n",
      "epoch: 30 step: 310, loss is 0.011103872209787369\n",
      "epoch: 30 step: 311, loss is 0.0003210395807400346\n",
      "epoch: 30 step: 312, loss is 0.005864843726158142\n",
      "epoch: 30 step: 313, loss is 0.0042014396749436855\n",
      "epoch: 30 step: 314, loss is 0.005221506580710411\n",
      "epoch: 30 step: 315, loss is 0.0006689270958304405\n",
      "epoch: 30 step: 316, loss is 0.09141983091831207\n",
      "epoch: 30 step: 317, loss is 0.003753458149731159\n",
      "epoch: 30 step: 318, loss is 0.00046836453839205205\n",
      "epoch: 30 step: 319, loss is 0.0008780379430390894\n",
      "epoch: 30 step: 320, loss is 0.001625289674848318\n",
      "epoch: 30 step: 321, loss is 7.513657328672707e-05\n",
      "epoch: 30 step: 322, loss is 0.0036998740397393703\n",
      "epoch: 30 step: 323, loss is 0.029129013419151306\n",
      "epoch: 30 step: 324, loss is 0.02198648452758789\n",
      "epoch: 30 step: 325, loss is 0.0020375452004373074\n",
      "epoch: 30 step: 326, loss is 0.43940216302871704\n",
      "epoch: 30 step: 327, loss is 1.2934286132804118e-05\n",
      "epoch: 30 step: 328, loss is 0.00066397525370121\n",
      "epoch: 30 step: 329, loss is 0.0013533018063753843\n",
      "epoch: 30 step: 330, loss is 0.035255540162324905\n",
      "epoch: 30 step: 331, loss is 5.9564801631495357e-05\n",
      "epoch: 30 step: 332, loss is 0.25294947624206543\n",
      "epoch: 30 step: 333, loss is 0.003747686743736267\n",
      "epoch: 30 step: 334, loss is 0.0002221337053924799\n",
      "epoch: 30 step: 335, loss is 7.750128133920953e-05\n",
      "epoch: 30 step: 336, loss is 0.009912251494824886\n",
      "epoch: 30 step: 337, loss is 0.0008928952738642693\n",
      "epoch: 30 step: 338, loss is 0.010711648501455784\n",
      "epoch: 30 step: 339, loss is 0.020648783072829247\n",
      "epoch: 30 step: 340, loss is 0.00011295820877421647\n",
      "epoch: 30 step: 341, loss is 0.00011633311805780977\n",
      "epoch: 30 step: 342, loss is 0.0022740724962204695\n",
      "epoch: 30 step: 343, loss is 0.04260781407356262\n",
      "epoch: 30 step: 344, loss is 0.004546335898339748\n",
      "epoch: 30 step: 345, loss is 0.00022973949671722949\n",
      "epoch: 30 step: 346, loss is 0.0016169581795111299\n",
      "epoch: 30 step: 347, loss is 0.0004834617720916867\n",
      "epoch: 30 step: 348, loss is 0.023997541517019272\n",
      "epoch: 30 step: 349, loss is 0.027327969670295715\n",
      "epoch: 30 step: 350, loss is 0.219419926404953\n",
      "epoch: 30 step: 351, loss is 0.02297954447567463\n",
      "epoch: 30 step: 352, loss is 0.00015807425370439887\n",
      "epoch: 30 step: 353, loss is 0.00025412291870452464\n",
      "epoch: 30 step: 354, loss is 0.0025274003855884075\n",
      "epoch: 30 step: 355, loss is 0.001000629854388535\n",
      "epoch: 30 step: 356, loss is 0.000124673024402\n",
      "epoch: 30 step: 357, loss is 0.00028957444010302424\n",
      "epoch: 30 step: 358, loss is 0.007228377275168896\n",
      "epoch: 30 step: 359, loss is 0.019502082839608192\n",
      "epoch: 30 step: 360, loss is 0.0003429984790273011\n",
      "epoch: 30 step: 361, loss is 0.0027452956419438124\n",
      "epoch: 30 step: 362, loss is 0.0003634443273767829\n",
      "epoch: 30 step: 363, loss is 0.02863870933651924\n",
      "epoch: 30 step: 364, loss is 0.015210511162877083\n",
      "epoch: 30 step: 365, loss is 0.008091116324067116\n",
      "epoch: 30 step: 366, loss is 0.028447704389691353\n",
      "epoch: 30 step: 367, loss is 0.00788882002234459\n",
      "epoch: 30 step: 368, loss is 0.0394127182662487\n",
      "epoch: 30 step: 369, loss is 0.00029807258397340775\n",
      "epoch: 30 step: 370, loss is 0.0005735376616939902\n",
      "epoch: 30 step: 371, loss is 0.002415296621620655\n",
      "epoch: 30 step: 372, loss is 0.0004854208091273904\n",
      "epoch: 30 step: 373, loss is 0.004256492480635643\n",
      "epoch: 30 step: 374, loss is 0.023607566952705383\n",
      "epoch: 30 step: 375, loss is 0.036835603415966034\n",
      "epoch: 30 step: 376, loss is 0.0003950187820009887\n",
      "epoch: 30 step: 377, loss is 3.0499944841722026e-05\n",
      "epoch: 30 step: 378, loss is 0.0003088566299993545\n",
      "epoch: 30 step: 379, loss is 0.03831077367067337\n",
      "epoch: 30 step: 380, loss is 0.011834804899990559\n",
      "epoch: 30 step: 381, loss is 0.0005596386617980897\n",
      "epoch: 30 step: 382, loss is 0.0003304115671198815\n",
      "epoch: 30 step: 383, loss is 0.011385481804609299\n",
      "epoch: 30 step: 384, loss is 0.007172619923949242\n",
      "epoch: 30 step: 385, loss is 0.0007659274851903319\n",
      "epoch: 30 step: 386, loss is 0.0008997710538096726\n",
      "epoch: 30 step: 387, loss is 0.0011218618601560593\n",
      "epoch: 30 step: 388, loss is 0.00010181360994465649\n",
      "epoch: 30 step: 389, loss is 0.0004579884698614478\n",
      "epoch: 30 step: 390, loss is 0.021048109978437424\n",
      "epoch: 30 step: 391, loss is 0.040725041180849075\n",
      "epoch: 30 step: 392, loss is 0.001926989876665175\n",
      "epoch: 30 step: 393, loss is 0.002846763702109456\n",
      "epoch: 30 step: 394, loss is 0.0001286050392081961\n",
      "epoch: 30 step: 395, loss is 0.0005500089027918875\n",
      "epoch: 30 step: 396, loss is 0.0004579559026751667\n",
      "epoch: 30 step: 397, loss is 0.00023175435489974916\n",
      "epoch: 30 step: 398, loss is 0.001949228229932487\n",
      "epoch: 30 step: 399, loss is 0.0001798089360818267\n",
      "epoch: 30 step: 400, loss is 0.0013986715348437428\n",
      "epoch: 30 step: 401, loss is 0.01124605629593134\n",
      "epoch: 30 step: 402, loss is 0.05326833948493004\n",
      "epoch: 30 step: 403, loss is 0.011071516200900078\n",
      "epoch: 30 step: 404, loss is 0.0018757539801299572\n",
      "epoch: 30 step: 405, loss is 0.0045769596472382545\n",
      "epoch: 30 step: 406, loss is 0.015148094855248928\n",
      "epoch: 30 step: 407, loss is 0.000976016279309988\n",
      "epoch: 30 step: 408, loss is 6.547120574396104e-05\n",
      "epoch: 30 step: 409, loss is 0.008380312472581863\n",
      "epoch: 30 step: 410, loss is 0.0012005483731627464\n",
      "epoch: 30 step: 411, loss is 0.0006183417281135917\n",
      "epoch: 30 step: 412, loss is 0.0016593383625149727\n",
      "epoch: 30 step: 413, loss is 0.14118440449237823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 414, loss is 0.00250641000457108\n",
      "epoch: 30 step: 415, loss is 0.015949402004480362\n",
      "epoch: 30 step: 416, loss is 0.004269372206181288\n",
      "epoch: 30 step: 417, loss is 8.407199493376538e-05\n",
      "epoch: 30 step: 418, loss is 7.806793291820213e-05\n",
      "epoch: 30 step: 419, loss is 0.05934890732169151\n",
      "epoch: 30 step: 420, loss is 0.0004196512163616717\n",
      "epoch: 30 step: 421, loss is 0.00032747609657235444\n",
      "epoch: 30 step: 422, loss is 0.000751179235521704\n",
      "epoch: 30 step: 423, loss is 0.007272996474057436\n",
      "epoch: 30 step: 424, loss is 0.041463445872068405\n",
      "epoch: 30 step: 425, loss is 0.00048531583161093295\n",
      "epoch: 30 step: 426, loss is 0.13748261332511902\n",
      "epoch: 30 step: 427, loss is 8.592692029196769e-05\n",
      "epoch: 30 step: 428, loss is 0.006251935847103596\n",
      "epoch: 30 step: 429, loss is 0.0008457114454358816\n",
      "epoch: 30 step: 430, loss is 0.00023703502665739506\n",
      "epoch: 30 step: 431, loss is 0.01227433979511261\n",
      "epoch: 30 step: 432, loss is 0.014489603228867054\n",
      "epoch: 30 step: 433, loss is 0.027255496010184288\n",
      "epoch: 30 step: 434, loss is 0.024447446689009666\n",
      "epoch: 30 step: 435, loss is 6.500173185486346e-05\n",
      "epoch: 30 step: 436, loss is 0.00014996572281233966\n",
      "epoch: 30 step: 437, loss is 0.03023470565676689\n",
      "epoch: 30 step: 438, loss is 0.04435867443680763\n",
      "epoch: 30 step: 439, loss is 0.007862483151257038\n",
      "epoch: 30 step: 440, loss is 0.00048237747978419065\n",
      "epoch: 30 step: 441, loss is 0.02246822975575924\n",
      "epoch: 30 step: 442, loss is 0.001037999289110303\n",
      "epoch: 30 step: 443, loss is 0.0014526632148772478\n",
      "epoch: 30 step: 444, loss is 0.011890139430761337\n",
      "epoch: 30 step: 445, loss is 0.0035303819458931684\n",
      "epoch: 30 step: 446, loss is 0.006633534096181393\n",
      "epoch: 30 step: 447, loss is 0.00021082707098685205\n",
      "epoch: 30 step: 448, loss is 0.014234686270356178\n",
      "epoch: 30 step: 449, loss is 0.00033372468897141516\n",
      "epoch: 30 step: 450, loss is 0.0013730464270338416\n",
      "epoch: 30 step: 451, loss is 0.005181140266358852\n",
      "epoch: 30 step: 452, loss is 0.00597668020054698\n",
      "epoch: 30 step: 453, loss is 0.008583956398069859\n",
      "epoch: 30 step: 454, loss is 0.00021679390920326114\n",
      "epoch: 30 step: 455, loss is 0.02117479220032692\n",
      "epoch: 30 step: 456, loss is 0.05856314301490784\n",
      "epoch: 30 step: 457, loss is 0.001896996283903718\n",
      "epoch: 30 step: 458, loss is 0.003642007475718856\n",
      "epoch: 30 step: 459, loss is 0.012910156510770321\n",
      "epoch: 30 step: 460, loss is 0.001619033981114626\n",
      "epoch: 30 step: 461, loss is 0.005405970383435488\n",
      "epoch: 30 step: 462, loss is 0.003972480073571205\n",
      "epoch: 30 step: 463, loss is 0.0005551549256779253\n",
      "epoch: 30 step: 464, loss is 0.014786755666136742\n",
      "epoch: 30 step: 465, loss is 0.001520383171737194\n",
      "epoch: 30 step: 466, loss is 0.018368860706686974\n",
      "epoch: 30 step: 467, loss is 0.00011047124280594289\n",
      "epoch: 30 step: 468, loss is 0.015048093162477016\n",
      "epoch: 30 step: 469, loss is 0.001249168999493122\n",
      "epoch: 30 step: 470, loss is 0.002370414324104786\n",
      "epoch: 30 step: 471, loss is 0.0015255574835464358\n",
      "epoch: 30 step: 472, loss is 0.00021168647799640894\n",
      "epoch: 30 step: 473, loss is 0.00349088991060853\n",
      "epoch: 30 step: 474, loss is 0.022784775123000145\n",
      "epoch: 30 step: 475, loss is 0.0007241438142955303\n",
      "epoch: 30 step: 476, loss is 0.05455174669623375\n",
      "epoch: 30 step: 477, loss is 0.0026193619705736637\n",
      "epoch: 30 step: 478, loss is 0.0006668103742413223\n",
      "epoch: 30 step: 479, loss is 0.00047627725871279836\n",
      "epoch: 30 step: 480, loss is 0.0020726362708956003\n",
      "epoch: 30 step: 481, loss is 0.0024001337587833405\n",
      "epoch: 30 step: 482, loss is 0.0005630866507999599\n",
      "epoch: 30 step: 483, loss is 0.0028219951782375574\n",
      "epoch: 30 step: 484, loss is 0.0009466062765568495\n",
      "epoch: 30 step: 485, loss is 0.0009245435940101743\n",
      "epoch: 30 step: 486, loss is 0.000206704149604775\n",
      "epoch: 30 step: 487, loss is 0.00028215456404723227\n",
      "epoch: 30 step: 488, loss is 0.02343946322798729\n",
      "epoch: 30 step: 489, loss is 0.004782743752002716\n",
      "epoch: 30 step: 490, loss is 0.00034765200689435005\n",
      "epoch: 30 step: 491, loss is 0.01604755036532879\n",
      "epoch: 30 step: 492, loss is 0.00047807645751163363\n",
      "epoch: 30 step: 493, loss is 0.010579373687505722\n",
      "epoch: 30 step: 494, loss is 0.00030576225253753364\n",
      "epoch: 30 step: 495, loss is 0.0029775083530694246\n",
      "epoch: 30 step: 496, loss is 0.00010003771603805944\n",
      "epoch: 30 step: 497, loss is 0.005186745896935463\n",
      "epoch: 30 step: 498, loss is 0.004967959132045507\n",
      "epoch: 30 step: 499, loss is 9.98244941001758e-05\n",
      "epoch: 30 step: 500, loss is 0.0014840206131339073\n",
      "epoch: 30 step: 501, loss is 0.00030248347320593894\n",
      "epoch: 30 step: 502, loss is 0.0044053783640265465\n",
      "epoch: 30 step: 503, loss is 0.0004877264436800033\n",
      "epoch: 30 step: 504, loss is 0.000657534459605813\n",
      "epoch: 30 step: 505, loss is 0.0006776420050300658\n",
      "epoch: 30 step: 506, loss is 0.00028839471633546054\n",
      "epoch: 30 step: 507, loss is 0.00013031699927523732\n",
      "epoch: 30 step: 508, loss is 0.0006957178120501339\n",
      "epoch: 30 step: 509, loss is 0.0006225183606147766\n",
      "epoch: 30 step: 510, loss is 0.001176616526208818\n",
      "epoch: 30 step: 511, loss is 0.0006436307448893785\n",
      "epoch: 30 step: 512, loss is 5.412736936705187e-05\n",
      "epoch: 30 step: 513, loss is 0.0063295806758105755\n",
      "epoch: 30 step: 514, loss is 0.000551066710613668\n",
      "epoch: 30 step: 515, loss is 0.00354409939609468\n",
      "epoch: 30 step: 516, loss is 0.0001316378911724314\n",
      "epoch: 30 step: 517, loss is 7.753928912279662e-06\n",
      "epoch: 30 step: 518, loss is 0.00017642344755586237\n",
      "epoch: 30 step: 519, loss is 0.00044688585330732167\n",
      "epoch: 30 step: 520, loss is 0.02259180136024952\n",
      "epoch: 30 step: 521, loss is 0.0037915098946541548\n",
      "epoch: 30 step: 522, loss is 7.871075467846822e-06\n",
      "epoch: 30 step: 523, loss is 2.1351697796490043e-05\n",
      "epoch: 30 step: 524, loss is 5.8875208196695894e-05\n",
      "epoch: 30 step: 525, loss is 0.00032779216417111456\n",
      "epoch: 30 step: 526, loss is 0.0009257458732463419\n",
      "epoch: 30 step: 527, loss is 0.001188257010653615\n",
      "epoch: 30 step: 528, loss is 0.014534926041960716\n",
      "epoch: 30 step: 529, loss is 0.000738594913855195\n",
      "epoch: 30 step: 530, loss is 6.482922617578879e-05\n",
      "epoch: 30 step: 531, loss is 8.962413266999647e-05\n",
      "epoch: 30 step: 532, loss is 0.0025762582663446665\n",
      "epoch: 30 step: 533, loss is 0.0007445328519679606\n",
      "epoch: 30 step: 534, loss is 7.077898771967739e-05\n",
      "epoch: 30 step: 535, loss is 0.0065124863758683205\n",
      "epoch: 30 step: 536, loss is 0.0024113846011459827\n",
      "epoch: 30 step: 537, loss is 0.00025432504480704665\n",
      "epoch: 30 step: 538, loss is 0.0001488555280957371\n",
      "epoch: 30 step: 539, loss is 0.0002005167625611648\n",
      "epoch: 30 step: 540, loss is 0.0007921530632302165\n",
      "epoch: 30 step: 541, loss is 0.0005024601123295724\n",
      "epoch: 30 step: 542, loss is 0.003901259507983923\n",
      "epoch: 30 step: 543, loss is 0.0033554560504853725\n",
      "epoch: 30 step: 544, loss is 0.0005229103844612837\n",
      "epoch: 30 step: 545, loss is 0.0018518113065510988\n",
      "epoch: 30 step: 546, loss is 5.388984936871566e-05\n",
      "epoch: 30 step: 547, loss is 2.6256488126819022e-05\n",
      "epoch: 30 step: 548, loss is 0.00017485830176156014\n",
      "epoch: 30 step: 549, loss is 0.00017735142318997532\n",
      "epoch: 30 step: 550, loss is 0.00026930717285722494\n",
      "epoch: 30 step: 551, loss is 7.137177453842014e-05\n",
      "epoch: 30 step: 552, loss is 0.00037930801045149565\n",
      "epoch: 30 step: 553, loss is 0.007897000759840012\n",
      "epoch: 30 step: 554, loss is 0.019644543528556824\n",
      "epoch: 30 step: 555, loss is 0.011085991747677326\n",
      "epoch: 30 step: 556, loss is 0.00028324054437689483\n",
      "epoch: 30 step: 557, loss is 0.00019123633683193475\n",
      "epoch: 30 step: 558, loss is 0.0007406247314065695\n",
      "epoch: 30 step: 559, loss is 0.0010209582978859544\n",
      "epoch: 30 step: 560, loss is 9.302642138209194e-05\n",
      "epoch: 30 step: 561, loss is 0.00013415611465461552\n",
      "epoch: 30 step: 562, loss is 0.007553098723292351\n",
      "epoch: 30 step: 563, loss is 0.005681824404746294\n",
      "epoch: 30 step: 564, loss is 0.0007828535162843764\n",
      "epoch: 30 step: 565, loss is 0.001048796926625073\n",
      "epoch: 30 step: 566, loss is 0.00011753321450669318\n",
      "epoch: 30 step: 567, loss is 0.00048183370381593704\n",
      "epoch: 30 step: 568, loss is 0.0008019147207960486\n",
      "epoch: 30 step: 569, loss is 4.1611310734879225e-05\n",
      "epoch: 30 step: 570, loss is 8.647250069770962e-06\n",
      "epoch: 30 step: 571, loss is 0.00023845447867643088\n",
      "epoch: 30 step: 572, loss is 8.227146463468671e-05\n",
      "epoch: 30 step: 573, loss is 0.00016466033412143588\n",
      "epoch: 30 step: 574, loss is 0.0007066123653203249\n",
      "epoch: 30 step: 575, loss is 0.02911405824124813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 576, loss is 0.005256393924355507\n",
      "epoch: 30 step: 577, loss is 0.0005084361764602363\n",
      "epoch: 30 step: 578, loss is 0.009257693774998188\n",
      "epoch: 30 step: 579, loss is 0.00035441675572656095\n",
      "epoch: 30 step: 580, loss is 0.0004831133410334587\n",
      "epoch: 30 step: 581, loss is 0.000514625571668148\n",
      "epoch: 30 step: 582, loss is 5.776029502158053e-05\n",
      "epoch: 30 step: 583, loss is 0.0036611068062484264\n",
      "epoch: 30 step: 584, loss is 0.0006127713713794947\n",
      "epoch: 30 step: 585, loss is 0.003733211662620306\n",
      "epoch: 30 step: 586, loss is 0.001117455423809588\n",
      "epoch: 30 step: 587, loss is 0.00245374976657331\n",
      "epoch: 30 step: 588, loss is 0.0013709592167288065\n",
      "epoch: 30 step: 589, loss is 0.04416253790259361\n",
      "epoch: 30 step: 590, loss is 0.0008086228044703603\n",
      "epoch: 30 step: 591, loss is 0.000400249584345147\n",
      "epoch: 30 step: 592, loss is 0.004016280174255371\n",
      "epoch: 30 step: 593, loss is 0.0019338297424837947\n",
      "epoch: 30 step: 594, loss is 6.728658263455145e-06\n",
      "epoch: 30 step: 595, loss is 0.000361021637218073\n",
      "epoch: 30 step: 596, loss is 0.0014677973231300712\n",
      "epoch: 30 step: 597, loss is 0.0007120300433598459\n",
      "epoch: 30 step: 598, loss is 3.490281960694119e-05\n",
      "epoch: 30 step: 599, loss is 0.0001657287939451635\n",
      "epoch: 30 step: 600, loss is 0.0028666220605373383\n",
      "epoch: 30 step: 601, loss is 0.05248667299747467\n",
      "epoch: 30 step: 602, loss is 0.0008983414736576378\n",
      "epoch: 30 step: 603, loss is 0.0009571103728376329\n",
      "epoch: 30 step: 604, loss is 0.003923158161342144\n",
      "epoch: 30 step: 605, loss is 0.00020144201698713005\n",
      "epoch: 30 step: 606, loss is 0.007571302354335785\n",
      "epoch: 30 step: 607, loss is 0.04374529793858528\n",
      "epoch: 30 step: 608, loss is 0.002147154649719596\n",
      "epoch: 30 step: 609, loss is 5.629231964121573e-05\n",
      "epoch: 30 step: 610, loss is 5.7683999330038205e-05\n",
      "epoch: 30 step: 611, loss is 0.004937403369694948\n",
      "epoch: 30 step: 612, loss is 0.028662674129009247\n",
      "epoch: 30 step: 613, loss is 3.1669515010435134e-05\n",
      "epoch: 30 step: 614, loss is 0.0003583076177164912\n",
      "epoch: 30 step: 615, loss is 0.002593729645013809\n",
      "epoch: 30 step: 616, loss is 6.403723818948492e-05\n",
      "epoch: 30 step: 617, loss is 0.0021667941473424435\n",
      "epoch: 30 step: 618, loss is 0.0017482545226812363\n",
      "epoch: 30 step: 619, loss is 0.00016043857613112777\n",
      "epoch: 30 step: 620, loss is 7.132016617106274e-05\n",
      "epoch: 30 step: 621, loss is 6.245769327506423e-05\n",
      "epoch: 30 step: 622, loss is 1.4161083527142182e-05\n",
      "epoch: 30 step: 623, loss is 0.01197026576846838\n",
      "epoch: 30 step: 624, loss is 0.0006747160223312676\n",
      "epoch: 30 step: 625, loss is 0.0022852157708257437\n",
      "epoch: 30 step: 626, loss is 0.0002574866230133921\n",
      "epoch: 30 step: 627, loss is 0.007919094525277615\n",
      "epoch: 30 step: 628, loss is 0.0008302005007863045\n",
      "epoch: 30 step: 629, loss is 0.00023183011217042804\n",
      "epoch: 30 step: 630, loss is 0.0003718051011674106\n",
      "epoch: 30 step: 631, loss is 0.006099761463701725\n",
      "epoch: 30 step: 632, loss is 0.0004261692811269313\n",
      "epoch: 30 step: 633, loss is 0.004813113249838352\n",
      "epoch: 30 step: 634, loss is 0.005160566885024309\n",
      "epoch: 30 step: 635, loss is 0.0004950774600729346\n",
      "epoch: 30 step: 636, loss is 0.01106232963502407\n",
      "epoch: 30 step: 637, loss is 0.000918810605071485\n",
      "epoch: 30 step: 638, loss is 1.1690736755554099e-05\n",
      "epoch: 30 step: 639, loss is 0.013325332663953304\n",
      "epoch: 30 step: 640, loss is 0.0005861588870175183\n",
      "epoch: 30 step: 641, loss is 0.00042588356882333755\n",
      "epoch: 30 step: 642, loss is 0.0027045728638768196\n",
      "epoch: 30 step: 643, loss is 1.5531722965533845e-05\n",
      "epoch: 30 step: 644, loss is 0.0007319436990655959\n",
      "epoch: 30 step: 645, loss is 0.0004253477673046291\n",
      "epoch: 30 step: 646, loss is 0.003549505490809679\n",
      "epoch: 30 step: 647, loss is 0.002017635153606534\n",
      "epoch: 30 step: 648, loss is 0.0006719579687342048\n",
      "epoch: 30 step: 649, loss is 1.875715133792255e-05\n",
      "epoch: 30 step: 650, loss is 0.022808972746133804\n",
      "epoch: 30 step: 651, loss is 0.0011023543775081635\n",
      "epoch: 30 step: 652, loss is 0.00011911099136341363\n",
      "epoch: 30 step: 653, loss is 0.0018348748562857509\n",
      "epoch: 30 step: 654, loss is 1.0971629308187403e-05\n",
      "epoch: 30 step: 655, loss is 8.276570588350296e-05\n",
      "epoch: 30 step: 656, loss is 0.0789543017745018\n",
      "epoch: 30 step: 657, loss is 0.0001551601744722575\n",
      "epoch: 30 step: 658, loss is 0.00019451873959042132\n",
      "epoch: 30 step: 659, loss is 0.020278822630643845\n",
      "epoch: 30 step: 660, loss is 0.0016297070542350411\n",
      "epoch: 30 step: 661, loss is 0.02264050766825676\n",
      "epoch: 30 step: 662, loss is 0.00012720574159175158\n",
      "epoch: 30 step: 663, loss is 0.0022303294390439987\n",
      "epoch: 30 step: 664, loss is 0.00793051440268755\n",
      "epoch: 30 step: 665, loss is 0.0016864635981619358\n",
      "epoch: 30 step: 666, loss is 0.0019485951634123921\n",
      "epoch: 30 step: 667, loss is 0.01728534884750843\n",
      "epoch: 30 step: 668, loss is 0.0011618356220424175\n",
      "epoch: 30 step: 669, loss is 0.01613631844520569\n",
      "epoch: 30 step: 670, loss is 0.003676528111100197\n",
      "epoch: 30 step: 671, loss is 0.0005551115027628839\n",
      "epoch: 30 step: 672, loss is 0.0009440115536563098\n",
      "epoch: 30 step: 673, loss is 0.0007529559661634266\n",
      "epoch: 30 step: 674, loss is 3.3682288631098345e-05\n",
      "epoch: 30 step: 675, loss is 0.0015762201510369778\n",
      "epoch: 30 step: 676, loss is 0.003005875740200281\n",
      "epoch: 30 step: 677, loss is 0.0021864571608603\n",
      "epoch: 30 step: 678, loss is 0.0011268995003774762\n",
      "epoch: 30 step: 679, loss is 0.00025775781250558794\n",
      "epoch: 30 step: 680, loss is 0.021833479404449463\n",
      "epoch: 30 step: 681, loss is 0.001944825635291636\n",
      "epoch: 30 step: 682, loss is 0.0015235326718539\n",
      "epoch: 30 step: 683, loss is 0.0002866891445592046\n",
      "epoch: 30 step: 684, loss is 0.0025106614921242\n",
      "epoch: 30 step: 685, loss is 0.00891303364187479\n",
      "epoch: 30 step: 686, loss is 0.0006956428987905383\n",
      "epoch: 30 step: 687, loss is 0.0038008377887308598\n",
      "epoch: 30 step: 688, loss is 0.02532387524843216\n",
      "epoch: 30 step: 689, loss is 0.0019680617842823267\n",
      "epoch: 30 step: 690, loss is 0.002707491163164377\n",
      "epoch: 30 step: 691, loss is 0.0006375365192070603\n",
      "epoch: 30 step: 692, loss is 0.0011894234921783209\n",
      "epoch: 30 step: 693, loss is 0.03855478763580322\n",
      "epoch: 30 step: 694, loss is 0.05408178269863129\n",
      "epoch: 30 step: 695, loss is 0.0009308308362960815\n",
      "epoch: 30 step: 696, loss is 0.015308521687984467\n",
      "epoch: 30 step: 697, loss is 0.0020035712514072657\n",
      "epoch: 30 step: 698, loss is 0.0006234404863789678\n",
      "epoch: 30 step: 699, loss is 0.06472622603178024\n",
      "epoch: 30 step: 700, loss is 0.0026079886592924595\n",
      "epoch: 30 step: 701, loss is 0.0037771507631987333\n",
      "epoch: 30 step: 702, loss is 0.014790295623242855\n",
      "epoch: 30 step: 703, loss is 0.007967944256961346\n",
      "epoch: 30 step: 704, loss is 0.0006465541664510965\n",
      "epoch: 30 step: 705, loss is 0.0027983456384390593\n",
      "epoch: 30 step: 706, loss is 0.0015890438808128238\n",
      "epoch: 30 step: 707, loss is 0.030500153079628944\n",
      "epoch: 30 step: 708, loss is 0.0015589292161166668\n",
      "epoch: 30 step: 709, loss is 0.004098532255738974\n",
      "epoch: 30 step: 710, loss is 0.001683720969595015\n",
      "epoch: 30 step: 711, loss is 0.008664185181260109\n",
      "epoch: 30 step: 712, loss is 0.006436893716454506\n",
      "epoch: 30 step: 713, loss is 0.00097565574105829\n",
      "epoch: 30 step: 714, loss is 0.0023811247665435076\n",
      "epoch: 30 step: 715, loss is 0.0030374289490282536\n",
      "epoch: 30 step: 716, loss is 0.0010397399310022593\n",
      "epoch: 30 step: 717, loss is 0.0007916553295217454\n",
      "epoch: 30 step: 718, loss is 0.002699128817766905\n",
      "epoch: 30 step: 719, loss is 0.007135832216590643\n",
      "epoch: 30 step: 720, loss is 0.024915320798754692\n",
      "epoch: 30 step: 721, loss is 0.00017566386668477207\n",
      "epoch: 30 step: 722, loss is 0.0007304672617465258\n",
      "epoch: 30 step: 723, loss is 0.0005019027157686651\n",
      "epoch: 30 step: 724, loss is 0.0013253276702016592\n",
      "epoch: 30 step: 725, loss is 0.00014244978956412524\n",
      "epoch: 30 step: 726, loss is 0.007832142524421215\n",
      "epoch: 30 step: 727, loss is 0.0002634664997458458\n",
      "epoch: 30 step: 728, loss is 4.43008539150469e-05\n",
      "epoch: 30 step: 729, loss is 0.001745913177728653\n",
      "epoch: 30 step: 730, loss is 0.0014679114101454616\n",
      "epoch: 30 step: 731, loss is 0.023839537054300308\n",
      "epoch: 30 step: 732, loss is 0.0001773416588548571\n",
      "epoch: 30 step: 733, loss is 0.011485280469059944\n",
      "epoch: 30 step: 734, loss is 0.010161240585148335\n",
      "epoch: 30 step: 735, loss is 0.00042281849891878664\n",
      "epoch: 30 step: 736, loss is 1.9414428606978618e-05\n",
      "epoch: 30 step: 737, loss is 0.0007541929953731596\n",
      "epoch: 30 step: 738, loss is 0.00019500298367347568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 739, loss is 1.0466923413332552e-05\n",
      "epoch: 30 step: 740, loss is 0.013067751191556454\n",
      "epoch: 30 step: 741, loss is 0.07603862136602402\n",
      "epoch: 30 step: 742, loss is 0.0029842674266546965\n",
      "epoch: 30 step: 743, loss is 0.020189695060253143\n",
      "epoch: 30 step: 744, loss is 0.003110442776232958\n",
      "epoch: 30 step: 745, loss is 0.004513409920036793\n",
      "epoch: 30 step: 746, loss is 0.00020698229491245002\n",
      "epoch: 30 step: 747, loss is 0.0007221922860480845\n",
      "epoch: 30 step: 748, loss is 0.03605898469686508\n",
      "epoch: 30 step: 749, loss is 0.003280260134488344\n",
      "epoch: 30 step: 750, loss is 0.0008573494269512594\n",
      "epoch: 30 step: 751, loss is 0.046227820217609406\n",
      "epoch: 30 step: 752, loss is 0.0003540769685059786\n",
      "epoch: 30 step: 753, loss is 0.14635305106639862\n",
      "epoch: 30 step: 754, loss is 0.0008952078642323613\n",
      "epoch: 30 step: 755, loss is 0.0005030051106587052\n",
      "epoch: 30 step: 756, loss is 0.0006375200464390218\n",
      "epoch: 30 step: 757, loss is 0.005459276959300041\n",
      "epoch: 30 step: 758, loss is 0.002333217067644\n",
      "epoch: 30 step: 759, loss is 0.0006286384304985404\n",
      "epoch: 30 step: 760, loss is 0.00025405915221199393\n",
      "epoch: 30 step: 761, loss is 0.00023521474213339388\n",
      "epoch: 30 step: 762, loss is 0.00014146137982606888\n",
      "epoch: 30 step: 763, loss is 0.00013078958727419376\n",
      "epoch: 30 step: 764, loss is 0.0028273824136704206\n",
      "epoch: 30 step: 765, loss is 0.006872153840959072\n",
      "epoch: 30 step: 766, loss is 0.005749462638050318\n",
      "epoch: 30 step: 767, loss is 0.02981891855597496\n",
      "epoch: 30 step: 768, loss is 0.033550214022397995\n",
      "epoch: 30 step: 769, loss is 0.008207052014768124\n",
      "epoch: 30 step: 770, loss is 0.04057692363858223\n",
      "epoch: 30 step: 771, loss is 0.0027907686308026314\n",
      "epoch: 30 step: 772, loss is 0.08434928208589554\n",
      "epoch: 30 step: 773, loss is 0.0013615150237455964\n",
      "epoch: 30 step: 774, loss is 0.0007029965054243803\n",
      "epoch: 30 step: 775, loss is 0.0064418562687933445\n",
      "epoch: 30 step: 776, loss is 0.00035601723357103765\n",
      "epoch: 30 step: 777, loss is 0.013399923220276833\n",
      "epoch: 30 step: 778, loss is 0.028025692328810692\n",
      "epoch: 30 step: 779, loss is 0.002634937409311533\n",
      "epoch: 30 step: 780, loss is 0.03431785851716995\n",
      "epoch: 30 step: 781, loss is 0.02129794843494892\n",
      "epoch: 30 step: 782, loss is 0.00031534783192910254\n",
      "epoch: 30 step: 783, loss is 0.0017636665143072605\n",
      "epoch: 30 step: 784, loss is 0.0006662487285211682\n",
      "epoch: 30 step: 785, loss is 0.0011795636964961886\n",
      "epoch: 30 step: 786, loss is 0.0009509401861578226\n",
      "epoch: 30 step: 787, loss is 0.005357660353183746\n",
      "epoch: 30 step: 788, loss is 0.005961043294519186\n",
      "epoch: 30 step: 789, loss is 0.0008066068403422832\n",
      "epoch: 30 step: 790, loss is 0.05494033172726631\n",
      "epoch: 30 step: 791, loss is 0.0007463343208655715\n",
      "epoch: 30 step: 792, loss is 0.02075066789984703\n",
      "epoch: 30 step: 793, loss is 0.004723703488707542\n",
      "epoch: 30 step: 794, loss is 0.0003222690720576793\n",
      "epoch: 30 step: 795, loss is 0.0014238439034670591\n",
      "epoch: 30 step: 796, loss is 0.00017704340280033648\n",
      "epoch: 30 step: 797, loss is 2.5473251298535615e-05\n",
      "epoch: 30 step: 798, loss is 0.0009740958921611309\n",
      "epoch: 30 step: 799, loss is 0.00025878087035380304\n",
      "epoch: 30 step: 800, loss is 0.0009261929662898183\n",
      "epoch: 30 step: 801, loss is 0.0026588812470436096\n",
      "epoch: 30 step: 802, loss is 0.0001980548695428297\n",
      "epoch: 30 step: 803, loss is 0.019695701077580452\n",
      "epoch: 30 step: 804, loss is 0.044397931545972824\n",
      "epoch: 30 step: 805, loss is 0.005453308578580618\n",
      "epoch: 30 step: 806, loss is 0.0027596347499638796\n",
      "epoch: 30 step: 807, loss is 7.618372183060274e-05\n",
      "epoch: 30 step: 808, loss is 0.004292636178433895\n",
      "epoch: 30 step: 809, loss is 0.0003742717090062797\n",
      "epoch: 30 step: 810, loss is 0.0009688268764875829\n",
      "epoch: 30 step: 811, loss is 0.001939845853485167\n",
      "epoch: 30 step: 812, loss is 0.03148035705089569\n",
      "epoch: 30 step: 813, loss is 0.007957343012094498\n",
      "epoch: 30 step: 814, loss is 0.0002026407019002363\n",
      "epoch: 30 step: 815, loss is 0.003319105599075556\n",
      "epoch: 30 step: 816, loss is 0.048245105892419815\n",
      "epoch: 30 step: 817, loss is 0.000503314018715173\n",
      "epoch: 30 step: 818, loss is 0.0002075444208458066\n",
      "epoch: 30 step: 819, loss is 0.006191819440573454\n",
      "epoch: 30 step: 820, loss is 0.02732836827635765\n",
      "epoch: 30 step: 821, loss is 0.00021759967785328627\n",
      "epoch: 30 step: 822, loss is 0.0012302102986723185\n",
      "epoch: 30 step: 823, loss is 0.096171073615551\n",
      "epoch: 30 step: 824, loss is 0.0014729694230481982\n",
      "epoch: 30 step: 825, loss is 0.0007497085025534034\n",
      "epoch: 30 step: 826, loss is 0.019629068672657013\n",
      "epoch: 30 step: 827, loss is 0.0006671390729025006\n",
      "epoch: 30 step: 828, loss is 0.000564980146009475\n",
      "epoch: 30 step: 829, loss is 0.0002660012396518141\n",
      "epoch: 30 step: 830, loss is 0.00012894086830783635\n",
      "epoch: 30 step: 831, loss is 0.006540917791426182\n",
      "epoch: 30 step: 832, loss is 0.12032310664653778\n",
      "epoch: 30 step: 833, loss is 0.02381800301373005\n",
      "epoch: 30 step: 834, loss is 0.09142126142978668\n",
      "epoch: 30 step: 835, loss is 0.0011530575575307012\n",
      "epoch: 30 step: 836, loss is 0.023086203262209892\n",
      "epoch: 30 step: 837, loss is 0.014994173310697079\n",
      "epoch: 30 step: 838, loss is 0.0003672686580102891\n",
      "epoch: 30 step: 839, loss is 0.00011884987907251343\n",
      "epoch: 30 step: 840, loss is 0.0008170768851414323\n",
      "epoch: 30 step: 841, loss is 0.00033828066079877317\n",
      "epoch: 30 step: 842, loss is 0.004060094244778156\n",
      "epoch: 30 step: 843, loss is 0.0015335993375629187\n",
      "epoch: 30 step: 844, loss is 0.005753859877586365\n",
      "epoch: 30 step: 845, loss is 5.894563946640119e-05\n",
      "epoch: 30 step: 846, loss is 0.00592353381216526\n",
      "epoch: 30 step: 847, loss is 0.0007305783219635487\n",
      "epoch: 30 step: 848, loss is 0.012592144310474396\n",
      "epoch: 30 step: 849, loss is 0.005081426817923784\n",
      "epoch: 30 step: 850, loss is 4.314716352382675e-05\n",
      "epoch: 30 step: 851, loss is 0.02500089444220066\n",
      "epoch: 30 step: 852, loss is 0.005410545971244574\n",
      "epoch: 30 step: 853, loss is 0.0005147315096110106\n",
      "epoch: 30 step: 854, loss is 0.0019675586372613907\n",
      "epoch: 30 step: 855, loss is 0.0019134539179503918\n",
      "epoch: 30 step: 856, loss is 0.05865668132901192\n",
      "epoch: 30 step: 857, loss is 0.00010258456313749775\n",
      "epoch: 30 step: 858, loss is 0.004128390457481146\n",
      "epoch: 30 step: 859, loss is 0.0010620267130434513\n",
      "epoch: 30 step: 860, loss is 0.010315137915313244\n",
      "epoch: 30 step: 861, loss is 0.00052419159328565\n",
      "epoch: 30 step: 862, loss is 0.0008249397506006062\n",
      "epoch: 30 step: 863, loss is 0.00017501333786640316\n",
      "epoch: 30 step: 864, loss is 0.0041719963774085045\n",
      "epoch: 30 step: 865, loss is 0.002808741992339492\n",
      "epoch: 30 step: 866, loss is 0.00237021641805768\n",
      "epoch: 30 step: 867, loss is 2.3079628590494394e-05\n",
      "epoch: 30 step: 868, loss is 0.03385412320494652\n",
      "epoch: 30 step: 869, loss is 0.18106837570667267\n",
      "epoch: 30 step: 870, loss is 9.89222971838899e-05\n",
      "epoch: 30 step: 871, loss is 0.0002107682084897533\n",
      "epoch: 30 step: 872, loss is 0.038823649287223816\n",
      "epoch: 30 step: 873, loss is 0.0003951090038754046\n",
      "epoch: 30 step: 874, loss is 0.00023487625003326684\n",
      "epoch: 30 step: 875, loss is 0.0037250060122460127\n",
      "epoch: 30 step: 876, loss is 0.0003789749462157488\n",
      "epoch: 30 step: 877, loss is 0.0042536514811217785\n",
      "epoch: 30 step: 878, loss is 2.9731074391747825e-05\n",
      "epoch: 30 step: 879, loss is 0.004857302643358707\n",
      "epoch: 30 step: 880, loss is 0.002415172290056944\n",
      "epoch: 30 step: 881, loss is 0.002205215860158205\n",
      "epoch: 30 step: 882, loss is 0.014651430770754814\n",
      "epoch: 30 step: 883, loss is 0.0020987619645893574\n",
      "epoch: 30 step: 884, loss is 0.004579502623528242\n",
      "epoch: 30 step: 885, loss is 0.007493589539080858\n",
      "epoch: 30 step: 886, loss is 0.00015593410353176296\n",
      "epoch: 30 step: 887, loss is 0.0015236991457641125\n",
      "epoch: 30 step: 888, loss is 0.00023339934705290943\n",
      "epoch: 30 step: 889, loss is 0.001880939700640738\n",
      "epoch: 30 step: 890, loss is 0.0012385289883241057\n",
      "epoch: 30 step: 891, loss is 0.013551454059779644\n",
      "epoch: 30 step: 892, loss is 0.0008949823095463216\n",
      "epoch: 30 step: 893, loss is 0.0056724390015006065\n",
      "epoch: 30 step: 894, loss is 0.0037169032730162144\n",
      "epoch: 30 step: 895, loss is 0.045566268265247345\n",
      "epoch: 30 step: 896, loss is 0.010251381434500217\n",
      "epoch: 30 step: 897, loss is 0.00035291819949634373\n",
      "epoch: 30 step: 898, loss is 0.09944695234298706\n",
      "epoch: 30 step: 899, loss is 0.030588747933506966\n",
      "epoch: 30 step: 900, loss is 0.02554868534207344\n",
      "epoch: 30 step: 901, loss is 0.0014279634924605489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 902, loss is 3.0695562600158155e-05\n",
      "epoch: 30 step: 903, loss is 0.0001827430387493223\n",
      "epoch: 30 step: 904, loss is 0.002842371352016926\n",
      "epoch: 30 step: 905, loss is 0.0021341030951589346\n",
      "epoch: 30 step: 906, loss is 0.0014678253792226315\n",
      "epoch: 30 step: 907, loss is 0.0010442659258842468\n",
      "epoch: 30 step: 908, loss is 0.06872071325778961\n",
      "epoch: 30 step: 909, loss is 0.0664169192314148\n",
      "epoch: 30 step: 910, loss is 6.110593676567078e-05\n",
      "epoch: 30 step: 911, loss is 0.001797516830265522\n",
      "epoch: 30 step: 912, loss is 0.02126471884548664\n",
      "epoch: 30 step: 913, loss is 0.008459141477942467\n",
      "epoch: 30 step: 914, loss is 0.0030407060403376818\n",
      "epoch: 30 step: 915, loss is 0.04405921325087547\n",
      "epoch: 30 step: 916, loss is 0.0041778977029025555\n",
      "epoch: 30 step: 917, loss is 0.001317957416176796\n",
      "epoch: 30 step: 918, loss is 0.016028542071580887\n",
      "epoch: 30 step: 919, loss is 0.0013336619595065713\n",
      "epoch: 30 step: 920, loss is 0.0011687525548040867\n",
      "epoch: 30 step: 921, loss is 0.048766955733299255\n",
      "epoch: 30 step: 922, loss is 0.034161921590566635\n",
      "epoch: 30 step: 923, loss is 0.007652998901903629\n",
      "epoch: 30 step: 924, loss is 0.005310824606567621\n",
      "epoch: 30 step: 925, loss is 0.002900270279496908\n",
      "epoch: 30 step: 926, loss is 0.0306938998401165\n",
      "epoch: 30 step: 927, loss is 0.00011689618258969858\n",
      "epoch: 30 step: 928, loss is 0.02181323803961277\n",
      "epoch: 30 step: 929, loss is 0.0030408164020627737\n",
      "epoch: 30 step: 930, loss is 0.003002554178237915\n",
      "epoch: 30 step: 931, loss is 0.007001618854701519\n",
      "epoch: 30 step: 932, loss is 0.027401475235819817\n",
      "epoch: 30 step: 933, loss is 0.030220238491892815\n",
      "epoch: 30 step: 934, loss is 3.543992716004141e-05\n",
      "epoch: 30 step: 935, loss is 0.022217854857444763\n",
      "epoch: 30 step: 936, loss is 0.002908021444454789\n",
      "epoch: 30 step: 937, loss is 0.003565501654520631\n",
      "{'acc': 0.914863782051282}\n"
     ]
    }
   ],
   "source": [
    "# 训练无正则化的网络\n",
    "model = train(ForwardFashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(976:16928,MainProcess):2022-10-17-15:34:11.682.364 [mindspore\\dataset\\engine\\datasets_user_defined.py:656] Python multiprocessing is not supported on Windows platform.\n",
      "[WARNING] ME(976:16928,MainProcess):2022-10-17-15:34:11.686.353 [mindspore\\dataset\\engine\\datasets_user_defined.py:656] Python multiprocessing is not supported on Windows platform.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.3020575046539307\n",
      "epoch: 1 step: 2, loss is 2.2959277629852295\n",
      "epoch: 1 step: 3, loss is 2.270437479019165\n",
      "epoch: 1 step: 4, loss is 2.2471110820770264\n",
      "epoch: 1 step: 5, loss is 2.215440273284912\n",
      "epoch: 1 step: 6, loss is 2.1484062671661377\n",
      "epoch: 1 step: 7, loss is 2.1010899543762207\n",
      "epoch: 1 step: 8, loss is 2.102116346359253\n",
      "epoch: 1 step: 9, loss is 2.079498767852783\n",
      "epoch: 1 step: 10, loss is 1.9992653131484985\n",
      "epoch: 1 step: 11, loss is 2.046281576156616\n",
      "epoch: 1 step: 12, loss is 1.9494364261627197\n",
      "epoch: 1 step: 13, loss is 1.9238234758377075\n",
      "epoch: 1 step: 14, loss is 1.9068684577941895\n",
      "epoch: 1 step: 15, loss is 1.8079833984375\n",
      "epoch: 1 step: 16, loss is 1.8377723693847656\n",
      "epoch: 1 step: 17, loss is 1.7819052934646606\n",
      "epoch: 1 step: 18, loss is 1.7726539373397827\n",
      "epoch: 1 step: 19, loss is 1.7068630456924438\n",
      "epoch: 1 step: 20, loss is 1.7907772064208984\n",
      "epoch: 1 step: 21, loss is 1.6517536640167236\n",
      "epoch: 1 step: 22, loss is 1.63987135887146\n",
      "epoch: 1 step: 23, loss is 1.6513770818710327\n",
      "epoch: 1 step: 24, loss is 1.7271591424942017\n",
      "epoch: 1 step: 25, loss is 1.620965600013733\n",
      "epoch: 1 step: 26, loss is 1.505929708480835\n",
      "epoch: 1 step: 27, loss is 1.4905340671539307\n",
      "epoch: 1 step: 28, loss is 1.478883147239685\n",
      "epoch: 1 step: 29, loss is 1.5436285734176636\n",
      "epoch: 1 step: 30, loss is 1.3474332094192505\n",
      "epoch: 1 step: 31, loss is 1.5515104532241821\n",
      "epoch: 1 step: 32, loss is 1.418293833732605\n",
      "epoch: 1 step: 33, loss is 1.4280024766921997\n",
      "epoch: 1 step: 34, loss is 1.3606913089752197\n",
      "epoch: 1 step: 35, loss is 1.3840879201889038\n",
      "epoch: 1 step: 36, loss is 1.4455623626708984\n",
      "epoch: 1 step: 37, loss is 1.3940982818603516\n",
      "epoch: 1 step: 38, loss is 1.350334644317627\n",
      "epoch: 1 step: 39, loss is 1.2837516069412231\n",
      "epoch: 1 step: 40, loss is 1.2525321245193481\n",
      "epoch: 1 step: 41, loss is 1.2569372653961182\n",
      "epoch: 1 step: 42, loss is 1.2982115745544434\n",
      "epoch: 1 step: 43, loss is 1.4239516258239746\n",
      "epoch: 1 step: 44, loss is 1.253326177597046\n",
      "epoch: 1 step: 45, loss is 1.2543375492095947\n",
      "epoch: 1 step: 46, loss is 1.1894696950912476\n",
      "epoch: 1 step: 47, loss is 1.1509729623794556\n",
      "epoch: 1 step: 48, loss is 1.2616239786148071\n",
      "epoch: 1 step: 49, loss is 1.1783225536346436\n",
      "epoch: 1 step: 50, loss is 1.1242284774780273\n",
      "epoch: 1 step: 51, loss is 1.053662657737732\n",
      "epoch: 1 step: 52, loss is 1.121317982673645\n",
      "epoch: 1 step: 53, loss is 1.1897886991500854\n",
      "epoch: 1 step: 54, loss is 1.0503607988357544\n",
      "epoch: 1 step: 55, loss is 1.0396132469177246\n",
      "epoch: 1 step: 56, loss is 1.0994318723678589\n",
      "epoch: 1 step: 57, loss is 1.051061987876892\n",
      "epoch: 1 step: 58, loss is 1.015463948249817\n",
      "epoch: 1 step: 59, loss is 1.083086609840393\n",
      "epoch: 1 step: 60, loss is 1.0824809074401855\n",
      "epoch: 1 step: 61, loss is 1.0204931497573853\n",
      "epoch: 1 step: 62, loss is 1.0077862739562988\n",
      "epoch: 1 step: 63, loss is 0.9737350344657898\n",
      "epoch: 1 step: 64, loss is 0.9325059056282043\n",
      "epoch: 1 step: 65, loss is 1.0237579345703125\n",
      "epoch: 1 step: 66, loss is 0.9555172324180603\n",
      "epoch: 1 step: 67, loss is 0.9736908674240112\n",
      "epoch: 1 step: 68, loss is 0.956268846988678\n",
      "epoch: 1 step: 69, loss is 1.1131625175476074\n",
      "epoch: 1 step: 70, loss is 0.964357852935791\n",
      "epoch: 1 step: 71, loss is 1.0049856901168823\n",
      "epoch: 1 step: 72, loss is 0.9471607804298401\n",
      "epoch: 1 step: 73, loss is 0.8097976446151733\n",
      "epoch: 1 step: 74, loss is 0.8621085286140442\n",
      "epoch: 1 step: 75, loss is 0.9810832738876343\n",
      "epoch: 1 step: 76, loss is 0.8504279255867004\n",
      "epoch: 1 step: 77, loss is 0.7499804496765137\n",
      "epoch: 1 step: 78, loss is 0.9445980787277222\n",
      "epoch: 1 step: 79, loss is 0.8427838087081909\n",
      "epoch: 1 step: 80, loss is 0.7380840182304382\n",
      "epoch: 1 step: 81, loss is 0.785195529460907\n",
      "epoch: 1 step: 82, loss is 0.9582419991493225\n",
      "epoch: 1 step: 83, loss is 0.7436947822570801\n",
      "epoch: 1 step: 84, loss is 0.8537217974662781\n",
      "epoch: 1 step: 85, loss is 0.7932010293006897\n",
      "epoch: 1 step: 86, loss is 0.8408669829368591\n",
      "epoch: 1 step: 87, loss is 0.9294402599334717\n",
      "epoch: 1 step: 88, loss is 0.869670033454895\n",
      "epoch: 1 step: 89, loss is 0.7075467109680176\n",
      "epoch: 1 step: 90, loss is 0.7696090340614319\n",
      "epoch: 1 step: 91, loss is 0.8400256633758545\n",
      "epoch: 1 step: 92, loss is 0.8287067413330078\n",
      "epoch: 1 step: 93, loss is 0.8120027780532837\n",
      "epoch: 1 step: 94, loss is 0.8452391624450684\n",
      "epoch: 1 step: 95, loss is 0.8917169570922852\n",
      "epoch: 1 step: 96, loss is 0.7573888301849365\n",
      "epoch: 1 step: 97, loss is 0.8165128827095032\n",
      "epoch: 1 step: 98, loss is 0.6607835292816162\n",
      "epoch: 1 step: 99, loss is 0.641976535320282\n",
      "epoch: 1 step: 100, loss is 0.8588898777961731\n",
      "epoch: 1 step: 101, loss is 0.8102184534072876\n",
      "epoch: 1 step: 102, loss is 0.647669792175293\n",
      "epoch: 1 step: 103, loss is 0.8225493431091309\n",
      "epoch: 1 step: 104, loss is 0.9062219262123108\n",
      "epoch: 1 step: 105, loss is 0.7662190794944763\n",
      "epoch: 1 step: 106, loss is 0.6676495671272278\n",
      "epoch: 1 step: 107, loss is 0.732643187046051\n",
      "epoch: 1 step: 108, loss is 0.6119268536567688\n",
      "epoch: 1 step: 109, loss is 0.7069092392921448\n",
      "epoch: 1 step: 110, loss is 0.6515179872512817\n",
      "epoch: 1 step: 111, loss is 0.7537569999694824\n",
      "epoch: 1 step: 112, loss is 0.644195556640625\n",
      "epoch: 1 step: 113, loss is 0.8696560263633728\n",
      "epoch: 1 step: 114, loss is 0.6419891715049744\n",
      "epoch: 1 step: 115, loss is 0.7424554228782654\n",
      "epoch: 1 step: 116, loss is 0.8231999278068542\n",
      "epoch: 1 step: 117, loss is 0.6142657995223999\n",
      "epoch: 1 step: 118, loss is 0.6289081573486328\n",
      "epoch: 1 step: 119, loss is 0.6889268755912781\n",
      "epoch: 1 step: 120, loss is 0.7009245157241821\n",
      "epoch: 1 step: 121, loss is 0.6480124592781067\n",
      "epoch: 1 step: 122, loss is 0.6357839703559875\n",
      "epoch: 1 step: 123, loss is 0.6802674531936646\n",
      "epoch: 1 step: 124, loss is 0.6201726794242859\n",
      "epoch: 1 step: 125, loss is 0.8369683027267456\n",
      "epoch: 1 step: 126, loss is 0.7204470038414001\n",
      "epoch: 1 step: 127, loss is 0.8041762709617615\n",
      "epoch: 1 step: 128, loss is 0.6011404395103455\n",
      "epoch: 1 step: 129, loss is 0.705054521560669\n",
      "epoch: 1 step: 130, loss is 0.7420312762260437\n",
      "epoch: 1 step: 131, loss is 0.6724514961242676\n",
      "epoch: 1 step: 132, loss is 0.611107349395752\n",
      "epoch: 1 step: 133, loss is 0.8688217997550964\n",
      "epoch: 1 step: 134, loss is 0.7212600111961365\n",
      "epoch: 1 step: 135, loss is 0.6906575560569763\n",
      "epoch: 1 step: 136, loss is 0.6311004161834717\n",
      "epoch: 1 step: 137, loss is 0.629377007484436\n",
      "epoch: 1 step: 138, loss is 0.7353758215904236\n",
      "epoch: 1 step: 139, loss is 0.8157010674476624\n",
      "epoch: 1 step: 140, loss is 0.7882493138313293\n",
      "epoch: 1 step: 141, loss is 0.6596482992172241\n",
      "epoch: 1 step: 142, loss is 0.7043191194534302\n",
      "epoch: 1 step: 143, loss is 0.5291986465454102\n",
      "epoch: 1 step: 144, loss is 0.7851771116256714\n",
      "epoch: 1 step: 145, loss is 0.7031114101409912\n",
      "epoch: 1 step: 146, loss is 0.6369438767433167\n",
      "epoch: 1 step: 147, loss is 0.6319475173950195\n",
      "epoch: 1 step: 148, loss is 0.6363251805305481\n",
      "epoch: 1 step: 149, loss is 0.6683358550071716\n",
      "epoch: 1 step: 150, loss is 0.6665039658546448\n",
      "epoch: 1 step: 151, loss is 0.6552910208702087\n",
      "epoch: 1 step: 152, loss is 0.4474017024040222\n",
      "epoch: 1 step: 153, loss is 0.790959358215332\n",
      "epoch: 1 step: 154, loss is 0.8671255111694336\n",
      "epoch: 1 step: 155, loss is 0.8411364555358887\n",
      "epoch: 1 step: 156, loss is 0.5000702738761902\n",
      "epoch: 1 step: 157, loss is 0.7614012360572815\n",
      "epoch: 1 step: 158, loss is 0.6161444783210754\n",
      "epoch: 1 step: 159, loss is 0.6443939805030823\n",
      "epoch: 1 step: 160, loss is 0.44259050488471985\n",
      "epoch: 1 step: 161, loss is 0.5804163813591003\n",
      "epoch: 1 step: 162, loss is 0.8474421501159668\n",
      "epoch: 1 step: 163, loss is 0.6735821962356567\n",
      "epoch: 1 step: 164, loss is 0.5434281826019287\n",
      "epoch: 1 step: 165, loss is 0.8967462778091431\n",
      "epoch: 1 step: 166, loss is 0.5789753198623657\n",
      "epoch: 1 step: 167, loss is 0.6965458989143372\n",
      "epoch: 1 step: 168, loss is 0.829025149345398\n",
      "epoch: 1 step: 169, loss is 0.5471055507659912\n",
      "epoch: 1 step: 170, loss is 0.7340276837348938\n",
      "epoch: 1 step: 171, loss is 0.5887072086334229\n",
      "epoch: 1 step: 172, loss is 0.6171584725379944\n",
      "epoch: 1 step: 173, loss is 0.6366463899612427\n",
      "epoch: 1 step: 174, loss is 0.6476411819458008\n",
      "epoch: 1 step: 175, loss is 0.7255988717079163\n",
      "epoch: 1 step: 176, loss is 0.6696565747261047\n",
      "epoch: 1 step: 177, loss is 0.6693340539932251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 178, loss is 0.5739101767539978\n",
      "epoch: 1 step: 179, loss is 0.9628711938858032\n",
      "epoch: 1 step: 180, loss is 0.7320249676704407\n",
      "epoch: 1 step: 181, loss is 0.5804867148399353\n",
      "epoch: 1 step: 182, loss is 0.6623972654342651\n",
      "epoch: 1 step: 183, loss is 0.625049352645874\n",
      "epoch: 1 step: 184, loss is 0.536794126033783\n",
      "epoch: 1 step: 185, loss is 0.651915431022644\n",
      "epoch: 1 step: 186, loss is 0.7575166821479797\n",
      "epoch: 1 step: 187, loss is 0.5233355164527893\n",
      "epoch: 1 step: 188, loss is 0.5399340987205505\n",
      "epoch: 1 step: 189, loss is 0.4971890151500702\n",
      "epoch: 1 step: 190, loss is 0.7932783365249634\n",
      "epoch: 1 step: 191, loss is 0.5257493257522583\n",
      "epoch: 1 step: 192, loss is 0.6126594543457031\n",
      "epoch: 1 step: 193, loss is 0.5642991065979004\n",
      "epoch: 1 step: 194, loss is 0.45970454812049866\n",
      "epoch: 1 step: 195, loss is 0.503703236579895\n",
      "epoch: 1 step: 196, loss is 0.5663501620292664\n",
      "epoch: 1 step: 197, loss is 0.48719021677970886\n",
      "epoch: 1 step: 198, loss is 0.6425681710243225\n",
      "epoch: 1 step: 199, loss is 0.6295266151428223\n",
      "epoch: 1 step: 200, loss is 0.44703054428100586\n",
      "epoch: 1 step: 201, loss is 0.6140146851539612\n",
      "epoch: 1 step: 202, loss is 0.5752231478691101\n",
      "epoch: 1 step: 203, loss is 0.5280792713165283\n",
      "epoch: 1 step: 204, loss is 0.568865180015564\n",
      "epoch: 1 step: 205, loss is 0.5596599578857422\n",
      "epoch: 1 step: 206, loss is 0.47050511837005615\n",
      "epoch: 1 step: 207, loss is 0.792756199836731\n",
      "epoch: 1 step: 208, loss is 0.6352729201316833\n",
      "epoch: 1 step: 209, loss is 0.43391942977905273\n",
      "epoch: 1 step: 210, loss is 0.6277445554733276\n",
      "epoch: 1 step: 211, loss is 0.6803895235061646\n",
      "epoch: 1 step: 212, loss is 0.5947009325027466\n",
      "epoch: 1 step: 213, loss is 0.4938953220844269\n",
      "epoch: 1 step: 214, loss is 0.665918231010437\n",
      "epoch: 1 step: 215, loss is 0.7177075743675232\n",
      "epoch: 1 step: 216, loss is 0.6940839290618896\n",
      "epoch: 1 step: 217, loss is 0.4491478502750397\n",
      "epoch: 1 step: 218, loss is 0.4474457800388336\n",
      "epoch: 1 step: 219, loss is 0.5311603546142578\n",
      "epoch: 1 step: 220, loss is 0.6985893249511719\n",
      "epoch: 1 step: 221, loss is 0.5583468079566956\n",
      "epoch: 1 step: 222, loss is 0.6714684367179871\n",
      "epoch: 1 step: 223, loss is 0.7946825623512268\n",
      "epoch: 1 step: 224, loss is 0.48228976130485535\n",
      "epoch: 1 step: 225, loss is 0.8204646706581116\n",
      "epoch: 1 step: 226, loss is 0.4448137879371643\n",
      "epoch: 1 step: 227, loss is 0.6152287721633911\n",
      "epoch: 1 step: 228, loss is 0.48714160919189453\n",
      "epoch: 1 step: 229, loss is 0.5200594067573547\n",
      "epoch: 1 step: 230, loss is 0.5027766823768616\n",
      "epoch: 1 step: 231, loss is 0.8010271787643433\n",
      "epoch: 1 step: 232, loss is 0.6273695826530457\n",
      "epoch: 1 step: 233, loss is 0.6299594044685364\n",
      "epoch: 1 step: 234, loss is 0.5040664076805115\n",
      "epoch: 1 step: 235, loss is 0.3853623867034912\n",
      "epoch: 1 step: 236, loss is 0.529007613658905\n",
      "epoch: 1 step: 237, loss is 0.6177801489830017\n",
      "epoch: 1 step: 238, loss is 0.6959949135780334\n",
      "epoch: 1 step: 239, loss is 0.5431182980537415\n",
      "epoch: 1 step: 240, loss is 0.6575678586959839\n",
      "epoch: 1 step: 241, loss is 0.8826321363449097\n",
      "epoch: 1 step: 242, loss is 0.7996969819068909\n",
      "epoch: 1 step: 243, loss is 0.5062176585197449\n",
      "epoch: 1 step: 244, loss is 0.7236754894256592\n",
      "epoch: 1 step: 245, loss is 0.7360292077064514\n",
      "epoch: 1 step: 246, loss is 0.4564497768878937\n",
      "epoch: 1 step: 247, loss is 0.6810975670814514\n",
      "epoch: 1 step: 248, loss is 0.5331376194953918\n",
      "epoch: 1 step: 249, loss is 0.800260603427887\n",
      "epoch: 1 step: 250, loss is 0.5228677988052368\n",
      "epoch: 1 step: 251, loss is 0.5125606060028076\n",
      "epoch: 1 step: 252, loss is 0.5372822284698486\n",
      "epoch: 1 step: 253, loss is 0.5226662158966064\n",
      "epoch: 1 step: 254, loss is 0.4995702803134918\n",
      "epoch: 1 step: 255, loss is 0.8207148909568787\n",
      "epoch: 1 step: 256, loss is 0.5887733101844788\n",
      "epoch: 1 step: 257, loss is 0.577556848526001\n",
      "epoch: 1 step: 258, loss is 0.6486007571220398\n",
      "epoch: 1 step: 259, loss is 0.44873759150505066\n",
      "epoch: 1 step: 260, loss is 0.35121649503707886\n",
      "epoch: 1 step: 261, loss is 0.6099163889884949\n",
      "epoch: 1 step: 262, loss is 0.6616126894950867\n",
      "epoch: 1 step: 263, loss is 0.44900989532470703\n",
      "epoch: 1 step: 264, loss is 0.6278390884399414\n",
      "epoch: 1 step: 265, loss is 0.48922762274742126\n",
      "epoch: 1 step: 266, loss is 0.6001455187797546\n",
      "epoch: 1 step: 267, loss is 0.5848397016525269\n",
      "epoch: 1 step: 268, loss is 0.6236901879310608\n",
      "epoch: 1 step: 269, loss is 0.45696574449539185\n",
      "epoch: 1 step: 270, loss is 0.6629350781440735\n",
      "epoch: 1 step: 271, loss is 0.8418964743614197\n",
      "epoch: 1 step: 272, loss is 0.5772082209587097\n",
      "epoch: 1 step: 273, loss is 0.5301352739334106\n",
      "epoch: 1 step: 274, loss is 0.4821578562259674\n",
      "epoch: 1 step: 275, loss is 0.5996692776679993\n",
      "epoch: 1 step: 276, loss is 0.655407726764679\n",
      "epoch: 1 step: 277, loss is 0.5382677912712097\n",
      "epoch: 1 step: 278, loss is 0.48312899470329285\n",
      "epoch: 1 step: 279, loss is 0.6167225241661072\n",
      "epoch: 1 step: 280, loss is 0.4773671627044678\n",
      "epoch: 1 step: 281, loss is 0.4706248939037323\n",
      "epoch: 1 step: 282, loss is 0.436727374792099\n",
      "epoch: 1 step: 283, loss is 0.5007114410400391\n",
      "epoch: 1 step: 284, loss is 0.616634726524353\n",
      "epoch: 1 step: 285, loss is 0.528675377368927\n",
      "epoch: 1 step: 286, loss is 0.5860546231269836\n",
      "epoch: 1 step: 287, loss is 0.5541075468063354\n",
      "epoch: 1 step: 288, loss is 0.595493733882904\n",
      "epoch: 1 step: 289, loss is 0.8058205842971802\n",
      "epoch: 1 step: 290, loss is 0.5357564091682434\n",
      "epoch: 1 step: 291, loss is 0.5564190745353699\n",
      "epoch: 1 step: 292, loss is 0.6794829368591309\n",
      "epoch: 1 step: 293, loss is 0.4973638951778412\n",
      "epoch: 1 step: 294, loss is 0.3485029637813568\n",
      "epoch: 1 step: 295, loss is 0.5023751854896545\n",
      "epoch: 1 step: 296, loss is 0.440965473651886\n",
      "epoch: 1 step: 297, loss is 0.6779007315635681\n",
      "epoch: 1 step: 298, loss is 0.5673297047615051\n",
      "epoch: 1 step: 299, loss is 0.5524916648864746\n",
      "epoch: 1 step: 300, loss is 0.6656474471092224\n",
      "epoch: 1 step: 301, loss is 0.5866595506668091\n",
      "epoch: 1 step: 302, loss is 0.5256364345550537\n",
      "epoch: 1 step: 303, loss is 0.5159159302711487\n",
      "epoch: 1 step: 304, loss is 0.570953369140625\n",
      "epoch: 1 step: 305, loss is 0.601504921913147\n",
      "epoch: 1 step: 306, loss is 0.6227392554283142\n",
      "epoch: 1 step: 307, loss is 0.4002724885940552\n",
      "epoch: 1 step: 308, loss is 0.5004953145980835\n",
      "epoch: 1 step: 309, loss is 0.5420712828636169\n",
      "epoch: 1 step: 310, loss is 0.624467134475708\n",
      "epoch: 1 step: 311, loss is 0.4891369044780731\n",
      "epoch: 1 step: 312, loss is 0.4185110926628113\n",
      "epoch: 1 step: 313, loss is 0.39936351776123047\n",
      "epoch: 1 step: 314, loss is 0.5230892896652222\n",
      "epoch: 1 step: 315, loss is 0.5858682990074158\n",
      "epoch: 1 step: 316, loss is 0.3105751574039459\n",
      "epoch: 1 step: 317, loss is 0.5683912038803101\n",
      "epoch: 1 step: 318, loss is 0.46549567580223083\n",
      "epoch: 1 step: 319, loss is 0.6180083751678467\n",
      "epoch: 1 step: 320, loss is 0.5223967432975769\n",
      "epoch: 1 step: 321, loss is 0.6690801978111267\n",
      "epoch: 1 step: 322, loss is 0.5080134868621826\n",
      "epoch: 1 step: 323, loss is 0.5074881315231323\n",
      "epoch: 1 step: 324, loss is 0.5982787013053894\n",
      "epoch: 1 step: 325, loss is 0.442549467086792\n",
      "epoch: 1 step: 326, loss is 0.4511970579624176\n",
      "epoch: 1 step: 327, loss is 0.4617808759212494\n",
      "epoch: 1 step: 328, loss is 0.4718366265296936\n",
      "epoch: 1 step: 329, loss is 0.6618301272392273\n",
      "epoch: 1 step: 330, loss is 0.5961619019508362\n",
      "epoch: 1 step: 331, loss is 0.46454837918281555\n",
      "epoch: 1 step: 332, loss is 0.5933787822723389\n",
      "epoch: 1 step: 333, loss is 0.5936993956565857\n",
      "epoch: 1 step: 334, loss is 0.6111258864402771\n",
      "epoch: 1 step: 335, loss is 0.528450608253479\n",
      "epoch: 1 step: 336, loss is 0.7123656868934631\n",
      "epoch: 1 step: 337, loss is 0.49000731110572815\n",
      "epoch: 1 step: 338, loss is 0.5853177905082703\n",
      "epoch: 1 step: 339, loss is 0.5133017301559448\n",
      "epoch: 1 step: 340, loss is 0.6387854218482971\n",
      "epoch: 1 step: 341, loss is 0.42154815793037415\n",
      "epoch: 1 step: 342, loss is 0.43214061856269836\n",
      "epoch: 1 step: 343, loss is 0.5195382833480835\n",
      "epoch: 1 step: 344, loss is 0.4634482264518738\n",
      "epoch: 1 step: 345, loss is 0.5559709072113037\n",
      "epoch: 1 step: 346, loss is 0.5829296112060547\n",
      "epoch: 1 step: 347, loss is 0.41538509726524353\n",
      "epoch: 1 step: 348, loss is 0.38192230463027954\n",
      "epoch: 1 step: 349, loss is 0.3902435898780823\n",
      "epoch: 1 step: 350, loss is 0.6714239716529846\n",
      "epoch: 1 step: 351, loss is 0.41315722465515137\n",
      "epoch: 1 step: 352, loss is 0.571471095085144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 353, loss is 0.5668156743049622\n",
      "epoch: 1 step: 354, loss is 0.491837739944458\n",
      "epoch: 1 step: 355, loss is 0.5132758617401123\n",
      "epoch: 1 step: 356, loss is 0.846571683883667\n",
      "epoch: 1 step: 357, loss is 0.4751027226448059\n",
      "epoch: 1 step: 358, loss is 0.40613603591918945\n",
      "epoch: 1 step: 359, loss is 0.41987040638923645\n",
      "epoch: 1 step: 360, loss is 0.4707048237323761\n",
      "epoch: 1 step: 361, loss is 0.5014020204544067\n",
      "epoch: 1 step: 362, loss is 0.43136343359947205\n",
      "epoch: 1 step: 363, loss is 0.4850411117076874\n",
      "epoch: 1 step: 364, loss is 0.4233570694923401\n",
      "epoch: 1 step: 365, loss is 0.5141554474830627\n",
      "epoch: 1 step: 366, loss is 0.49821627140045166\n",
      "epoch: 1 step: 367, loss is 0.681929886341095\n",
      "epoch: 1 step: 368, loss is 0.5799787044525146\n",
      "epoch: 1 step: 369, loss is 0.6660873889923096\n",
      "epoch: 1 step: 370, loss is 0.5828636288642883\n",
      "epoch: 1 step: 371, loss is 0.4214351773262024\n",
      "epoch: 1 step: 372, loss is 0.5088115334510803\n",
      "epoch: 1 step: 373, loss is 0.5978075861930847\n",
      "epoch: 1 step: 374, loss is 0.5272300839424133\n",
      "epoch: 1 step: 375, loss is 0.47504955530166626\n",
      "epoch: 1 step: 376, loss is 0.3406178951263428\n",
      "epoch: 1 step: 377, loss is 0.6278144121170044\n",
      "epoch: 1 step: 378, loss is 0.6892490983009338\n",
      "epoch: 1 step: 379, loss is 0.46725228428840637\n",
      "epoch: 1 step: 380, loss is 0.5638280510902405\n",
      "epoch: 1 step: 381, loss is 0.40051835775375366\n",
      "epoch: 1 step: 382, loss is 0.4553063213825226\n",
      "epoch: 1 step: 383, loss is 0.6039431691169739\n",
      "epoch: 1 step: 384, loss is 0.5331579446792603\n",
      "epoch: 1 step: 385, loss is 0.3842286467552185\n",
      "epoch: 1 step: 386, loss is 0.5262612700462341\n",
      "epoch: 1 step: 387, loss is 0.5639474391937256\n",
      "epoch: 1 step: 388, loss is 0.34097298979759216\n",
      "epoch: 1 step: 389, loss is 0.33870476484298706\n",
      "epoch: 1 step: 390, loss is 0.5242420434951782\n",
      "epoch: 1 step: 391, loss is 0.4023225009441376\n",
      "epoch: 1 step: 392, loss is 0.662135899066925\n",
      "epoch: 1 step: 393, loss is 0.5879303216934204\n",
      "epoch: 1 step: 394, loss is 0.4673622250556946\n",
      "epoch: 1 step: 395, loss is 0.408713698387146\n",
      "epoch: 1 step: 396, loss is 0.4022127091884613\n",
      "epoch: 1 step: 397, loss is 0.45827314257621765\n",
      "epoch: 1 step: 398, loss is 0.4962168335914612\n",
      "epoch: 1 step: 399, loss is 0.49738267064094543\n",
      "epoch: 1 step: 400, loss is 0.6132740378379822\n",
      "epoch: 1 step: 401, loss is 0.42599114775657654\n",
      "epoch: 1 step: 402, loss is 0.3559142053127289\n",
      "epoch: 1 step: 403, loss is 0.5067722797393799\n",
      "epoch: 1 step: 404, loss is 0.7275814414024353\n",
      "epoch: 1 step: 405, loss is 0.44900792837142944\n",
      "epoch: 1 step: 406, loss is 0.6274910569190979\n",
      "epoch: 1 step: 407, loss is 0.5953716039657593\n",
      "epoch: 1 step: 408, loss is 0.4823397099971771\n",
      "epoch: 1 step: 409, loss is 0.36260610818862915\n",
      "epoch: 1 step: 410, loss is 0.5434006452560425\n",
      "epoch: 1 step: 411, loss is 0.7028792500495911\n",
      "epoch: 1 step: 412, loss is 0.552701473236084\n",
      "epoch: 1 step: 413, loss is 0.37143218517303467\n",
      "epoch: 1 step: 414, loss is 0.3762008547782898\n",
      "epoch: 1 step: 415, loss is 0.609070360660553\n",
      "epoch: 1 step: 416, loss is 0.5126514434814453\n",
      "epoch: 1 step: 417, loss is 0.5002845525741577\n",
      "epoch: 1 step: 418, loss is 0.44258126616477966\n",
      "epoch: 1 step: 419, loss is 0.352066308259964\n",
      "epoch: 1 step: 420, loss is 0.61238032579422\n",
      "epoch: 1 step: 421, loss is 0.4392552375793457\n",
      "epoch: 1 step: 422, loss is 0.4242967665195465\n",
      "epoch: 1 step: 423, loss is 0.5581108927726746\n",
      "epoch: 1 step: 424, loss is 0.5610827803611755\n",
      "epoch: 1 step: 425, loss is 0.45042362809181213\n",
      "epoch: 1 step: 426, loss is 0.3994397222995758\n",
      "epoch: 1 step: 427, loss is 0.43629178404808044\n",
      "epoch: 1 step: 428, loss is 0.6436333656311035\n",
      "epoch: 1 step: 429, loss is 0.5312575101852417\n",
      "epoch: 1 step: 430, loss is 0.4468684196472168\n",
      "epoch: 1 step: 431, loss is 0.5471304655075073\n",
      "epoch: 1 step: 432, loss is 0.5485156774520874\n",
      "epoch: 1 step: 433, loss is 0.6240794658660889\n",
      "epoch: 1 step: 434, loss is 0.4482644498348236\n",
      "epoch: 1 step: 435, loss is 0.7183219790458679\n",
      "epoch: 1 step: 436, loss is 0.38249051570892334\n",
      "epoch: 1 step: 437, loss is 0.4034464359283447\n",
      "epoch: 1 step: 438, loss is 0.6066610217094421\n",
      "epoch: 1 step: 439, loss is 0.49849945306777954\n",
      "epoch: 1 step: 440, loss is 0.36399146914482117\n",
      "epoch: 1 step: 441, loss is 0.4434117376804352\n",
      "epoch: 1 step: 442, loss is 0.40962398052215576\n",
      "epoch: 1 step: 443, loss is 0.30543163418769836\n",
      "epoch: 1 step: 444, loss is 0.5505963563919067\n",
      "epoch: 1 step: 445, loss is 0.47952908277511597\n",
      "epoch: 1 step: 446, loss is 0.5521530508995056\n",
      "epoch: 1 step: 447, loss is 0.336108535528183\n",
      "epoch: 1 step: 448, loss is 0.3734690248966217\n",
      "epoch: 1 step: 449, loss is 0.557727575302124\n",
      "epoch: 1 step: 450, loss is 0.5503116846084595\n",
      "epoch: 1 step: 451, loss is 0.7708780169487\n",
      "epoch: 1 step: 452, loss is 0.39479345083236694\n",
      "epoch: 1 step: 453, loss is 0.4440365135669708\n",
      "epoch: 1 step: 454, loss is 0.43835362792015076\n",
      "epoch: 1 step: 455, loss is 0.40838223695755005\n",
      "epoch: 1 step: 456, loss is 0.5147973299026489\n",
      "epoch: 1 step: 457, loss is 0.6651588082313538\n",
      "epoch: 1 step: 458, loss is 0.3764928877353668\n",
      "epoch: 1 step: 459, loss is 0.5465238094329834\n",
      "epoch: 1 step: 460, loss is 0.5234696865081787\n",
      "epoch: 1 step: 461, loss is 0.37855401635169983\n",
      "epoch: 1 step: 462, loss is 0.5853991508483887\n",
      "epoch: 1 step: 463, loss is 0.52206951379776\n",
      "epoch: 1 step: 464, loss is 0.5453393459320068\n",
      "epoch: 1 step: 465, loss is 0.5264674425125122\n",
      "epoch: 1 step: 466, loss is 0.6524624824523926\n",
      "epoch: 1 step: 467, loss is 0.6541523933410645\n",
      "epoch: 1 step: 468, loss is 0.5572519898414612\n",
      "epoch: 1 step: 469, loss is 0.5894506573677063\n",
      "epoch: 1 step: 470, loss is 0.35704556107521057\n",
      "epoch: 1 step: 471, loss is 0.597145676612854\n",
      "epoch: 1 step: 472, loss is 0.4238457977771759\n",
      "epoch: 1 step: 473, loss is 0.596418023109436\n",
      "epoch: 1 step: 474, loss is 0.4592592418193817\n",
      "epoch: 1 step: 475, loss is 0.4629932641983032\n",
      "epoch: 1 step: 476, loss is 0.48046550154685974\n",
      "epoch: 1 step: 477, loss is 0.5434780120849609\n",
      "epoch: 1 step: 478, loss is 0.6966825723648071\n",
      "epoch: 1 step: 479, loss is 0.28678804636001587\n",
      "epoch: 1 step: 480, loss is 0.664322555065155\n",
      "epoch: 1 step: 481, loss is 0.5766199231147766\n",
      "epoch: 1 step: 482, loss is 0.3805452883243561\n",
      "epoch: 1 step: 483, loss is 0.49043142795562744\n",
      "epoch: 1 step: 484, loss is 0.4895012676715851\n",
      "epoch: 1 step: 485, loss is 0.4362989068031311\n",
      "epoch: 1 step: 486, loss is 0.5395998954772949\n",
      "epoch: 1 step: 487, loss is 0.5389524102210999\n",
      "epoch: 1 step: 488, loss is 0.5123216509819031\n",
      "epoch: 1 step: 489, loss is 0.3742363154888153\n",
      "epoch: 1 step: 490, loss is 0.47265321016311646\n",
      "epoch: 1 step: 491, loss is 0.43747296929359436\n",
      "epoch: 1 step: 492, loss is 0.45253440737724304\n",
      "epoch: 1 step: 493, loss is 0.5260031223297119\n",
      "epoch: 1 step: 494, loss is 0.40134161710739136\n",
      "epoch: 1 step: 495, loss is 0.5178539156913757\n",
      "epoch: 1 step: 496, loss is 0.6731132864952087\n",
      "epoch: 1 step: 497, loss is 0.4582637846469879\n",
      "epoch: 1 step: 498, loss is 0.5954257249832153\n",
      "epoch: 1 step: 499, loss is 0.4810931384563446\n",
      "epoch: 1 step: 500, loss is 0.5865312814712524\n",
      "epoch: 1 step: 501, loss is 0.29647234082221985\n",
      "epoch: 1 step: 502, loss is 0.43256425857543945\n",
      "epoch: 1 step: 503, loss is 0.41013672947883606\n",
      "epoch: 1 step: 504, loss is 0.6343268156051636\n",
      "epoch: 1 step: 505, loss is 0.554950475692749\n",
      "epoch: 1 step: 506, loss is 0.6316060423851013\n",
      "epoch: 1 step: 507, loss is 0.42373019456863403\n",
      "epoch: 1 step: 508, loss is 0.3672156035900116\n",
      "epoch: 1 step: 509, loss is 0.40422895550727844\n",
      "epoch: 1 step: 510, loss is 0.42785534262657166\n",
      "epoch: 1 step: 511, loss is 0.5564439296722412\n",
      "epoch: 1 step: 512, loss is 0.39814093708992004\n",
      "epoch: 1 step: 513, loss is 0.39294517040252686\n",
      "epoch: 1 step: 514, loss is 0.4279080629348755\n",
      "epoch: 1 step: 515, loss is 0.3420391082763672\n",
      "epoch: 1 step: 516, loss is 0.4220503568649292\n",
      "epoch: 1 step: 517, loss is 0.5381360054016113\n",
      "epoch: 1 step: 518, loss is 0.5618045330047607\n",
      "epoch: 1 step: 519, loss is 0.48560959100723267\n",
      "epoch: 1 step: 520, loss is 0.3808673620223999\n",
      "epoch: 1 step: 521, loss is 0.667141318321228\n",
      "epoch: 1 step: 522, loss is 0.38163450360298157\n",
      "epoch: 1 step: 523, loss is 0.5548246502876282\n",
      "epoch: 1 step: 524, loss is 0.5156263709068298\n",
      "epoch: 1 step: 525, loss is 0.38425910472869873\n",
      "epoch: 1 step: 526, loss is 0.5527446866035461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 527, loss is 0.36357244849205017\n",
      "epoch: 1 step: 528, loss is 0.3586622178554535\n",
      "epoch: 1 step: 529, loss is 0.6570996046066284\n",
      "epoch: 1 step: 530, loss is 0.3096027076244354\n",
      "epoch: 1 step: 531, loss is 0.4932595491409302\n",
      "epoch: 1 step: 532, loss is 0.3583742082118988\n",
      "epoch: 1 step: 533, loss is 0.5004245638847351\n",
      "epoch: 1 step: 534, loss is 0.5437094569206238\n",
      "epoch: 1 step: 535, loss is 0.45809414982795715\n",
      "epoch: 1 step: 536, loss is 0.5736086964607239\n",
      "epoch: 1 step: 537, loss is 0.46904879808425903\n",
      "epoch: 1 step: 538, loss is 0.5442205667495728\n",
      "epoch: 1 step: 539, loss is 0.3767048418521881\n",
      "epoch: 1 step: 540, loss is 0.483156681060791\n",
      "epoch: 1 step: 541, loss is 0.30563274025917053\n",
      "epoch: 1 step: 542, loss is 0.4302496016025543\n",
      "epoch: 1 step: 543, loss is 0.3609115779399872\n",
      "epoch: 1 step: 544, loss is 0.37889364361763\n",
      "epoch: 1 step: 545, loss is 0.41686341166496277\n",
      "epoch: 1 step: 546, loss is 0.47295114398002625\n",
      "epoch: 1 step: 547, loss is 0.39677101373672485\n",
      "epoch: 1 step: 548, loss is 0.517320990562439\n",
      "epoch: 1 step: 549, loss is 0.5725338459014893\n",
      "epoch: 1 step: 550, loss is 0.2738615572452545\n",
      "epoch: 1 step: 551, loss is 0.5740987062454224\n",
      "epoch: 1 step: 552, loss is 0.2906944453716278\n",
      "epoch: 1 step: 553, loss is 0.3747108280658722\n",
      "epoch: 1 step: 554, loss is 0.4058719277381897\n",
      "epoch: 1 step: 555, loss is 0.44164955615997314\n",
      "epoch: 1 step: 556, loss is 0.5028032660484314\n",
      "epoch: 1 step: 557, loss is 0.3830496668815613\n",
      "epoch: 1 step: 558, loss is 0.3669455349445343\n",
      "epoch: 1 step: 559, loss is 0.3876780569553375\n",
      "epoch: 1 step: 560, loss is 0.34780600666999817\n",
      "epoch: 1 step: 561, loss is 0.44769516587257385\n",
      "epoch: 1 step: 562, loss is 0.4630584418773651\n",
      "epoch: 1 step: 563, loss is 0.47922173142433167\n",
      "epoch: 1 step: 564, loss is 0.4705292284488678\n",
      "epoch: 1 step: 565, loss is 0.5071917772293091\n",
      "epoch: 1 step: 566, loss is 0.4741774797439575\n",
      "epoch: 1 step: 567, loss is 0.380536288022995\n",
      "epoch: 1 step: 568, loss is 0.41976064443588257\n",
      "epoch: 1 step: 569, loss is 0.41823992133140564\n",
      "epoch: 1 step: 570, loss is 0.4880382716655731\n",
      "epoch: 1 step: 571, loss is 0.5350106358528137\n",
      "epoch: 1 step: 572, loss is 0.6220837235450745\n",
      "epoch: 1 step: 573, loss is 0.3509238362312317\n",
      "epoch: 1 step: 574, loss is 0.32301732897758484\n",
      "epoch: 1 step: 575, loss is 0.44857048988342285\n",
      "epoch: 1 step: 576, loss is 0.3951856791973114\n",
      "epoch: 1 step: 577, loss is 0.522367537021637\n",
      "epoch: 1 step: 578, loss is 0.4889567196369171\n",
      "epoch: 1 step: 579, loss is 0.507923424243927\n",
      "epoch: 1 step: 580, loss is 0.4305025041103363\n",
      "epoch: 1 step: 581, loss is 0.4995056092739105\n",
      "epoch: 1 step: 582, loss is 0.6289224028587341\n",
      "epoch: 1 step: 583, loss is 0.5392466187477112\n",
      "epoch: 1 step: 584, loss is 0.4311615526676178\n",
      "epoch: 1 step: 585, loss is 0.4904956519603729\n",
      "epoch: 1 step: 586, loss is 0.6318175196647644\n",
      "epoch: 1 step: 587, loss is 0.4197433292865753\n",
      "epoch: 1 step: 588, loss is 0.3611358404159546\n",
      "epoch: 1 step: 589, loss is 0.3523138463497162\n",
      "epoch: 1 step: 590, loss is 0.49200043082237244\n",
      "epoch: 1 step: 591, loss is 0.5134828090667725\n",
      "epoch: 1 step: 592, loss is 0.4320261478424072\n",
      "epoch: 1 step: 593, loss is 0.4092828929424286\n",
      "epoch: 1 step: 594, loss is 0.3943691551685333\n",
      "epoch: 1 step: 595, loss is 0.603952169418335\n",
      "epoch: 1 step: 596, loss is 0.4612363576889038\n",
      "epoch: 1 step: 597, loss is 0.5565198659896851\n",
      "epoch: 1 step: 598, loss is 0.38525551557540894\n",
      "epoch: 1 step: 599, loss is 0.5560874342918396\n",
      "epoch: 1 step: 600, loss is 0.3223263919353485\n",
      "epoch: 1 step: 601, loss is 0.430559366941452\n",
      "epoch: 1 step: 602, loss is 0.46696388721466064\n",
      "epoch: 1 step: 603, loss is 0.4956904649734497\n",
      "epoch: 1 step: 604, loss is 0.4013485908508301\n",
      "epoch: 1 step: 605, loss is 0.42459022998809814\n",
      "epoch: 1 step: 606, loss is 0.30595940351486206\n",
      "epoch: 1 step: 607, loss is 0.3569422662258148\n",
      "epoch: 1 step: 608, loss is 0.5612472891807556\n",
      "epoch: 1 step: 609, loss is 0.3029010593891144\n",
      "epoch: 1 step: 610, loss is 0.3519107699394226\n",
      "epoch: 1 step: 611, loss is 0.42690399289131165\n",
      "epoch: 1 step: 612, loss is 0.4043631851673126\n",
      "epoch: 1 step: 613, loss is 0.4508419334888458\n",
      "epoch: 1 step: 614, loss is 0.4086208939552307\n",
      "epoch: 1 step: 615, loss is 0.44388556480407715\n",
      "epoch: 1 step: 616, loss is 0.5019977688789368\n",
      "epoch: 1 step: 617, loss is 0.3901824951171875\n",
      "epoch: 1 step: 618, loss is 0.4721481204032898\n",
      "epoch: 1 step: 619, loss is 0.6140702366828918\n",
      "epoch: 1 step: 620, loss is 0.43763983249664307\n",
      "epoch: 1 step: 621, loss is 0.5520631074905396\n",
      "epoch: 1 step: 622, loss is 0.4015128016471863\n",
      "epoch: 1 step: 623, loss is 0.4318346679210663\n",
      "epoch: 1 step: 624, loss is 0.27728626132011414\n",
      "epoch: 1 step: 625, loss is 0.4461243748664856\n",
      "epoch: 1 step: 626, loss is 0.618763267993927\n",
      "epoch: 1 step: 627, loss is 0.5355319976806641\n",
      "epoch: 1 step: 628, loss is 0.3969828188419342\n",
      "epoch: 1 step: 629, loss is 0.40362146496772766\n",
      "epoch: 1 step: 630, loss is 0.2784424424171448\n",
      "epoch: 1 step: 631, loss is 0.3911140561103821\n",
      "epoch: 1 step: 632, loss is 0.3883829712867737\n",
      "epoch: 1 step: 633, loss is 0.4426012933254242\n",
      "epoch: 1 step: 634, loss is 0.14313741028308868\n",
      "epoch: 1 step: 635, loss is 0.4029732942581177\n",
      "epoch: 1 step: 636, loss is 0.5331811308860779\n",
      "epoch: 1 step: 637, loss is 0.5404037237167358\n",
      "epoch: 1 step: 638, loss is 0.42266011238098145\n",
      "epoch: 1 step: 639, loss is 0.43560880422592163\n",
      "epoch: 1 step: 640, loss is 0.434216171503067\n",
      "epoch: 1 step: 641, loss is 0.39744600653648376\n",
      "epoch: 1 step: 642, loss is 0.33400958776474\n",
      "epoch: 1 step: 643, loss is 0.40125855803489685\n",
      "epoch: 1 step: 644, loss is 0.307472825050354\n",
      "epoch: 1 step: 645, loss is 0.3396266996860504\n",
      "epoch: 1 step: 646, loss is 0.48488128185272217\n",
      "epoch: 1 step: 647, loss is 0.43203356862068176\n",
      "epoch: 1 step: 648, loss is 0.3764626979827881\n",
      "epoch: 1 step: 649, loss is 0.4534911811351776\n",
      "epoch: 1 step: 650, loss is 0.3412513732910156\n",
      "epoch: 1 step: 651, loss is 0.46737930178642273\n",
      "epoch: 1 step: 652, loss is 0.47495004534721375\n",
      "epoch: 1 step: 653, loss is 0.3342060148715973\n",
      "epoch: 1 step: 654, loss is 0.4999954104423523\n",
      "epoch: 1 step: 655, loss is 0.45904526114463806\n",
      "epoch: 1 step: 656, loss is 0.5179004073143005\n",
      "epoch: 1 step: 657, loss is 0.543129563331604\n",
      "epoch: 1 step: 658, loss is 0.43590670824050903\n",
      "epoch: 1 step: 659, loss is 0.2094992846250534\n",
      "epoch: 1 step: 660, loss is 0.5381841063499451\n",
      "epoch: 1 step: 661, loss is 0.5853538513183594\n",
      "epoch: 1 step: 662, loss is 0.5234746932983398\n",
      "epoch: 1 step: 663, loss is 0.4256265163421631\n",
      "epoch: 1 step: 664, loss is 0.2674401104450226\n",
      "epoch: 1 step: 665, loss is 0.3953551650047302\n",
      "epoch: 1 step: 666, loss is 0.45663681626319885\n",
      "epoch: 1 step: 667, loss is 0.24406442046165466\n",
      "epoch: 1 step: 668, loss is 0.49049142003059387\n",
      "epoch: 1 step: 669, loss is 0.4667423665523529\n",
      "epoch: 1 step: 670, loss is 0.3300996422767639\n",
      "epoch: 1 step: 671, loss is 0.5001833438873291\n",
      "epoch: 1 step: 672, loss is 0.3801005780696869\n",
      "epoch: 1 step: 673, loss is 0.24144631624221802\n",
      "epoch: 1 step: 674, loss is 0.23638419806957245\n",
      "epoch: 1 step: 675, loss is 0.46326562762260437\n",
      "epoch: 1 step: 676, loss is 0.5414198637008667\n",
      "epoch: 1 step: 677, loss is 0.3184794485569\n",
      "epoch: 1 step: 678, loss is 0.38234251737594604\n",
      "epoch: 1 step: 679, loss is 0.7464290261268616\n",
      "epoch: 1 step: 680, loss is 0.4811047315597534\n",
      "epoch: 1 step: 681, loss is 0.40544578433036804\n",
      "epoch: 1 step: 682, loss is 0.4616643190383911\n",
      "epoch: 1 step: 683, loss is 0.4230261743068695\n",
      "epoch: 1 step: 684, loss is 0.398538738489151\n",
      "epoch: 1 step: 685, loss is 0.37232351303100586\n",
      "epoch: 1 step: 686, loss is 0.37276560068130493\n",
      "epoch: 1 step: 687, loss is 0.4949212074279785\n",
      "epoch: 1 step: 688, loss is 0.3952324092388153\n",
      "epoch: 1 step: 689, loss is 0.5010036826133728\n",
      "epoch: 1 step: 690, loss is 0.44589802622795105\n",
      "epoch: 1 step: 691, loss is 0.37041357159614563\n",
      "epoch: 1 step: 692, loss is 0.38788264989852905\n",
      "epoch: 1 step: 693, loss is 0.3849968910217285\n",
      "epoch: 1 step: 694, loss is 0.2948586046695709\n",
      "epoch: 1 step: 695, loss is 0.4549501836299896\n",
      "epoch: 1 step: 696, loss is 0.41539889574050903\n",
      "epoch: 1 step: 697, loss is 0.5480639934539795\n",
      "epoch: 1 step: 698, loss is 0.46131765842437744\n",
      "epoch: 1 step: 699, loss is 0.4977699816226959\n",
      "epoch: 1 step: 700, loss is 0.49383091926574707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 701, loss is 0.40543293952941895\n",
      "epoch: 1 step: 702, loss is 0.49000081419944763\n",
      "epoch: 1 step: 703, loss is 0.4052189588546753\n",
      "epoch: 1 step: 704, loss is 0.3366502523422241\n",
      "epoch: 1 step: 705, loss is 0.4906873404979706\n",
      "epoch: 1 step: 706, loss is 0.5041311383247375\n",
      "epoch: 1 step: 707, loss is 0.3905191719532013\n",
      "epoch: 1 step: 708, loss is 0.5570642948150635\n",
      "epoch: 1 step: 709, loss is 0.350538432598114\n",
      "epoch: 1 step: 710, loss is 0.45304742455482483\n",
      "epoch: 1 step: 711, loss is 0.43168342113494873\n",
      "epoch: 1 step: 712, loss is 0.4929145276546478\n",
      "epoch: 1 step: 713, loss is 0.3489239513874054\n",
      "epoch: 1 step: 714, loss is 0.4281541109085083\n",
      "epoch: 1 step: 715, loss is 0.394607812166214\n",
      "epoch: 1 step: 716, loss is 0.433147132396698\n",
      "epoch: 1 step: 717, loss is 0.45436346530914307\n",
      "epoch: 1 step: 718, loss is 0.3684645891189575\n",
      "epoch: 1 step: 719, loss is 0.47534307837486267\n",
      "epoch: 1 step: 720, loss is 0.3347966969013214\n",
      "epoch: 1 step: 721, loss is 0.3432063162326813\n",
      "epoch: 1 step: 722, loss is 0.4062678813934326\n",
      "epoch: 1 step: 723, loss is 0.30564895272254944\n",
      "epoch: 1 step: 724, loss is 0.47490358352661133\n",
      "epoch: 1 step: 725, loss is 0.3814142644405365\n",
      "epoch: 1 step: 726, loss is 0.5402320027351379\n",
      "epoch: 1 step: 727, loss is 0.4783863425254822\n",
      "epoch: 1 step: 728, loss is 0.4150165319442749\n",
      "epoch: 1 step: 729, loss is 0.27326640486717224\n",
      "epoch: 1 step: 730, loss is 0.4509546160697937\n",
      "epoch: 1 step: 731, loss is 0.5522135496139526\n",
      "epoch: 1 step: 732, loss is 0.6502597332000732\n",
      "epoch: 1 step: 733, loss is 0.3296813666820526\n",
      "epoch: 1 step: 734, loss is 0.444230854511261\n",
      "epoch: 1 step: 735, loss is 0.47615399956703186\n",
      "epoch: 1 step: 736, loss is 0.3435481786727905\n",
      "epoch: 1 step: 737, loss is 0.4308375120162964\n",
      "epoch: 1 step: 738, loss is 0.46300119161605835\n",
      "epoch: 1 step: 739, loss is 0.436690092086792\n",
      "epoch: 1 step: 740, loss is 0.4460781514644623\n",
      "epoch: 1 step: 741, loss is 0.4948772192001343\n",
      "epoch: 1 step: 742, loss is 0.40339091420173645\n",
      "epoch: 1 step: 743, loss is 0.6735101938247681\n",
      "epoch: 1 step: 744, loss is 0.3546152710914612\n",
      "epoch: 1 step: 745, loss is 0.33029577136039734\n",
      "epoch: 1 step: 746, loss is 0.3561273217201233\n",
      "epoch: 1 step: 747, loss is 0.2471565455198288\n",
      "epoch: 1 step: 748, loss is 0.27489253878593445\n",
      "epoch: 1 step: 749, loss is 0.4389234483242035\n",
      "epoch: 1 step: 750, loss is 0.38956359028816223\n",
      "epoch: 1 step: 751, loss is 0.4358777105808258\n",
      "epoch: 1 step: 752, loss is 0.31989607214927673\n",
      "epoch: 1 step: 753, loss is 0.5310505628585815\n",
      "epoch: 1 step: 754, loss is 0.5464398860931396\n",
      "epoch: 1 step: 755, loss is 0.2831580340862274\n",
      "epoch: 1 step: 756, loss is 0.389963835477829\n",
      "epoch: 1 step: 757, loss is 0.33875778317451477\n",
      "epoch: 1 step: 758, loss is 0.5399599075317383\n",
      "epoch: 1 step: 759, loss is 0.36571183800697327\n",
      "epoch: 1 step: 760, loss is 0.33067426085472107\n",
      "epoch: 1 step: 761, loss is 0.339383065700531\n",
      "epoch: 1 step: 762, loss is 0.3536709249019623\n",
      "epoch: 1 step: 763, loss is 0.5604943037033081\n",
      "epoch: 1 step: 764, loss is 0.39950138330459595\n",
      "epoch: 1 step: 765, loss is 0.4382297396659851\n",
      "epoch: 1 step: 766, loss is 0.28247010707855225\n",
      "epoch: 1 step: 767, loss is 0.47224947810173035\n",
      "epoch: 1 step: 768, loss is 0.6033997535705566\n",
      "epoch: 1 step: 769, loss is 0.4101755917072296\n",
      "epoch: 1 step: 770, loss is 0.3203957974910736\n",
      "epoch: 1 step: 771, loss is 0.3033842444419861\n",
      "epoch: 1 step: 772, loss is 0.3809899389743805\n",
      "epoch: 1 step: 773, loss is 0.6481857895851135\n",
      "epoch: 1 step: 774, loss is 0.4187682867050171\n",
      "epoch: 1 step: 775, loss is 0.26002055406570435\n",
      "epoch: 1 step: 776, loss is 0.2396448403596878\n",
      "epoch: 1 step: 777, loss is 0.2869409918785095\n",
      "epoch: 1 step: 778, loss is 0.2962644398212433\n",
      "epoch: 1 step: 779, loss is 0.2715730667114258\n",
      "epoch: 1 step: 780, loss is 0.38538798689842224\n",
      "epoch: 1 step: 781, loss is 0.6990013122558594\n",
      "epoch: 1 step: 782, loss is 0.5081578493118286\n",
      "epoch: 1 step: 783, loss is 0.4765615165233612\n",
      "epoch: 1 step: 784, loss is 0.4348401427268982\n",
      "epoch: 1 step: 785, loss is 0.2894502878189087\n",
      "epoch: 1 step: 786, loss is 0.30496326088905334\n",
      "epoch: 1 step: 787, loss is 0.421143114566803\n",
      "epoch: 1 step: 788, loss is 0.32801857590675354\n",
      "epoch: 1 step: 789, loss is 0.3812638223171234\n",
      "epoch: 1 step: 790, loss is 0.36743083596229553\n",
      "epoch: 1 step: 791, loss is 0.4062156081199646\n",
      "epoch: 1 step: 792, loss is 0.4951134920120239\n",
      "epoch: 1 step: 793, loss is 0.6162025928497314\n",
      "epoch: 1 step: 794, loss is 0.34360048174858093\n",
      "epoch: 1 step: 795, loss is 0.382720410823822\n",
      "epoch: 1 step: 796, loss is 0.4267856478691101\n",
      "epoch: 1 step: 797, loss is 0.4274631440639496\n",
      "epoch: 1 step: 798, loss is 0.38568195700645447\n",
      "epoch: 1 step: 799, loss is 0.5307605266571045\n",
      "epoch: 1 step: 800, loss is 0.4519994258880615\n",
      "epoch: 1 step: 801, loss is 0.32208317518234253\n",
      "epoch: 1 step: 802, loss is 0.537833034992218\n",
      "epoch: 1 step: 803, loss is 0.6575282216072083\n",
      "epoch: 1 step: 804, loss is 0.4376264810562134\n",
      "epoch: 1 step: 805, loss is 0.4692516624927521\n",
      "epoch: 1 step: 806, loss is 0.5026835203170776\n",
      "epoch: 1 step: 807, loss is 0.49464941024780273\n",
      "epoch: 1 step: 808, loss is 0.42713993787765503\n",
      "epoch: 1 step: 809, loss is 0.44513630867004395\n",
      "epoch: 1 step: 810, loss is 0.3571140468120575\n",
      "epoch: 1 step: 811, loss is 0.48721054196357727\n",
      "epoch: 1 step: 812, loss is 0.5397008061408997\n",
      "epoch: 1 step: 813, loss is 0.2445800006389618\n",
      "epoch: 1 step: 814, loss is 0.4554811120033264\n",
      "epoch: 1 step: 815, loss is 0.3502799868583679\n",
      "epoch: 1 step: 816, loss is 0.481219619512558\n",
      "epoch: 1 step: 817, loss is 0.3138487935066223\n",
      "epoch: 1 step: 818, loss is 0.2975901663303375\n",
      "epoch: 1 step: 819, loss is 0.5998976826667786\n",
      "epoch: 1 step: 820, loss is 0.46453315019607544\n",
      "epoch: 1 step: 821, loss is 0.5891627073287964\n",
      "epoch: 1 step: 822, loss is 0.2951335608959198\n",
      "epoch: 1 step: 823, loss is 0.5551264882087708\n",
      "epoch: 1 step: 824, loss is 0.505491316318512\n",
      "epoch: 1 step: 825, loss is 0.30277493596076965\n",
      "epoch: 1 step: 826, loss is 0.41568949818611145\n",
      "epoch: 1 step: 827, loss is 0.4459362328052521\n",
      "epoch: 1 step: 828, loss is 0.3486683964729309\n",
      "epoch: 1 step: 829, loss is 0.5621510744094849\n",
      "epoch: 1 step: 830, loss is 0.40181758999824524\n",
      "epoch: 1 step: 831, loss is 0.47896265983581543\n",
      "epoch: 1 step: 832, loss is 0.3848840296268463\n",
      "epoch: 1 step: 833, loss is 0.47715312242507935\n",
      "epoch: 1 step: 834, loss is 0.5553842782974243\n",
      "epoch: 1 step: 835, loss is 0.5228758454322815\n",
      "epoch: 1 step: 836, loss is 0.2617349922657013\n",
      "epoch: 1 step: 837, loss is 0.4136921763420105\n",
      "epoch: 1 step: 838, loss is 0.34349310398101807\n",
      "epoch: 1 step: 839, loss is 0.3780565857887268\n",
      "epoch: 1 step: 840, loss is 0.47246843576431274\n",
      "epoch: 1 step: 841, loss is 0.4246591031551361\n",
      "epoch: 1 step: 842, loss is 0.6079803109169006\n",
      "epoch: 1 step: 843, loss is 0.507077693939209\n",
      "epoch: 1 step: 844, loss is 0.3223724961280823\n",
      "epoch: 1 step: 845, loss is 0.5662015080451965\n",
      "epoch: 1 step: 846, loss is 0.38855230808258057\n",
      "epoch: 1 step: 847, loss is 0.5196644067764282\n",
      "epoch: 1 step: 848, loss is 0.3715999722480774\n",
      "epoch: 1 step: 849, loss is 0.44647055864334106\n",
      "epoch: 1 step: 850, loss is 0.669249415397644\n",
      "epoch: 1 step: 851, loss is 0.26781025528907776\n",
      "epoch: 1 step: 852, loss is 0.34997662901878357\n",
      "epoch: 1 step: 853, loss is 0.2581602931022644\n",
      "epoch: 1 step: 854, loss is 0.47250789403915405\n",
      "epoch: 1 step: 855, loss is 0.36561620235443115\n",
      "epoch: 1 step: 856, loss is 0.40459710359573364\n",
      "epoch: 1 step: 857, loss is 0.4805585741996765\n",
      "epoch: 1 step: 858, loss is 0.31861448287963867\n",
      "epoch: 1 step: 859, loss is 0.4006474018096924\n",
      "epoch: 1 step: 860, loss is 0.4898192584514618\n",
      "epoch: 1 step: 861, loss is 0.387145072221756\n",
      "epoch: 1 step: 862, loss is 0.5317888259887695\n",
      "epoch: 1 step: 863, loss is 0.39717528223991394\n",
      "epoch: 1 step: 864, loss is 0.4999062716960907\n",
      "epoch: 1 step: 865, loss is 0.2834048569202423\n",
      "epoch: 1 step: 866, loss is 0.49160632491111755\n",
      "epoch: 1 step: 867, loss is 0.587460458278656\n",
      "epoch: 1 step: 868, loss is 0.41132527589797974\n",
      "epoch: 1 step: 869, loss is 0.3599174916744232\n",
      "epoch: 1 step: 870, loss is 0.42770078778266907\n",
      "epoch: 1 step: 871, loss is 0.5491161346435547\n",
      "epoch: 1 step: 872, loss is 0.33717599511146545\n",
      "epoch: 1 step: 873, loss is 0.3818216025829315\n",
      "epoch: 1 step: 874, loss is 0.3367644250392914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 875, loss is 0.4249420464038849\n",
      "epoch: 1 step: 876, loss is 0.47879281640052795\n",
      "epoch: 1 step: 877, loss is 0.4209090769290924\n",
      "epoch: 1 step: 878, loss is 0.2728094756603241\n",
      "epoch: 1 step: 879, loss is 0.4162726402282715\n",
      "epoch: 1 step: 880, loss is 0.45873817801475525\n",
      "epoch: 1 step: 881, loss is 0.18939855694770813\n",
      "epoch: 1 step: 882, loss is 0.35699909925460815\n",
      "epoch: 1 step: 883, loss is 0.5137500762939453\n",
      "epoch: 1 step: 884, loss is 0.3313716650009155\n",
      "epoch: 1 step: 885, loss is 0.39545395970344543\n",
      "epoch: 1 step: 886, loss is 0.5135388970375061\n",
      "epoch: 1 step: 887, loss is 0.3727049231529236\n",
      "epoch: 1 step: 888, loss is 0.36531201004981995\n",
      "epoch: 1 step: 889, loss is 0.5238115191459656\n",
      "epoch: 1 step: 890, loss is 0.4129818081855774\n",
      "epoch: 1 step: 891, loss is 0.33113059401512146\n",
      "epoch: 1 step: 892, loss is 0.25259536504745483\n",
      "epoch: 1 step: 893, loss is 0.43133243918418884\n",
      "epoch: 1 step: 894, loss is 0.4292902946472168\n",
      "epoch: 1 step: 895, loss is 0.3108732998371124\n",
      "epoch: 1 step: 896, loss is 0.6353292465209961\n",
      "epoch: 1 step: 897, loss is 0.4618975520133972\n",
      "epoch: 1 step: 898, loss is 0.389831006526947\n",
      "epoch: 1 step: 899, loss is 0.5341625213623047\n",
      "epoch: 1 step: 900, loss is 0.42671167850494385\n",
      "epoch: 1 step: 901, loss is 0.4022827446460724\n",
      "epoch: 1 step: 902, loss is 0.25865811109542847\n",
      "epoch: 1 step: 903, loss is 0.39106690883636475\n",
      "epoch: 1 step: 904, loss is 0.39787179231643677\n",
      "epoch: 1 step: 905, loss is 0.30854296684265137\n",
      "epoch: 1 step: 906, loss is 0.3059934377670288\n",
      "epoch: 1 step: 907, loss is 0.46150508522987366\n",
      "epoch: 1 step: 908, loss is 0.3650300204753876\n",
      "epoch: 1 step: 909, loss is 0.4838126003742218\n",
      "epoch: 1 step: 910, loss is 0.38787078857421875\n",
      "epoch: 1 step: 911, loss is 0.312838077545166\n",
      "epoch: 1 step: 912, loss is 0.39156582951545715\n",
      "epoch: 1 step: 913, loss is 0.3807726204395294\n",
      "epoch: 1 step: 914, loss is 0.501386284828186\n",
      "epoch: 1 step: 915, loss is 0.2951003313064575\n",
      "epoch: 1 step: 916, loss is 0.2171577364206314\n",
      "epoch: 1 step: 917, loss is 0.417043536901474\n",
      "epoch: 1 step: 918, loss is 0.32985180616378784\n",
      "epoch: 1 step: 919, loss is 0.4924427270889282\n",
      "epoch: 1 step: 920, loss is 0.4722088575363159\n",
      "epoch: 1 step: 921, loss is 0.41480937600135803\n",
      "epoch: 1 step: 922, loss is 0.33440738916397095\n",
      "epoch: 1 step: 923, loss is 0.3345302939414978\n",
      "epoch: 1 step: 924, loss is 0.4642040431499481\n",
      "epoch: 1 step: 925, loss is 0.4425320625305176\n",
      "epoch: 1 step: 926, loss is 0.5232679843902588\n",
      "epoch: 1 step: 927, loss is 0.3937091529369354\n",
      "epoch: 1 step: 928, loss is 0.5436442494392395\n",
      "epoch: 1 step: 929, loss is 0.39321765303611755\n",
      "epoch: 1 step: 930, loss is 0.38976407051086426\n",
      "epoch: 1 step: 931, loss is 0.4789530336856842\n",
      "epoch: 1 step: 932, loss is 0.28613290190696716\n",
      "epoch: 1 step: 933, loss is 0.333729088306427\n",
      "epoch: 1 step: 934, loss is 0.3249194622039795\n",
      "epoch: 1 step: 935, loss is 0.43504270911216736\n",
      "epoch: 1 step: 936, loss is 0.4109889566898346\n",
      "epoch: 1 step: 937, loss is 0.3138655126094818\n",
      "epoch: 2 step: 1, loss is 0.5441704988479614\n",
      "epoch: 2 step: 2, loss is 0.44036808609962463\n",
      "epoch: 2 step: 3, loss is 0.26257991790771484\n",
      "epoch: 2 step: 4, loss is 0.36743760108947754\n",
      "epoch: 2 step: 5, loss is 0.31735724210739136\n",
      "epoch: 2 step: 6, loss is 0.5653608441352844\n",
      "epoch: 2 step: 7, loss is 0.4261206090450287\n",
      "epoch: 2 step: 8, loss is 0.3298618495464325\n",
      "epoch: 2 step: 9, loss is 0.2629612684249878\n",
      "epoch: 2 step: 10, loss is 0.23013897240161896\n",
      "epoch: 2 step: 11, loss is 0.5205278396606445\n",
      "epoch: 2 step: 12, loss is 0.6520572304725647\n",
      "epoch: 2 step: 13, loss is 0.5769644379615784\n",
      "epoch: 2 step: 14, loss is 0.32006317377090454\n",
      "epoch: 2 step: 15, loss is 0.4018605947494507\n",
      "epoch: 2 step: 16, loss is 0.29866474866867065\n",
      "epoch: 2 step: 17, loss is 0.40755295753479004\n",
      "epoch: 2 step: 18, loss is 0.25464779138565063\n",
      "epoch: 2 step: 19, loss is 0.4859277904033661\n",
      "epoch: 2 step: 20, loss is 0.45590919256210327\n",
      "epoch: 2 step: 21, loss is 0.4638371467590332\n",
      "epoch: 2 step: 22, loss is 0.3420445919036865\n",
      "epoch: 2 step: 23, loss is 0.4364205598831177\n",
      "epoch: 2 step: 24, loss is 0.47485101222991943\n",
      "epoch: 2 step: 25, loss is 0.24350839853286743\n",
      "epoch: 2 step: 26, loss is 0.3613499701023102\n",
      "epoch: 2 step: 27, loss is 0.36780548095703125\n",
      "epoch: 2 step: 28, loss is 0.47013333439826965\n",
      "epoch: 2 step: 29, loss is 0.3435174822807312\n",
      "epoch: 2 step: 30, loss is 0.25023195147514343\n",
      "epoch: 2 step: 31, loss is 0.47482675313949585\n",
      "epoch: 2 step: 32, loss is 0.2343343198299408\n",
      "epoch: 2 step: 33, loss is 0.2731500267982483\n",
      "epoch: 2 step: 34, loss is 0.37513744831085205\n",
      "epoch: 2 step: 35, loss is 0.4596639573574066\n",
      "epoch: 2 step: 36, loss is 0.3222607374191284\n",
      "epoch: 2 step: 37, loss is 0.41136881709098816\n",
      "epoch: 2 step: 38, loss is 0.49898529052734375\n",
      "epoch: 2 step: 39, loss is 0.5448509454727173\n",
      "epoch: 2 step: 40, loss is 0.6415528655052185\n",
      "epoch: 2 step: 41, loss is 0.3654709756374359\n",
      "epoch: 2 step: 42, loss is 0.4498710334300995\n",
      "epoch: 2 step: 43, loss is 0.4650104343891144\n",
      "epoch: 2 step: 44, loss is 0.35857561230659485\n",
      "epoch: 2 step: 45, loss is 0.26768794655799866\n",
      "epoch: 2 step: 46, loss is 0.2795155942440033\n",
      "epoch: 2 step: 47, loss is 0.46210911870002747\n",
      "epoch: 2 step: 48, loss is 0.34710291028022766\n",
      "epoch: 2 step: 49, loss is 0.39543119072914124\n",
      "epoch: 2 step: 50, loss is 0.4046027362346649\n",
      "epoch: 2 step: 51, loss is 0.4839235842227936\n",
      "epoch: 2 step: 52, loss is 0.3159710466861725\n",
      "epoch: 2 step: 53, loss is 0.4546653628349304\n",
      "epoch: 2 step: 54, loss is 0.29512569308280945\n",
      "epoch: 2 step: 55, loss is 0.2254127413034439\n",
      "epoch: 2 step: 56, loss is 0.3791525661945343\n",
      "epoch: 2 step: 57, loss is 0.3938184976577759\n",
      "epoch: 2 step: 58, loss is 0.6873955726623535\n",
      "epoch: 2 step: 59, loss is 0.504482626914978\n",
      "epoch: 2 step: 60, loss is 0.41371023654937744\n",
      "epoch: 2 step: 61, loss is 0.37332862615585327\n",
      "epoch: 2 step: 62, loss is 0.47752565145492554\n",
      "epoch: 2 step: 63, loss is 0.6073263883590698\n",
      "epoch: 2 step: 64, loss is 0.3995664119720459\n",
      "epoch: 2 step: 65, loss is 0.43825143575668335\n",
      "epoch: 2 step: 66, loss is 0.4486265778541565\n",
      "epoch: 2 step: 67, loss is 0.40147554874420166\n",
      "epoch: 2 step: 68, loss is 0.4265126585960388\n",
      "epoch: 2 step: 69, loss is 0.30104222893714905\n",
      "epoch: 2 step: 70, loss is 0.39277687668800354\n",
      "epoch: 2 step: 71, loss is 0.45084407925605774\n",
      "epoch: 2 step: 72, loss is 0.24600671231746674\n",
      "epoch: 2 step: 73, loss is 0.3003828823566437\n",
      "epoch: 2 step: 74, loss is 0.46381309628486633\n",
      "epoch: 2 step: 75, loss is 0.5693562626838684\n",
      "epoch: 2 step: 76, loss is 0.4549974501132965\n",
      "epoch: 2 step: 77, loss is 0.40442317724227905\n",
      "epoch: 2 step: 78, loss is 0.3895925283432007\n",
      "epoch: 2 step: 79, loss is 0.4020710587501526\n",
      "epoch: 2 step: 80, loss is 0.33067449927330017\n",
      "epoch: 2 step: 81, loss is 0.3775998651981354\n",
      "epoch: 2 step: 82, loss is 0.3561350405216217\n",
      "epoch: 2 step: 83, loss is 0.45883849263191223\n",
      "epoch: 2 step: 84, loss is 0.3065796494483948\n",
      "epoch: 2 step: 85, loss is 0.35278621315956116\n",
      "epoch: 2 step: 86, loss is 0.3721064329147339\n",
      "epoch: 2 step: 87, loss is 0.4025413691997528\n",
      "epoch: 2 step: 88, loss is 0.27273881435394287\n",
      "epoch: 2 step: 89, loss is 0.6690296530723572\n",
      "epoch: 2 step: 90, loss is 0.5189644694328308\n",
      "epoch: 2 step: 91, loss is 0.33338144421577454\n",
      "epoch: 2 step: 92, loss is 0.2669501006603241\n",
      "epoch: 2 step: 93, loss is 0.41288599371910095\n",
      "epoch: 2 step: 94, loss is 0.4625169634819031\n",
      "epoch: 2 step: 95, loss is 0.36193159222602844\n",
      "epoch: 2 step: 96, loss is 0.2670744061470032\n",
      "epoch: 2 step: 97, loss is 0.38387858867645264\n",
      "epoch: 2 step: 98, loss is 0.3370240032672882\n",
      "epoch: 2 step: 99, loss is 0.3143680691719055\n",
      "epoch: 2 step: 100, loss is 0.5165530443191528\n",
      "epoch: 2 step: 101, loss is 0.4749268591403961\n",
      "epoch: 2 step: 102, loss is 0.41676032543182373\n",
      "epoch: 2 step: 103, loss is 0.5147771835327148\n",
      "epoch: 2 step: 104, loss is 0.3996933400630951\n",
      "epoch: 2 step: 105, loss is 0.386366605758667\n",
      "epoch: 2 step: 106, loss is 0.3509230613708496\n",
      "epoch: 2 step: 107, loss is 0.4710994362831116\n",
      "epoch: 2 step: 108, loss is 0.3539983034133911\n",
      "epoch: 2 step: 109, loss is 0.20012453198432922\n",
      "epoch: 2 step: 110, loss is 0.4096810221672058\n",
      "epoch: 2 step: 111, loss is 0.418600857257843\n",
      "epoch: 2 step: 112, loss is 0.40873482823371887\n",
      "epoch: 2 step: 113, loss is 0.339002400636673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 114, loss is 0.4855712056159973\n",
      "epoch: 2 step: 115, loss is 0.3735593855381012\n",
      "epoch: 2 step: 116, loss is 0.27426958084106445\n",
      "epoch: 2 step: 117, loss is 0.43392783403396606\n",
      "epoch: 2 step: 118, loss is 0.4379482865333557\n",
      "epoch: 2 step: 119, loss is 0.2891845405101776\n",
      "epoch: 2 step: 120, loss is 0.47043997049331665\n",
      "epoch: 2 step: 121, loss is 0.45258012413978577\n",
      "epoch: 2 step: 122, loss is 0.43045008182525635\n",
      "epoch: 2 step: 123, loss is 0.3610682487487793\n",
      "epoch: 2 step: 124, loss is 0.4141820967197418\n",
      "epoch: 2 step: 125, loss is 0.32444021105766296\n",
      "epoch: 2 step: 126, loss is 0.3212859332561493\n",
      "epoch: 2 step: 127, loss is 0.36694639921188354\n",
      "epoch: 2 step: 128, loss is 0.36446449160575867\n",
      "epoch: 2 step: 129, loss is 0.22493284940719604\n",
      "epoch: 2 step: 130, loss is 0.31727465987205505\n",
      "epoch: 2 step: 131, loss is 0.37591007351875305\n",
      "epoch: 2 step: 132, loss is 0.22015275061130524\n",
      "epoch: 2 step: 133, loss is 0.41291674971580505\n",
      "epoch: 2 step: 134, loss is 0.2482990026473999\n",
      "epoch: 2 step: 135, loss is 0.5202272534370422\n",
      "epoch: 2 step: 136, loss is 0.3146260678768158\n",
      "epoch: 2 step: 137, loss is 0.31975290179252625\n",
      "epoch: 2 step: 138, loss is 0.2953316271305084\n",
      "epoch: 2 step: 139, loss is 0.2011839598417282\n",
      "epoch: 2 step: 140, loss is 0.34604594111442566\n",
      "epoch: 2 step: 141, loss is 0.35538896918296814\n",
      "epoch: 2 step: 142, loss is 0.35035455226898193\n",
      "epoch: 2 step: 143, loss is 0.4933762848377228\n",
      "epoch: 2 step: 144, loss is 0.3255240321159363\n",
      "epoch: 2 step: 145, loss is 0.3846535384654999\n",
      "epoch: 2 step: 146, loss is 0.37162086367607117\n",
      "epoch: 2 step: 147, loss is 0.4028138816356659\n",
      "epoch: 2 step: 148, loss is 0.4337014853954315\n",
      "epoch: 2 step: 149, loss is 0.3377077281475067\n",
      "epoch: 2 step: 150, loss is 0.3710586726665497\n",
      "epoch: 2 step: 151, loss is 0.3016276955604553\n",
      "epoch: 2 step: 152, loss is 0.40381309390068054\n",
      "epoch: 2 step: 153, loss is 0.6089954972267151\n",
      "epoch: 2 step: 154, loss is 0.2898033857345581\n",
      "epoch: 2 step: 155, loss is 0.2721517086029053\n",
      "epoch: 2 step: 156, loss is 0.4972575902938843\n",
      "epoch: 2 step: 157, loss is 0.37866905331611633\n",
      "epoch: 2 step: 158, loss is 0.20281937718391418\n",
      "epoch: 2 step: 159, loss is 0.2886784076690674\n",
      "epoch: 2 step: 160, loss is 0.575183093547821\n",
      "epoch: 2 step: 161, loss is 0.3182009160518646\n",
      "epoch: 2 step: 162, loss is 0.2781786322593689\n",
      "epoch: 2 step: 163, loss is 0.39129117131233215\n",
      "epoch: 2 step: 164, loss is 0.41544824838638306\n",
      "epoch: 2 step: 165, loss is 0.29497385025024414\n",
      "epoch: 2 step: 166, loss is 0.255498468875885\n",
      "epoch: 2 step: 167, loss is 0.4558945894241333\n",
      "epoch: 2 step: 168, loss is 0.44819003343582153\n",
      "epoch: 2 step: 169, loss is 0.41488194465637207\n",
      "epoch: 2 step: 170, loss is 0.4695165753364563\n",
      "epoch: 2 step: 171, loss is 0.523346483707428\n",
      "epoch: 2 step: 172, loss is 0.433280885219574\n",
      "epoch: 2 step: 173, loss is 0.4162806570529938\n",
      "epoch: 2 step: 174, loss is 0.43322497606277466\n",
      "epoch: 2 step: 175, loss is 0.4062032103538513\n",
      "epoch: 2 step: 176, loss is 0.504016101360321\n",
      "epoch: 2 step: 177, loss is 0.3232713043689728\n",
      "epoch: 2 step: 178, loss is 0.2844466269016266\n",
      "epoch: 2 step: 179, loss is 0.5246509909629822\n",
      "epoch: 2 step: 180, loss is 0.3891746699810028\n",
      "epoch: 2 step: 181, loss is 0.42554131150245667\n",
      "epoch: 2 step: 182, loss is 0.41111934185028076\n",
      "epoch: 2 step: 183, loss is 0.36187073588371277\n",
      "epoch: 2 step: 184, loss is 0.46618786454200745\n",
      "epoch: 2 step: 185, loss is 0.2689111530780792\n",
      "epoch: 2 step: 186, loss is 0.49366694688796997\n",
      "epoch: 2 step: 187, loss is 0.33615729212760925\n",
      "epoch: 2 step: 188, loss is 0.245886892080307\n",
      "epoch: 2 step: 189, loss is 0.4003308415412903\n",
      "epoch: 2 step: 190, loss is 0.2526471018791199\n",
      "epoch: 2 step: 191, loss is 0.405943363904953\n",
      "epoch: 2 step: 192, loss is 0.35273870825767517\n",
      "epoch: 2 step: 193, loss is 0.3461499810218811\n",
      "epoch: 2 step: 194, loss is 0.41893938183784485\n",
      "epoch: 2 step: 195, loss is 0.40556105971336365\n",
      "epoch: 2 step: 196, loss is 0.5538197159767151\n",
      "epoch: 2 step: 197, loss is 0.21926046907901764\n",
      "epoch: 2 step: 198, loss is 0.4320421516895294\n",
      "epoch: 2 step: 199, loss is 0.4753411114215851\n",
      "epoch: 2 step: 200, loss is 0.4586632549762726\n",
      "epoch: 2 step: 201, loss is 0.6199869513511658\n",
      "epoch: 2 step: 202, loss is 0.3008599281311035\n",
      "epoch: 2 step: 203, loss is 0.31085291504859924\n",
      "epoch: 2 step: 204, loss is 0.40415364503860474\n",
      "epoch: 2 step: 205, loss is 0.37634560465812683\n",
      "epoch: 2 step: 206, loss is 0.42552411556243896\n",
      "epoch: 2 step: 207, loss is 0.2676526606082916\n",
      "epoch: 2 step: 208, loss is 0.3956488370895386\n",
      "epoch: 2 step: 209, loss is 0.299556702375412\n",
      "epoch: 2 step: 210, loss is 0.20781072974205017\n",
      "epoch: 2 step: 211, loss is 0.4569070339202881\n",
      "epoch: 2 step: 212, loss is 0.37551349401474\n",
      "epoch: 2 step: 213, loss is 0.4245193898677826\n",
      "epoch: 2 step: 214, loss is 0.2516559064388275\n",
      "epoch: 2 step: 215, loss is 0.42443326115608215\n",
      "epoch: 2 step: 216, loss is 0.36234310269355774\n",
      "epoch: 2 step: 217, loss is 0.3733491003513336\n",
      "epoch: 2 step: 218, loss is 0.38006842136383057\n",
      "epoch: 2 step: 219, loss is 0.38012245297431946\n",
      "epoch: 2 step: 220, loss is 0.4557236135005951\n",
      "epoch: 2 step: 221, loss is 0.29138419032096863\n",
      "epoch: 2 step: 222, loss is 0.39618077874183655\n",
      "epoch: 2 step: 223, loss is 0.34712299704551697\n",
      "epoch: 2 step: 224, loss is 0.5851194262504578\n",
      "epoch: 2 step: 225, loss is 0.31742480397224426\n",
      "epoch: 2 step: 226, loss is 0.3732084035873413\n",
      "epoch: 2 step: 227, loss is 0.40717458724975586\n",
      "epoch: 2 step: 228, loss is 0.3423658013343811\n",
      "epoch: 2 step: 229, loss is 0.4811001420021057\n",
      "epoch: 2 step: 230, loss is 0.2333032637834549\n",
      "epoch: 2 step: 231, loss is 0.3578891158103943\n",
      "epoch: 2 step: 232, loss is 0.5222479104995728\n",
      "epoch: 2 step: 233, loss is 0.3412967920303345\n",
      "epoch: 2 step: 234, loss is 0.3325697183609009\n",
      "epoch: 2 step: 235, loss is 0.3064606189727783\n",
      "epoch: 2 step: 236, loss is 0.408796101808548\n",
      "epoch: 2 step: 237, loss is 0.3411365747451782\n",
      "epoch: 2 step: 238, loss is 0.45234158635139465\n",
      "epoch: 2 step: 239, loss is 0.5834287405014038\n",
      "epoch: 2 step: 240, loss is 0.4092593193054199\n",
      "epoch: 2 step: 241, loss is 0.4494420886039734\n",
      "epoch: 2 step: 242, loss is 0.33121541142463684\n",
      "epoch: 2 step: 243, loss is 0.3728104829788208\n",
      "epoch: 2 step: 244, loss is 0.4018077552318573\n",
      "epoch: 2 step: 245, loss is 0.3277917802333832\n",
      "epoch: 2 step: 246, loss is 0.39820387959480286\n",
      "epoch: 2 step: 247, loss is 0.31933391094207764\n",
      "epoch: 2 step: 248, loss is 0.43063586950302124\n",
      "epoch: 2 step: 249, loss is 0.36254552006721497\n",
      "epoch: 2 step: 250, loss is 0.30832532048225403\n",
      "epoch: 2 step: 251, loss is 0.4639591574668884\n",
      "epoch: 2 step: 252, loss is 0.34952718019485474\n",
      "epoch: 2 step: 253, loss is 0.3750589191913605\n",
      "epoch: 2 step: 254, loss is 0.2699833810329437\n",
      "epoch: 2 step: 255, loss is 0.38963988423347473\n",
      "epoch: 2 step: 256, loss is 0.29238706827163696\n",
      "epoch: 2 step: 257, loss is 0.446353942155838\n",
      "epoch: 2 step: 258, loss is 0.3321748375892639\n",
      "epoch: 2 step: 259, loss is 0.37074965238571167\n",
      "epoch: 2 step: 260, loss is 0.34039855003356934\n",
      "epoch: 2 step: 261, loss is 0.335780531167984\n",
      "epoch: 2 step: 262, loss is 0.23403169214725494\n",
      "epoch: 2 step: 263, loss is 0.33948951959609985\n",
      "epoch: 2 step: 264, loss is 0.37033453583717346\n",
      "epoch: 2 step: 265, loss is 0.2720792889595032\n",
      "epoch: 2 step: 266, loss is 0.3997405469417572\n",
      "epoch: 2 step: 267, loss is 0.35724198818206787\n",
      "epoch: 2 step: 268, loss is 0.2584361135959625\n",
      "epoch: 2 step: 269, loss is 0.3477829396724701\n",
      "epoch: 2 step: 270, loss is 0.2995584309101105\n",
      "epoch: 2 step: 271, loss is 0.4752539396286011\n",
      "epoch: 2 step: 272, loss is 0.5199589133262634\n",
      "epoch: 2 step: 273, loss is 0.41842183470726013\n",
      "epoch: 2 step: 274, loss is 0.33692681789398193\n",
      "epoch: 2 step: 275, loss is 0.4170324504375458\n",
      "epoch: 2 step: 276, loss is 0.36560913920402527\n",
      "epoch: 2 step: 277, loss is 0.28909048438072205\n",
      "epoch: 2 step: 278, loss is 0.30813780426979065\n",
      "epoch: 2 step: 279, loss is 0.4212310314178467\n",
      "epoch: 2 step: 280, loss is 0.33900031447410583\n",
      "epoch: 2 step: 281, loss is 0.34068071842193604\n",
      "epoch: 2 step: 282, loss is 0.3168397843837738\n",
      "epoch: 2 step: 283, loss is 0.2710581421852112\n",
      "epoch: 2 step: 284, loss is 0.31721240282058716\n",
      "epoch: 2 step: 285, loss is 0.2874315083026886\n",
      "epoch: 2 step: 286, loss is 0.29105067253112793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 287, loss is 0.39873015880584717\n",
      "epoch: 2 step: 288, loss is 0.6546134948730469\n",
      "epoch: 2 step: 289, loss is 0.3979221284389496\n",
      "epoch: 2 step: 290, loss is 0.3588189482688904\n",
      "epoch: 2 step: 291, loss is 0.18578514456748962\n",
      "epoch: 2 step: 292, loss is 0.6867704391479492\n",
      "epoch: 2 step: 293, loss is 0.19628022611141205\n",
      "epoch: 2 step: 294, loss is 0.35377851128578186\n",
      "epoch: 2 step: 295, loss is 0.4318379759788513\n",
      "epoch: 2 step: 296, loss is 0.2560542821884155\n",
      "epoch: 2 step: 297, loss is 0.23827339708805084\n",
      "epoch: 2 step: 298, loss is 0.28362932801246643\n",
      "epoch: 2 step: 299, loss is 0.3649062514305115\n",
      "epoch: 2 step: 300, loss is 0.41305872797966003\n",
      "epoch: 2 step: 301, loss is 0.30514654517173767\n",
      "epoch: 2 step: 302, loss is 0.46334561705589294\n",
      "epoch: 2 step: 303, loss is 0.3956323266029358\n",
      "epoch: 2 step: 304, loss is 0.3549453616142273\n",
      "epoch: 2 step: 305, loss is 0.443012535572052\n",
      "epoch: 2 step: 306, loss is 0.34031936526298523\n",
      "epoch: 2 step: 307, loss is 0.5406596064567566\n",
      "epoch: 2 step: 308, loss is 0.38453078269958496\n",
      "epoch: 2 step: 309, loss is 0.39548206329345703\n",
      "epoch: 2 step: 310, loss is 0.377991259098053\n",
      "epoch: 2 step: 311, loss is 0.3130760192871094\n",
      "epoch: 2 step: 312, loss is 0.33289363980293274\n",
      "epoch: 2 step: 313, loss is 0.2923261523246765\n",
      "epoch: 2 step: 314, loss is 0.27581194043159485\n",
      "epoch: 2 step: 315, loss is 0.3154090642929077\n",
      "epoch: 2 step: 316, loss is 0.326300710439682\n",
      "epoch: 2 step: 317, loss is 0.2736205756664276\n",
      "epoch: 2 step: 318, loss is 0.22701680660247803\n",
      "epoch: 2 step: 319, loss is 0.3285136818885803\n",
      "epoch: 2 step: 320, loss is 0.6648849844932556\n",
      "epoch: 2 step: 321, loss is 0.2973859906196594\n",
      "epoch: 2 step: 322, loss is 0.30265024304389954\n",
      "epoch: 2 step: 323, loss is 0.3380454480648041\n",
      "epoch: 2 step: 324, loss is 0.31537166237831116\n",
      "epoch: 2 step: 325, loss is 0.3080863356590271\n",
      "epoch: 2 step: 326, loss is 0.47382888197898865\n",
      "epoch: 2 step: 327, loss is 0.31900063157081604\n",
      "epoch: 2 step: 328, loss is 0.32247161865234375\n",
      "epoch: 2 step: 329, loss is 0.4467294216156006\n",
      "epoch: 2 step: 330, loss is 0.4383312463760376\n",
      "epoch: 2 step: 331, loss is 0.5300071239471436\n",
      "epoch: 2 step: 332, loss is 0.3458706736564636\n",
      "epoch: 2 step: 333, loss is 0.46557825803756714\n",
      "epoch: 2 step: 334, loss is 0.3246280550956726\n",
      "epoch: 2 step: 335, loss is 0.42599356174468994\n",
      "epoch: 2 step: 336, loss is 0.40881797671318054\n",
      "epoch: 2 step: 337, loss is 0.4210316240787506\n",
      "epoch: 2 step: 338, loss is 0.25529900193214417\n",
      "epoch: 2 step: 339, loss is 0.3125627338886261\n",
      "epoch: 2 step: 340, loss is 0.2795718014240265\n",
      "epoch: 2 step: 341, loss is 0.3958759009838104\n",
      "epoch: 2 step: 342, loss is 0.2985295057296753\n",
      "epoch: 2 step: 343, loss is 0.3478510081768036\n",
      "epoch: 2 step: 344, loss is 0.43694767355918884\n",
      "epoch: 2 step: 345, loss is 0.350959450006485\n",
      "epoch: 2 step: 346, loss is 0.3277524411678314\n",
      "epoch: 2 step: 347, loss is 0.4070032835006714\n",
      "epoch: 2 step: 348, loss is 0.30912792682647705\n",
      "epoch: 2 step: 349, loss is 0.445597380399704\n",
      "epoch: 2 step: 350, loss is 0.32585200667381287\n",
      "epoch: 2 step: 351, loss is 0.5971397757530212\n",
      "epoch: 2 step: 352, loss is 0.2794838845729828\n",
      "epoch: 2 step: 353, loss is 0.49535703659057617\n",
      "epoch: 2 step: 354, loss is 0.41176649928092957\n",
      "epoch: 2 step: 355, loss is 0.4172365963459015\n",
      "epoch: 2 step: 356, loss is 0.3375771939754486\n",
      "epoch: 2 step: 357, loss is 0.43047431111335754\n",
      "epoch: 2 step: 358, loss is 0.38806936144828796\n",
      "epoch: 2 step: 359, loss is 0.4218537211418152\n",
      "epoch: 2 step: 360, loss is 0.36395642161369324\n",
      "epoch: 2 step: 361, loss is 0.22350606322288513\n",
      "epoch: 2 step: 362, loss is 0.33419501781463623\n",
      "epoch: 2 step: 363, loss is 0.3835621178150177\n",
      "epoch: 2 step: 364, loss is 0.29984050989151\n",
      "epoch: 2 step: 365, loss is 0.3774992525577545\n",
      "epoch: 2 step: 366, loss is 0.4384589195251465\n",
      "epoch: 2 step: 367, loss is 0.2509544789791107\n",
      "epoch: 2 step: 368, loss is 0.31182312965393066\n",
      "epoch: 2 step: 369, loss is 0.32907000184059143\n",
      "epoch: 2 step: 370, loss is 0.3579135835170746\n",
      "epoch: 2 step: 371, loss is 0.34301549196243286\n",
      "epoch: 2 step: 372, loss is 0.3041805624961853\n",
      "epoch: 2 step: 373, loss is 0.36711859703063965\n",
      "epoch: 2 step: 374, loss is 0.2679658830165863\n",
      "epoch: 2 step: 375, loss is 0.33782270550727844\n",
      "epoch: 2 step: 376, loss is 0.33406275510787964\n",
      "epoch: 2 step: 377, loss is 0.4402228891849518\n",
      "epoch: 2 step: 378, loss is 0.33156076073646545\n",
      "epoch: 2 step: 379, loss is 0.39139196276664734\n",
      "epoch: 2 step: 380, loss is 0.29987359046936035\n",
      "epoch: 2 step: 381, loss is 0.34028270840644836\n",
      "epoch: 2 step: 382, loss is 0.13605424761772156\n",
      "epoch: 2 step: 383, loss is 0.3328142762184143\n",
      "epoch: 2 step: 384, loss is 0.3060600757598877\n",
      "epoch: 2 step: 385, loss is 0.49000459909439087\n",
      "epoch: 2 step: 386, loss is 0.3391715884208679\n",
      "epoch: 2 step: 387, loss is 0.34615159034729004\n",
      "epoch: 2 step: 388, loss is 0.44328662753105164\n",
      "epoch: 2 step: 389, loss is 0.5282737016677856\n",
      "epoch: 2 step: 390, loss is 0.4569462239742279\n",
      "epoch: 2 step: 391, loss is 0.3598788380622864\n",
      "epoch: 2 step: 392, loss is 0.341842383146286\n",
      "epoch: 2 step: 393, loss is 0.45463457703590393\n",
      "epoch: 2 step: 394, loss is 0.22701600193977356\n",
      "epoch: 2 step: 395, loss is 0.43623146414756775\n",
      "epoch: 2 step: 396, loss is 0.32189181447029114\n",
      "epoch: 2 step: 397, loss is 0.32060322165489197\n",
      "epoch: 2 step: 398, loss is 0.36968061327934265\n",
      "epoch: 2 step: 399, loss is 0.2643195390701294\n",
      "epoch: 2 step: 400, loss is 0.5132521390914917\n",
      "epoch: 2 step: 401, loss is 0.3229096829891205\n",
      "epoch: 2 step: 402, loss is 0.2902010381221771\n",
      "epoch: 2 step: 403, loss is 0.38601958751678467\n",
      "epoch: 2 step: 404, loss is 0.35595011711120605\n",
      "epoch: 2 step: 405, loss is 0.22540006041526794\n",
      "epoch: 2 step: 406, loss is 0.2247510403394699\n",
      "epoch: 2 step: 407, loss is 0.4401865601539612\n",
      "epoch: 2 step: 408, loss is 0.45887041091918945\n",
      "epoch: 2 step: 409, loss is 0.4749470055103302\n",
      "epoch: 2 step: 410, loss is 0.2939346730709076\n",
      "epoch: 2 step: 411, loss is 0.4954914152622223\n",
      "epoch: 2 step: 412, loss is 0.3919677138328552\n",
      "epoch: 2 step: 413, loss is 0.23439741134643555\n",
      "epoch: 2 step: 414, loss is 0.25162234902381897\n",
      "epoch: 2 step: 415, loss is 0.299185574054718\n",
      "epoch: 2 step: 416, loss is 0.30112266540527344\n",
      "epoch: 2 step: 417, loss is 0.44695785641670227\n",
      "epoch: 2 step: 418, loss is 0.31114038825035095\n",
      "epoch: 2 step: 419, loss is 0.3612956404685974\n",
      "epoch: 2 step: 420, loss is 0.39368364214897156\n",
      "epoch: 2 step: 421, loss is 0.32245001196861267\n",
      "epoch: 2 step: 422, loss is 0.33580881357192993\n",
      "epoch: 2 step: 423, loss is 0.4977652430534363\n",
      "epoch: 2 step: 424, loss is 0.4365760087966919\n",
      "epoch: 2 step: 425, loss is 0.39547738432884216\n",
      "epoch: 2 step: 426, loss is 0.30026984214782715\n",
      "epoch: 2 step: 427, loss is 0.2486349493265152\n",
      "epoch: 2 step: 428, loss is 0.4130632281303406\n",
      "epoch: 2 step: 429, loss is 0.3607199490070343\n",
      "epoch: 2 step: 430, loss is 0.3669167459011078\n",
      "epoch: 2 step: 431, loss is 0.5084876418113708\n",
      "epoch: 2 step: 432, loss is 0.4447647035121918\n",
      "epoch: 2 step: 433, loss is 0.30145981907844543\n",
      "epoch: 2 step: 434, loss is 0.7593271732330322\n",
      "epoch: 2 step: 435, loss is 0.4498327374458313\n",
      "epoch: 2 step: 436, loss is 0.46006879210472107\n",
      "epoch: 2 step: 437, loss is 0.3975432813167572\n",
      "epoch: 2 step: 438, loss is 0.605697751045227\n",
      "epoch: 2 step: 439, loss is 0.4066898226737976\n",
      "epoch: 2 step: 440, loss is 0.30846279859542847\n",
      "epoch: 2 step: 441, loss is 0.35744708776474\n",
      "epoch: 2 step: 442, loss is 0.30769082903862\n",
      "epoch: 2 step: 443, loss is 0.22929975390434265\n",
      "epoch: 2 step: 444, loss is 0.4297981858253479\n",
      "epoch: 2 step: 445, loss is 0.2999836206436157\n",
      "epoch: 2 step: 446, loss is 0.2994377315044403\n",
      "epoch: 2 step: 447, loss is 0.4227723479270935\n",
      "epoch: 2 step: 448, loss is 0.2513309121131897\n",
      "epoch: 2 step: 449, loss is 0.20244625210762024\n",
      "epoch: 2 step: 450, loss is 0.3523811399936676\n",
      "epoch: 2 step: 451, loss is 0.5055907964706421\n",
      "epoch: 2 step: 452, loss is 0.4163433909416199\n",
      "epoch: 2 step: 453, loss is 0.3421556353569031\n",
      "epoch: 2 step: 454, loss is 0.4739278554916382\n",
      "epoch: 2 step: 455, loss is 0.3169686496257782\n",
      "epoch: 2 step: 456, loss is 0.42406192421913147\n",
      "epoch: 2 step: 457, loss is 0.18732336163520813\n",
      "epoch: 2 step: 458, loss is 0.32483184337615967\n",
      "epoch: 2 step: 459, loss is 0.3485182821750641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 460, loss is 0.39095401763916016\n",
      "epoch: 2 step: 461, loss is 0.32313084602355957\n",
      "epoch: 2 step: 462, loss is 0.23160547018051147\n",
      "epoch: 2 step: 463, loss is 0.3135536313056946\n",
      "epoch: 2 step: 464, loss is 0.376579225063324\n",
      "epoch: 2 step: 465, loss is 0.34495723247528076\n",
      "epoch: 2 step: 466, loss is 0.4704046845436096\n",
      "epoch: 2 step: 467, loss is 0.22831326723098755\n",
      "epoch: 2 step: 468, loss is 0.3752172291278839\n",
      "epoch: 2 step: 469, loss is 0.244810089468956\n",
      "epoch: 2 step: 470, loss is 0.4417979121208191\n",
      "epoch: 2 step: 471, loss is 0.5367746949195862\n",
      "epoch: 2 step: 472, loss is 0.38505277037620544\n",
      "epoch: 2 step: 473, loss is 0.3224544823169708\n",
      "epoch: 2 step: 474, loss is 0.3311709463596344\n",
      "epoch: 2 step: 475, loss is 0.2666621506214142\n",
      "epoch: 2 step: 476, loss is 0.38221046328544617\n",
      "epoch: 2 step: 477, loss is 0.6716350317001343\n",
      "epoch: 2 step: 478, loss is 0.24465809762477875\n",
      "epoch: 2 step: 479, loss is 0.5099154710769653\n",
      "epoch: 2 step: 480, loss is 0.2347104251384735\n",
      "epoch: 2 step: 481, loss is 0.38229215145111084\n",
      "epoch: 2 step: 482, loss is 0.5060926675796509\n",
      "epoch: 2 step: 483, loss is 0.4180642366409302\n",
      "epoch: 2 step: 484, loss is 0.2290463000535965\n",
      "epoch: 2 step: 485, loss is 0.30734536051750183\n",
      "epoch: 2 step: 486, loss is 0.5241948962211609\n",
      "epoch: 2 step: 487, loss is 0.4389655888080597\n",
      "epoch: 2 step: 488, loss is 0.2833418846130371\n",
      "epoch: 2 step: 489, loss is 0.31841835379600525\n",
      "epoch: 2 step: 490, loss is 0.5152799487113953\n",
      "epoch: 2 step: 491, loss is 0.49160879850387573\n",
      "epoch: 2 step: 492, loss is 0.37813106179237366\n",
      "epoch: 2 step: 493, loss is 0.2678532004356384\n",
      "epoch: 2 step: 494, loss is 0.39809393882751465\n",
      "epoch: 2 step: 495, loss is 0.29247021675109863\n",
      "epoch: 2 step: 496, loss is 0.3033551275730133\n",
      "epoch: 2 step: 497, loss is 0.3346693515777588\n",
      "epoch: 2 step: 498, loss is 0.3756667673587799\n",
      "epoch: 2 step: 499, loss is 0.2693922817707062\n",
      "epoch: 2 step: 500, loss is 0.3217940628528595\n",
      "epoch: 2 step: 501, loss is 0.2556145489215851\n",
      "epoch: 2 step: 502, loss is 0.2184901386499405\n",
      "epoch: 2 step: 503, loss is 0.5073958039283752\n",
      "epoch: 2 step: 504, loss is 0.21524561941623688\n",
      "epoch: 2 step: 505, loss is 0.39592862129211426\n",
      "epoch: 2 step: 506, loss is 0.29351338744163513\n",
      "epoch: 2 step: 507, loss is 0.3588041067123413\n",
      "epoch: 2 step: 508, loss is 0.3777005970478058\n",
      "epoch: 2 step: 509, loss is 0.26841047406196594\n",
      "epoch: 2 step: 510, loss is 0.2726197838783264\n",
      "epoch: 2 step: 511, loss is 0.4139273762702942\n",
      "epoch: 2 step: 512, loss is 0.4791180491447449\n",
      "epoch: 2 step: 513, loss is 0.22572669386863708\n",
      "epoch: 2 step: 514, loss is 0.3945052921772003\n",
      "epoch: 2 step: 515, loss is 0.43171146512031555\n",
      "epoch: 2 step: 516, loss is 0.3551442325115204\n",
      "epoch: 2 step: 517, loss is 0.3694321811199188\n",
      "epoch: 2 step: 518, loss is 0.36842867732048035\n",
      "epoch: 2 step: 519, loss is 0.47564807534217834\n",
      "epoch: 2 step: 520, loss is 0.4591928720474243\n",
      "epoch: 2 step: 521, loss is 0.40059953927993774\n",
      "epoch: 2 step: 522, loss is 0.5611380934715271\n",
      "epoch: 2 step: 523, loss is 0.4434160590171814\n",
      "epoch: 2 step: 524, loss is 0.2308485060930252\n",
      "epoch: 2 step: 525, loss is 0.39612534642219543\n",
      "epoch: 2 step: 526, loss is 0.41558143496513367\n",
      "epoch: 2 step: 527, loss is 0.26053887605667114\n",
      "epoch: 2 step: 528, loss is 0.30678150057792664\n",
      "epoch: 2 step: 529, loss is 0.32750508189201355\n",
      "epoch: 2 step: 530, loss is 0.27209052443504333\n",
      "epoch: 2 step: 531, loss is 0.3901168406009674\n",
      "epoch: 2 step: 532, loss is 0.5091738700866699\n",
      "epoch: 2 step: 533, loss is 0.3708871006965637\n",
      "epoch: 2 step: 534, loss is 0.27773815393447876\n",
      "epoch: 2 step: 535, loss is 0.4461781680583954\n",
      "epoch: 2 step: 536, loss is 0.2894906997680664\n",
      "epoch: 2 step: 537, loss is 0.299198716878891\n",
      "epoch: 2 step: 538, loss is 0.35790568590164185\n",
      "epoch: 2 step: 539, loss is 0.49421972036361694\n",
      "epoch: 2 step: 540, loss is 0.20253229141235352\n",
      "epoch: 2 step: 541, loss is 0.33049890398979187\n",
      "epoch: 2 step: 542, loss is 0.2608144283294678\n",
      "epoch: 2 step: 543, loss is 0.35221779346466064\n",
      "epoch: 2 step: 544, loss is 0.2593865692615509\n",
      "epoch: 2 step: 545, loss is 0.4966345727443695\n",
      "epoch: 2 step: 546, loss is 0.26143330335617065\n",
      "epoch: 2 step: 547, loss is 0.4125434160232544\n",
      "epoch: 2 step: 548, loss is 0.36745959520339966\n",
      "epoch: 2 step: 549, loss is 0.3908248841762543\n",
      "epoch: 2 step: 550, loss is 0.34718984365463257\n",
      "epoch: 2 step: 551, loss is 0.3675682544708252\n",
      "epoch: 2 step: 552, loss is 0.202856183052063\n",
      "epoch: 2 step: 553, loss is 0.18434327840805054\n",
      "epoch: 2 step: 554, loss is 0.24526625871658325\n",
      "epoch: 2 step: 555, loss is 0.4028068780899048\n",
      "epoch: 2 step: 556, loss is 0.4100262224674225\n",
      "epoch: 2 step: 557, loss is 0.37385040521621704\n",
      "epoch: 2 step: 558, loss is 0.3338702917098999\n",
      "epoch: 2 step: 559, loss is 0.33400216698646545\n",
      "epoch: 2 step: 560, loss is 0.387875497341156\n",
      "epoch: 2 step: 561, loss is 0.2532615065574646\n",
      "epoch: 2 step: 562, loss is 0.27181753516197205\n",
      "epoch: 2 step: 563, loss is 0.3612078130245209\n",
      "epoch: 2 step: 564, loss is 0.3123830556869507\n",
      "epoch: 2 step: 565, loss is 0.29334068298339844\n",
      "epoch: 2 step: 566, loss is 0.240248903632164\n",
      "epoch: 2 step: 567, loss is 0.4136597514152527\n",
      "epoch: 2 step: 568, loss is 0.3047064244747162\n",
      "epoch: 2 step: 569, loss is 0.33865979313850403\n",
      "epoch: 2 step: 570, loss is 0.4476250410079956\n",
      "epoch: 2 step: 571, loss is 0.3528623878955841\n",
      "epoch: 2 step: 572, loss is 0.2906149923801422\n",
      "epoch: 2 step: 573, loss is 0.26563867926597595\n",
      "epoch: 2 step: 574, loss is 0.3032478094100952\n",
      "epoch: 2 step: 575, loss is 0.26380759477615356\n",
      "epoch: 2 step: 576, loss is 0.2417563945055008\n",
      "epoch: 2 step: 577, loss is 0.29916325211524963\n",
      "epoch: 2 step: 578, loss is 0.5443482398986816\n",
      "epoch: 2 step: 579, loss is 0.2585254907608032\n",
      "epoch: 2 step: 580, loss is 0.3670019507408142\n",
      "epoch: 2 step: 581, loss is 0.15310262143611908\n",
      "epoch: 2 step: 582, loss is 0.4698152244091034\n",
      "epoch: 2 step: 583, loss is 0.22840569913387299\n",
      "epoch: 2 step: 584, loss is 0.34418484568595886\n",
      "epoch: 2 step: 585, loss is 0.37977126240730286\n",
      "epoch: 2 step: 586, loss is 0.33467531204223633\n",
      "epoch: 2 step: 587, loss is 0.6419784426689148\n",
      "epoch: 2 step: 588, loss is 0.2944103181362152\n",
      "epoch: 2 step: 589, loss is 0.36649441719055176\n",
      "epoch: 2 step: 590, loss is 0.27211830019950867\n",
      "epoch: 2 step: 591, loss is 0.37058696150779724\n",
      "epoch: 2 step: 592, loss is 0.41620510816574097\n",
      "epoch: 2 step: 593, loss is 0.36461618542671204\n",
      "epoch: 2 step: 594, loss is 0.30191221833229065\n",
      "epoch: 2 step: 595, loss is 0.3871241807937622\n",
      "epoch: 2 step: 596, loss is 0.32982975244522095\n",
      "epoch: 2 step: 597, loss is 0.3428695499897003\n",
      "epoch: 2 step: 598, loss is 0.33724695444107056\n",
      "epoch: 2 step: 599, loss is 0.4567300081253052\n",
      "epoch: 2 step: 600, loss is 0.2807520627975464\n",
      "epoch: 2 step: 601, loss is 0.2068946659564972\n",
      "epoch: 2 step: 602, loss is 0.2674616575241089\n",
      "epoch: 2 step: 603, loss is 0.5329364538192749\n",
      "epoch: 2 step: 604, loss is 0.27765634655952454\n",
      "epoch: 2 step: 605, loss is 0.3112970292568207\n",
      "epoch: 2 step: 606, loss is 0.4560478925704956\n",
      "epoch: 2 step: 607, loss is 0.6179161667823792\n",
      "epoch: 2 step: 608, loss is 0.33022812008857727\n",
      "epoch: 2 step: 609, loss is 0.2263757586479187\n",
      "epoch: 2 step: 610, loss is 0.25014954805374146\n",
      "epoch: 2 step: 611, loss is 0.4485110640525818\n",
      "epoch: 2 step: 612, loss is 0.30469146370887756\n",
      "epoch: 2 step: 613, loss is 0.36297404766082764\n",
      "epoch: 2 step: 614, loss is 0.3658401370048523\n",
      "epoch: 2 step: 615, loss is 0.15500327944755554\n",
      "epoch: 2 step: 616, loss is 0.31262606382369995\n",
      "epoch: 2 step: 617, loss is 0.277919739484787\n",
      "epoch: 2 step: 618, loss is 0.2955760061740875\n",
      "epoch: 2 step: 619, loss is 0.3232421875\n",
      "epoch: 2 step: 620, loss is 0.2986183762550354\n",
      "epoch: 2 step: 621, loss is 0.2682780921459198\n",
      "epoch: 2 step: 622, loss is 0.3761995732784271\n",
      "epoch: 2 step: 623, loss is 0.3421863317489624\n",
      "epoch: 2 step: 624, loss is 0.34575751423835754\n",
      "epoch: 2 step: 625, loss is 0.30058059096336365\n",
      "epoch: 2 step: 626, loss is 0.41530516743659973\n",
      "epoch: 2 step: 627, loss is 0.3760784864425659\n",
      "epoch: 2 step: 628, loss is 0.35776957869529724\n",
      "epoch: 2 step: 629, loss is 0.3806147575378418\n",
      "epoch: 2 step: 630, loss is 0.3121401369571686\n",
      "epoch: 2 step: 631, loss is 0.5537055134773254\n",
      "epoch: 2 step: 632, loss is 0.30063921213150024\n",
      "epoch: 2 step: 633, loss is 0.24045094847679138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 634, loss is 0.2599236071109772\n",
      "epoch: 2 step: 635, loss is 0.27541807293891907\n",
      "epoch: 2 step: 636, loss is 0.2769947350025177\n",
      "epoch: 2 step: 637, loss is 0.3560505509376526\n",
      "epoch: 2 step: 638, loss is 0.2683679759502411\n",
      "epoch: 2 step: 639, loss is 0.2580550014972687\n",
      "epoch: 2 step: 640, loss is 0.343746542930603\n",
      "epoch: 2 step: 641, loss is 0.45247310400009155\n",
      "epoch: 2 step: 642, loss is 0.38232100009918213\n",
      "epoch: 2 step: 643, loss is 0.2927717864513397\n",
      "epoch: 2 step: 644, loss is 0.37552785873413086\n",
      "epoch: 2 step: 645, loss is 0.44048938155174255\n",
      "epoch: 2 step: 646, loss is 0.35825932025909424\n",
      "epoch: 2 step: 647, loss is 0.2862756848335266\n",
      "epoch: 2 step: 648, loss is 0.33495721220970154\n",
      "epoch: 2 step: 649, loss is 0.5383672118186951\n",
      "epoch: 2 step: 650, loss is 0.4304417371749878\n",
      "epoch: 2 step: 651, loss is 0.42057859897613525\n",
      "epoch: 2 step: 652, loss is 0.21897529065608978\n",
      "epoch: 2 step: 653, loss is 0.3935851752758026\n",
      "epoch: 2 step: 654, loss is 0.2767520844936371\n",
      "epoch: 2 step: 655, loss is 0.4153442680835724\n",
      "epoch: 2 step: 656, loss is 0.5217021703720093\n",
      "epoch: 2 step: 657, loss is 0.6690424680709839\n",
      "epoch: 2 step: 658, loss is 0.58918696641922\n",
      "epoch: 2 step: 659, loss is 0.3154105544090271\n",
      "epoch: 2 step: 660, loss is 0.36268872022628784\n",
      "epoch: 2 step: 661, loss is 0.42712903022766113\n",
      "epoch: 2 step: 662, loss is 0.318787157535553\n",
      "epoch: 2 step: 663, loss is 0.22009913623332977\n",
      "epoch: 2 step: 664, loss is 0.3892884850502014\n",
      "epoch: 2 step: 665, loss is 0.5142161250114441\n",
      "epoch: 2 step: 666, loss is 0.3464597463607788\n",
      "epoch: 2 step: 667, loss is 0.39390885829925537\n",
      "epoch: 2 step: 668, loss is 0.33660581707954407\n",
      "epoch: 2 step: 669, loss is 0.42202648520469666\n",
      "epoch: 2 step: 670, loss is 0.46579092741012573\n",
      "epoch: 2 step: 671, loss is 0.3050468862056732\n",
      "epoch: 2 step: 672, loss is 0.19070574641227722\n",
      "epoch: 2 step: 673, loss is 0.3397064208984375\n",
      "epoch: 2 step: 674, loss is 0.45442238450050354\n",
      "epoch: 2 step: 675, loss is 0.3423519730567932\n",
      "epoch: 2 step: 676, loss is 0.46322035789489746\n",
      "epoch: 2 step: 677, loss is 0.3577072322368622\n",
      "epoch: 2 step: 678, loss is 0.3100747764110565\n",
      "epoch: 2 step: 679, loss is 0.27252498269081116\n",
      "epoch: 2 step: 680, loss is 0.39462971687316895\n",
      "epoch: 2 step: 681, loss is 0.545604407787323\n",
      "epoch: 2 step: 682, loss is 0.2717190682888031\n",
      "epoch: 2 step: 683, loss is 0.30167675018310547\n",
      "epoch: 2 step: 684, loss is 0.33833572268486023\n",
      "epoch: 2 step: 685, loss is 0.22580689191818237\n",
      "epoch: 2 step: 686, loss is 0.22755493223667145\n",
      "epoch: 2 step: 687, loss is 0.23515872657299042\n",
      "epoch: 2 step: 688, loss is 0.2635270059108734\n",
      "epoch: 2 step: 689, loss is 0.4281669855117798\n",
      "epoch: 2 step: 690, loss is 0.3885952830314636\n",
      "epoch: 2 step: 691, loss is 0.3129521906375885\n",
      "epoch: 2 step: 692, loss is 0.4419857859611511\n",
      "epoch: 2 step: 693, loss is 0.41717931628227234\n",
      "epoch: 2 step: 694, loss is 0.29142245650291443\n",
      "epoch: 2 step: 695, loss is 0.3746286630630493\n",
      "epoch: 2 step: 696, loss is 0.2706819176673889\n",
      "epoch: 2 step: 697, loss is 0.2724077105522156\n",
      "epoch: 2 step: 698, loss is 0.32088881731033325\n",
      "epoch: 2 step: 699, loss is 0.3677116334438324\n",
      "epoch: 2 step: 700, loss is 0.43205398321151733\n",
      "epoch: 2 step: 701, loss is 0.29616662859916687\n",
      "epoch: 2 step: 702, loss is 0.3202303349971771\n",
      "epoch: 2 step: 703, loss is 0.4079011082649231\n",
      "epoch: 2 step: 704, loss is 0.20711912214756012\n",
      "epoch: 2 step: 705, loss is 0.2811342477798462\n",
      "epoch: 2 step: 706, loss is 0.27997973561286926\n",
      "epoch: 2 step: 707, loss is 0.3284018039703369\n",
      "epoch: 2 step: 708, loss is 0.38968557119369507\n",
      "epoch: 2 step: 709, loss is 0.3555436134338379\n",
      "epoch: 2 step: 710, loss is 0.4860363006591797\n",
      "epoch: 2 step: 711, loss is 0.3311796486377716\n",
      "epoch: 2 step: 712, loss is 0.4160107374191284\n",
      "epoch: 2 step: 713, loss is 0.31070348620414734\n",
      "epoch: 2 step: 714, loss is 0.34954655170440674\n",
      "epoch: 2 step: 715, loss is 0.37552863359451294\n",
      "epoch: 2 step: 716, loss is 0.31403857469558716\n",
      "epoch: 2 step: 717, loss is 0.38394278287887573\n",
      "epoch: 2 step: 718, loss is 0.3676663041114807\n",
      "epoch: 2 step: 719, loss is 0.33600348234176636\n",
      "epoch: 2 step: 720, loss is 0.41175878047943115\n",
      "epoch: 2 step: 721, loss is 0.3860069811344147\n",
      "epoch: 2 step: 722, loss is 0.3148207366466522\n",
      "epoch: 2 step: 723, loss is 0.1676645278930664\n",
      "epoch: 2 step: 724, loss is 0.35865160822868347\n",
      "epoch: 2 step: 725, loss is 0.2734438478946686\n",
      "epoch: 2 step: 726, loss is 0.2851233184337616\n",
      "epoch: 2 step: 727, loss is 0.4195072054862976\n",
      "epoch: 2 step: 728, loss is 0.3352423906326294\n",
      "epoch: 2 step: 729, loss is 0.3165099620819092\n",
      "epoch: 2 step: 730, loss is 0.2464059740304947\n",
      "epoch: 2 step: 731, loss is 0.37577980756759644\n",
      "epoch: 2 step: 732, loss is 0.26119643449783325\n",
      "epoch: 2 step: 733, loss is 0.25702905654907227\n",
      "epoch: 2 step: 734, loss is 0.40373915433883667\n",
      "epoch: 2 step: 735, loss is 0.4705776870250702\n",
      "epoch: 2 step: 736, loss is 0.3464866876602173\n",
      "epoch: 2 step: 737, loss is 0.3202253580093384\n",
      "epoch: 2 step: 738, loss is 0.33663731813430786\n",
      "epoch: 2 step: 739, loss is 0.2774536907672882\n",
      "epoch: 2 step: 740, loss is 0.5293791890144348\n",
      "epoch: 2 step: 741, loss is 0.4559890329837799\n",
      "epoch: 2 step: 742, loss is 0.22021779417991638\n",
      "epoch: 2 step: 743, loss is 0.37385499477386475\n",
      "epoch: 2 step: 744, loss is 0.5599634647369385\n",
      "epoch: 2 step: 745, loss is 0.37627407908439636\n",
      "epoch: 2 step: 746, loss is 0.5221673846244812\n",
      "epoch: 2 step: 747, loss is 0.4868801236152649\n",
      "epoch: 2 step: 748, loss is 0.47754424810409546\n",
      "epoch: 2 step: 749, loss is 0.32985225319862366\n",
      "epoch: 2 step: 750, loss is 0.4536757171154022\n",
      "epoch: 2 step: 751, loss is 0.20694714784622192\n",
      "epoch: 2 step: 752, loss is 0.37719014286994934\n",
      "epoch: 2 step: 753, loss is 0.3234765827655792\n",
      "epoch: 2 step: 754, loss is 0.2950597405433655\n",
      "epoch: 2 step: 755, loss is 0.4114188849925995\n",
      "epoch: 2 step: 756, loss is 0.3546501696109772\n",
      "epoch: 2 step: 757, loss is 0.3325095474720001\n",
      "epoch: 2 step: 758, loss is 0.2861716151237488\n",
      "epoch: 2 step: 759, loss is 0.2664942741394043\n",
      "epoch: 2 step: 760, loss is 0.4450685679912567\n",
      "epoch: 2 step: 761, loss is 0.4465571939945221\n",
      "epoch: 2 step: 762, loss is 0.43479612469673157\n",
      "epoch: 2 step: 763, loss is 0.3396153151988983\n",
      "epoch: 2 step: 764, loss is 0.3483724594116211\n",
      "epoch: 2 step: 765, loss is 0.4288320541381836\n",
      "epoch: 2 step: 766, loss is 0.3496803641319275\n",
      "epoch: 2 step: 767, loss is 0.4734151363372803\n",
      "epoch: 2 step: 768, loss is 0.36557209491729736\n",
      "epoch: 2 step: 769, loss is 0.29885947704315186\n",
      "epoch: 2 step: 770, loss is 0.46517348289489746\n",
      "epoch: 2 step: 771, loss is 0.33664557337760925\n",
      "epoch: 2 step: 772, loss is 0.4989885985851288\n",
      "epoch: 2 step: 773, loss is 0.29357948899269104\n",
      "epoch: 2 step: 774, loss is 0.3198583126068115\n",
      "epoch: 2 step: 775, loss is 0.31329795718193054\n",
      "epoch: 2 step: 776, loss is 0.3554018437862396\n",
      "epoch: 2 step: 777, loss is 0.2999117970466614\n",
      "epoch: 2 step: 778, loss is 0.4082389175891876\n",
      "epoch: 2 step: 779, loss is 0.2768820524215698\n",
      "epoch: 2 step: 780, loss is 0.5150393843650818\n",
      "epoch: 2 step: 781, loss is 0.31114041805267334\n",
      "epoch: 2 step: 782, loss is 0.3226691782474518\n",
      "epoch: 2 step: 783, loss is 0.4962576925754547\n",
      "epoch: 2 step: 784, loss is 0.27572980523109436\n",
      "epoch: 2 step: 785, loss is 0.4455815851688385\n",
      "epoch: 2 step: 786, loss is 0.5437387228012085\n",
      "epoch: 2 step: 787, loss is 0.2945384383201599\n",
      "epoch: 2 step: 788, loss is 0.27047228813171387\n",
      "epoch: 2 step: 789, loss is 0.34386903047561646\n",
      "epoch: 2 step: 790, loss is 0.42318111658096313\n",
      "epoch: 2 step: 791, loss is 0.29668375849723816\n",
      "epoch: 2 step: 792, loss is 0.26105114817619324\n",
      "epoch: 2 step: 793, loss is 0.3274911344051361\n",
      "epoch: 2 step: 794, loss is 0.3163382411003113\n",
      "epoch: 2 step: 795, loss is 0.43165525794029236\n",
      "epoch: 2 step: 796, loss is 0.38240760564804077\n",
      "epoch: 2 step: 797, loss is 0.38361451029777527\n",
      "epoch: 2 step: 798, loss is 0.2723604142665863\n",
      "epoch: 2 step: 799, loss is 0.46350499987602234\n",
      "epoch: 2 step: 800, loss is 0.19398824870586395\n",
      "epoch: 2 step: 801, loss is 0.33400392532348633\n",
      "epoch: 2 step: 802, loss is 0.2654561996459961\n",
      "epoch: 2 step: 803, loss is 0.29978278279304504\n",
      "epoch: 2 step: 804, loss is 0.3633749485015869\n",
      "epoch: 2 step: 805, loss is 0.3309208154678345\n",
      "epoch: 2 step: 806, loss is 0.25386330485343933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 807, loss is 0.314948707818985\n",
      "epoch: 2 step: 808, loss is 0.5265882611274719\n",
      "epoch: 2 step: 809, loss is 0.32384514808654785\n",
      "epoch: 2 step: 810, loss is 0.3685707747936249\n",
      "epoch: 2 step: 811, loss is 0.28134414553642273\n",
      "epoch: 2 step: 812, loss is 0.24806714057922363\n",
      "epoch: 2 step: 813, loss is 0.23192676901817322\n",
      "epoch: 2 step: 814, loss is 0.2624775171279907\n",
      "epoch: 2 step: 815, loss is 0.25942477583885193\n",
      "epoch: 2 step: 816, loss is 0.34383389353752136\n",
      "epoch: 2 step: 817, loss is 0.3982951045036316\n",
      "epoch: 2 step: 818, loss is 0.19359628856182098\n",
      "epoch: 2 step: 819, loss is 0.3131364583969116\n",
      "epoch: 2 step: 820, loss is 0.2488168627023697\n",
      "epoch: 2 step: 821, loss is 0.1975231170654297\n",
      "epoch: 2 step: 822, loss is 0.39673107862472534\n",
      "epoch: 2 step: 823, loss is 0.6304240226745605\n",
      "epoch: 2 step: 824, loss is 0.34198012948036194\n",
      "epoch: 2 step: 825, loss is 0.33650317788124084\n",
      "epoch: 2 step: 826, loss is 0.4938197731971741\n",
      "epoch: 2 step: 827, loss is 0.2900207042694092\n",
      "epoch: 2 step: 828, loss is 0.3766222596168518\n",
      "epoch: 2 step: 829, loss is 0.3312739431858063\n",
      "epoch: 2 step: 830, loss is 0.21185609698295593\n",
      "epoch: 2 step: 831, loss is 0.36746200919151306\n",
      "epoch: 2 step: 832, loss is 0.2949761748313904\n",
      "epoch: 2 step: 833, loss is 0.48484089970588684\n",
      "epoch: 2 step: 834, loss is 0.26166659593582153\n",
      "epoch: 2 step: 835, loss is 0.5061172246932983\n",
      "epoch: 2 step: 836, loss is 0.3184404969215393\n",
      "epoch: 2 step: 837, loss is 0.3162856698036194\n",
      "epoch: 2 step: 838, loss is 0.2957257628440857\n",
      "epoch: 2 step: 839, loss is 0.188667893409729\n",
      "epoch: 2 step: 840, loss is 0.30618318915367126\n",
      "epoch: 2 step: 841, loss is 0.46741026639938354\n",
      "epoch: 2 step: 842, loss is 0.23035134375095367\n",
      "epoch: 2 step: 843, loss is 0.2818314731121063\n",
      "epoch: 2 step: 844, loss is 0.2519834339618683\n",
      "epoch: 2 step: 845, loss is 0.22885426878929138\n",
      "epoch: 2 step: 846, loss is 0.3007121980190277\n",
      "epoch: 2 step: 847, loss is 0.33563244342803955\n",
      "epoch: 2 step: 848, loss is 0.3327544033527374\n",
      "epoch: 2 step: 849, loss is 0.22923791408538818\n",
      "epoch: 2 step: 850, loss is 0.5755202174186707\n",
      "epoch: 2 step: 851, loss is 0.30042991042137146\n",
      "epoch: 2 step: 852, loss is 0.37966498732566833\n",
      "epoch: 2 step: 853, loss is 0.405272901058197\n",
      "epoch: 2 step: 854, loss is 0.5624879598617554\n",
      "epoch: 2 step: 855, loss is 0.16401803493499756\n",
      "epoch: 2 step: 856, loss is 0.27559077739715576\n",
      "epoch: 2 step: 857, loss is 0.28314849734306335\n",
      "epoch: 2 step: 858, loss is 0.3879888653755188\n",
      "epoch: 2 step: 859, loss is 0.4373492896556854\n",
      "epoch: 2 step: 860, loss is 0.22242258489131927\n",
      "epoch: 2 step: 861, loss is 0.27132800221443176\n",
      "epoch: 2 step: 862, loss is 0.29979968070983887\n",
      "epoch: 2 step: 863, loss is 0.2227250039577484\n",
      "epoch: 2 step: 864, loss is 0.31964626908302307\n",
      "epoch: 2 step: 865, loss is 0.48571252822875977\n",
      "epoch: 2 step: 866, loss is 0.36527615785598755\n",
      "epoch: 2 step: 867, loss is 0.39008691906929016\n",
      "epoch: 2 step: 868, loss is 0.6889796853065491\n",
      "epoch: 2 step: 869, loss is 0.7859044075012207\n",
      "epoch: 2 step: 870, loss is 0.2487860471010208\n",
      "epoch: 2 step: 871, loss is 0.33028650283813477\n",
      "epoch: 2 step: 872, loss is 0.357024222612381\n",
      "epoch: 2 step: 873, loss is 0.23519688844680786\n",
      "epoch: 2 step: 874, loss is 0.3491780161857605\n",
      "epoch: 2 step: 875, loss is 0.2766419053077698\n",
      "epoch: 2 step: 876, loss is 0.3115171194076538\n",
      "epoch: 2 step: 877, loss is 0.3848969340324402\n",
      "epoch: 2 step: 878, loss is 0.3769396245479584\n",
      "epoch: 2 step: 879, loss is 0.4073640704154968\n",
      "epoch: 2 step: 880, loss is 0.5245402455329895\n",
      "epoch: 2 step: 881, loss is 0.16979418694972992\n",
      "epoch: 2 step: 882, loss is 0.15877902507781982\n",
      "epoch: 2 step: 883, loss is 0.332302987575531\n",
      "epoch: 2 step: 884, loss is 0.4725588858127594\n",
      "epoch: 2 step: 885, loss is 0.22020259499549866\n",
      "epoch: 2 step: 886, loss is 0.38775694370269775\n",
      "epoch: 2 step: 887, loss is 0.2679611146450043\n",
      "epoch: 2 step: 888, loss is 0.2995877265930176\n",
      "epoch: 2 step: 889, loss is 0.24916285276412964\n",
      "epoch: 2 step: 890, loss is 0.316625714302063\n",
      "epoch: 2 step: 891, loss is 0.3349303901195526\n",
      "epoch: 2 step: 892, loss is 0.38662901520729065\n",
      "epoch: 2 step: 893, loss is 0.24885714054107666\n",
      "epoch: 2 step: 894, loss is 0.24740852415561676\n",
      "epoch: 2 step: 895, loss is 0.30539390444755554\n",
      "epoch: 2 step: 896, loss is 0.3706872761249542\n",
      "epoch: 2 step: 897, loss is 0.44679003953933716\n",
      "epoch: 2 step: 898, loss is 0.40590977668762207\n",
      "epoch: 2 step: 899, loss is 0.34243103861808777\n",
      "epoch: 2 step: 900, loss is 0.33294108510017395\n",
      "epoch: 2 step: 901, loss is 0.29964521527290344\n",
      "epoch: 2 step: 902, loss is 0.30079954862594604\n",
      "epoch: 2 step: 903, loss is 0.350532203912735\n",
      "epoch: 2 step: 904, loss is 0.33290693163871765\n",
      "epoch: 2 step: 905, loss is 0.3963986337184906\n",
      "epoch: 2 step: 906, loss is 0.44064849615097046\n",
      "epoch: 2 step: 907, loss is 0.29024696350097656\n",
      "epoch: 2 step: 908, loss is 0.39557525515556335\n",
      "epoch: 2 step: 909, loss is 0.3472973704338074\n",
      "epoch: 2 step: 910, loss is 0.4148024320602417\n",
      "epoch: 2 step: 911, loss is 0.2944948375225067\n",
      "epoch: 2 step: 912, loss is 0.3111051917076111\n",
      "epoch: 2 step: 913, loss is 0.4844038486480713\n",
      "epoch: 2 step: 914, loss is 0.28651851415634155\n",
      "epoch: 2 step: 915, loss is 0.44785869121551514\n",
      "epoch: 2 step: 916, loss is 0.25242483615875244\n",
      "epoch: 2 step: 917, loss is 0.39502301812171936\n",
      "epoch: 2 step: 918, loss is 0.22356443107128143\n",
      "epoch: 2 step: 919, loss is 0.46927595138549805\n",
      "epoch: 2 step: 920, loss is 0.36283063888549805\n",
      "epoch: 2 step: 921, loss is 0.39409202337265015\n",
      "epoch: 2 step: 922, loss is 0.2770676910877228\n",
      "epoch: 2 step: 923, loss is 0.11946949362754822\n",
      "epoch: 2 step: 924, loss is 0.4415486454963684\n",
      "epoch: 2 step: 925, loss is 0.15787775814533234\n",
      "epoch: 2 step: 926, loss is 0.37929511070251465\n",
      "epoch: 2 step: 927, loss is 0.1902584582567215\n",
      "epoch: 2 step: 928, loss is 0.37878602743148804\n",
      "epoch: 2 step: 929, loss is 0.23953492939472198\n",
      "epoch: 2 step: 930, loss is 0.3260124623775482\n",
      "epoch: 2 step: 931, loss is 0.41070762276649475\n",
      "epoch: 2 step: 932, loss is 0.16848739981651306\n",
      "epoch: 2 step: 933, loss is 0.45087337493896484\n",
      "epoch: 2 step: 934, loss is 0.32512497901916504\n",
      "epoch: 2 step: 935, loss is 0.5370790958404541\n",
      "epoch: 2 step: 936, loss is 0.30510371923446655\n",
      "epoch: 2 step: 937, loss is 0.28751957416534424\n",
      "epoch: 3 step: 1, loss is 0.4421688914299011\n",
      "epoch: 3 step: 2, loss is 0.2951769530773163\n",
      "epoch: 3 step: 3, loss is 0.48958835005760193\n",
      "epoch: 3 step: 4, loss is 0.4587251543998718\n",
      "epoch: 3 step: 5, loss is 0.2921532392501831\n",
      "epoch: 3 step: 6, loss is 0.3759008049964905\n",
      "epoch: 3 step: 7, loss is 0.36923617124557495\n",
      "epoch: 3 step: 8, loss is 0.22887684404850006\n",
      "epoch: 3 step: 9, loss is 0.35057127475738525\n",
      "epoch: 3 step: 10, loss is 0.31494107842445374\n",
      "epoch: 3 step: 11, loss is 0.3381823003292084\n",
      "epoch: 3 step: 12, loss is 0.26959678530693054\n",
      "epoch: 3 step: 13, loss is 0.5066511631011963\n",
      "epoch: 3 step: 14, loss is 0.1920931339263916\n",
      "epoch: 3 step: 15, loss is 0.1741398125886917\n",
      "epoch: 3 step: 16, loss is 0.2929703891277313\n",
      "epoch: 3 step: 17, loss is 0.1954607218503952\n",
      "epoch: 3 step: 18, loss is 0.35123878717422485\n",
      "epoch: 3 step: 19, loss is 0.3038782477378845\n",
      "epoch: 3 step: 20, loss is 0.23767468333244324\n",
      "epoch: 3 step: 21, loss is 0.2689518332481384\n",
      "epoch: 3 step: 22, loss is 0.3071596920490265\n",
      "epoch: 3 step: 23, loss is 0.3844904899597168\n",
      "epoch: 3 step: 24, loss is 0.563916802406311\n",
      "epoch: 3 step: 25, loss is 0.28847065567970276\n",
      "epoch: 3 step: 26, loss is 0.24141375720500946\n",
      "epoch: 3 step: 27, loss is 0.20003370940685272\n",
      "epoch: 3 step: 28, loss is 0.5192162394523621\n",
      "epoch: 3 step: 29, loss is 0.3728339374065399\n",
      "epoch: 3 step: 30, loss is 0.33364829421043396\n",
      "epoch: 3 step: 31, loss is 0.435819149017334\n",
      "epoch: 3 step: 32, loss is 0.40109801292419434\n",
      "epoch: 3 step: 33, loss is 0.3283417820930481\n",
      "epoch: 3 step: 34, loss is 0.4203184247016907\n",
      "epoch: 3 step: 35, loss is 0.3408862054347992\n",
      "epoch: 3 step: 36, loss is 0.31313782930374146\n",
      "epoch: 3 step: 37, loss is 0.22373956441879272\n",
      "epoch: 3 step: 38, loss is 0.3921974301338196\n",
      "epoch: 3 step: 39, loss is 0.3595389127731323\n",
      "epoch: 3 step: 40, loss is 0.42919301986694336\n",
      "epoch: 3 step: 41, loss is 0.5016326904296875\n",
      "epoch: 3 step: 42, loss is 0.2661956250667572\n",
      "epoch: 3 step: 43, loss is 0.31971657276153564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 44, loss is 0.39957135915756226\n",
      "epoch: 3 step: 45, loss is 0.21206431090831757\n",
      "epoch: 3 step: 46, loss is 0.3745015859603882\n",
      "epoch: 3 step: 47, loss is 0.2627873718738556\n",
      "epoch: 3 step: 48, loss is 0.28054940700531006\n",
      "epoch: 3 step: 49, loss is 0.3913600444793701\n",
      "epoch: 3 step: 50, loss is 0.2901451289653778\n",
      "epoch: 3 step: 51, loss is 0.3088644742965698\n",
      "epoch: 3 step: 52, loss is 0.35081005096435547\n",
      "epoch: 3 step: 53, loss is 0.33570948243141174\n",
      "epoch: 3 step: 54, loss is 0.21311961114406586\n",
      "epoch: 3 step: 55, loss is 0.36071842908859253\n",
      "epoch: 3 step: 56, loss is 0.1928371638059616\n",
      "epoch: 3 step: 57, loss is 0.3441823124885559\n",
      "epoch: 3 step: 58, loss is 0.37202897667884827\n",
      "epoch: 3 step: 59, loss is 0.670971155166626\n",
      "epoch: 3 step: 60, loss is 0.3251420855522156\n",
      "epoch: 3 step: 61, loss is 0.3910447061061859\n",
      "epoch: 3 step: 62, loss is 0.42907804250717163\n",
      "epoch: 3 step: 63, loss is 0.29436662793159485\n",
      "epoch: 3 step: 64, loss is 0.36618804931640625\n",
      "epoch: 3 step: 65, loss is 0.3204604387283325\n",
      "epoch: 3 step: 66, loss is 0.20650744438171387\n",
      "epoch: 3 step: 67, loss is 0.3387436866760254\n",
      "epoch: 3 step: 68, loss is 0.39185336232185364\n",
      "epoch: 3 step: 69, loss is 0.3660091459751129\n",
      "epoch: 3 step: 70, loss is 0.32363465428352356\n",
      "epoch: 3 step: 71, loss is 0.362710177898407\n",
      "epoch: 3 step: 72, loss is 0.30658215284347534\n",
      "epoch: 3 step: 73, loss is 0.19060498476028442\n",
      "epoch: 3 step: 74, loss is 0.2841533124446869\n",
      "epoch: 3 step: 75, loss is 0.1924542337656021\n",
      "epoch: 3 step: 76, loss is 0.16064730286598206\n",
      "epoch: 3 step: 77, loss is 0.3039572238922119\n",
      "epoch: 3 step: 78, loss is 0.44714900851249695\n",
      "epoch: 3 step: 79, loss is 0.35506191849708557\n",
      "epoch: 3 step: 80, loss is 0.38807177543640137\n",
      "epoch: 3 step: 81, loss is 0.36545461416244507\n",
      "epoch: 3 step: 82, loss is 0.3782356083393097\n",
      "epoch: 3 step: 83, loss is 0.49682170152664185\n",
      "epoch: 3 step: 84, loss is 0.24915984272956848\n",
      "epoch: 3 step: 85, loss is 0.30815041065216064\n",
      "epoch: 3 step: 86, loss is 0.41009247303009033\n",
      "epoch: 3 step: 87, loss is 0.18339718878269196\n",
      "epoch: 3 step: 88, loss is 0.43431341648101807\n",
      "epoch: 3 step: 89, loss is 0.2521742582321167\n",
      "epoch: 3 step: 90, loss is 0.3189675211906433\n",
      "epoch: 3 step: 91, loss is 0.29558947682380676\n",
      "epoch: 3 step: 92, loss is 0.325149804353714\n",
      "epoch: 3 step: 93, loss is 0.24926191568374634\n",
      "epoch: 3 step: 94, loss is 0.23083657026290894\n",
      "epoch: 3 step: 95, loss is 0.21536915004253387\n",
      "epoch: 3 step: 96, loss is 0.2773609161376953\n",
      "epoch: 3 step: 97, loss is 0.2381138652563095\n",
      "epoch: 3 step: 98, loss is 0.3216734826564789\n",
      "epoch: 3 step: 99, loss is 0.4010075628757477\n",
      "epoch: 3 step: 100, loss is 0.35117289423942566\n",
      "epoch: 3 step: 101, loss is 0.5208290815353394\n",
      "epoch: 3 step: 102, loss is 0.49115079641342163\n",
      "epoch: 3 step: 103, loss is 0.4257684648036957\n",
      "epoch: 3 step: 104, loss is 0.31299492716789246\n",
      "epoch: 3 step: 105, loss is 0.12556500732898712\n",
      "epoch: 3 step: 106, loss is 0.3259263038635254\n",
      "epoch: 3 step: 107, loss is 0.1938956081867218\n",
      "epoch: 3 step: 108, loss is 0.3262091279029846\n",
      "epoch: 3 step: 109, loss is 0.15556707978248596\n",
      "epoch: 3 step: 110, loss is 0.3658846318721771\n",
      "epoch: 3 step: 111, loss is 0.5053767561912537\n",
      "epoch: 3 step: 112, loss is 0.18771754205226898\n",
      "epoch: 3 step: 113, loss is 0.3010130822658539\n",
      "epoch: 3 step: 114, loss is 0.48945629596710205\n",
      "epoch: 3 step: 115, loss is 0.13276977837085724\n",
      "epoch: 3 step: 116, loss is 0.4368534982204437\n",
      "epoch: 3 step: 117, loss is 0.21981585025787354\n",
      "epoch: 3 step: 118, loss is 0.3152759373188019\n",
      "epoch: 3 step: 119, loss is 0.3121952414512634\n",
      "epoch: 3 step: 120, loss is 0.23095445334911346\n",
      "epoch: 3 step: 121, loss is 0.27540063858032227\n",
      "epoch: 3 step: 122, loss is 0.4041379690170288\n",
      "epoch: 3 step: 123, loss is 0.4122660756111145\n",
      "epoch: 3 step: 124, loss is 0.45525071024894714\n",
      "epoch: 3 step: 125, loss is 0.23834222555160522\n",
      "epoch: 3 step: 126, loss is 0.21332788467407227\n",
      "epoch: 3 step: 127, loss is 0.3231394588947296\n",
      "epoch: 3 step: 128, loss is 0.31163495779037476\n",
      "epoch: 3 step: 129, loss is 0.32658883929252625\n",
      "epoch: 3 step: 130, loss is 0.24956253170967102\n",
      "epoch: 3 step: 131, loss is 0.3222678303718567\n",
      "epoch: 3 step: 132, loss is 0.3783271312713623\n",
      "epoch: 3 step: 133, loss is 0.28686395287513733\n",
      "epoch: 3 step: 134, loss is 0.23472189903259277\n",
      "epoch: 3 step: 135, loss is 0.2732934057712555\n",
      "epoch: 3 step: 136, loss is 0.21727314591407776\n",
      "epoch: 3 step: 137, loss is 0.46667030453681946\n",
      "epoch: 3 step: 138, loss is 0.3365272581577301\n",
      "epoch: 3 step: 139, loss is 0.3317868113517761\n",
      "epoch: 3 step: 140, loss is 0.24577148258686066\n",
      "epoch: 3 step: 141, loss is 0.2823031544685364\n",
      "epoch: 3 step: 142, loss is 0.4716215133666992\n",
      "epoch: 3 step: 143, loss is 0.5018686652183533\n",
      "epoch: 3 step: 144, loss is 0.3751034140586853\n",
      "epoch: 3 step: 145, loss is 0.3829847574234009\n",
      "epoch: 3 step: 146, loss is 0.2549346685409546\n",
      "epoch: 3 step: 147, loss is 0.34391629695892334\n",
      "epoch: 3 step: 148, loss is 0.39988598227500916\n",
      "epoch: 3 step: 149, loss is 0.3226146697998047\n",
      "epoch: 3 step: 150, loss is 0.25651422142982483\n",
      "epoch: 3 step: 151, loss is 0.44439876079559326\n",
      "epoch: 3 step: 152, loss is 0.2125493437051773\n",
      "epoch: 3 step: 153, loss is 0.32284384965896606\n",
      "epoch: 3 step: 154, loss is 0.4218437075614929\n",
      "epoch: 3 step: 155, loss is 0.35688069462776184\n",
      "epoch: 3 step: 156, loss is 0.4534190595149994\n",
      "epoch: 3 step: 157, loss is 0.512759268283844\n",
      "epoch: 3 step: 158, loss is 0.3118009567260742\n",
      "epoch: 3 step: 159, loss is 0.2279442995786667\n",
      "epoch: 3 step: 160, loss is 0.33658215403556824\n",
      "epoch: 3 step: 161, loss is 0.5312967300415039\n",
      "epoch: 3 step: 162, loss is 0.17137764394283295\n",
      "epoch: 3 step: 163, loss is 0.4041743874549866\n",
      "epoch: 3 step: 164, loss is 0.3133912980556488\n",
      "epoch: 3 step: 165, loss is 0.2344110906124115\n",
      "epoch: 3 step: 166, loss is 0.501715898513794\n",
      "epoch: 3 step: 167, loss is 0.35937657952308655\n",
      "epoch: 3 step: 168, loss is 0.3035508692264557\n",
      "epoch: 3 step: 169, loss is 0.3532443046569824\n",
      "epoch: 3 step: 170, loss is 0.29915356636047363\n",
      "epoch: 3 step: 171, loss is 0.5002047419548035\n",
      "epoch: 3 step: 172, loss is 0.3199034631252289\n",
      "epoch: 3 step: 173, loss is 0.37389203906059265\n",
      "epoch: 3 step: 174, loss is 0.39885661005973816\n",
      "epoch: 3 step: 175, loss is 0.36532723903656006\n",
      "epoch: 3 step: 176, loss is 0.3231322765350342\n",
      "epoch: 3 step: 177, loss is 0.26493334770202637\n",
      "epoch: 3 step: 178, loss is 0.2890162765979767\n",
      "epoch: 3 step: 179, loss is 0.34604865312576294\n",
      "epoch: 3 step: 180, loss is 0.34402504563331604\n",
      "epoch: 3 step: 181, loss is 0.3027562201023102\n",
      "epoch: 3 step: 182, loss is 0.38536959886550903\n",
      "epoch: 3 step: 183, loss is 0.31017959117889404\n",
      "epoch: 3 step: 184, loss is 0.31185877323150635\n",
      "epoch: 3 step: 185, loss is 0.19409388303756714\n",
      "epoch: 3 step: 186, loss is 0.37824711203575134\n",
      "epoch: 3 step: 187, loss is 0.29632383584976196\n",
      "epoch: 3 step: 188, loss is 0.46440452337265015\n",
      "epoch: 3 step: 189, loss is 0.2546294331550598\n",
      "epoch: 3 step: 190, loss is 0.2331267148256302\n",
      "epoch: 3 step: 191, loss is 0.32753682136535645\n",
      "epoch: 3 step: 192, loss is 0.34457656741142273\n",
      "epoch: 3 step: 193, loss is 0.36643001437187195\n",
      "epoch: 3 step: 194, loss is 0.3142348825931549\n",
      "epoch: 3 step: 195, loss is 0.4436095356941223\n",
      "epoch: 3 step: 196, loss is 0.31116271018981934\n",
      "epoch: 3 step: 197, loss is 0.196008563041687\n",
      "epoch: 3 step: 198, loss is 0.4337351620197296\n",
      "epoch: 3 step: 199, loss is 0.3875676095485687\n",
      "epoch: 3 step: 200, loss is 0.3941587805747986\n",
      "epoch: 3 step: 201, loss is 0.39349794387817383\n",
      "epoch: 3 step: 202, loss is 0.1963721662759781\n",
      "epoch: 3 step: 203, loss is 0.38841357827186584\n",
      "epoch: 3 step: 204, loss is 0.40832260251045227\n",
      "epoch: 3 step: 205, loss is 0.28498318791389465\n",
      "epoch: 3 step: 206, loss is 0.35707688331604004\n",
      "epoch: 3 step: 207, loss is 0.3112376928329468\n",
      "epoch: 3 step: 208, loss is 0.39953914284706116\n",
      "epoch: 3 step: 209, loss is 0.4402611553668976\n",
      "epoch: 3 step: 210, loss is 0.2895287573337555\n",
      "epoch: 3 step: 211, loss is 0.26092395186424255\n",
      "epoch: 3 step: 212, loss is 0.5675058960914612\n",
      "epoch: 3 step: 213, loss is 0.35146984457969666\n",
      "epoch: 3 step: 214, loss is 0.5194501280784607\n",
      "epoch: 3 step: 215, loss is 0.22716689109802246\n",
      "epoch: 3 step: 216, loss is 0.2705378532409668\n",
      "epoch: 3 step: 217, loss is 0.3261682391166687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 218, loss is 0.5027405023574829\n",
      "epoch: 3 step: 219, loss is 0.2981896996498108\n",
      "epoch: 3 step: 220, loss is 0.5011388659477234\n",
      "epoch: 3 step: 221, loss is 0.38452568650245667\n",
      "epoch: 3 step: 222, loss is 0.5984476208686829\n",
      "epoch: 3 step: 223, loss is 0.42390263080596924\n",
      "epoch: 3 step: 224, loss is 0.3864898383617401\n",
      "epoch: 3 step: 225, loss is 0.288906991481781\n",
      "epoch: 3 step: 226, loss is 0.33111077547073364\n",
      "epoch: 3 step: 227, loss is 0.32812440395355225\n",
      "epoch: 3 step: 228, loss is 0.4499489367008209\n",
      "epoch: 3 step: 229, loss is 0.459261417388916\n",
      "epoch: 3 step: 230, loss is 0.2229900062084198\n",
      "epoch: 3 step: 231, loss is 0.3052486181259155\n",
      "epoch: 3 step: 232, loss is 0.3708348274230957\n",
      "epoch: 3 step: 233, loss is 0.326323539018631\n",
      "epoch: 3 step: 234, loss is 0.24393230676651\n",
      "epoch: 3 step: 235, loss is 0.2920920252799988\n",
      "epoch: 3 step: 236, loss is 0.2721587121486664\n",
      "epoch: 3 step: 237, loss is 0.2835961878299713\n",
      "epoch: 3 step: 238, loss is 0.5345540046691895\n",
      "epoch: 3 step: 239, loss is 0.22588244080543518\n",
      "epoch: 3 step: 240, loss is 0.22287818789482117\n",
      "epoch: 3 step: 241, loss is 0.16988281905651093\n",
      "epoch: 3 step: 242, loss is 0.30279216170310974\n",
      "epoch: 3 step: 243, loss is 0.5221655964851379\n",
      "epoch: 3 step: 244, loss is 0.29459962248802185\n",
      "epoch: 3 step: 245, loss is 0.30295485258102417\n",
      "epoch: 3 step: 246, loss is 0.23887433111667633\n",
      "epoch: 3 step: 247, loss is 0.28082677721977234\n",
      "epoch: 3 step: 248, loss is 0.33952784538269043\n",
      "epoch: 3 step: 249, loss is 0.7192612290382385\n",
      "epoch: 3 step: 250, loss is 0.2506089508533478\n",
      "epoch: 3 step: 251, loss is 0.16411317884922028\n",
      "epoch: 3 step: 252, loss is 0.4850292205810547\n",
      "epoch: 3 step: 253, loss is 0.35578981041908264\n",
      "epoch: 3 step: 254, loss is 0.4261055290699005\n",
      "epoch: 3 step: 255, loss is 0.4592210650444031\n",
      "epoch: 3 step: 256, loss is 0.3402225971221924\n",
      "epoch: 3 step: 257, loss is 0.44442617893218994\n",
      "epoch: 3 step: 258, loss is 0.35816556215286255\n",
      "epoch: 3 step: 259, loss is 0.32833728194236755\n",
      "epoch: 3 step: 260, loss is 0.43560028076171875\n",
      "epoch: 3 step: 261, loss is 0.3189579248428345\n",
      "epoch: 3 step: 262, loss is 0.30249276757240295\n",
      "epoch: 3 step: 263, loss is 0.23220503330230713\n",
      "epoch: 3 step: 264, loss is 0.3532313406467438\n",
      "epoch: 3 step: 265, loss is 0.4149572551250458\n",
      "epoch: 3 step: 266, loss is 0.5240201354026794\n",
      "epoch: 3 step: 267, loss is 0.25915801525115967\n",
      "epoch: 3 step: 268, loss is 0.2616492509841919\n",
      "epoch: 3 step: 269, loss is 0.4734155535697937\n",
      "epoch: 3 step: 270, loss is 0.30194106698036194\n",
      "epoch: 3 step: 271, loss is 0.45048820972442627\n",
      "epoch: 3 step: 272, loss is 0.3472120463848114\n",
      "epoch: 3 step: 273, loss is 0.1560935527086258\n",
      "epoch: 3 step: 274, loss is 0.37091514468193054\n",
      "epoch: 3 step: 275, loss is 0.2741369605064392\n",
      "epoch: 3 step: 276, loss is 0.37448644638061523\n",
      "epoch: 3 step: 277, loss is 0.34644514322280884\n",
      "epoch: 3 step: 278, loss is 0.38052108883857727\n",
      "epoch: 3 step: 279, loss is 0.2380281686782837\n",
      "epoch: 3 step: 280, loss is 0.39065998792648315\n",
      "epoch: 3 step: 281, loss is 0.25149059295654297\n",
      "epoch: 3 step: 282, loss is 0.294669508934021\n",
      "epoch: 3 step: 283, loss is 0.27002841234207153\n",
      "epoch: 3 step: 284, loss is 0.4109954833984375\n",
      "epoch: 3 step: 285, loss is 0.4405550956726074\n",
      "epoch: 3 step: 286, loss is 0.24025914072990417\n",
      "epoch: 3 step: 287, loss is 0.30267447233200073\n",
      "epoch: 3 step: 288, loss is 0.3784317672252655\n",
      "epoch: 3 step: 289, loss is 0.2673898935317993\n",
      "epoch: 3 step: 290, loss is 0.12531237304210663\n",
      "epoch: 3 step: 291, loss is 0.44950658082962036\n",
      "epoch: 3 step: 292, loss is 0.40799567103385925\n",
      "epoch: 3 step: 293, loss is 0.30232614278793335\n",
      "epoch: 3 step: 294, loss is 0.4230372905731201\n",
      "epoch: 3 step: 295, loss is 0.37806782126426697\n",
      "epoch: 3 step: 296, loss is 0.2997535467147827\n",
      "epoch: 3 step: 297, loss is 0.22447293996810913\n",
      "epoch: 3 step: 298, loss is 0.3575940430164337\n",
      "epoch: 3 step: 299, loss is 0.356922447681427\n",
      "epoch: 3 step: 300, loss is 0.2772284150123596\n",
      "epoch: 3 step: 301, loss is 0.21967345476150513\n",
      "epoch: 3 step: 302, loss is 0.40861496329307556\n",
      "epoch: 3 step: 303, loss is 0.40747594833374023\n",
      "epoch: 3 step: 304, loss is 0.2805251181125641\n",
      "epoch: 3 step: 305, loss is 0.3026497960090637\n",
      "epoch: 3 step: 306, loss is 0.3124467432498932\n",
      "epoch: 3 step: 307, loss is 0.24665158987045288\n",
      "epoch: 3 step: 308, loss is 0.30463719367980957\n",
      "epoch: 3 step: 309, loss is 0.33611777424812317\n",
      "epoch: 3 step: 310, loss is 0.5180038213729858\n",
      "epoch: 3 step: 311, loss is 0.29046201705932617\n",
      "epoch: 3 step: 312, loss is 0.46268779039382935\n",
      "epoch: 3 step: 313, loss is 0.33474498987197876\n",
      "epoch: 3 step: 314, loss is 0.2267722487449646\n",
      "epoch: 3 step: 315, loss is 0.14529642462730408\n",
      "epoch: 3 step: 316, loss is 0.22933799028396606\n",
      "epoch: 3 step: 317, loss is 0.1516459584236145\n",
      "epoch: 3 step: 318, loss is 0.31182166934013367\n",
      "epoch: 3 step: 319, loss is 0.3606790602207184\n",
      "epoch: 3 step: 320, loss is 0.3939817249774933\n",
      "epoch: 3 step: 321, loss is 0.3073568344116211\n",
      "epoch: 3 step: 322, loss is 0.3599472939968109\n",
      "epoch: 3 step: 323, loss is 0.2345070242881775\n",
      "epoch: 3 step: 324, loss is 0.22669243812561035\n",
      "epoch: 3 step: 325, loss is 0.4580531418323517\n",
      "epoch: 3 step: 326, loss is 0.3181852698326111\n",
      "epoch: 3 step: 327, loss is 0.3273860216140747\n",
      "epoch: 3 step: 328, loss is 0.30819934606552124\n",
      "epoch: 3 step: 329, loss is 0.4122818112373352\n",
      "epoch: 3 step: 330, loss is 0.30239978432655334\n",
      "epoch: 3 step: 331, loss is 0.3079318404197693\n",
      "epoch: 3 step: 332, loss is 0.3536892831325531\n",
      "epoch: 3 step: 333, loss is 0.3271380364894867\n",
      "epoch: 3 step: 334, loss is 0.2016613632440567\n",
      "epoch: 3 step: 335, loss is 0.21617498993873596\n",
      "epoch: 3 step: 336, loss is 0.32779085636138916\n",
      "epoch: 3 step: 337, loss is 0.40281087160110474\n",
      "epoch: 3 step: 338, loss is 0.29000645875930786\n",
      "epoch: 3 step: 339, loss is 0.44237011671066284\n",
      "epoch: 3 step: 340, loss is 0.33221182227134705\n",
      "epoch: 3 step: 341, loss is 0.3809211254119873\n",
      "epoch: 3 step: 342, loss is 0.4629386365413666\n",
      "epoch: 3 step: 343, loss is 0.23700962960720062\n",
      "epoch: 3 step: 344, loss is 0.3102934956550598\n",
      "epoch: 3 step: 345, loss is 0.28519871830940247\n",
      "epoch: 3 step: 346, loss is 0.3477899134159088\n",
      "epoch: 3 step: 347, loss is 0.4566119909286499\n",
      "epoch: 3 step: 348, loss is 0.20250675082206726\n",
      "epoch: 3 step: 349, loss is 0.2947014272212982\n",
      "epoch: 3 step: 350, loss is 0.4139085114002228\n",
      "epoch: 3 step: 351, loss is 0.4394216239452362\n",
      "epoch: 3 step: 352, loss is 0.27689245343208313\n",
      "epoch: 3 step: 353, loss is 0.3176174461841583\n",
      "epoch: 3 step: 354, loss is 0.33490845561027527\n",
      "epoch: 3 step: 355, loss is 0.2464953511953354\n",
      "epoch: 3 step: 356, loss is 0.33273860812187195\n",
      "epoch: 3 step: 357, loss is 0.2455296665430069\n",
      "epoch: 3 step: 358, loss is 0.5357177257537842\n",
      "epoch: 3 step: 359, loss is 0.28632134199142456\n",
      "epoch: 3 step: 360, loss is 0.2686275541782379\n",
      "epoch: 3 step: 361, loss is 0.23137573897838593\n",
      "epoch: 3 step: 362, loss is 0.2535715103149414\n",
      "epoch: 3 step: 363, loss is 0.4934804141521454\n",
      "epoch: 3 step: 364, loss is 0.2618083357810974\n",
      "epoch: 3 step: 365, loss is 0.3300372064113617\n",
      "epoch: 3 step: 366, loss is 0.28061431646347046\n",
      "epoch: 3 step: 367, loss is 0.3728286325931549\n",
      "epoch: 3 step: 368, loss is 0.45635300874710083\n",
      "epoch: 3 step: 369, loss is 0.3167326748371124\n",
      "epoch: 3 step: 370, loss is 0.42183226346969604\n",
      "epoch: 3 step: 371, loss is 0.17587876319885254\n",
      "epoch: 3 step: 372, loss is 0.2700826823711395\n",
      "epoch: 3 step: 373, loss is 0.28546208143234253\n",
      "epoch: 3 step: 374, loss is 0.34685012698173523\n",
      "epoch: 3 step: 375, loss is 0.3304235637187958\n",
      "epoch: 3 step: 376, loss is 0.4816884696483612\n",
      "epoch: 3 step: 377, loss is 0.19250251352787018\n",
      "epoch: 3 step: 378, loss is 0.2699335515499115\n",
      "epoch: 3 step: 379, loss is 0.271826833486557\n",
      "epoch: 3 step: 380, loss is 0.3151521682739258\n",
      "epoch: 3 step: 381, loss is 0.23270587623119354\n",
      "epoch: 3 step: 382, loss is 0.410773903131485\n",
      "epoch: 3 step: 383, loss is 0.2588772773742676\n",
      "epoch: 3 step: 384, loss is 0.3455567955970764\n",
      "epoch: 3 step: 385, loss is 0.32804709672927856\n",
      "epoch: 3 step: 386, loss is 0.20574796199798584\n",
      "epoch: 3 step: 387, loss is 0.19558778405189514\n",
      "epoch: 3 step: 388, loss is 0.22545258700847626\n",
      "epoch: 3 step: 389, loss is 0.4592527747154236\n",
      "epoch: 3 step: 390, loss is 0.34259283542633057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 391, loss is 0.43887123465538025\n",
      "epoch: 3 step: 392, loss is 0.3046364188194275\n",
      "epoch: 3 step: 393, loss is 0.1328948736190796\n",
      "epoch: 3 step: 394, loss is 0.3282029926776886\n",
      "epoch: 3 step: 395, loss is 0.24090953171253204\n",
      "epoch: 3 step: 396, loss is 0.2928512394428253\n",
      "epoch: 3 step: 397, loss is 0.5237994194030762\n",
      "epoch: 3 step: 398, loss is 0.3228336274623871\n",
      "epoch: 3 step: 399, loss is 0.2145594358444214\n",
      "epoch: 3 step: 400, loss is 0.38697293400764465\n",
      "epoch: 3 step: 401, loss is 0.21035121381282806\n",
      "epoch: 3 step: 402, loss is 0.4016563594341278\n",
      "epoch: 3 step: 403, loss is 0.43746334314346313\n",
      "epoch: 3 step: 404, loss is 0.26413437724113464\n",
      "epoch: 3 step: 405, loss is 0.35911643505096436\n",
      "epoch: 3 step: 406, loss is 0.4297383725643158\n",
      "epoch: 3 step: 407, loss is 0.23365312814712524\n",
      "epoch: 3 step: 408, loss is 0.28782713413238525\n",
      "epoch: 3 step: 409, loss is 0.2851578891277313\n",
      "epoch: 3 step: 410, loss is 0.47048264741897583\n",
      "epoch: 3 step: 411, loss is 0.28270241618156433\n",
      "epoch: 3 step: 412, loss is 0.3552263379096985\n",
      "epoch: 3 step: 413, loss is 0.27927690744400024\n",
      "epoch: 3 step: 414, loss is 0.18142761290073395\n",
      "epoch: 3 step: 415, loss is 0.3250150680541992\n",
      "epoch: 3 step: 416, loss is 0.28717830777168274\n",
      "epoch: 3 step: 417, loss is 0.17600926756858826\n",
      "epoch: 3 step: 418, loss is 0.25167617201805115\n",
      "epoch: 3 step: 419, loss is 0.5639315247535706\n",
      "epoch: 3 step: 420, loss is 0.4182604253292084\n",
      "epoch: 3 step: 421, loss is 0.32547906041145325\n",
      "epoch: 3 step: 422, loss is 0.37906262278556824\n",
      "epoch: 3 step: 423, loss is 0.33894774317741394\n",
      "epoch: 3 step: 424, loss is 0.3032883107662201\n",
      "epoch: 3 step: 425, loss is 0.3210848271846771\n",
      "epoch: 3 step: 426, loss is 0.25974810123443604\n",
      "epoch: 3 step: 427, loss is 0.291448712348938\n",
      "epoch: 3 step: 428, loss is 0.33860135078430176\n",
      "epoch: 3 step: 429, loss is 0.35491126775741577\n",
      "epoch: 3 step: 430, loss is 0.4888714551925659\n",
      "epoch: 3 step: 431, loss is 0.20169758796691895\n",
      "epoch: 3 step: 432, loss is 0.25297361612319946\n",
      "epoch: 3 step: 433, loss is 0.40334609150886536\n",
      "epoch: 3 step: 434, loss is 0.26240894198417664\n",
      "epoch: 3 step: 435, loss is 0.270136296749115\n",
      "epoch: 3 step: 436, loss is 0.21910610795021057\n",
      "epoch: 3 step: 437, loss is 0.3518944978713989\n",
      "epoch: 3 step: 438, loss is 0.3906487226486206\n",
      "epoch: 3 step: 439, loss is 0.37999898195266724\n",
      "epoch: 3 step: 440, loss is 0.44386979937553406\n",
      "epoch: 3 step: 441, loss is 0.45521754026412964\n",
      "epoch: 3 step: 442, loss is 0.22071929275989532\n",
      "epoch: 3 step: 443, loss is 0.5745910406112671\n",
      "epoch: 3 step: 444, loss is 0.3479344844818115\n",
      "epoch: 3 step: 445, loss is 0.37281668186187744\n",
      "epoch: 3 step: 446, loss is 0.23145848512649536\n",
      "epoch: 3 step: 447, loss is 0.18102975189685822\n",
      "epoch: 3 step: 448, loss is 0.21720005571842194\n",
      "epoch: 3 step: 449, loss is 0.5655447840690613\n",
      "epoch: 3 step: 450, loss is 0.2225762903690338\n",
      "epoch: 3 step: 451, loss is 0.30172353982925415\n",
      "epoch: 3 step: 452, loss is 0.35536232590675354\n",
      "epoch: 3 step: 453, loss is 0.3341224193572998\n",
      "epoch: 3 step: 454, loss is 0.15866290032863617\n",
      "epoch: 3 step: 455, loss is 0.49027150869369507\n",
      "epoch: 3 step: 456, loss is 0.32901889085769653\n",
      "epoch: 3 step: 457, loss is 0.28249233961105347\n",
      "epoch: 3 step: 458, loss is 0.331364244222641\n",
      "epoch: 3 step: 459, loss is 0.5386595726013184\n",
      "epoch: 3 step: 460, loss is 0.2867504358291626\n",
      "epoch: 3 step: 461, loss is 0.4091072976589203\n",
      "epoch: 3 step: 462, loss is 0.25520122051239014\n",
      "epoch: 3 step: 463, loss is 0.16865424811840057\n",
      "epoch: 3 step: 464, loss is 0.33735209703445435\n",
      "epoch: 3 step: 465, loss is 0.2799736261367798\n",
      "epoch: 3 step: 466, loss is 0.2244054526090622\n",
      "epoch: 3 step: 467, loss is 0.23186825215816498\n",
      "epoch: 3 step: 468, loss is 0.4192376732826233\n",
      "epoch: 3 step: 469, loss is 0.43914511799812317\n",
      "epoch: 3 step: 470, loss is 0.2505449950695038\n",
      "epoch: 3 step: 471, loss is 0.23047015070915222\n",
      "epoch: 3 step: 472, loss is 0.15642745792865753\n",
      "epoch: 3 step: 473, loss is 0.3057352304458618\n",
      "epoch: 3 step: 474, loss is 0.3524308204650879\n",
      "epoch: 3 step: 475, loss is 0.2073863297700882\n",
      "epoch: 3 step: 476, loss is 0.3260173499584198\n",
      "epoch: 3 step: 477, loss is 0.449905663728714\n",
      "epoch: 3 step: 478, loss is 0.29427212476730347\n",
      "epoch: 3 step: 479, loss is 0.3389662802219391\n",
      "epoch: 3 step: 480, loss is 0.3012198507785797\n",
      "epoch: 3 step: 481, loss is 0.2530140280723572\n",
      "epoch: 3 step: 482, loss is 0.3083907663822174\n",
      "epoch: 3 step: 483, loss is 0.4275573492050171\n",
      "epoch: 3 step: 484, loss is 0.24735993146896362\n",
      "epoch: 3 step: 485, loss is 0.3876802325248718\n",
      "epoch: 3 step: 486, loss is 0.2333938479423523\n",
      "epoch: 3 step: 487, loss is 0.18479514122009277\n",
      "epoch: 3 step: 488, loss is 0.363025039434433\n",
      "epoch: 3 step: 489, loss is 0.32950982451438904\n",
      "epoch: 3 step: 490, loss is 0.39605751633644104\n",
      "epoch: 3 step: 491, loss is 0.19738224148750305\n",
      "epoch: 3 step: 492, loss is 0.3927353322505951\n",
      "epoch: 3 step: 493, loss is 0.2551487982273102\n",
      "epoch: 3 step: 494, loss is 0.2104427069425583\n",
      "epoch: 3 step: 495, loss is 0.34253090620040894\n",
      "epoch: 3 step: 496, loss is 0.43302249908447266\n",
      "epoch: 3 step: 497, loss is 0.2645546793937683\n",
      "epoch: 3 step: 498, loss is 0.24748243391513824\n",
      "epoch: 3 step: 499, loss is 0.3032945990562439\n",
      "epoch: 3 step: 500, loss is 0.19776089489459991\n",
      "epoch: 3 step: 501, loss is 0.33047235012054443\n",
      "epoch: 3 step: 502, loss is 0.17098242044448853\n",
      "epoch: 3 step: 503, loss is 0.4900034964084625\n",
      "epoch: 3 step: 504, loss is 0.27090680599212646\n",
      "epoch: 3 step: 505, loss is 0.37482360005378723\n",
      "epoch: 3 step: 506, loss is 0.5250678062438965\n",
      "epoch: 3 step: 507, loss is 0.47929492592811584\n",
      "epoch: 3 step: 508, loss is 0.5643672943115234\n",
      "epoch: 3 step: 509, loss is 0.3137911558151245\n",
      "epoch: 3 step: 510, loss is 0.28448835015296936\n",
      "epoch: 3 step: 511, loss is 0.2365930676460266\n",
      "epoch: 3 step: 512, loss is 0.475616991519928\n",
      "epoch: 3 step: 513, loss is 0.22453030943870544\n",
      "epoch: 3 step: 514, loss is 0.3057054281234741\n",
      "epoch: 3 step: 515, loss is 0.21183978021144867\n",
      "epoch: 3 step: 516, loss is 0.38755226135253906\n",
      "epoch: 3 step: 517, loss is 0.4343125820159912\n",
      "epoch: 3 step: 518, loss is 0.2991204261779785\n",
      "epoch: 3 step: 519, loss is 0.13953465223312378\n",
      "epoch: 3 step: 520, loss is 0.18120597302913666\n",
      "epoch: 3 step: 521, loss is 0.28170982003211975\n",
      "epoch: 3 step: 522, loss is 0.4228129982948303\n",
      "epoch: 3 step: 523, loss is 0.3815299868583679\n",
      "epoch: 3 step: 524, loss is 0.4304291605949402\n",
      "epoch: 3 step: 525, loss is 0.39692404866218567\n",
      "epoch: 3 step: 526, loss is 0.2003658413887024\n",
      "epoch: 3 step: 527, loss is 0.2762415409088135\n",
      "epoch: 3 step: 528, loss is 0.24443228542804718\n",
      "epoch: 3 step: 529, loss is 0.45559680461883545\n",
      "epoch: 3 step: 530, loss is 0.3768446743488312\n",
      "epoch: 3 step: 531, loss is 0.45365476608276367\n",
      "epoch: 3 step: 532, loss is 0.2871093153953552\n",
      "epoch: 3 step: 533, loss is 0.19035038352012634\n",
      "epoch: 3 step: 534, loss is 0.30941957235336304\n",
      "epoch: 3 step: 535, loss is 0.23682361841201782\n",
      "epoch: 3 step: 536, loss is 0.22734028100967407\n",
      "epoch: 3 step: 537, loss is 0.43333351612091064\n",
      "epoch: 3 step: 538, loss is 0.31998690962791443\n",
      "epoch: 3 step: 539, loss is 0.639142632484436\n",
      "epoch: 3 step: 540, loss is 0.37704166769981384\n",
      "epoch: 3 step: 541, loss is 0.2668517827987671\n",
      "epoch: 3 step: 542, loss is 0.36003321409225464\n",
      "epoch: 3 step: 543, loss is 0.5066608190536499\n",
      "epoch: 3 step: 544, loss is 0.3786059617996216\n",
      "epoch: 3 step: 545, loss is 0.3735836148262024\n",
      "epoch: 3 step: 546, loss is 0.45939579606056213\n",
      "epoch: 3 step: 547, loss is 0.26973122358322144\n",
      "epoch: 3 step: 548, loss is 0.3318796157836914\n",
      "epoch: 3 step: 549, loss is 0.45310065150260925\n",
      "epoch: 3 step: 550, loss is 0.36609774827957153\n",
      "epoch: 3 step: 551, loss is 0.33854249119758606\n",
      "epoch: 3 step: 552, loss is 0.40475407242774963\n",
      "epoch: 3 step: 553, loss is 0.3237169086933136\n",
      "epoch: 3 step: 554, loss is 0.17714828252792358\n",
      "epoch: 3 step: 555, loss is 0.22002452611923218\n",
      "epoch: 3 step: 556, loss is 0.39849376678466797\n",
      "epoch: 3 step: 557, loss is 0.2773498296737671\n",
      "epoch: 3 step: 558, loss is 0.3298841118812561\n",
      "epoch: 3 step: 559, loss is 0.4093541204929352\n",
      "epoch: 3 step: 560, loss is 0.1994696706533432\n",
      "epoch: 3 step: 561, loss is 0.4528409540653229\n",
      "epoch: 3 step: 562, loss is 0.3723032474517822\n",
      "epoch: 3 step: 563, loss is 0.2153027355670929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 564, loss is 0.30792027711868286\n",
      "epoch: 3 step: 565, loss is 0.24948309361934662\n",
      "epoch: 3 step: 566, loss is 0.3951968252658844\n",
      "epoch: 3 step: 567, loss is 0.4109402000904083\n",
      "epoch: 3 step: 568, loss is 0.17692802846431732\n",
      "epoch: 3 step: 569, loss is 0.3589220941066742\n",
      "epoch: 3 step: 570, loss is 0.4448026120662689\n",
      "epoch: 3 step: 571, loss is 0.35909706354141235\n",
      "epoch: 3 step: 572, loss is 0.3480193614959717\n",
      "epoch: 3 step: 573, loss is 0.3148958683013916\n",
      "epoch: 3 step: 574, loss is 0.3104117810726166\n",
      "epoch: 3 step: 575, loss is 0.35182246565818787\n",
      "epoch: 3 step: 576, loss is 0.4005732238292694\n",
      "epoch: 3 step: 577, loss is 0.38091418147087097\n",
      "epoch: 3 step: 578, loss is 0.20403848588466644\n",
      "epoch: 3 step: 579, loss is 0.3103158175945282\n",
      "epoch: 3 step: 580, loss is 0.594942569732666\n",
      "epoch: 3 step: 581, loss is 0.2687641680240631\n",
      "epoch: 3 step: 582, loss is 0.34464019536972046\n",
      "epoch: 3 step: 583, loss is 0.32931214570999146\n",
      "epoch: 3 step: 584, loss is 0.3899204134941101\n",
      "epoch: 3 step: 585, loss is 0.2422465980052948\n",
      "epoch: 3 step: 586, loss is 0.537726104259491\n",
      "epoch: 3 step: 587, loss is 0.206471249461174\n",
      "epoch: 3 step: 588, loss is 0.3600611090660095\n",
      "epoch: 3 step: 589, loss is 0.3650766909122467\n",
      "epoch: 3 step: 590, loss is 0.30147573351860046\n",
      "epoch: 3 step: 591, loss is 0.30699020624160767\n",
      "epoch: 3 step: 592, loss is 0.27424174547195435\n",
      "epoch: 3 step: 593, loss is 0.30095580220222473\n",
      "epoch: 3 step: 594, loss is 0.3509361147880554\n",
      "epoch: 3 step: 595, loss is 0.5116115212440491\n",
      "epoch: 3 step: 596, loss is 0.3228533864021301\n",
      "epoch: 3 step: 597, loss is 0.18104985356330872\n",
      "epoch: 3 step: 598, loss is 0.4625696837902069\n",
      "epoch: 3 step: 599, loss is 0.2832591235637665\n",
      "epoch: 3 step: 600, loss is 0.5119706988334656\n",
      "epoch: 3 step: 601, loss is 0.33650192618370056\n",
      "epoch: 3 step: 602, loss is 0.20352308452129364\n",
      "epoch: 3 step: 603, loss is 0.3719869554042816\n",
      "epoch: 3 step: 604, loss is 0.45305579900741577\n",
      "epoch: 3 step: 605, loss is 0.2984161376953125\n",
      "epoch: 3 step: 606, loss is 0.42378342151641846\n",
      "epoch: 3 step: 607, loss is 0.34952443838119507\n",
      "epoch: 3 step: 608, loss is 0.3296718895435333\n",
      "epoch: 3 step: 609, loss is 0.28489822149276733\n",
      "epoch: 3 step: 610, loss is 0.3471218943595886\n",
      "epoch: 3 step: 611, loss is 0.3339332640171051\n",
      "epoch: 3 step: 612, loss is 0.2704964280128479\n",
      "epoch: 3 step: 613, loss is 0.28679969906806946\n",
      "epoch: 3 step: 614, loss is 0.28943461179733276\n",
      "epoch: 3 step: 615, loss is 0.6192247867584229\n",
      "epoch: 3 step: 616, loss is 0.3694015443325043\n",
      "epoch: 3 step: 617, loss is 0.39423736929893494\n",
      "epoch: 3 step: 618, loss is 0.4225391745567322\n",
      "epoch: 3 step: 619, loss is 0.16218307614326477\n",
      "epoch: 3 step: 620, loss is 0.35740774869918823\n",
      "epoch: 3 step: 621, loss is 0.2318248152732849\n",
      "epoch: 3 step: 622, loss is 0.2932228446006775\n",
      "epoch: 3 step: 623, loss is 0.3197092115879059\n",
      "epoch: 3 step: 624, loss is 0.4953475892543793\n",
      "epoch: 3 step: 625, loss is 0.1659490019083023\n",
      "epoch: 3 step: 626, loss is 0.39337438344955444\n",
      "epoch: 3 step: 627, loss is 0.28990229964256287\n",
      "epoch: 3 step: 628, loss is 0.2188415378332138\n",
      "epoch: 3 step: 629, loss is 0.4208400845527649\n",
      "epoch: 3 step: 630, loss is 0.43680697679519653\n",
      "epoch: 3 step: 631, loss is 0.5155226588249207\n",
      "epoch: 3 step: 632, loss is 0.3244926631450653\n",
      "epoch: 3 step: 633, loss is 0.5105571150779724\n",
      "epoch: 3 step: 634, loss is 0.34691697359085083\n",
      "epoch: 3 step: 635, loss is 0.3608419597148895\n",
      "epoch: 3 step: 636, loss is 0.17376163601875305\n",
      "epoch: 3 step: 637, loss is 0.41924890875816345\n",
      "epoch: 3 step: 638, loss is 0.3550971448421478\n",
      "epoch: 3 step: 639, loss is 0.4168359935283661\n",
      "epoch: 3 step: 640, loss is 0.2554994225502014\n",
      "epoch: 3 step: 641, loss is 0.3032313585281372\n",
      "epoch: 3 step: 642, loss is 0.25061336159706116\n",
      "epoch: 3 step: 643, loss is 0.3314847946166992\n",
      "epoch: 3 step: 644, loss is 0.5047720074653625\n",
      "epoch: 3 step: 645, loss is 0.3001236915588379\n",
      "epoch: 3 step: 646, loss is 0.2257082313299179\n",
      "epoch: 3 step: 647, loss is 0.35707834362983704\n",
      "epoch: 3 step: 648, loss is 0.34145408868789673\n",
      "epoch: 3 step: 649, loss is 0.3477778732776642\n",
      "epoch: 3 step: 650, loss is 0.17253045737743378\n",
      "epoch: 3 step: 651, loss is 0.33911675214767456\n",
      "epoch: 3 step: 652, loss is 0.43399885296821594\n",
      "epoch: 3 step: 653, loss is 0.22243644297122955\n",
      "epoch: 3 step: 654, loss is 0.16015887260437012\n",
      "epoch: 3 step: 655, loss is 0.17174148559570312\n",
      "epoch: 3 step: 656, loss is 0.3009154498577118\n",
      "epoch: 3 step: 657, loss is 0.288004606962204\n",
      "epoch: 3 step: 658, loss is 0.2933620810508728\n",
      "epoch: 3 step: 659, loss is 0.49759629368782043\n",
      "epoch: 3 step: 660, loss is 0.4892190098762512\n",
      "epoch: 3 step: 661, loss is 0.2679819166660309\n",
      "epoch: 3 step: 662, loss is 0.3113413155078888\n",
      "epoch: 3 step: 663, loss is 0.3444433808326721\n",
      "epoch: 3 step: 664, loss is 0.5112156867980957\n",
      "epoch: 3 step: 665, loss is 0.30040064454078674\n",
      "epoch: 3 step: 666, loss is 0.2860695719718933\n",
      "epoch: 3 step: 667, loss is 0.24991686642169952\n",
      "epoch: 3 step: 668, loss is 0.2858666181564331\n",
      "epoch: 3 step: 669, loss is 0.449995756149292\n",
      "epoch: 3 step: 670, loss is 0.3289623558521271\n",
      "epoch: 3 step: 671, loss is 0.38562339544296265\n",
      "epoch: 3 step: 672, loss is 0.19545641541481018\n",
      "epoch: 3 step: 673, loss is 0.2896597385406494\n",
      "epoch: 3 step: 674, loss is 0.3818373680114746\n",
      "epoch: 3 step: 675, loss is 0.3569101095199585\n",
      "epoch: 3 step: 676, loss is 0.42920076847076416\n",
      "epoch: 3 step: 677, loss is 0.19723276793956757\n",
      "epoch: 3 step: 678, loss is 0.3085078299045563\n",
      "epoch: 3 step: 679, loss is 0.2962746322154999\n",
      "epoch: 3 step: 680, loss is 0.23057565093040466\n",
      "epoch: 3 step: 681, loss is 0.2683296799659729\n",
      "epoch: 3 step: 682, loss is 0.2771981656551361\n",
      "epoch: 3 step: 683, loss is 0.2662133276462555\n",
      "epoch: 3 step: 684, loss is 0.1971893608570099\n",
      "epoch: 3 step: 685, loss is 0.30676519870758057\n",
      "epoch: 3 step: 686, loss is 0.4832472801208496\n",
      "epoch: 3 step: 687, loss is 0.27037513256073\n",
      "epoch: 3 step: 688, loss is 0.38113850355148315\n",
      "epoch: 3 step: 689, loss is 0.4754916727542877\n",
      "epoch: 3 step: 690, loss is 0.3975106179714203\n",
      "epoch: 3 step: 691, loss is 0.39422881603240967\n",
      "epoch: 3 step: 692, loss is 0.3184545934200287\n",
      "epoch: 3 step: 693, loss is 0.4928036332130432\n",
      "epoch: 3 step: 694, loss is 0.28891924023628235\n",
      "epoch: 3 step: 695, loss is 0.3606388568878174\n",
      "epoch: 3 step: 696, loss is 0.3904631435871124\n",
      "epoch: 3 step: 697, loss is 0.20180438458919525\n",
      "epoch: 3 step: 698, loss is 0.37458229064941406\n",
      "epoch: 3 step: 699, loss is 0.37654730677604675\n",
      "epoch: 3 step: 700, loss is 0.2826886773109436\n",
      "epoch: 3 step: 701, loss is 0.4502491354942322\n",
      "epoch: 3 step: 702, loss is 0.41674473881721497\n",
      "epoch: 3 step: 703, loss is 0.38900670409202576\n",
      "epoch: 3 step: 704, loss is 0.2542622983455658\n",
      "epoch: 3 step: 705, loss is 0.29387348890304565\n",
      "epoch: 3 step: 706, loss is 0.29679226875305176\n",
      "epoch: 3 step: 707, loss is 0.26282981038093567\n",
      "epoch: 3 step: 708, loss is 0.3363904356956482\n",
      "epoch: 3 step: 709, loss is 0.2518197298049927\n",
      "epoch: 3 step: 710, loss is 0.5095893740653992\n",
      "epoch: 3 step: 711, loss is 0.187041237950325\n",
      "epoch: 3 step: 712, loss is 0.25813692808151245\n",
      "epoch: 3 step: 713, loss is 0.34114283323287964\n",
      "epoch: 3 step: 714, loss is 0.26149266958236694\n",
      "epoch: 3 step: 715, loss is 0.3057810366153717\n",
      "epoch: 3 step: 716, loss is 0.16513283550739288\n",
      "epoch: 3 step: 717, loss is 0.27585098147392273\n",
      "epoch: 3 step: 718, loss is 0.4109046459197998\n",
      "epoch: 3 step: 719, loss is 0.1966015249490738\n",
      "epoch: 3 step: 720, loss is 0.37863975763320923\n",
      "epoch: 3 step: 721, loss is 0.2937764823436737\n",
      "epoch: 3 step: 722, loss is 0.18801181018352509\n",
      "epoch: 3 step: 723, loss is 0.49469444155693054\n",
      "epoch: 3 step: 724, loss is 0.25653839111328125\n",
      "epoch: 3 step: 725, loss is 0.24016772210597992\n",
      "epoch: 3 step: 726, loss is 0.2627234160900116\n",
      "epoch: 3 step: 727, loss is 0.32923179864883423\n",
      "epoch: 3 step: 728, loss is 0.3401321768760681\n",
      "epoch: 3 step: 729, loss is 0.22803270816802979\n",
      "epoch: 3 step: 730, loss is 0.4642163813114166\n",
      "epoch: 3 step: 731, loss is 0.35934942960739136\n",
      "epoch: 3 step: 732, loss is 0.13590240478515625\n",
      "epoch: 3 step: 733, loss is 0.3583947718143463\n",
      "epoch: 3 step: 734, loss is 0.3418450355529785\n",
      "epoch: 3 step: 735, loss is 0.3644583225250244\n",
      "epoch: 3 step: 736, loss is 0.2616390287876129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 737, loss is 0.18735307455062866\n",
      "epoch: 3 step: 738, loss is 0.4103342890739441\n",
      "epoch: 3 step: 739, loss is 0.5078719258308411\n",
      "epoch: 3 step: 740, loss is 0.1886492520570755\n",
      "epoch: 3 step: 741, loss is 0.4711654782295227\n",
      "epoch: 3 step: 742, loss is 0.35298481583595276\n",
      "epoch: 3 step: 743, loss is 0.20058634877204895\n",
      "epoch: 3 step: 744, loss is 0.4175543189048767\n",
      "epoch: 3 step: 745, loss is 0.509734570980072\n",
      "epoch: 3 step: 746, loss is 0.19522646069526672\n",
      "epoch: 3 step: 747, loss is 0.3044140636920929\n",
      "epoch: 3 step: 748, loss is 0.33885106444358826\n",
      "epoch: 3 step: 749, loss is 0.2618260979652405\n",
      "epoch: 3 step: 750, loss is 0.2878541052341461\n",
      "epoch: 3 step: 751, loss is 0.27165687084198\n",
      "epoch: 3 step: 752, loss is 0.25503844022750854\n",
      "epoch: 3 step: 753, loss is 0.36863234639167786\n",
      "epoch: 3 step: 754, loss is 0.5617479681968689\n",
      "epoch: 3 step: 755, loss is 0.472552090883255\n",
      "epoch: 3 step: 756, loss is 0.25954580307006836\n",
      "epoch: 3 step: 757, loss is 0.3284464478492737\n",
      "epoch: 3 step: 758, loss is 0.33597075939178467\n",
      "epoch: 3 step: 759, loss is 0.38341599702835083\n",
      "epoch: 3 step: 760, loss is 0.4129645526409149\n",
      "epoch: 3 step: 761, loss is 0.28958266973495483\n",
      "epoch: 3 step: 762, loss is 0.3947094678878784\n",
      "epoch: 3 step: 763, loss is 0.41977494955062866\n",
      "epoch: 3 step: 764, loss is 0.1906116008758545\n",
      "epoch: 3 step: 765, loss is 0.3103308081626892\n",
      "epoch: 3 step: 766, loss is 0.2802625596523285\n",
      "epoch: 3 step: 767, loss is 0.21838496625423431\n",
      "epoch: 3 step: 768, loss is 0.2797880172729492\n",
      "epoch: 3 step: 769, loss is 0.2729925215244293\n",
      "epoch: 3 step: 770, loss is 0.4065234661102295\n",
      "epoch: 3 step: 771, loss is 0.3185814619064331\n",
      "epoch: 3 step: 772, loss is 0.2992810308933258\n",
      "epoch: 3 step: 773, loss is 0.38678178191185\n",
      "epoch: 3 step: 774, loss is 0.2925899624824524\n",
      "epoch: 3 step: 775, loss is 0.39009687304496765\n",
      "epoch: 3 step: 776, loss is 0.261972576379776\n",
      "epoch: 3 step: 777, loss is 0.32151487469673157\n",
      "epoch: 3 step: 778, loss is 0.35149529576301575\n",
      "epoch: 3 step: 779, loss is 0.3723718225955963\n",
      "epoch: 3 step: 780, loss is 0.3165898025035858\n",
      "epoch: 3 step: 781, loss is 0.21500511467456818\n",
      "epoch: 3 step: 782, loss is 0.47312644124031067\n",
      "epoch: 3 step: 783, loss is 0.170162633061409\n",
      "epoch: 3 step: 784, loss is 0.30673426389694214\n",
      "epoch: 3 step: 785, loss is 0.2788180708885193\n",
      "epoch: 3 step: 786, loss is 0.2903769612312317\n",
      "epoch: 3 step: 787, loss is 0.37589722871780396\n",
      "epoch: 3 step: 788, loss is 0.18482579290866852\n",
      "epoch: 3 step: 789, loss is 0.23279555141925812\n",
      "epoch: 3 step: 790, loss is 0.36233392357826233\n",
      "epoch: 3 step: 791, loss is 0.29812055826187134\n",
      "epoch: 3 step: 792, loss is 0.4105338752269745\n",
      "epoch: 3 step: 793, loss is 0.19037777185440063\n",
      "epoch: 3 step: 794, loss is 0.43021973967552185\n",
      "epoch: 3 step: 795, loss is 0.46295177936553955\n",
      "epoch: 3 step: 796, loss is 0.3550616204738617\n",
      "epoch: 3 step: 797, loss is 0.44303932785987854\n",
      "epoch: 3 step: 798, loss is 0.27476805448532104\n",
      "epoch: 3 step: 799, loss is 0.29092079401016235\n",
      "epoch: 3 step: 800, loss is 0.38816726207733154\n",
      "epoch: 3 step: 801, loss is 0.3013926148414612\n",
      "epoch: 3 step: 802, loss is 0.2876356840133667\n",
      "epoch: 3 step: 803, loss is 0.5259204506874084\n",
      "epoch: 3 step: 804, loss is 0.11909816414117813\n",
      "epoch: 3 step: 805, loss is 0.28417477011680603\n",
      "epoch: 3 step: 806, loss is 0.32143133878707886\n",
      "epoch: 3 step: 807, loss is 0.3195928931236267\n",
      "epoch: 3 step: 808, loss is 0.15301865339279175\n",
      "epoch: 3 step: 809, loss is 0.3070198595523834\n",
      "epoch: 3 step: 810, loss is 0.24464912712574005\n",
      "epoch: 3 step: 811, loss is 0.2935802638530731\n",
      "epoch: 3 step: 812, loss is 0.18456043303012848\n",
      "epoch: 3 step: 813, loss is 0.27421170473098755\n",
      "epoch: 3 step: 814, loss is 0.33177027106285095\n",
      "epoch: 3 step: 815, loss is 0.2566429674625397\n",
      "epoch: 3 step: 816, loss is 0.4199346601963043\n",
      "epoch: 3 step: 817, loss is 0.4098692238330841\n",
      "epoch: 3 step: 818, loss is 0.33984997868537903\n",
      "epoch: 3 step: 819, loss is 0.31291699409484863\n",
      "epoch: 3 step: 820, loss is 0.4836738705635071\n",
      "epoch: 3 step: 821, loss is 0.5595584511756897\n",
      "epoch: 3 step: 822, loss is 0.21852323412895203\n",
      "epoch: 3 step: 823, loss is 0.173776775598526\n",
      "epoch: 3 step: 824, loss is 0.29117730259895325\n",
      "epoch: 3 step: 825, loss is 0.24329228699207306\n",
      "epoch: 3 step: 826, loss is 0.19538387656211853\n",
      "epoch: 3 step: 827, loss is 0.24774116277694702\n",
      "epoch: 3 step: 828, loss is 0.23406489193439484\n",
      "epoch: 3 step: 829, loss is 0.39843907952308655\n",
      "epoch: 3 step: 830, loss is 0.3163267970085144\n",
      "epoch: 3 step: 831, loss is 0.2511592209339142\n",
      "epoch: 3 step: 832, loss is 0.43350666761398315\n",
      "epoch: 3 step: 833, loss is 0.32750171422958374\n",
      "epoch: 3 step: 834, loss is 0.3118225038051605\n",
      "epoch: 3 step: 835, loss is 0.23163992166519165\n",
      "epoch: 3 step: 836, loss is 0.2831539511680603\n",
      "epoch: 3 step: 837, loss is 0.18847128748893738\n",
      "epoch: 3 step: 838, loss is 0.335014283657074\n",
      "epoch: 3 step: 839, loss is 0.3708953261375427\n",
      "epoch: 3 step: 840, loss is 0.42223116755485535\n",
      "epoch: 3 step: 841, loss is 0.3099910318851471\n",
      "epoch: 3 step: 842, loss is 0.37852194905281067\n",
      "epoch: 3 step: 843, loss is 0.3009527623653412\n",
      "epoch: 3 step: 844, loss is 0.2549183964729309\n",
      "epoch: 3 step: 845, loss is 0.35103321075439453\n",
      "epoch: 3 step: 846, loss is 0.3069828450679779\n",
      "epoch: 3 step: 847, loss is 0.1655258983373642\n",
      "epoch: 3 step: 848, loss is 0.1695723533630371\n",
      "epoch: 3 step: 849, loss is 0.528512716293335\n",
      "epoch: 3 step: 850, loss is 0.48567309975624084\n",
      "epoch: 3 step: 851, loss is 0.33472299575805664\n",
      "epoch: 3 step: 852, loss is 0.5363208651542664\n",
      "epoch: 3 step: 853, loss is 0.3063981533050537\n",
      "epoch: 3 step: 854, loss is 0.2478092759847641\n",
      "epoch: 3 step: 855, loss is 0.2557365894317627\n",
      "epoch: 3 step: 856, loss is 0.2541692554950714\n",
      "epoch: 3 step: 857, loss is 0.45580315589904785\n",
      "epoch: 3 step: 858, loss is 0.25275102257728577\n",
      "epoch: 3 step: 859, loss is 0.2994677722454071\n",
      "epoch: 3 step: 860, loss is 0.23768579959869385\n",
      "epoch: 3 step: 861, loss is 0.40124526619911194\n",
      "epoch: 3 step: 862, loss is 0.3145153522491455\n",
      "epoch: 3 step: 863, loss is 0.36750367283821106\n",
      "epoch: 3 step: 864, loss is 0.1878986358642578\n",
      "epoch: 3 step: 865, loss is 0.30122560262680054\n",
      "epoch: 3 step: 866, loss is 0.2645168900489807\n",
      "epoch: 3 step: 867, loss is 0.28736281394958496\n",
      "epoch: 3 step: 868, loss is 0.27260881662368774\n",
      "epoch: 3 step: 869, loss is 0.19451864063739777\n",
      "epoch: 3 step: 870, loss is 0.19053559005260468\n",
      "epoch: 3 step: 871, loss is 0.28713494539260864\n",
      "epoch: 3 step: 872, loss is 0.2748394310474396\n",
      "epoch: 3 step: 873, loss is 0.34815722703933716\n",
      "epoch: 3 step: 874, loss is 0.4205154776573181\n",
      "epoch: 3 step: 875, loss is 0.2924085259437561\n",
      "epoch: 3 step: 876, loss is 0.3229202628135681\n",
      "epoch: 3 step: 877, loss is 0.33739665150642395\n",
      "epoch: 3 step: 878, loss is 0.33252811431884766\n",
      "epoch: 3 step: 879, loss is 0.35210514068603516\n",
      "epoch: 3 step: 880, loss is 0.33238330483436584\n",
      "epoch: 3 step: 881, loss is 0.2706540822982788\n",
      "epoch: 3 step: 882, loss is 0.3761323094367981\n",
      "epoch: 3 step: 883, loss is 0.2931971549987793\n",
      "epoch: 3 step: 884, loss is 0.2828839421272278\n",
      "epoch: 3 step: 885, loss is 0.20382218062877655\n",
      "epoch: 3 step: 886, loss is 0.3488544225692749\n",
      "epoch: 3 step: 887, loss is 0.2774173319339752\n",
      "epoch: 3 step: 888, loss is 0.6010104417800903\n",
      "epoch: 3 step: 889, loss is 0.2616281807422638\n",
      "epoch: 3 step: 890, loss is 0.4364315867424011\n",
      "epoch: 3 step: 891, loss is 0.3008705973625183\n",
      "epoch: 3 step: 892, loss is 0.24858078360557556\n",
      "epoch: 3 step: 893, loss is 0.3382893204689026\n",
      "epoch: 3 step: 894, loss is 0.3917381763458252\n",
      "epoch: 3 step: 895, loss is 0.3898645043373108\n",
      "epoch: 3 step: 896, loss is 0.2938239574432373\n",
      "epoch: 3 step: 897, loss is 0.2038949877023697\n",
      "epoch: 3 step: 898, loss is 0.37059494853019714\n",
      "epoch: 3 step: 899, loss is 0.341221421957016\n",
      "epoch: 3 step: 900, loss is 0.371226966381073\n",
      "epoch: 3 step: 901, loss is 0.35499870777130127\n",
      "epoch: 3 step: 902, loss is 0.38215115666389465\n",
      "epoch: 3 step: 903, loss is 0.18138247728347778\n",
      "epoch: 3 step: 904, loss is 0.21066761016845703\n",
      "epoch: 3 step: 905, loss is 0.29274073243141174\n",
      "epoch: 3 step: 906, loss is 0.22431230545043945\n",
      "epoch: 3 step: 907, loss is 0.44697603583335876\n",
      "epoch: 3 step: 908, loss is 0.19427862763404846\n",
      "epoch: 3 step: 909, loss is 0.4057711362838745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 910, loss is 0.14996519684791565\n",
      "epoch: 3 step: 911, loss is 0.2135004997253418\n",
      "epoch: 3 step: 912, loss is 0.328684002161026\n",
      "epoch: 3 step: 913, loss is 0.18346050381660461\n",
      "epoch: 3 step: 914, loss is 0.3493434488773346\n",
      "epoch: 3 step: 915, loss is 0.37822425365448\n",
      "epoch: 3 step: 916, loss is 0.2460351288318634\n",
      "epoch: 3 step: 917, loss is 0.329639732837677\n",
      "epoch: 3 step: 918, loss is 0.5078805685043335\n",
      "epoch: 3 step: 919, loss is 0.1891084611415863\n",
      "epoch: 3 step: 920, loss is 0.2521187961101532\n",
      "epoch: 3 step: 921, loss is 0.3657037317752838\n",
      "epoch: 3 step: 922, loss is 0.3169117569923401\n",
      "epoch: 3 step: 923, loss is 0.43627074360847473\n",
      "epoch: 3 step: 924, loss is 0.3633313775062561\n",
      "epoch: 3 step: 925, loss is 0.1964511126279831\n",
      "epoch: 3 step: 926, loss is 0.18860745429992676\n",
      "epoch: 3 step: 927, loss is 0.18368493020534515\n",
      "epoch: 3 step: 928, loss is 0.3992023766040802\n",
      "epoch: 3 step: 929, loss is 0.32914626598358154\n",
      "epoch: 3 step: 930, loss is 0.34936732053756714\n",
      "epoch: 3 step: 931, loss is 0.414869099855423\n",
      "epoch: 3 step: 932, loss is 0.2802906036376953\n",
      "epoch: 3 step: 933, loss is 0.33536580204963684\n",
      "epoch: 3 step: 934, loss is 0.2609867751598358\n",
      "epoch: 3 step: 935, loss is 0.27370405197143555\n",
      "epoch: 3 step: 936, loss is 0.5244135856628418\n",
      "epoch: 3 step: 937, loss is 0.5015189051628113\n",
      "epoch: 4 step: 1, loss is 0.32440581917762756\n",
      "epoch: 4 step: 2, loss is 0.5332726836204529\n",
      "epoch: 4 step: 3, loss is 0.2920449674129486\n",
      "epoch: 4 step: 4, loss is 0.38149556517601013\n",
      "epoch: 4 step: 5, loss is 0.3097970187664032\n",
      "epoch: 4 step: 6, loss is 0.3064517676830292\n",
      "epoch: 4 step: 7, loss is 0.37542983889579773\n",
      "epoch: 4 step: 8, loss is 0.30681902170181274\n",
      "epoch: 4 step: 9, loss is 0.3229321241378784\n",
      "epoch: 4 step: 10, loss is 0.3783564269542694\n",
      "epoch: 4 step: 11, loss is 0.2974955439567566\n",
      "epoch: 4 step: 12, loss is 0.45000308752059937\n",
      "epoch: 4 step: 13, loss is 0.2035307139158249\n",
      "epoch: 4 step: 14, loss is 0.2966758906841278\n",
      "epoch: 4 step: 15, loss is 0.2620183229446411\n",
      "epoch: 4 step: 16, loss is 0.11163903027772903\n",
      "epoch: 4 step: 17, loss is 0.28206300735473633\n",
      "epoch: 4 step: 18, loss is 0.10231015086174011\n",
      "epoch: 4 step: 19, loss is 0.27803102135658264\n",
      "epoch: 4 step: 20, loss is 0.26060548424720764\n",
      "epoch: 4 step: 21, loss is 0.2175949364900589\n",
      "epoch: 4 step: 22, loss is 0.48242825269699097\n",
      "epoch: 4 step: 23, loss is 0.32484060525894165\n",
      "epoch: 4 step: 24, loss is 0.38229888677597046\n",
      "epoch: 4 step: 25, loss is 0.3955468535423279\n",
      "epoch: 4 step: 26, loss is 0.31856316328048706\n",
      "epoch: 4 step: 27, loss is 0.2891818881034851\n",
      "epoch: 4 step: 28, loss is 0.39211317896842957\n",
      "epoch: 4 step: 29, loss is 0.3118725121021271\n",
      "epoch: 4 step: 30, loss is 0.17195762693881989\n",
      "epoch: 4 step: 31, loss is 0.13233280181884766\n",
      "epoch: 4 step: 32, loss is 0.2602110207080841\n",
      "epoch: 4 step: 33, loss is 0.20520265400409698\n",
      "epoch: 4 step: 34, loss is 0.3494391441345215\n",
      "epoch: 4 step: 35, loss is 0.2270742654800415\n",
      "epoch: 4 step: 36, loss is 0.3745265603065491\n",
      "epoch: 4 step: 37, loss is 0.2907537519931793\n",
      "epoch: 4 step: 38, loss is 0.33990854024887085\n",
      "epoch: 4 step: 39, loss is 0.20915956795215607\n",
      "epoch: 4 step: 40, loss is 0.4009621739387512\n",
      "epoch: 4 step: 41, loss is 0.27780064940452576\n",
      "epoch: 4 step: 42, loss is 0.317813515663147\n",
      "epoch: 4 step: 43, loss is 0.3857330083847046\n",
      "epoch: 4 step: 44, loss is 0.27075710892677307\n",
      "epoch: 4 step: 45, loss is 0.2665235102176666\n",
      "epoch: 4 step: 46, loss is 0.34743374586105347\n",
      "epoch: 4 step: 47, loss is 0.30093249678611755\n",
      "epoch: 4 step: 48, loss is 0.4466768503189087\n",
      "epoch: 4 step: 49, loss is 0.17819662392139435\n",
      "epoch: 4 step: 50, loss is 0.23357336223125458\n",
      "epoch: 4 step: 51, loss is 0.2356141358613968\n",
      "epoch: 4 step: 52, loss is 0.2425576150417328\n",
      "epoch: 4 step: 53, loss is 0.3621145784854889\n",
      "epoch: 4 step: 54, loss is 0.4716465473175049\n",
      "epoch: 4 step: 55, loss is 0.30640265345573425\n",
      "epoch: 4 step: 56, loss is 0.2630680799484253\n",
      "epoch: 4 step: 57, loss is 0.28595170378685\n",
      "epoch: 4 step: 58, loss is 0.46359992027282715\n",
      "epoch: 4 step: 59, loss is 0.38334715366363525\n",
      "epoch: 4 step: 60, loss is 0.38224393129348755\n",
      "epoch: 4 step: 61, loss is 0.3559577763080597\n",
      "epoch: 4 step: 62, loss is 0.46765291690826416\n",
      "epoch: 4 step: 63, loss is 0.45031535625457764\n",
      "epoch: 4 step: 64, loss is 0.17721861600875854\n",
      "epoch: 4 step: 65, loss is 0.38434484601020813\n",
      "epoch: 4 step: 66, loss is 0.3102205693721771\n",
      "epoch: 4 step: 67, loss is 0.4430038332939148\n",
      "epoch: 4 step: 68, loss is 0.2747483551502228\n",
      "epoch: 4 step: 69, loss is 0.3037259578704834\n",
      "epoch: 4 step: 70, loss is 0.29727691411972046\n",
      "epoch: 4 step: 71, loss is 0.2418956160545349\n",
      "epoch: 4 step: 72, loss is 0.38488951325416565\n",
      "epoch: 4 step: 73, loss is 0.295731782913208\n",
      "epoch: 4 step: 74, loss is 0.17948991060256958\n",
      "epoch: 4 step: 75, loss is 0.24591472744941711\n",
      "epoch: 4 step: 76, loss is 0.41138604283332825\n",
      "epoch: 4 step: 77, loss is 0.4214010536670685\n",
      "epoch: 4 step: 78, loss is 0.31528234481811523\n",
      "epoch: 4 step: 79, loss is 0.11343210935592651\n",
      "epoch: 4 step: 80, loss is 0.29373759031295776\n",
      "epoch: 4 step: 81, loss is 0.5204337239265442\n",
      "epoch: 4 step: 82, loss is 0.27603524923324585\n",
      "epoch: 4 step: 83, loss is 0.37519052624702454\n",
      "epoch: 4 step: 84, loss is 0.23693013191223145\n",
      "epoch: 4 step: 85, loss is 0.42283084988594055\n",
      "epoch: 4 step: 86, loss is 0.24105733633041382\n",
      "epoch: 4 step: 87, loss is 0.1915149688720703\n",
      "epoch: 4 step: 88, loss is 0.4041014313697815\n",
      "epoch: 4 step: 89, loss is 0.2915485203266144\n",
      "epoch: 4 step: 90, loss is 0.5068488121032715\n",
      "epoch: 4 step: 91, loss is 0.41433051228523254\n",
      "epoch: 4 step: 92, loss is 0.2859079837799072\n",
      "epoch: 4 step: 93, loss is 0.12490011751651764\n",
      "epoch: 4 step: 94, loss is 0.21629957854747772\n",
      "epoch: 4 step: 95, loss is 0.3407406508922577\n",
      "epoch: 4 step: 96, loss is 0.36042535305023193\n",
      "epoch: 4 step: 97, loss is 0.384299099445343\n",
      "epoch: 4 step: 98, loss is 0.3001735806465149\n",
      "epoch: 4 step: 99, loss is 0.24829061329364777\n",
      "epoch: 4 step: 100, loss is 0.2988344430923462\n",
      "epoch: 4 step: 101, loss is 0.3580077290534973\n",
      "epoch: 4 step: 102, loss is 0.285003662109375\n",
      "epoch: 4 step: 103, loss is 0.2621252238750458\n",
      "epoch: 4 step: 104, loss is 0.24835579097270966\n",
      "epoch: 4 step: 105, loss is 0.23486024141311646\n",
      "epoch: 4 step: 106, loss is 0.3133407235145569\n",
      "epoch: 4 step: 107, loss is 0.3472259044647217\n",
      "epoch: 4 step: 108, loss is 0.2700459361076355\n",
      "epoch: 4 step: 109, loss is 0.3755677342414856\n",
      "epoch: 4 step: 110, loss is 0.3401572108268738\n",
      "epoch: 4 step: 111, loss is 0.16579847037792206\n",
      "epoch: 4 step: 112, loss is 0.12712661921977997\n",
      "epoch: 4 step: 113, loss is 0.2905988097190857\n",
      "epoch: 4 step: 114, loss is 0.1940869688987732\n",
      "epoch: 4 step: 115, loss is 0.24647702276706696\n",
      "epoch: 4 step: 116, loss is 0.23344546556472778\n",
      "epoch: 4 step: 117, loss is 0.2543933391571045\n",
      "epoch: 4 step: 118, loss is 0.20184430480003357\n",
      "epoch: 4 step: 119, loss is 0.38971802592277527\n",
      "epoch: 4 step: 120, loss is 0.33907070755958557\n",
      "epoch: 4 step: 121, loss is 0.33691298961639404\n",
      "epoch: 4 step: 122, loss is 0.39148828387260437\n",
      "epoch: 4 step: 123, loss is 0.2574027180671692\n",
      "epoch: 4 step: 124, loss is 0.15602636337280273\n",
      "epoch: 4 step: 125, loss is 0.3134020268917084\n",
      "epoch: 4 step: 126, loss is 0.21769389510154724\n",
      "epoch: 4 step: 127, loss is 0.4361143410205841\n",
      "epoch: 4 step: 128, loss is 0.2851158380508423\n",
      "epoch: 4 step: 129, loss is 0.34761613607406616\n",
      "epoch: 4 step: 130, loss is 0.30997544527053833\n",
      "epoch: 4 step: 131, loss is 0.2219592183828354\n",
      "epoch: 4 step: 132, loss is 0.2183978110551834\n",
      "epoch: 4 step: 133, loss is 0.19656097888946533\n",
      "epoch: 4 step: 134, loss is 0.2102869153022766\n",
      "epoch: 4 step: 135, loss is 0.2486003041267395\n",
      "epoch: 4 step: 136, loss is 0.28323355317115784\n",
      "epoch: 4 step: 137, loss is 0.3230222165584564\n",
      "epoch: 4 step: 138, loss is 0.2259574681520462\n",
      "epoch: 4 step: 139, loss is 0.19175313413143158\n",
      "epoch: 4 step: 140, loss is 0.21286429464817047\n",
      "epoch: 4 step: 141, loss is 0.3486338257789612\n",
      "epoch: 4 step: 142, loss is 0.1932568997144699\n",
      "epoch: 4 step: 143, loss is 0.2860133945941925\n",
      "epoch: 4 step: 144, loss is 0.2982213795185089\n",
      "epoch: 4 step: 145, loss is 0.42592597007751465\n",
      "epoch: 4 step: 146, loss is 0.38070741295814514\n",
      "epoch: 4 step: 147, loss is 0.26209262013435364\n",
      "epoch: 4 step: 148, loss is 0.33218637108802795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 149, loss is 0.4594081938266754\n",
      "epoch: 4 step: 150, loss is 0.2015649825334549\n",
      "epoch: 4 step: 151, loss is 0.3284342586994171\n",
      "epoch: 4 step: 152, loss is 0.1836741715669632\n",
      "epoch: 4 step: 153, loss is 0.49282994866371155\n",
      "epoch: 4 step: 154, loss is 0.3264402151107788\n",
      "epoch: 4 step: 155, loss is 0.4004000723361969\n",
      "epoch: 4 step: 156, loss is 0.37820523977279663\n",
      "epoch: 4 step: 157, loss is 0.5047531127929688\n",
      "epoch: 4 step: 158, loss is 0.35960930585861206\n",
      "epoch: 4 step: 159, loss is 0.22085867822170258\n",
      "epoch: 4 step: 160, loss is 0.2529674768447876\n",
      "epoch: 4 step: 161, loss is 0.2516072988510132\n",
      "epoch: 4 step: 162, loss is 0.19054628908634186\n",
      "epoch: 4 step: 163, loss is 0.3918745219707489\n",
      "epoch: 4 step: 164, loss is 0.4230611026287079\n",
      "epoch: 4 step: 165, loss is 0.584324061870575\n",
      "epoch: 4 step: 166, loss is 0.42512062191963196\n",
      "epoch: 4 step: 167, loss is 0.2711329162120819\n",
      "epoch: 4 step: 168, loss is 0.25073450803756714\n",
      "epoch: 4 step: 169, loss is 0.1438521444797516\n",
      "epoch: 4 step: 170, loss is 0.1813131868839264\n",
      "epoch: 4 step: 171, loss is 0.34697937965393066\n",
      "epoch: 4 step: 172, loss is 0.21765410900115967\n",
      "epoch: 4 step: 173, loss is 0.2495812177658081\n",
      "epoch: 4 step: 174, loss is 0.3514471650123596\n",
      "epoch: 4 step: 175, loss is 0.26477161049842834\n",
      "epoch: 4 step: 176, loss is 0.21114182472229004\n",
      "epoch: 4 step: 177, loss is 0.2159488946199417\n",
      "epoch: 4 step: 178, loss is 0.3870580792427063\n",
      "epoch: 4 step: 179, loss is 0.3245922923088074\n",
      "epoch: 4 step: 180, loss is 0.2534298002719879\n",
      "epoch: 4 step: 181, loss is 0.3476848006248474\n",
      "epoch: 4 step: 182, loss is 0.3486000895500183\n",
      "epoch: 4 step: 183, loss is 0.4089180529117584\n",
      "epoch: 4 step: 184, loss is 0.2645108997821808\n",
      "epoch: 4 step: 185, loss is 0.2726139724254608\n",
      "epoch: 4 step: 186, loss is 0.360838383436203\n",
      "epoch: 4 step: 187, loss is 0.43744441866874695\n",
      "epoch: 4 step: 188, loss is 0.3425288796424866\n",
      "epoch: 4 step: 189, loss is 0.24258266389369965\n",
      "epoch: 4 step: 190, loss is 0.35209810733795166\n",
      "epoch: 4 step: 191, loss is 0.23338940739631653\n",
      "epoch: 4 step: 192, loss is 0.421188622713089\n",
      "epoch: 4 step: 193, loss is 0.26906564831733704\n",
      "epoch: 4 step: 194, loss is 0.3097582459449768\n",
      "epoch: 4 step: 195, loss is 0.3164495527744293\n",
      "epoch: 4 step: 196, loss is 0.23762404918670654\n",
      "epoch: 4 step: 197, loss is 0.23509952425956726\n",
      "epoch: 4 step: 198, loss is 0.4067051410675049\n",
      "epoch: 4 step: 199, loss is 0.1831594705581665\n",
      "epoch: 4 step: 200, loss is 0.3528752624988556\n",
      "epoch: 4 step: 201, loss is 0.29424580931663513\n",
      "epoch: 4 step: 202, loss is 0.4593373239040375\n",
      "epoch: 4 step: 203, loss is 0.2658858001232147\n",
      "epoch: 4 step: 204, loss is 0.22038698196411133\n",
      "epoch: 4 step: 205, loss is 0.2218988686800003\n",
      "epoch: 4 step: 206, loss is 0.23880749940872192\n",
      "epoch: 4 step: 207, loss is 0.2950406074523926\n",
      "epoch: 4 step: 208, loss is 0.2285981923341751\n",
      "epoch: 4 step: 209, loss is 0.2632676064968109\n",
      "epoch: 4 step: 210, loss is 0.2860850691795349\n",
      "epoch: 4 step: 211, loss is 0.32804927229881287\n",
      "epoch: 4 step: 212, loss is 0.35347306728363037\n",
      "epoch: 4 step: 213, loss is 0.2738187611103058\n",
      "epoch: 4 step: 214, loss is 0.3057868778705597\n",
      "epoch: 4 step: 215, loss is 0.17884178459644318\n",
      "epoch: 4 step: 216, loss is 0.19437851011753082\n",
      "epoch: 4 step: 217, loss is 0.3094154894351959\n",
      "epoch: 4 step: 218, loss is 0.19148169457912445\n",
      "epoch: 4 step: 219, loss is 0.32259997725486755\n",
      "epoch: 4 step: 220, loss is 0.21629196405410767\n",
      "epoch: 4 step: 221, loss is 0.28494343161582947\n",
      "epoch: 4 step: 222, loss is 0.42682933807373047\n",
      "epoch: 4 step: 223, loss is 0.2628733515739441\n",
      "epoch: 4 step: 224, loss is 0.3469710350036621\n",
      "epoch: 4 step: 225, loss is 0.32431674003601074\n",
      "epoch: 4 step: 226, loss is 0.20677776634693146\n",
      "epoch: 4 step: 227, loss is 0.2966737747192383\n",
      "epoch: 4 step: 228, loss is 0.17614783346652985\n",
      "epoch: 4 step: 229, loss is 0.1930747777223587\n",
      "epoch: 4 step: 230, loss is 0.31983986496925354\n",
      "epoch: 4 step: 231, loss is 0.20744124054908752\n",
      "epoch: 4 step: 232, loss is 0.32995402812957764\n",
      "epoch: 4 step: 233, loss is 0.30947965383529663\n",
      "epoch: 4 step: 234, loss is 0.31894686818122864\n",
      "epoch: 4 step: 235, loss is 0.27947884798049927\n",
      "epoch: 4 step: 236, loss is 0.26168301701545715\n",
      "epoch: 4 step: 237, loss is 0.48597532510757446\n",
      "epoch: 4 step: 238, loss is 0.36509308218955994\n",
      "epoch: 4 step: 239, loss is 0.28569310903549194\n",
      "epoch: 4 step: 240, loss is 0.35549595952033997\n",
      "epoch: 4 step: 241, loss is 0.25148430466651917\n",
      "epoch: 4 step: 242, loss is 0.22762373089790344\n",
      "epoch: 4 step: 243, loss is 0.2668166160583496\n",
      "epoch: 4 step: 244, loss is 0.12896834313869476\n",
      "epoch: 4 step: 245, loss is 0.14025099575519562\n",
      "epoch: 4 step: 246, loss is 0.2563200294971466\n",
      "epoch: 4 step: 247, loss is 0.47225093841552734\n",
      "epoch: 4 step: 248, loss is 0.24999113380908966\n",
      "epoch: 4 step: 249, loss is 0.20886285603046417\n",
      "epoch: 4 step: 250, loss is 0.3747199475765228\n",
      "epoch: 4 step: 251, loss is 0.24707354605197906\n",
      "epoch: 4 step: 252, loss is 0.30050328373908997\n",
      "epoch: 4 step: 253, loss is 0.18677648901939392\n",
      "epoch: 4 step: 254, loss is 0.1820453554391861\n",
      "epoch: 4 step: 255, loss is 0.37097230553627014\n",
      "epoch: 4 step: 256, loss is 0.2947534918785095\n",
      "epoch: 4 step: 257, loss is 0.2498844712972641\n",
      "epoch: 4 step: 258, loss is 0.62502521276474\n",
      "epoch: 4 step: 259, loss is 0.34820449352264404\n",
      "epoch: 4 step: 260, loss is 0.25319263339042664\n",
      "epoch: 4 step: 261, loss is 0.2953905761241913\n",
      "epoch: 4 step: 262, loss is 0.20037846267223358\n",
      "epoch: 4 step: 263, loss is 0.25446808338165283\n",
      "epoch: 4 step: 264, loss is 0.3473502993583679\n",
      "epoch: 4 step: 265, loss is 0.1961168646812439\n",
      "epoch: 4 step: 266, loss is 0.2576877176761627\n",
      "epoch: 4 step: 267, loss is 0.2300843894481659\n",
      "epoch: 4 step: 268, loss is 0.4520275592803955\n",
      "epoch: 4 step: 269, loss is 0.191648930311203\n",
      "epoch: 4 step: 270, loss is 0.6451323628425598\n",
      "epoch: 4 step: 271, loss is 0.20532195270061493\n",
      "epoch: 4 step: 272, loss is 0.20202979445457458\n",
      "epoch: 4 step: 273, loss is 0.35261863470077515\n",
      "epoch: 4 step: 274, loss is 0.17719325423240662\n",
      "epoch: 4 step: 275, loss is 0.17612092196941376\n",
      "epoch: 4 step: 276, loss is 0.637092113494873\n",
      "epoch: 4 step: 277, loss is 0.35899507999420166\n",
      "epoch: 4 step: 278, loss is 0.4156925678253174\n",
      "epoch: 4 step: 279, loss is 0.22074224054813385\n",
      "epoch: 4 step: 280, loss is 0.24833659827709198\n",
      "epoch: 4 step: 281, loss is 0.21048328280448914\n",
      "epoch: 4 step: 282, loss is 0.17246660590171814\n",
      "epoch: 4 step: 283, loss is 0.20645259320735931\n",
      "epoch: 4 step: 284, loss is 0.2570562958717346\n",
      "epoch: 4 step: 285, loss is 0.31145569682121277\n",
      "epoch: 4 step: 286, loss is 0.343835711479187\n",
      "epoch: 4 step: 287, loss is 0.21364492177963257\n",
      "epoch: 4 step: 288, loss is 0.3591989576816559\n",
      "epoch: 4 step: 289, loss is 0.42615512013435364\n",
      "epoch: 4 step: 290, loss is 0.3189374506473541\n",
      "epoch: 4 step: 291, loss is 0.314196914434433\n",
      "epoch: 4 step: 292, loss is 0.23193304240703583\n",
      "epoch: 4 step: 293, loss is 0.43196263909339905\n",
      "epoch: 4 step: 294, loss is 0.368855357170105\n",
      "epoch: 4 step: 295, loss is 0.262686163187027\n",
      "epoch: 4 step: 296, loss is 0.21266411244869232\n",
      "epoch: 4 step: 297, loss is 0.2576826214790344\n",
      "epoch: 4 step: 298, loss is 0.22180777788162231\n",
      "epoch: 4 step: 299, loss is 0.41071194410324097\n",
      "epoch: 4 step: 300, loss is 0.3152739405632019\n",
      "epoch: 4 step: 301, loss is 0.3270803391933441\n",
      "epoch: 4 step: 302, loss is 0.3612368106842041\n",
      "epoch: 4 step: 303, loss is 0.22329528629779816\n",
      "epoch: 4 step: 304, loss is 0.30228716135025024\n",
      "epoch: 4 step: 305, loss is 0.2493317574262619\n",
      "epoch: 4 step: 306, loss is 0.3482254147529602\n",
      "epoch: 4 step: 307, loss is 0.3164741098880768\n",
      "epoch: 4 step: 308, loss is 0.30075812339782715\n",
      "epoch: 4 step: 309, loss is 0.29686522483825684\n",
      "epoch: 4 step: 310, loss is 0.22561204433441162\n",
      "epoch: 4 step: 311, loss is 0.238064706325531\n",
      "epoch: 4 step: 312, loss is 0.16992178559303284\n",
      "epoch: 4 step: 313, loss is 0.17601925134658813\n",
      "epoch: 4 step: 314, loss is 0.30956506729125977\n",
      "epoch: 4 step: 315, loss is 0.46101683378219604\n",
      "epoch: 4 step: 316, loss is 0.33943745493888855\n",
      "epoch: 4 step: 317, loss is 0.2278808355331421\n",
      "epoch: 4 step: 318, loss is 0.24135726690292358\n",
      "epoch: 4 step: 319, loss is 0.17425213754177094\n",
      "epoch: 4 step: 320, loss is 0.24796929955482483\n",
      "epoch: 4 step: 321, loss is 0.32628145813941956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 322, loss is 0.2727571427822113\n",
      "epoch: 4 step: 323, loss is 0.3298380374908447\n",
      "epoch: 4 step: 324, loss is 0.2043389081954956\n",
      "epoch: 4 step: 325, loss is 0.5854949951171875\n",
      "epoch: 4 step: 326, loss is 0.2525472342967987\n",
      "epoch: 4 step: 327, loss is 0.18343015015125275\n",
      "epoch: 4 step: 328, loss is 0.4278421700000763\n",
      "epoch: 4 step: 329, loss is 0.3425677716732025\n",
      "epoch: 4 step: 330, loss is 0.6058075428009033\n",
      "epoch: 4 step: 331, loss is 0.5105103850364685\n",
      "epoch: 4 step: 332, loss is 0.3646988272666931\n",
      "epoch: 4 step: 333, loss is 0.30494409799575806\n",
      "epoch: 4 step: 334, loss is 0.5810359716415405\n",
      "epoch: 4 step: 335, loss is 0.3565882742404938\n",
      "epoch: 4 step: 336, loss is 0.42793479561805725\n",
      "epoch: 4 step: 337, loss is 0.12620232999324799\n",
      "epoch: 4 step: 338, loss is 0.2786051332950592\n",
      "epoch: 4 step: 339, loss is 0.2835036516189575\n",
      "epoch: 4 step: 340, loss is 0.19205069541931152\n",
      "epoch: 4 step: 341, loss is 0.5288515686988831\n",
      "epoch: 4 step: 342, loss is 0.32006558775901794\n",
      "epoch: 4 step: 343, loss is 0.2285599410533905\n",
      "epoch: 4 step: 344, loss is 0.3707908093929291\n",
      "epoch: 4 step: 345, loss is 0.30691471695899963\n",
      "epoch: 4 step: 346, loss is 0.4189590811729431\n",
      "epoch: 4 step: 347, loss is 0.2858642637729645\n",
      "epoch: 4 step: 348, loss is 0.4832572937011719\n",
      "epoch: 4 step: 349, loss is 0.25840502977371216\n",
      "epoch: 4 step: 350, loss is 0.19265961647033691\n",
      "epoch: 4 step: 351, loss is 0.2621580958366394\n",
      "epoch: 4 step: 352, loss is 0.19133451581001282\n",
      "epoch: 4 step: 353, loss is 0.34099462628364563\n",
      "epoch: 4 step: 354, loss is 0.38758930563926697\n",
      "epoch: 4 step: 355, loss is 0.20161817967891693\n",
      "epoch: 4 step: 356, loss is 0.4013769030570984\n",
      "epoch: 4 step: 357, loss is 0.2875898480415344\n",
      "epoch: 4 step: 358, loss is 0.37415871024131775\n",
      "epoch: 4 step: 359, loss is 0.35862648487091064\n",
      "epoch: 4 step: 360, loss is 0.20653694868087769\n",
      "epoch: 4 step: 361, loss is 0.4554249048233032\n",
      "epoch: 4 step: 362, loss is 0.15859486162662506\n",
      "epoch: 4 step: 363, loss is 0.23519080877304077\n",
      "epoch: 4 step: 364, loss is 0.3057863116264343\n",
      "epoch: 4 step: 365, loss is 0.3726442754268646\n",
      "epoch: 4 step: 366, loss is 0.25676196813583374\n",
      "epoch: 4 step: 367, loss is 0.29220208525657654\n",
      "epoch: 4 step: 368, loss is 0.28621646761894226\n",
      "epoch: 4 step: 369, loss is 0.3210844099521637\n",
      "epoch: 4 step: 370, loss is 0.2731221914291382\n",
      "epoch: 4 step: 371, loss is 0.3427853286266327\n",
      "epoch: 4 step: 372, loss is 0.40450167655944824\n",
      "epoch: 4 step: 373, loss is 0.31873321533203125\n",
      "epoch: 4 step: 374, loss is 0.4044466018676758\n",
      "epoch: 4 step: 375, loss is 0.2809545397758484\n",
      "epoch: 4 step: 376, loss is 0.17821957170963287\n",
      "epoch: 4 step: 377, loss is 0.27396735548973083\n",
      "epoch: 4 step: 378, loss is 0.33189964294433594\n",
      "epoch: 4 step: 379, loss is 0.20248357951641083\n",
      "epoch: 4 step: 380, loss is 0.4719724655151367\n",
      "epoch: 4 step: 381, loss is 0.28588470816612244\n",
      "epoch: 4 step: 382, loss is 0.17800851166248322\n",
      "epoch: 4 step: 383, loss is 0.20496393740177155\n",
      "epoch: 4 step: 384, loss is 0.40486037731170654\n",
      "epoch: 4 step: 385, loss is 0.2343025654554367\n",
      "epoch: 4 step: 386, loss is 0.37397271394729614\n",
      "epoch: 4 step: 387, loss is 0.14633430540561676\n",
      "epoch: 4 step: 388, loss is 0.38860100507736206\n",
      "epoch: 4 step: 389, loss is 0.3533390462398529\n",
      "epoch: 4 step: 390, loss is 0.5643837451934814\n",
      "epoch: 4 step: 391, loss is 0.2428445816040039\n",
      "epoch: 4 step: 392, loss is 0.2951596677303314\n",
      "epoch: 4 step: 393, loss is 0.23194840550422668\n",
      "epoch: 4 step: 394, loss is 0.4915417730808258\n",
      "epoch: 4 step: 395, loss is 0.3224160373210907\n",
      "epoch: 4 step: 396, loss is 0.2343662977218628\n",
      "epoch: 4 step: 397, loss is 0.3090618848800659\n",
      "epoch: 4 step: 398, loss is 0.5640872120857239\n",
      "epoch: 4 step: 399, loss is 0.23202745616436005\n",
      "epoch: 4 step: 400, loss is 0.19331927597522736\n",
      "epoch: 4 step: 401, loss is 0.25595787167549133\n",
      "epoch: 4 step: 402, loss is 0.40565383434295654\n",
      "epoch: 4 step: 403, loss is 0.28039976954460144\n",
      "epoch: 4 step: 404, loss is 0.2190890908241272\n",
      "epoch: 4 step: 405, loss is 0.20393595099449158\n",
      "epoch: 4 step: 406, loss is 0.359062135219574\n",
      "epoch: 4 step: 407, loss is 0.49331915378570557\n",
      "epoch: 4 step: 408, loss is 0.30138781666755676\n",
      "epoch: 4 step: 409, loss is 0.4229159355163574\n",
      "epoch: 4 step: 410, loss is 0.18264566361904144\n",
      "epoch: 4 step: 411, loss is 0.31934407353401184\n",
      "epoch: 4 step: 412, loss is 0.34959155321121216\n",
      "epoch: 4 step: 413, loss is 0.23380398750305176\n",
      "epoch: 4 step: 414, loss is 0.2808976173400879\n",
      "epoch: 4 step: 415, loss is 0.3215324878692627\n",
      "epoch: 4 step: 416, loss is 0.2098081409931183\n",
      "epoch: 4 step: 417, loss is 0.2250269502401352\n",
      "epoch: 4 step: 418, loss is 0.2742811143398285\n",
      "epoch: 4 step: 419, loss is 0.2896779179573059\n",
      "epoch: 4 step: 420, loss is 0.25970250368118286\n",
      "epoch: 4 step: 421, loss is 0.38159239292144775\n",
      "epoch: 4 step: 422, loss is 0.28997963666915894\n",
      "epoch: 4 step: 423, loss is 0.24235883355140686\n",
      "epoch: 4 step: 424, loss is 0.3066527545452118\n",
      "epoch: 4 step: 425, loss is 0.26366761326789856\n",
      "epoch: 4 step: 426, loss is 0.20833386480808258\n",
      "epoch: 4 step: 427, loss is 0.27243152260780334\n",
      "epoch: 4 step: 428, loss is 0.27031809091567993\n",
      "epoch: 4 step: 429, loss is 0.2207416594028473\n",
      "epoch: 4 step: 430, loss is 0.13338303565979004\n",
      "epoch: 4 step: 431, loss is 0.4331270456314087\n",
      "epoch: 4 step: 432, loss is 0.31954923272132874\n",
      "epoch: 4 step: 433, loss is 0.33014005422592163\n",
      "epoch: 4 step: 434, loss is 0.3260876536369324\n",
      "epoch: 4 step: 435, loss is 0.4088914394378662\n",
      "epoch: 4 step: 436, loss is 0.33981752395629883\n",
      "epoch: 4 step: 437, loss is 0.3125249147415161\n",
      "epoch: 4 step: 438, loss is 0.49703478813171387\n",
      "epoch: 4 step: 439, loss is 0.35286930203437805\n",
      "epoch: 4 step: 440, loss is 0.37253761291503906\n",
      "epoch: 4 step: 441, loss is 0.3139025866985321\n",
      "epoch: 4 step: 442, loss is 0.24164405465126038\n",
      "epoch: 4 step: 443, loss is 0.3829103410243988\n",
      "epoch: 4 step: 444, loss is 0.19317381083965302\n",
      "epoch: 4 step: 445, loss is 0.2680232524871826\n",
      "epoch: 4 step: 446, loss is 0.1993355005979538\n",
      "epoch: 4 step: 447, loss is 0.5197266340255737\n",
      "epoch: 4 step: 448, loss is 0.2880319058895111\n",
      "epoch: 4 step: 449, loss is 0.3350221514701843\n",
      "epoch: 4 step: 450, loss is 0.2722385823726654\n",
      "epoch: 4 step: 451, loss is 0.375679075717926\n",
      "epoch: 4 step: 452, loss is 0.4035358130931854\n",
      "epoch: 4 step: 453, loss is 0.29375991225242615\n",
      "epoch: 4 step: 454, loss is 0.2637515366077423\n",
      "epoch: 4 step: 455, loss is 0.36416617035865784\n",
      "epoch: 4 step: 456, loss is 0.3509253263473511\n",
      "epoch: 4 step: 457, loss is 0.3979118764400482\n",
      "epoch: 4 step: 458, loss is 0.3643983006477356\n",
      "epoch: 4 step: 459, loss is 0.31946495175361633\n",
      "epoch: 4 step: 460, loss is 0.2651257812976837\n",
      "epoch: 4 step: 461, loss is 0.3291732966899872\n",
      "epoch: 4 step: 462, loss is 0.2975386679172516\n",
      "epoch: 4 step: 463, loss is 0.4978540539741516\n",
      "epoch: 4 step: 464, loss is 0.2621139585971832\n",
      "epoch: 4 step: 465, loss is 0.48934534192085266\n",
      "epoch: 4 step: 466, loss is 0.3059481382369995\n",
      "epoch: 4 step: 467, loss is 0.4295153319835663\n",
      "epoch: 4 step: 468, loss is 0.44120195508003235\n",
      "epoch: 4 step: 469, loss is 0.346711665391922\n",
      "epoch: 4 step: 470, loss is 0.14504913985729218\n",
      "epoch: 4 step: 471, loss is 0.42558807134628296\n",
      "epoch: 4 step: 472, loss is 0.2333383411169052\n",
      "epoch: 4 step: 473, loss is 0.3006371557712555\n",
      "epoch: 4 step: 474, loss is 0.3281259834766388\n",
      "epoch: 4 step: 475, loss is 0.30134502053260803\n",
      "epoch: 4 step: 476, loss is 0.3167594075202942\n",
      "epoch: 4 step: 477, loss is 0.29818102717399597\n",
      "epoch: 4 step: 478, loss is 0.28095564246177673\n",
      "epoch: 4 step: 479, loss is 0.27342936396598816\n",
      "epoch: 4 step: 480, loss is 0.3393353819847107\n",
      "epoch: 4 step: 481, loss is 0.24408960342407227\n",
      "epoch: 4 step: 482, loss is 0.30480584502220154\n",
      "epoch: 4 step: 483, loss is 0.41433462500572205\n",
      "epoch: 4 step: 484, loss is 0.42423179745674133\n",
      "epoch: 4 step: 485, loss is 0.38182440400123596\n",
      "epoch: 4 step: 486, loss is 0.2528856098651886\n",
      "epoch: 4 step: 487, loss is 0.3470429480075836\n",
      "epoch: 4 step: 488, loss is 0.23966951668262482\n",
      "epoch: 4 step: 489, loss is 0.2794536352157593\n",
      "epoch: 4 step: 490, loss is 0.43873220682144165\n",
      "epoch: 4 step: 491, loss is 0.5524719953536987\n",
      "epoch: 4 step: 492, loss is 0.16123729944229126\n",
      "epoch: 4 step: 493, loss is 0.23358488082885742\n",
      "epoch: 4 step: 494, loss is 0.24761074781417847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 495, loss is 0.43267714977264404\n",
      "epoch: 4 step: 496, loss is 0.15409135818481445\n",
      "epoch: 4 step: 497, loss is 0.23554983735084534\n",
      "epoch: 4 step: 498, loss is 0.26656442880630493\n",
      "epoch: 4 step: 499, loss is 0.2824877202510834\n",
      "epoch: 4 step: 500, loss is 0.3233281970024109\n",
      "epoch: 4 step: 501, loss is 0.23141206800937653\n",
      "epoch: 4 step: 502, loss is 0.3253532350063324\n",
      "epoch: 4 step: 503, loss is 0.38113704323768616\n",
      "epoch: 4 step: 504, loss is 0.3609831631183624\n",
      "epoch: 4 step: 505, loss is 0.4044967293739319\n",
      "epoch: 4 step: 506, loss is 0.435028612613678\n",
      "epoch: 4 step: 507, loss is 0.24818205833435059\n",
      "epoch: 4 step: 508, loss is 0.38367003202438354\n",
      "epoch: 4 step: 509, loss is 0.2632783353328705\n",
      "epoch: 4 step: 510, loss is 0.3305470645427704\n",
      "epoch: 4 step: 511, loss is 0.3266236484050751\n",
      "epoch: 4 step: 512, loss is 0.2412426471710205\n",
      "epoch: 4 step: 513, loss is 0.2601301968097687\n",
      "epoch: 4 step: 514, loss is 0.27928614616394043\n",
      "epoch: 4 step: 515, loss is 0.33925536274909973\n",
      "epoch: 4 step: 516, loss is 0.2246592789888382\n",
      "epoch: 4 step: 517, loss is 0.31890347599983215\n",
      "epoch: 4 step: 518, loss is 0.3818051815032959\n",
      "epoch: 4 step: 519, loss is 0.30088844895362854\n",
      "epoch: 4 step: 520, loss is 0.31076887249946594\n",
      "epoch: 4 step: 521, loss is 0.27883487939834595\n",
      "epoch: 4 step: 522, loss is 0.33736783266067505\n",
      "epoch: 4 step: 523, loss is 0.319929838180542\n",
      "epoch: 4 step: 524, loss is 0.38258928060531616\n",
      "epoch: 4 step: 525, loss is 0.2913849353790283\n",
      "epoch: 4 step: 526, loss is 0.48168715834617615\n",
      "epoch: 4 step: 527, loss is 0.34978339076042175\n",
      "epoch: 4 step: 528, loss is 0.42879244685173035\n",
      "epoch: 4 step: 529, loss is 0.36852964758872986\n",
      "epoch: 4 step: 530, loss is 0.16680985689163208\n",
      "epoch: 4 step: 531, loss is 0.4876541793346405\n",
      "epoch: 4 step: 532, loss is 0.20781312882900238\n",
      "epoch: 4 step: 533, loss is 0.40821003913879395\n",
      "epoch: 4 step: 534, loss is 0.4876590371131897\n",
      "epoch: 4 step: 535, loss is 0.37676525115966797\n",
      "epoch: 4 step: 536, loss is 0.21392934024333954\n",
      "epoch: 4 step: 537, loss is 0.3058199882507324\n",
      "epoch: 4 step: 538, loss is 0.33384063839912415\n",
      "epoch: 4 step: 539, loss is 0.3931528925895691\n",
      "epoch: 4 step: 540, loss is 0.20812708139419556\n",
      "epoch: 4 step: 541, loss is 0.4321334958076477\n",
      "epoch: 4 step: 542, loss is 0.33240634202957153\n",
      "epoch: 4 step: 543, loss is 0.26401445269584656\n",
      "epoch: 4 step: 544, loss is 0.204330712556839\n",
      "epoch: 4 step: 545, loss is 0.31926167011260986\n",
      "epoch: 4 step: 546, loss is 0.2917654812335968\n",
      "epoch: 4 step: 547, loss is 0.24357597529888153\n",
      "epoch: 4 step: 548, loss is 0.15861155092716217\n",
      "epoch: 4 step: 549, loss is 0.3513619899749756\n",
      "epoch: 4 step: 550, loss is 0.352318674325943\n",
      "epoch: 4 step: 551, loss is 0.23071753978729248\n",
      "epoch: 4 step: 552, loss is 0.2302839457988739\n",
      "epoch: 4 step: 553, loss is 0.6025490164756775\n",
      "epoch: 4 step: 554, loss is 0.4951062798500061\n",
      "epoch: 4 step: 555, loss is 0.2658744156360626\n",
      "epoch: 4 step: 556, loss is 0.15498825907707214\n",
      "epoch: 4 step: 557, loss is 0.35731691122055054\n",
      "epoch: 4 step: 558, loss is 0.16173425316810608\n",
      "epoch: 4 step: 559, loss is 0.33650124073028564\n",
      "epoch: 4 step: 560, loss is 0.33204078674316406\n",
      "epoch: 4 step: 561, loss is 0.14166465401649475\n",
      "epoch: 4 step: 562, loss is 0.22427019476890564\n",
      "epoch: 4 step: 563, loss is 0.26106807589530945\n",
      "epoch: 4 step: 564, loss is 0.3400074541568756\n",
      "epoch: 4 step: 565, loss is 0.3175497353076935\n",
      "epoch: 4 step: 566, loss is 0.20253786444664001\n",
      "epoch: 4 step: 567, loss is 0.39035674929618835\n",
      "epoch: 4 step: 568, loss is 0.38143211603164673\n",
      "epoch: 4 step: 569, loss is 0.2889149785041809\n",
      "epoch: 4 step: 570, loss is 0.2980795204639435\n",
      "epoch: 4 step: 571, loss is 0.2485114187002182\n",
      "epoch: 4 step: 572, loss is 0.3124798834323883\n",
      "epoch: 4 step: 573, loss is 0.3528110384941101\n",
      "epoch: 4 step: 574, loss is 0.38359397649765015\n",
      "epoch: 4 step: 575, loss is 0.32397520542144775\n",
      "epoch: 4 step: 576, loss is 0.20502060651779175\n",
      "epoch: 4 step: 577, loss is 0.2730763554573059\n",
      "epoch: 4 step: 578, loss is 0.20826922357082367\n",
      "epoch: 4 step: 579, loss is 0.26414787769317627\n",
      "epoch: 4 step: 580, loss is 0.40876504778862\n",
      "epoch: 4 step: 581, loss is 0.21470309793949127\n",
      "epoch: 4 step: 582, loss is 0.35454243421554565\n",
      "epoch: 4 step: 583, loss is 0.294737309217453\n",
      "epoch: 4 step: 584, loss is 0.16411910951137543\n",
      "epoch: 4 step: 585, loss is 0.28520599007606506\n",
      "epoch: 4 step: 586, loss is 0.2501690089702606\n",
      "epoch: 4 step: 587, loss is 0.47571226954460144\n",
      "epoch: 4 step: 588, loss is 0.11067348718643188\n",
      "epoch: 4 step: 589, loss is 0.17210271954536438\n",
      "epoch: 4 step: 590, loss is 0.45735323429107666\n",
      "epoch: 4 step: 591, loss is 0.29252690076828003\n",
      "epoch: 4 step: 592, loss is 0.34006911516189575\n",
      "epoch: 4 step: 593, loss is 0.2916412353515625\n",
      "epoch: 4 step: 594, loss is 0.24287566542625427\n",
      "epoch: 4 step: 595, loss is 0.27659013867378235\n",
      "epoch: 4 step: 596, loss is 0.306965708732605\n",
      "epoch: 4 step: 597, loss is 0.31749626994132996\n",
      "epoch: 4 step: 598, loss is 0.3450677990913391\n",
      "epoch: 4 step: 599, loss is 0.18899944424629211\n",
      "epoch: 4 step: 600, loss is 0.33665478229522705\n",
      "epoch: 4 step: 601, loss is 0.3458108603954315\n",
      "epoch: 4 step: 602, loss is 0.19518780708312988\n",
      "epoch: 4 step: 603, loss is 0.285605788230896\n",
      "epoch: 4 step: 604, loss is 0.35226431488990784\n",
      "epoch: 4 step: 605, loss is 0.49688786268234253\n",
      "epoch: 4 step: 606, loss is 0.25078094005584717\n",
      "epoch: 4 step: 607, loss is 0.3376106321811676\n",
      "epoch: 4 step: 608, loss is 0.47260281443595886\n",
      "epoch: 4 step: 609, loss is 0.21541811525821686\n",
      "epoch: 4 step: 610, loss is 0.23339922726154327\n",
      "epoch: 4 step: 611, loss is 0.2161010056734085\n",
      "epoch: 4 step: 612, loss is 0.3891158998012543\n",
      "epoch: 4 step: 613, loss is 0.38303762674331665\n",
      "epoch: 4 step: 614, loss is 0.2319371998310089\n",
      "epoch: 4 step: 615, loss is 0.24313615262508392\n",
      "epoch: 4 step: 616, loss is 0.1690557599067688\n",
      "epoch: 4 step: 617, loss is 0.36500614881515503\n",
      "epoch: 4 step: 618, loss is 0.18860435485839844\n",
      "epoch: 4 step: 619, loss is 0.3839339315891266\n",
      "epoch: 4 step: 620, loss is 0.3517552316188812\n",
      "epoch: 4 step: 621, loss is 0.42565295100212097\n",
      "epoch: 4 step: 622, loss is 0.45418646931648254\n",
      "epoch: 4 step: 623, loss is 0.17823737859725952\n",
      "epoch: 4 step: 624, loss is 0.19375155866146088\n",
      "epoch: 4 step: 625, loss is 0.2941589653491974\n",
      "epoch: 4 step: 626, loss is 0.2802947163581848\n",
      "epoch: 4 step: 627, loss is 0.24325348436832428\n",
      "epoch: 4 step: 628, loss is 0.32722458243370056\n",
      "epoch: 4 step: 629, loss is 0.28717800974845886\n",
      "epoch: 4 step: 630, loss is 0.1750268191099167\n",
      "epoch: 4 step: 631, loss is 0.2915395200252533\n",
      "epoch: 4 step: 632, loss is 0.35416334867477417\n",
      "epoch: 4 step: 633, loss is 0.4829772710800171\n",
      "epoch: 4 step: 634, loss is 0.2032528966665268\n",
      "epoch: 4 step: 635, loss is 0.2501077950000763\n",
      "epoch: 4 step: 636, loss is 0.21842215955257416\n",
      "epoch: 4 step: 637, loss is 0.39172542095184326\n",
      "epoch: 4 step: 638, loss is 0.4072559177875519\n",
      "epoch: 4 step: 639, loss is 0.2968805432319641\n",
      "epoch: 4 step: 640, loss is 0.5692477226257324\n",
      "epoch: 4 step: 641, loss is 0.5235385894775391\n",
      "epoch: 4 step: 642, loss is 0.2685057520866394\n",
      "epoch: 4 step: 643, loss is 0.2805878520011902\n",
      "epoch: 4 step: 644, loss is 0.3318840265274048\n",
      "epoch: 4 step: 645, loss is 0.21495535969734192\n",
      "epoch: 4 step: 646, loss is 0.3436073362827301\n",
      "epoch: 4 step: 647, loss is 0.377025306224823\n",
      "epoch: 4 step: 648, loss is 0.3434678614139557\n",
      "epoch: 4 step: 649, loss is 0.33532413840293884\n",
      "epoch: 4 step: 650, loss is 0.6100422739982605\n",
      "epoch: 4 step: 651, loss is 0.2356431633234024\n",
      "epoch: 4 step: 652, loss is 0.3701189458370209\n",
      "epoch: 4 step: 653, loss is 0.5212491750717163\n",
      "epoch: 4 step: 654, loss is 0.40774768590927124\n",
      "epoch: 4 step: 655, loss is 0.22164364159107208\n",
      "epoch: 4 step: 656, loss is 0.407884806394577\n",
      "epoch: 4 step: 657, loss is 0.3136107623577118\n",
      "epoch: 4 step: 658, loss is 0.30891236662864685\n",
      "epoch: 4 step: 659, loss is 0.3676670491695404\n",
      "epoch: 4 step: 660, loss is 0.19139066338539124\n",
      "epoch: 4 step: 661, loss is 0.27191266417503357\n",
      "epoch: 4 step: 662, loss is 0.6326426267623901\n",
      "epoch: 4 step: 663, loss is 0.24652104079723358\n",
      "epoch: 4 step: 664, loss is 0.38631001114845276\n",
      "epoch: 4 step: 665, loss is 0.43232181668281555\n",
      "epoch: 4 step: 666, loss is 0.38760536909103394\n",
      "epoch: 4 step: 667, loss is 0.21619227528572083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 668, loss is 0.3093714118003845\n",
      "epoch: 4 step: 669, loss is 0.2689692974090576\n",
      "epoch: 4 step: 670, loss is 0.2826887369155884\n",
      "epoch: 4 step: 671, loss is 0.22977904975414276\n",
      "epoch: 4 step: 672, loss is 0.29143455624580383\n",
      "epoch: 4 step: 673, loss is 0.17224973440170288\n",
      "epoch: 4 step: 674, loss is 0.22500494122505188\n",
      "epoch: 4 step: 675, loss is 0.2594369053840637\n",
      "epoch: 4 step: 676, loss is 0.3850959539413452\n",
      "epoch: 4 step: 677, loss is 0.28591832518577576\n",
      "epoch: 4 step: 678, loss is 0.44587385654449463\n",
      "epoch: 4 step: 679, loss is 0.2507525682449341\n",
      "epoch: 4 step: 680, loss is 0.2532302737236023\n",
      "epoch: 4 step: 681, loss is 0.3771146833896637\n",
      "epoch: 4 step: 682, loss is 0.38268229365348816\n",
      "epoch: 4 step: 683, loss is 0.22950664162635803\n",
      "epoch: 4 step: 684, loss is 0.3660602569580078\n",
      "epoch: 4 step: 685, loss is 0.2313184291124344\n",
      "epoch: 4 step: 686, loss is 0.20430323481559753\n",
      "epoch: 4 step: 687, loss is 0.22567841410636902\n",
      "epoch: 4 step: 688, loss is 0.37712526321411133\n",
      "epoch: 4 step: 689, loss is 0.1738847941160202\n",
      "epoch: 4 step: 690, loss is 0.22551412880420685\n",
      "epoch: 4 step: 691, loss is 0.29530027508735657\n",
      "epoch: 4 step: 692, loss is 0.21579977869987488\n",
      "epoch: 4 step: 693, loss is 0.17935879528522491\n",
      "epoch: 4 step: 694, loss is 0.4387807250022888\n",
      "epoch: 4 step: 695, loss is 0.19340406358242035\n",
      "epoch: 4 step: 696, loss is 0.23259827494621277\n",
      "epoch: 4 step: 697, loss is 0.12077149003744125\n",
      "epoch: 4 step: 698, loss is 0.10177507996559143\n",
      "epoch: 4 step: 699, loss is 0.20965805649757385\n",
      "epoch: 4 step: 700, loss is 0.2404908388853073\n",
      "epoch: 4 step: 701, loss is 0.1460181027650833\n",
      "epoch: 4 step: 702, loss is 0.2877293527126312\n",
      "epoch: 4 step: 703, loss is 0.3354557156562805\n",
      "epoch: 4 step: 704, loss is 0.20481812953948975\n",
      "epoch: 4 step: 705, loss is 0.2484043389558792\n",
      "epoch: 4 step: 706, loss is 0.395131379365921\n",
      "epoch: 4 step: 707, loss is 0.2896310091018677\n",
      "epoch: 4 step: 708, loss is 0.29976117610931396\n",
      "epoch: 4 step: 709, loss is 0.3179432451725006\n",
      "epoch: 4 step: 710, loss is 0.2741091549396515\n",
      "epoch: 4 step: 711, loss is 0.15109789371490479\n",
      "epoch: 4 step: 712, loss is 0.3773690164089203\n",
      "epoch: 4 step: 713, loss is 0.2456391155719757\n",
      "epoch: 4 step: 714, loss is 0.3291475474834442\n",
      "epoch: 4 step: 715, loss is 0.2965914011001587\n",
      "epoch: 4 step: 716, loss is 0.23706559836864471\n",
      "epoch: 4 step: 717, loss is 0.24160362780094147\n",
      "epoch: 4 step: 718, loss is 0.4454750716686249\n",
      "epoch: 4 step: 719, loss is 0.40266960859298706\n",
      "epoch: 4 step: 720, loss is 0.29542267322540283\n",
      "epoch: 4 step: 721, loss is 0.32711029052734375\n",
      "epoch: 4 step: 722, loss is 0.35215261578559875\n",
      "epoch: 4 step: 723, loss is 0.35402217507362366\n",
      "epoch: 4 step: 724, loss is 0.16718506813049316\n",
      "epoch: 4 step: 725, loss is 0.15910865366458893\n",
      "epoch: 4 step: 726, loss is 0.21738409996032715\n",
      "epoch: 4 step: 727, loss is 0.43170103430747986\n",
      "epoch: 4 step: 728, loss is 0.14563220739364624\n",
      "epoch: 4 step: 729, loss is 0.30895984172821045\n",
      "epoch: 4 step: 730, loss is 0.2090027779340744\n",
      "epoch: 4 step: 731, loss is 0.3285260796546936\n",
      "epoch: 4 step: 732, loss is 0.2702637314796448\n",
      "epoch: 4 step: 733, loss is 0.21866166591644287\n",
      "epoch: 4 step: 734, loss is 0.2654268741607666\n",
      "epoch: 4 step: 735, loss is 0.20599214732646942\n",
      "epoch: 4 step: 736, loss is 0.32136139273643494\n",
      "epoch: 4 step: 737, loss is 0.3325083553791046\n",
      "epoch: 4 step: 738, loss is 0.19483903050422668\n",
      "epoch: 4 step: 739, loss is 0.3227464258670807\n",
      "epoch: 4 step: 740, loss is 0.3342099189758301\n",
      "epoch: 4 step: 741, loss is 0.18097159266471863\n",
      "epoch: 4 step: 742, loss is 0.40100035071372986\n",
      "epoch: 4 step: 743, loss is 0.35875239968299866\n",
      "epoch: 4 step: 744, loss is 0.24712927639484406\n",
      "epoch: 4 step: 745, loss is 0.3118683993816376\n",
      "epoch: 4 step: 746, loss is 0.3087849020957947\n",
      "epoch: 4 step: 747, loss is 0.35873866081237793\n",
      "epoch: 4 step: 748, loss is 0.4147985577583313\n",
      "epoch: 4 step: 749, loss is 0.44666919112205505\n",
      "epoch: 4 step: 750, loss is 0.22677335143089294\n",
      "epoch: 4 step: 751, loss is 0.27031436562538147\n",
      "epoch: 4 step: 752, loss is 0.21329694986343384\n",
      "epoch: 4 step: 753, loss is 0.45255985856056213\n",
      "epoch: 4 step: 754, loss is 0.22536224126815796\n",
      "epoch: 4 step: 755, loss is 0.3867245316505432\n",
      "epoch: 4 step: 756, loss is 0.2747085988521576\n",
      "epoch: 4 step: 757, loss is 0.405087947845459\n",
      "epoch: 4 step: 758, loss is 0.2128719836473465\n",
      "epoch: 4 step: 759, loss is 0.2576712369918823\n",
      "epoch: 4 step: 760, loss is 0.37531229853630066\n",
      "epoch: 4 step: 761, loss is 0.21431472897529602\n",
      "epoch: 4 step: 762, loss is 0.1655687689781189\n",
      "epoch: 4 step: 763, loss is 0.2708323299884796\n",
      "epoch: 4 step: 764, loss is 0.3649635910987854\n",
      "epoch: 4 step: 765, loss is 0.20182769000530243\n",
      "epoch: 4 step: 766, loss is 0.3123061954975128\n",
      "epoch: 4 step: 767, loss is 0.194497212767601\n",
      "epoch: 4 step: 768, loss is 0.4167339503765106\n",
      "epoch: 4 step: 769, loss is 0.24821291863918304\n",
      "epoch: 4 step: 770, loss is 0.26408150792121887\n",
      "epoch: 4 step: 771, loss is 0.5016552209854126\n",
      "epoch: 4 step: 772, loss is 0.1837075650691986\n",
      "epoch: 4 step: 773, loss is 0.3042440116405487\n",
      "epoch: 4 step: 774, loss is 0.2903575003147125\n",
      "epoch: 4 step: 775, loss is 0.25930023193359375\n",
      "epoch: 4 step: 776, loss is 0.2730594575405121\n",
      "epoch: 4 step: 777, loss is 0.1331644058227539\n",
      "epoch: 4 step: 778, loss is 0.27739417552948\n",
      "epoch: 4 step: 779, loss is 0.31554415822029114\n",
      "epoch: 4 step: 780, loss is 0.4370820224285126\n",
      "epoch: 4 step: 781, loss is 0.23538081347942352\n",
      "epoch: 4 step: 782, loss is 0.3000950217247009\n",
      "epoch: 4 step: 783, loss is 0.22428834438323975\n",
      "epoch: 4 step: 784, loss is 0.2648685872554779\n",
      "epoch: 4 step: 785, loss is 0.20819315314292908\n",
      "epoch: 4 step: 786, loss is 0.280216783285141\n",
      "epoch: 4 step: 787, loss is 0.28729739785194397\n",
      "epoch: 4 step: 788, loss is 0.26854363083839417\n",
      "epoch: 4 step: 789, loss is 0.38821613788604736\n",
      "epoch: 4 step: 790, loss is 0.34393247961997986\n",
      "epoch: 4 step: 791, loss is 0.21225722134113312\n",
      "epoch: 4 step: 792, loss is 0.2019709050655365\n",
      "epoch: 4 step: 793, loss is 0.2434656172990799\n",
      "epoch: 4 step: 794, loss is 0.24540027976036072\n",
      "epoch: 4 step: 795, loss is 0.3054913580417633\n",
      "epoch: 4 step: 796, loss is 0.16805973649024963\n",
      "epoch: 4 step: 797, loss is 0.3180258572101593\n",
      "epoch: 4 step: 798, loss is 0.21597059071063995\n",
      "epoch: 4 step: 799, loss is 0.4074917435646057\n",
      "epoch: 4 step: 800, loss is 0.4307980537414551\n",
      "epoch: 4 step: 801, loss is 0.21061992645263672\n",
      "epoch: 4 step: 802, loss is 0.3644024133682251\n",
      "epoch: 4 step: 803, loss is 0.518062949180603\n",
      "epoch: 4 step: 804, loss is 0.23397037386894226\n",
      "epoch: 4 step: 805, loss is 0.4464991092681885\n",
      "epoch: 4 step: 806, loss is 0.21229135990142822\n",
      "epoch: 4 step: 807, loss is 0.31434130668640137\n",
      "epoch: 4 step: 808, loss is 0.2716042995452881\n",
      "epoch: 4 step: 809, loss is 0.33421704173088074\n",
      "epoch: 4 step: 810, loss is 0.1758269965648651\n",
      "epoch: 4 step: 811, loss is 0.2877865135669708\n",
      "epoch: 4 step: 812, loss is 0.3641715347766876\n",
      "epoch: 4 step: 813, loss is 0.41094329953193665\n",
      "epoch: 4 step: 814, loss is 0.1687902808189392\n",
      "epoch: 4 step: 815, loss is 0.3049919903278351\n",
      "epoch: 4 step: 816, loss is 0.5400267839431763\n",
      "epoch: 4 step: 817, loss is 0.23097723722457886\n",
      "epoch: 4 step: 818, loss is 0.20898117125034332\n",
      "epoch: 4 step: 819, loss is 0.33152031898498535\n",
      "epoch: 4 step: 820, loss is 0.16785679757595062\n",
      "epoch: 4 step: 821, loss is 0.3936179280281067\n",
      "epoch: 4 step: 822, loss is 0.2014528065919876\n",
      "epoch: 4 step: 823, loss is 0.2733304798603058\n",
      "epoch: 4 step: 824, loss is 0.2879250645637512\n",
      "epoch: 4 step: 825, loss is 0.3720298111438751\n",
      "epoch: 4 step: 826, loss is 0.21270695328712463\n",
      "epoch: 4 step: 827, loss is 0.20426273345947266\n",
      "epoch: 4 step: 828, loss is 0.334093302488327\n",
      "epoch: 4 step: 829, loss is 0.25642579793930054\n",
      "epoch: 4 step: 830, loss is 0.26529911160469055\n",
      "epoch: 4 step: 831, loss is 0.21528056263923645\n",
      "epoch: 4 step: 832, loss is 0.25990304350852966\n",
      "epoch: 4 step: 833, loss is 0.18320173025131226\n",
      "epoch: 4 step: 834, loss is 0.3246777355670929\n",
      "epoch: 4 step: 835, loss is 0.27874502539634705\n",
      "epoch: 4 step: 836, loss is 0.30774742364883423\n",
      "epoch: 4 step: 837, loss is 0.18764999508857727\n",
      "epoch: 4 step: 838, loss is 0.5136256217956543\n",
      "epoch: 4 step: 839, loss is 0.20833651721477509\n",
      "epoch: 4 step: 840, loss is 0.31323352456092834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 841, loss is 0.4382418990135193\n",
      "epoch: 4 step: 842, loss is 0.466104656457901\n",
      "epoch: 4 step: 843, loss is 0.3627440631389618\n",
      "epoch: 4 step: 844, loss is 0.11873463541269302\n",
      "epoch: 4 step: 845, loss is 0.19561925530433655\n",
      "epoch: 4 step: 846, loss is 0.5021103620529175\n",
      "epoch: 4 step: 847, loss is 0.19409799575805664\n",
      "epoch: 4 step: 848, loss is 0.2988431751728058\n",
      "epoch: 4 step: 849, loss is 0.3616444766521454\n",
      "epoch: 4 step: 850, loss is 0.1588088870048523\n",
      "epoch: 4 step: 851, loss is 0.24455326795578003\n",
      "epoch: 4 step: 852, loss is 0.23801599442958832\n",
      "epoch: 4 step: 853, loss is 0.2821873128414154\n",
      "epoch: 4 step: 854, loss is 0.1728438287973404\n",
      "epoch: 4 step: 855, loss is 0.35320693254470825\n",
      "epoch: 4 step: 856, loss is 0.4113214313983917\n",
      "epoch: 4 step: 857, loss is 0.29141947627067566\n",
      "epoch: 4 step: 858, loss is 0.21964377164840698\n",
      "epoch: 4 step: 859, loss is 0.32651013135910034\n",
      "epoch: 4 step: 860, loss is 0.2381737381219864\n",
      "epoch: 4 step: 861, loss is 0.3115319609642029\n",
      "epoch: 4 step: 862, loss is 0.26016005873680115\n",
      "epoch: 4 step: 863, loss is 0.4079444408416748\n",
      "epoch: 4 step: 864, loss is 0.34911298751831055\n",
      "epoch: 4 step: 865, loss is 0.21832971274852753\n",
      "epoch: 4 step: 866, loss is 0.45898330211639404\n",
      "epoch: 4 step: 867, loss is 0.20969533920288086\n",
      "epoch: 4 step: 868, loss is 0.28814569115638733\n",
      "epoch: 4 step: 869, loss is 0.4896884560585022\n",
      "epoch: 4 step: 870, loss is 0.3186308741569519\n",
      "epoch: 4 step: 871, loss is 0.3072894513607025\n",
      "epoch: 4 step: 872, loss is 0.25963279604911804\n",
      "epoch: 4 step: 873, loss is 0.4266149401664734\n",
      "epoch: 4 step: 874, loss is 0.28332576155662537\n",
      "epoch: 4 step: 875, loss is 0.1769612580537796\n",
      "epoch: 4 step: 876, loss is 0.37754470109939575\n",
      "epoch: 4 step: 877, loss is 0.23686037957668304\n",
      "epoch: 4 step: 878, loss is 0.1464652568101883\n",
      "epoch: 4 step: 879, loss is 0.2714906632900238\n",
      "epoch: 4 step: 880, loss is 0.22222411632537842\n",
      "epoch: 4 step: 881, loss is 0.3414483368396759\n",
      "epoch: 4 step: 882, loss is 0.31687402725219727\n",
      "epoch: 4 step: 883, loss is 0.34226059913635254\n",
      "epoch: 4 step: 884, loss is 0.41842707991600037\n",
      "epoch: 4 step: 885, loss is 0.4601753056049347\n",
      "epoch: 4 step: 886, loss is 0.15372595191001892\n",
      "epoch: 4 step: 887, loss is 0.16479507088661194\n",
      "epoch: 4 step: 888, loss is 0.299606591463089\n",
      "epoch: 4 step: 889, loss is 0.35846713185310364\n",
      "epoch: 4 step: 890, loss is 0.146675243973732\n",
      "epoch: 4 step: 891, loss is 0.25666239857673645\n",
      "epoch: 4 step: 892, loss is 0.17937630414962769\n",
      "epoch: 4 step: 893, loss is 0.4105144441127777\n",
      "epoch: 4 step: 894, loss is 0.22886736690998077\n",
      "epoch: 4 step: 895, loss is 0.3023185431957245\n",
      "epoch: 4 step: 896, loss is 0.27564388513565063\n",
      "epoch: 4 step: 897, loss is 0.2395104616880417\n",
      "epoch: 4 step: 898, loss is 0.19298963248729706\n",
      "epoch: 4 step: 899, loss is 0.17221751809120178\n",
      "epoch: 4 step: 900, loss is 0.2360629141330719\n",
      "epoch: 4 step: 901, loss is 0.34344005584716797\n",
      "epoch: 4 step: 902, loss is 0.3624909520149231\n",
      "epoch: 4 step: 903, loss is 0.26089242100715637\n",
      "epoch: 4 step: 904, loss is 0.206291064620018\n",
      "epoch: 4 step: 905, loss is 0.21880905330181122\n",
      "epoch: 4 step: 906, loss is 0.13646066188812256\n",
      "epoch: 4 step: 907, loss is 0.281823992729187\n",
      "epoch: 4 step: 908, loss is 0.4009575843811035\n",
      "epoch: 4 step: 909, loss is 0.17628046870231628\n",
      "epoch: 4 step: 910, loss is 0.21010933816432953\n",
      "epoch: 4 step: 911, loss is 0.21129514276981354\n",
      "epoch: 4 step: 912, loss is 0.3948659300804138\n",
      "epoch: 4 step: 913, loss is 0.3205258548259735\n",
      "epoch: 4 step: 914, loss is 0.39546653628349304\n",
      "epoch: 4 step: 915, loss is 0.22689220309257507\n",
      "epoch: 4 step: 916, loss is 0.19016996026039124\n",
      "epoch: 4 step: 917, loss is 0.3382706642150879\n",
      "epoch: 4 step: 918, loss is 0.3375173807144165\n",
      "epoch: 4 step: 919, loss is 0.34207049012184143\n",
      "epoch: 4 step: 920, loss is 0.2870734632015228\n",
      "epoch: 4 step: 921, loss is 0.13368719816207886\n",
      "epoch: 4 step: 922, loss is 0.35351690649986267\n",
      "epoch: 4 step: 923, loss is 0.2627907395362854\n",
      "epoch: 4 step: 924, loss is 0.47370246052742004\n",
      "epoch: 4 step: 925, loss is 0.17972597479820251\n",
      "epoch: 4 step: 926, loss is 0.2514105439186096\n",
      "epoch: 4 step: 927, loss is 0.27010831236839294\n",
      "epoch: 4 step: 928, loss is 0.210702046751976\n",
      "epoch: 4 step: 929, loss is 0.23697756230831146\n",
      "epoch: 4 step: 930, loss is 0.559962272644043\n",
      "epoch: 4 step: 931, loss is 0.28659212589263916\n",
      "epoch: 4 step: 932, loss is 0.21343772113323212\n",
      "epoch: 4 step: 933, loss is 0.3867233693599701\n",
      "epoch: 4 step: 934, loss is 0.4036615192890167\n",
      "epoch: 4 step: 935, loss is 0.30173560976982117\n",
      "epoch: 4 step: 936, loss is 0.27833476662635803\n",
      "epoch: 4 step: 937, loss is 0.22353114187717438\n",
      "epoch: 5 step: 1, loss is 0.3877360224723816\n",
      "epoch: 5 step: 2, loss is 0.46604079008102417\n",
      "epoch: 5 step: 3, loss is 0.16324518620967865\n",
      "epoch: 5 step: 4, loss is 0.2735028862953186\n",
      "epoch: 5 step: 5, loss is 0.30445826053619385\n",
      "epoch: 5 step: 6, loss is 0.3971048891544342\n",
      "epoch: 5 step: 7, loss is 0.19851791858673096\n",
      "epoch: 5 step: 8, loss is 0.30751660466194153\n",
      "epoch: 5 step: 9, loss is 0.19860029220581055\n",
      "epoch: 5 step: 10, loss is 0.3864089250564575\n",
      "epoch: 5 step: 11, loss is 0.31990164518356323\n",
      "epoch: 5 step: 12, loss is 0.36075153946876526\n",
      "epoch: 5 step: 13, loss is 0.45319512486457825\n",
      "epoch: 5 step: 14, loss is 0.30881670117378235\n",
      "epoch: 5 step: 15, loss is 0.18783551454544067\n",
      "epoch: 5 step: 16, loss is 0.16402526199817657\n",
      "epoch: 5 step: 17, loss is 0.1620485782623291\n",
      "epoch: 5 step: 18, loss is 0.2670042812824249\n",
      "epoch: 5 step: 19, loss is 0.29036980867385864\n",
      "epoch: 5 step: 20, loss is 0.18257509171962738\n",
      "epoch: 5 step: 21, loss is 0.3282640278339386\n",
      "epoch: 5 step: 22, loss is 0.22248299419879913\n",
      "epoch: 5 step: 23, loss is 0.2698061168193817\n",
      "epoch: 5 step: 24, loss is 0.34806039929389954\n",
      "epoch: 5 step: 25, loss is 0.3362314999103546\n",
      "epoch: 5 step: 26, loss is 0.1706501841545105\n",
      "epoch: 5 step: 27, loss is 0.3583866059780121\n",
      "epoch: 5 step: 28, loss is 0.150680273771286\n",
      "epoch: 5 step: 29, loss is 0.22625991702079773\n",
      "epoch: 5 step: 30, loss is 0.2954958379268646\n",
      "epoch: 5 step: 31, loss is 0.2108078896999359\n",
      "epoch: 5 step: 32, loss is 0.16946204006671906\n",
      "epoch: 5 step: 33, loss is 0.3053085505962372\n",
      "epoch: 5 step: 34, loss is 0.30906644463539124\n",
      "epoch: 5 step: 35, loss is 0.2640089690685272\n",
      "epoch: 5 step: 36, loss is 0.31179916858673096\n",
      "epoch: 5 step: 37, loss is 0.3369702100753784\n",
      "epoch: 5 step: 38, loss is 0.1744687557220459\n",
      "epoch: 5 step: 39, loss is 0.2549965977668762\n",
      "epoch: 5 step: 40, loss is 0.4033386707305908\n",
      "epoch: 5 step: 41, loss is 0.20639409124851227\n",
      "epoch: 5 step: 42, loss is 0.12312141060829163\n",
      "epoch: 5 step: 43, loss is 0.12687063217163086\n",
      "epoch: 5 step: 44, loss is 0.1870807707309723\n",
      "epoch: 5 step: 45, loss is 0.2474348545074463\n",
      "epoch: 5 step: 46, loss is 0.2722010910511017\n",
      "epoch: 5 step: 47, loss is 0.3921718895435333\n",
      "epoch: 5 step: 48, loss is 0.35515594482421875\n",
      "epoch: 5 step: 49, loss is 0.21519535779953003\n",
      "epoch: 5 step: 50, loss is 0.35195809602737427\n",
      "epoch: 5 step: 51, loss is 0.21226267516613007\n",
      "epoch: 5 step: 52, loss is 0.3591059744358063\n",
      "epoch: 5 step: 53, loss is 0.4044276475906372\n",
      "epoch: 5 step: 54, loss is 0.20533500611782074\n",
      "epoch: 5 step: 55, loss is 0.44010332226753235\n",
      "epoch: 5 step: 56, loss is 0.29156702756881714\n",
      "epoch: 5 step: 57, loss is 0.21816012263298035\n",
      "epoch: 5 step: 58, loss is 0.2442941665649414\n",
      "epoch: 5 step: 59, loss is 0.1945059448480606\n",
      "epoch: 5 step: 60, loss is 0.2253415733575821\n",
      "epoch: 5 step: 61, loss is 0.3519049882888794\n",
      "epoch: 5 step: 62, loss is 0.2217983454465866\n",
      "epoch: 5 step: 63, loss is 0.24152883887290955\n",
      "epoch: 5 step: 64, loss is 0.2909431755542755\n",
      "epoch: 5 step: 65, loss is 0.186955526471138\n",
      "epoch: 5 step: 66, loss is 0.12190095335245132\n",
      "epoch: 5 step: 67, loss is 0.2477884590625763\n",
      "epoch: 5 step: 68, loss is 0.24517259001731873\n",
      "epoch: 5 step: 69, loss is 0.3071514964103699\n",
      "epoch: 5 step: 70, loss is 0.17116767168045044\n",
      "epoch: 5 step: 71, loss is 0.22846414148807526\n",
      "epoch: 5 step: 72, loss is 0.21612313389778137\n",
      "epoch: 5 step: 73, loss is 0.2768757939338684\n",
      "epoch: 5 step: 74, loss is 0.4557795524597168\n",
      "epoch: 5 step: 75, loss is 0.2502518594264984\n",
      "epoch: 5 step: 76, loss is 0.25589221715927124\n",
      "epoch: 5 step: 77, loss is 0.2795364260673523\n",
      "epoch: 5 step: 78, loss is 0.21282222867012024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 79, loss is 0.2843163013458252\n",
      "epoch: 5 step: 80, loss is 0.3401626944541931\n",
      "epoch: 5 step: 81, loss is 0.37253934144973755\n",
      "epoch: 5 step: 82, loss is 0.24927420914173126\n",
      "epoch: 5 step: 83, loss is 0.3033277094364166\n",
      "epoch: 5 step: 84, loss is 0.2230941355228424\n",
      "epoch: 5 step: 85, loss is 0.2364571988582611\n",
      "epoch: 5 step: 86, loss is 0.1392773538827896\n",
      "epoch: 5 step: 87, loss is 0.2528080642223358\n",
      "epoch: 5 step: 88, loss is 0.2751740515232086\n",
      "epoch: 5 step: 89, loss is 0.2758752703666687\n",
      "epoch: 5 step: 90, loss is 0.3446830213069916\n",
      "epoch: 5 step: 91, loss is 0.2714076042175293\n",
      "epoch: 5 step: 92, loss is 0.24398981034755707\n",
      "epoch: 5 step: 93, loss is 0.2409992516040802\n",
      "epoch: 5 step: 94, loss is 0.19718989729881287\n",
      "epoch: 5 step: 95, loss is 0.25060924887657166\n",
      "epoch: 5 step: 96, loss is 0.23341584205627441\n",
      "epoch: 5 step: 97, loss is 0.3341614603996277\n",
      "epoch: 5 step: 98, loss is 0.2991800010204315\n",
      "epoch: 5 step: 99, loss is 0.398720920085907\n",
      "epoch: 5 step: 100, loss is 0.29894447326660156\n",
      "epoch: 5 step: 101, loss is 0.16952450573444366\n",
      "epoch: 5 step: 102, loss is 0.3300613760948181\n",
      "epoch: 5 step: 103, loss is 0.38913825154304504\n",
      "epoch: 5 step: 104, loss is 0.2758274972438812\n",
      "epoch: 5 step: 105, loss is 0.3421984612941742\n",
      "epoch: 5 step: 106, loss is 0.24429307878017426\n",
      "epoch: 5 step: 107, loss is 0.31261876225471497\n",
      "epoch: 5 step: 108, loss is 0.20207270979881287\n",
      "epoch: 5 step: 109, loss is 0.2349267601966858\n",
      "epoch: 5 step: 110, loss is 0.2584265172481537\n",
      "epoch: 5 step: 111, loss is 0.1964426040649414\n",
      "epoch: 5 step: 112, loss is 0.3721520006656647\n",
      "epoch: 5 step: 113, loss is 0.23266583681106567\n",
      "epoch: 5 step: 114, loss is 0.23535217344760895\n",
      "epoch: 5 step: 115, loss is 0.436169296503067\n",
      "epoch: 5 step: 116, loss is 0.4463392496109009\n",
      "epoch: 5 step: 117, loss is 0.27501240372657776\n",
      "epoch: 5 step: 118, loss is 0.3779146075248718\n",
      "epoch: 5 step: 119, loss is 0.28329986333847046\n",
      "epoch: 5 step: 120, loss is 0.28668007254600525\n",
      "epoch: 5 step: 121, loss is 0.35469526052474976\n",
      "epoch: 5 step: 122, loss is 0.4506658613681793\n",
      "epoch: 5 step: 123, loss is 0.21653981506824493\n",
      "epoch: 5 step: 124, loss is 0.14044548571109772\n",
      "epoch: 5 step: 125, loss is 0.2865607440471649\n",
      "epoch: 5 step: 126, loss is 0.32378074526786804\n",
      "epoch: 5 step: 127, loss is 0.20248199999332428\n",
      "epoch: 5 step: 128, loss is 0.2619403004646301\n",
      "epoch: 5 step: 129, loss is 0.16543135046958923\n",
      "epoch: 5 step: 130, loss is 0.29374897480010986\n",
      "epoch: 5 step: 131, loss is 0.24212493002414703\n",
      "epoch: 5 step: 132, loss is 0.32860562205314636\n",
      "epoch: 5 step: 133, loss is 0.23182404041290283\n",
      "epoch: 5 step: 134, loss is 0.39077070355415344\n",
      "epoch: 5 step: 135, loss is 0.29711177945137024\n",
      "epoch: 5 step: 136, loss is 0.3900168538093567\n",
      "epoch: 5 step: 137, loss is 0.3092299997806549\n",
      "epoch: 5 step: 138, loss is 0.22591866552829742\n",
      "epoch: 5 step: 139, loss is 0.4378095269203186\n",
      "epoch: 5 step: 140, loss is 0.6097202301025391\n",
      "epoch: 5 step: 141, loss is 0.30277031660079956\n",
      "epoch: 5 step: 142, loss is 0.40791910886764526\n",
      "epoch: 5 step: 143, loss is 0.3917083740234375\n",
      "epoch: 5 step: 144, loss is 0.1915873885154724\n",
      "epoch: 5 step: 145, loss is 0.38014188408851624\n",
      "epoch: 5 step: 146, loss is 0.31023964285850525\n",
      "epoch: 5 step: 147, loss is 0.2660205364227295\n",
      "epoch: 5 step: 148, loss is 0.22187204658985138\n",
      "epoch: 5 step: 149, loss is 0.25205761194229126\n",
      "epoch: 5 step: 150, loss is 0.2704342007637024\n",
      "epoch: 5 step: 151, loss is 0.32580217719078064\n",
      "epoch: 5 step: 152, loss is 0.2646994888782501\n",
      "epoch: 5 step: 153, loss is 0.3115922510623932\n",
      "epoch: 5 step: 154, loss is 0.3791487514972687\n",
      "epoch: 5 step: 155, loss is 0.32706183195114136\n",
      "epoch: 5 step: 156, loss is 0.22445353865623474\n",
      "epoch: 5 step: 157, loss is 0.2705819308757782\n",
      "epoch: 5 step: 158, loss is 0.1791374832391739\n",
      "epoch: 5 step: 159, loss is 0.16367335617542267\n",
      "epoch: 5 step: 160, loss is 0.29194340109825134\n",
      "epoch: 5 step: 161, loss is 0.19514240324497223\n",
      "epoch: 5 step: 162, loss is 0.40295153856277466\n",
      "epoch: 5 step: 163, loss is 0.18178881704807281\n",
      "epoch: 5 step: 164, loss is 0.10729101300239563\n",
      "epoch: 5 step: 165, loss is 0.31349071860313416\n",
      "epoch: 5 step: 166, loss is 0.2588188648223877\n",
      "epoch: 5 step: 167, loss is 0.10383374989032745\n",
      "epoch: 5 step: 168, loss is 0.20983761548995972\n",
      "epoch: 5 step: 169, loss is 0.279766708612442\n",
      "epoch: 5 step: 170, loss is 0.23162773251533508\n",
      "epoch: 5 step: 171, loss is 0.20344214141368866\n",
      "epoch: 5 step: 172, loss is 0.35115328431129456\n",
      "epoch: 5 step: 173, loss is 0.25249820947647095\n",
      "epoch: 5 step: 174, loss is 0.33111676573753357\n",
      "epoch: 5 step: 175, loss is 0.3213365972042084\n",
      "epoch: 5 step: 176, loss is 0.28147363662719727\n",
      "epoch: 5 step: 177, loss is 0.29460152983665466\n",
      "epoch: 5 step: 178, loss is 0.33807000517845154\n",
      "epoch: 5 step: 179, loss is 0.3744945526123047\n",
      "epoch: 5 step: 180, loss is 0.31619977951049805\n",
      "epoch: 5 step: 181, loss is 0.23225049674510956\n",
      "epoch: 5 step: 182, loss is 0.3280857801437378\n",
      "epoch: 5 step: 183, loss is 0.17224572598934174\n",
      "epoch: 5 step: 184, loss is 0.21515510976314545\n",
      "epoch: 5 step: 185, loss is 0.42601487040519714\n",
      "epoch: 5 step: 186, loss is 0.3469224274158478\n",
      "epoch: 5 step: 187, loss is 0.11648742854595184\n",
      "epoch: 5 step: 188, loss is 0.3740573823451996\n",
      "epoch: 5 step: 189, loss is 0.42058926820755005\n",
      "epoch: 5 step: 190, loss is 0.2082929164171219\n",
      "epoch: 5 step: 191, loss is 0.2645190954208374\n",
      "epoch: 5 step: 192, loss is 0.46769389510154724\n",
      "epoch: 5 step: 193, loss is 0.17129401862621307\n",
      "epoch: 5 step: 194, loss is 0.244822159409523\n",
      "epoch: 5 step: 195, loss is 0.4654124677181244\n",
      "epoch: 5 step: 196, loss is 0.3245162069797516\n",
      "epoch: 5 step: 197, loss is 0.2510840892791748\n",
      "epoch: 5 step: 198, loss is 0.4991036653518677\n",
      "epoch: 5 step: 199, loss is 0.3652024269104004\n",
      "epoch: 5 step: 200, loss is 0.32988807559013367\n",
      "epoch: 5 step: 201, loss is 0.42024630308151245\n",
      "epoch: 5 step: 202, loss is 0.40482619404792786\n",
      "epoch: 5 step: 203, loss is 0.43206602334976196\n",
      "epoch: 5 step: 204, loss is 0.15860623121261597\n",
      "epoch: 5 step: 205, loss is 0.4321271479129791\n",
      "epoch: 5 step: 206, loss is 0.23457244038581848\n",
      "epoch: 5 step: 207, loss is 0.22581106424331665\n",
      "epoch: 5 step: 208, loss is 0.34085506200790405\n",
      "epoch: 5 step: 209, loss is 0.133955180644989\n",
      "epoch: 5 step: 210, loss is 0.30462732911109924\n",
      "epoch: 5 step: 211, loss is 0.19780436158180237\n",
      "epoch: 5 step: 212, loss is 0.2326459437608719\n",
      "epoch: 5 step: 213, loss is 0.2684226930141449\n",
      "epoch: 5 step: 214, loss is 0.272316038608551\n",
      "epoch: 5 step: 215, loss is 0.13630342483520508\n",
      "epoch: 5 step: 216, loss is 0.2419327050447464\n",
      "epoch: 5 step: 217, loss is 0.18196451663970947\n",
      "epoch: 5 step: 218, loss is 0.2754467725753784\n",
      "epoch: 5 step: 219, loss is 0.26163819432258606\n",
      "epoch: 5 step: 220, loss is 0.331454336643219\n",
      "epoch: 5 step: 221, loss is 0.3199031949043274\n",
      "epoch: 5 step: 222, loss is 0.3998073935508728\n",
      "epoch: 5 step: 223, loss is 0.19905264675617218\n",
      "epoch: 5 step: 224, loss is 0.261703222990036\n",
      "epoch: 5 step: 225, loss is 0.34898754954338074\n",
      "epoch: 5 step: 226, loss is 0.19325529038906097\n",
      "epoch: 5 step: 227, loss is 0.15015076100826263\n",
      "epoch: 5 step: 228, loss is 0.2030685693025589\n",
      "epoch: 5 step: 229, loss is 0.3874731659889221\n",
      "epoch: 5 step: 230, loss is 0.22858203947544098\n",
      "epoch: 5 step: 231, loss is 0.3067378103733063\n",
      "epoch: 5 step: 232, loss is 0.3781786859035492\n",
      "epoch: 5 step: 233, loss is 0.1876550018787384\n",
      "epoch: 5 step: 234, loss is 0.18913334608078003\n",
      "epoch: 5 step: 235, loss is 0.2052263617515564\n",
      "epoch: 5 step: 236, loss is 0.1740756630897522\n",
      "epoch: 5 step: 237, loss is 0.32016971707344055\n",
      "epoch: 5 step: 238, loss is 0.3435564935207367\n",
      "epoch: 5 step: 239, loss is 0.3662698566913605\n",
      "epoch: 5 step: 240, loss is 0.2645286023616791\n",
      "epoch: 5 step: 241, loss is 0.23925423622131348\n",
      "epoch: 5 step: 242, loss is 0.25620099902153015\n",
      "epoch: 5 step: 243, loss is 0.29667139053344727\n",
      "epoch: 5 step: 244, loss is 0.3044545352458954\n",
      "epoch: 5 step: 245, loss is 0.26303815841674805\n",
      "epoch: 5 step: 246, loss is 0.17275550961494446\n",
      "epoch: 5 step: 247, loss is 0.24518848955631256\n",
      "epoch: 5 step: 248, loss is 0.21926166117191315\n",
      "epoch: 5 step: 249, loss is 0.22088825702667236\n",
      "epoch: 5 step: 250, loss is 0.3103500008583069\n",
      "epoch: 5 step: 251, loss is 0.4666665196418762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 252, loss is 0.17458245158195496\n",
      "epoch: 5 step: 253, loss is 0.40572458505630493\n",
      "epoch: 5 step: 254, loss is 0.34559252858161926\n",
      "epoch: 5 step: 255, loss is 0.20173101127147675\n",
      "epoch: 5 step: 256, loss is 0.29500412940979004\n",
      "epoch: 5 step: 257, loss is 0.3537522852420807\n",
      "epoch: 5 step: 258, loss is 0.30527207255363464\n",
      "epoch: 5 step: 259, loss is 0.16431254148483276\n",
      "epoch: 5 step: 260, loss is 0.2659025192260742\n",
      "epoch: 5 step: 261, loss is 0.20439830422401428\n",
      "epoch: 5 step: 262, loss is 0.2895868718624115\n",
      "epoch: 5 step: 263, loss is 0.4206766188144684\n",
      "epoch: 5 step: 264, loss is 0.3763093054294586\n",
      "epoch: 5 step: 265, loss is 0.37197625637054443\n",
      "epoch: 5 step: 266, loss is 0.19418318569660187\n",
      "epoch: 5 step: 267, loss is 0.2958119809627533\n",
      "epoch: 5 step: 268, loss is 0.2703532874584198\n",
      "epoch: 5 step: 269, loss is 0.2312433272600174\n",
      "epoch: 5 step: 270, loss is 0.24690470099449158\n",
      "epoch: 5 step: 271, loss is 0.1586257964372635\n",
      "epoch: 5 step: 272, loss is 0.18363919854164124\n",
      "epoch: 5 step: 273, loss is 0.2394876331090927\n",
      "epoch: 5 step: 274, loss is 0.23286357522010803\n",
      "epoch: 5 step: 275, loss is 0.2385169267654419\n",
      "epoch: 5 step: 276, loss is 0.34576743841171265\n",
      "epoch: 5 step: 277, loss is 0.4235652983188629\n",
      "epoch: 5 step: 278, loss is 0.23621481657028198\n",
      "epoch: 5 step: 279, loss is 0.2057005763053894\n",
      "epoch: 5 step: 280, loss is 0.38451433181762695\n",
      "epoch: 5 step: 281, loss is 0.4117918014526367\n",
      "epoch: 5 step: 282, loss is 0.34560906887054443\n",
      "epoch: 5 step: 283, loss is 0.3902229964733124\n",
      "epoch: 5 step: 284, loss is 0.19737005233764648\n",
      "epoch: 5 step: 285, loss is 0.22085915505886078\n",
      "epoch: 5 step: 286, loss is 0.16990318894386292\n",
      "epoch: 5 step: 287, loss is 0.4237467348575592\n",
      "epoch: 5 step: 288, loss is 0.2962089776992798\n",
      "epoch: 5 step: 289, loss is 0.2641340494155884\n",
      "epoch: 5 step: 290, loss is 0.25670596957206726\n",
      "epoch: 5 step: 291, loss is 0.24077652394771576\n",
      "epoch: 5 step: 292, loss is 0.16419734060764313\n",
      "epoch: 5 step: 293, loss is 0.2017066925764084\n",
      "epoch: 5 step: 294, loss is 0.2170519083738327\n",
      "epoch: 5 step: 295, loss is 0.19026702642440796\n",
      "epoch: 5 step: 296, loss is 0.31584757566452026\n",
      "epoch: 5 step: 297, loss is 0.20318463444709778\n",
      "epoch: 5 step: 298, loss is 0.27108636498451233\n",
      "epoch: 5 step: 299, loss is 0.2727506756782532\n",
      "epoch: 5 step: 300, loss is 0.20621731877326965\n",
      "epoch: 5 step: 301, loss is 0.23268014192581177\n",
      "epoch: 5 step: 302, loss is 0.3604676127433777\n",
      "epoch: 5 step: 303, loss is 0.42728814482688904\n",
      "epoch: 5 step: 304, loss is 0.2324366569519043\n",
      "epoch: 5 step: 305, loss is 0.3383257985115051\n",
      "epoch: 5 step: 306, loss is 0.1911928653717041\n",
      "epoch: 5 step: 307, loss is 0.36225685477256775\n",
      "epoch: 5 step: 308, loss is 0.3137335181236267\n",
      "epoch: 5 step: 309, loss is 0.22272177040576935\n",
      "epoch: 5 step: 310, loss is 0.5617561340332031\n",
      "epoch: 5 step: 311, loss is 0.2662307620048523\n",
      "epoch: 5 step: 312, loss is 0.2391451746225357\n",
      "epoch: 5 step: 313, loss is 0.3567882180213928\n",
      "epoch: 5 step: 314, loss is 0.32650116086006165\n",
      "epoch: 5 step: 315, loss is 0.436046302318573\n",
      "epoch: 5 step: 316, loss is 0.25144538283348083\n",
      "epoch: 5 step: 317, loss is 0.15290939807891846\n",
      "epoch: 5 step: 318, loss is 0.1305360496044159\n",
      "epoch: 5 step: 319, loss is 0.3019062280654907\n",
      "epoch: 5 step: 320, loss is 0.13129593431949615\n",
      "epoch: 5 step: 321, loss is 0.29323655366897583\n",
      "epoch: 5 step: 322, loss is 0.3215656578540802\n",
      "epoch: 5 step: 323, loss is 0.37342581152915955\n",
      "epoch: 5 step: 324, loss is 0.2782135307788849\n",
      "epoch: 5 step: 325, loss is 0.3538146913051605\n",
      "epoch: 5 step: 326, loss is 0.3953073024749756\n",
      "epoch: 5 step: 327, loss is 0.3823723793029785\n",
      "epoch: 5 step: 328, loss is 0.2662986218929291\n",
      "epoch: 5 step: 329, loss is 0.17753514647483826\n",
      "epoch: 5 step: 330, loss is 0.14432062208652496\n",
      "epoch: 5 step: 331, loss is 0.27544131875038147\n",
      "epoch: 5 step: 332, loss is 0.37372681498527527\n",
      "epoch: 5 step: 333, loss is 0.5214858055114746\n",
      "epoch: 5 step: 334, loss is 0.3872452676296234\n",
      "epoch: 5 step: 335, loss is 0.4352128803730011\n",
      "epoch: 5 step: 336, loss is 0.3447711765766144\n",
      "epoch: 5 step: 337, loss is 0.3691059648990631\n",
      "epoch: 5 step: 338, loss is 0.3305806517601013\n",
      "epoch: 5 step: 339, loss is 0.33942699432373047\n",
      "epoch: 5 step: 340, loss is 0.2780933678150177\n",
      "epoch: 5 step: 341, loss is 0.4006027281284332\n",
      "epoch: 5 step: 342, loss is 0.14654572308063507\n",
      "epoch: 5 step: 343, loss is 0.24034757912158966\n",
      "epoch: 5 step: 344, loss is 0.2863328754901886\n",
      "epoch: 5 step: 345, loss is 0.546301007270813\n",
      "epoch: 5 step: 346, loss is 0.4529057741165161\n",
      "epoch: 5 step: 347, loss is 0.16703741252422333\n",
      "epoch: 5 step: 348, loss is 0.22140450775623322\n",
      "epoch: 5 step: 349, loss is 0.15951105952262878\n",
      "epoch: 5 step: 350, loss is 0.22811362147331238\n",
      "epoch: 5 step: 351, loss is 0.13446806371212006\n",
      "epoch: 5 step: 352, loss is 0.21336974203586578\n",
      "epoch: 5 step: 353, loss is 0.2992005944252014\n",
      "epoch: 5 step: 354, loss is 0.3448597192764282\n",
      "epoch: 5 step: 355, loss is 0.14219139516353607\n",
      "epoch: 5 step: 356, loss is 0.34345850348472595\n",
      "epoch: 5 step: 357, loss is 0.3191268742084503\n",
      "epoch: 5 step: 358, loss is 0.26180240511894226\n",
      "epoch: 5 step: 359, loss is 0.2755299210548401\n",
      "epoch: 5 step: 360, loss is 0.1669345498085022\n",
      "epoch: 5 step: 361, loss is 0.25915515422821045\n",
      "epoch: 5 step: 362, loss is 0.28613516688346863\n",
      "epoch: 5 step: 363, loss is 0.17136885225772858\n",
      "epoch: 5 step: 364, loss is 0.29349759221076965\n",
      "epoch: 5 step: 365, loss is 0.2555011212825775\n",
      "epoch: 5 step: 366, loss is 0.4177071750164032\n",
      "epoch: 5 step: 367, loss is 0.6242265701293945\n",
      "epoch: 5 step: 368, loss is 0.23544912040233612\n",
      "epoch: 5 step: 369, loss is 0.4255531430244446\n",
      "epoch: 5 step: 370, loss is 0.4599388539791107\n",
      "epoch: 5 step: 371, loss is 0.3081296384334564\n",
      "epoch: 5 step: 372, loss is 0.13239014148712158\n",
      "epoch: 5 step: 373, loss is 0.48404011130332947\n",
      "epoch: 5 step: 374, loss is 0.18010829389095306\n",
      "epoch: 5 step: 375, loss is 0.3273148536682129\n",
      "epoch: 5 step: 376, loss is 0.2644575536251068\n",
      "epoch: 5 step: 377, loss is 0.30679845809936523\n",
      "epoch: 5 step: 378, loss is 0.22173205018043518\n",
      "epoch: 5 step: 379, loss is 0.19725912809371948\n",
      "epoch: 5 step: 380, loss is 0.366875559091568\n",
      "epoch: 5 step: 381, loss is 0.2645363509654999\n",
      "epoch: 5 step: 382, loss is 0.14224380254745483\n",
      "epoch: 5 step: 383, loss is 0.22777168452739716\n",
      "epoch: 5 step: 384, loss is 0.1914808452129364\n",
      "epoch: 5 step: 385, loss is 0.18818551301956177\n",
      "epoch: 5 step: 386, loss is 0.17526875436306\n",
      "epoch: 5 step: 387, loss is 0.2883216440677643\n",
      "epoch: 5 step: 388, loss is 0.18466489017009735\n",
      "epoch: 5 step: 389, loss is 0.33608293533325195\n",
      "epoch: 5 step: 390, loss is 0.3208047151565552\n",
      "epoch: 5 step: 391, loss is 0.2086314857006073\n",
      "epoch: 5 step: 392, loss is 0.3675040304660797\n",
      "epoch: 5 step: 393, loss is 0.3152913451194763\n",
      "epoch: 5 step: 394, loss is 0.2918063700199127\n",
      "epoch: 5 step: 395, loss is 0.18440505862236023\n",
      "epoch: 5 step: 396, loss is 0.316215842962265\n",
      "epoch: 5 step: 397, loss is 0.14955992996692657\n",
      "epoch: 5 step: 398, loss is 0.2043948769569397\n",
      "epoch: 5 step: 399, loss is 0.18995937705039978\n",
      "epoch: 5 step: 400, loss is 0.23062275350093842\n",
      "epoch: 5 step: 401, loss is 0.260871946811676\n",
      "epoch: 5 step: 402, loss is 0.3333853483200073\n",
      "epoch: 5 step: 403, loss is 0.2883548438549042\n",
      "epoch: 5 step: 404, loss is 0.46055495738983154\n",
      "epoch: 5 step: 405, loss is 0.3638598322868347\n",
      "epoch: 5 step: 406, loss is 0.365035742521286\n",
      "epoch: 5 step: 407, loss is 0.26241186261177063\n",
      "epoch: 5 step: 408, loss is 0.4229296147823334\n",
      "epoch: 5 step: 409, loss is 0.17362743616104126\n",
      "epoch: 5 step: 410, loss is 0.3643699586391449\n",
      "epoch: 5 step: 411, loss is 0.1910506784915924\n",
      "epoch: 5 step: 412, loss is 0.1430172324180603\n",
      "epoch: 5 step: 413, loss is 0.2837428152561188\n",
      "epoch: 5 step: 414, loss is 0.331758588552475\n",
      "epoch: 5 step: 415, loss is 0.4727923572063446\n",
      "epoch: 5 step: 416, loss is 0.2085968554019928\n",
      "epoch: 5 step: 417, loss is 0.4086070656776428\n",
      "epoch: 5 step: 418, loss is 0.1680777370929718\n",
      "epoch: 5 step: 419, loss is 0.430075079202652\n",
      "epoch: 5 step: 420, loss is 0.21406061947345734\n",
      "epoch: 5 step: 421, loss is 0.17094305157661438\n",
      "epoch: 5 step: 422, loss is 0.22658757865428925\n",
      "epoch: 5 step: 423, loss is 0.21770581603050232\n",
      "epoch: 5 step: 424, loss is 0.38318848609924316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 425, loss is 0.31888794898986816\n",
      "epoch: 5 step: 426, loss is 0.19560320675373077\n",
      "epoch: 5 step: 427, loss is 0.18222638964653015\n",
      "epoch: 5 step: 428, loss is 0.2942287027835846\n",
      "epoch: 5 step: 429, loss is 0.26765120029449463\n",
      "epoch: 5 step: 430, loss is 0.23384273052215576\n",
      "epoch: 5 step: 431, loss is 0.1854502558708191\n",
      "epoch: 5 step: 432, loss is 0.31250494718551636\n",
      "epoch: 5 step: 433, loss is 0.29073774814605713\n",
      "epoch: 5 step: 434, loss is 0.288560688495636\n",
      "epoch: 5 step: 435, loss is 0.264373242855072\n",
      "epoch: 5 step: 436, loss is 0.20809273421764374\n",
      "epoch: 5 step: 437, loss is 0.25766655802726746\n",
      "epoch: 5 step: 438, loss is 0.24858519434928894\n",
      "epoch: 5 step: 439, loss is 0.24939747154712677\n",
      "epoch: 5 step: 440, loss is 0.18206681311130524\n",
      "epoch: 5 step: 441, loss is 0.25523531436920166\n",
      "epoch: 5 step: 442, loss is 0.20133505761623383\n",
      "epoch: 5 step: 443, loss is 0.26627546548843384\n",
      "epoch: 5 step: 444, loss is 0.30655646324157715\n",
      "epoch: 5 step: 445, loss is 0.2719310224056244\n",
      "epoch: 5 step: 446, loss is 0.2509232759475708\n",
      "epoch: 5 step: 447, loss is 0.36989250779151917\n",
      "epoch: 5 step: 448, loss is 0.2695074677467346\n",
      "epoch: 5 step: 449, loss is 0.20529600977897644\n",
      "epoch: 5 step: 450, loss is 0.5501426458358765\n",
      "epoch: 5 step: 451, loss is 0.23146729171276093\n",
      "epoch: 5 step: 452, loss is 0.24235183000564575\n",
      "epoch: 5 step: 453, loss is 0.38184988498687744\n",
      "epoch: 5 step: 454, loss is 0.27635207772254944\n",
      "epoch: 5 step: 455, loss is 0.2948017120361328\n",
      "epoch: 5 step: 456, loss is 0.24766872823238373\n",
      "epoch: 5 step: 457, loss is 0.17927773296833038\n",
      "epoch: 5 step: 458, loss is 0.1286017745733261\n",
      "epoch: 5 step: 459, loss is 0.30924496054649353\n",
      "epoch: 5 step: 460, loss is 0.35480475425720215\n",
      "epoch: 5 step: 461, loss is 0.3692537248134613\n",
      "epoch: 5 step: 462, loss is 0.28128349781036377\n",
      "epoch: 5 step: 463, loss is 0.4944990575313568\n",
      "epoch: 5 step: 464, loss is 0.30028101801872253\n",
      "epoch: 5 step: 465, loss is 0.23065350949764252\n",
      "epoch: 5 step: 466, loss is 0.3504936099052429\n",
      "epoch: 5 step: 467, loss is 0.22794969379901886\n",
      "epoch: 5 step: 468, loss is 0.45906710624694824\n",
      "epoch: 5 step: 469, loss is 0.29627636075019836\n",
      "epoch: 5 step: 470, loss is 0.2610359489917755\n",
      "epoch: 5 step: 471, loss is 0.15156111121177673\n",
      "epoch: 5 step: 472, loss is 0.241411492228508\n",
      "epoch: 5 step: 473, loss is 0.4033007323741913\n",
      "epoch: 5 step: 474, loss is 0.18414637446403503\n",
      "epoch: 5 step: 475, loss is 0.1365237981081009\n",
      "epoch: 5 step: 476, loss is 0.21120133996009827\n",
      "epoch: 5 step: 477, loss is 0.5024533271789551\n",
      "epoch: 5 step: 478, loss is 0.4211816191673279\n",
      "epoch: 5 step: 479, loss is 0.257193922996521\n",
      "epoch: 5 step: 480, loss is 0.35088759660720825\n",
      "epoch: 5 step: 481, loss is 0.3282508850097656\n",
      "epoch: 5 step: 482, loss is 0.19647951424121857\n",
      "epoch: 5 step: 483, loss is 0.2132876068353653\n",
      "epoch: 5 step: 484, loss is 0.3055403530597687\n",
      "epoch: 5 step: 485, loss is 0.22159840166568756\n",
      "epoch: 5 step: 486, loss is 0.13343442976474762\n",
      "epoch: 5 step: 487, loss is 0.2572326362133026\n",
      "epoch: 5 step: 488, loss is 0.23807023465633392\n",
      "epoch: 5 step: 489, loss is 0.31749022006988525\n",
      "epoch: 5 step: 490, loss is 0.2551898956298828\n",
      "epoch: 5 step: 491, loss is 0.39789193868637085\n",
      "epoch: 5 step: 492, loss is 0.3321998417377472\n",
      "epoch: 5 step: 493, loss is 0.22892913222312927\n",
      "epoch: 5 step: 494, loss is 0.2759507894515991\n",
      "epoch: 5 step: 495, loss is 0.27901196479797363\n",
      "epoch: 5 step: 496, loss is 0.3225448727607727\n",
      "epoch: 5 step: 497, loss is 0.1792190819978714\n",
      "epoch: 5 step: 498, loss is 0.2969624996185303\n",
      "epoch: 5 step: 499, loss is 0.5781242251396179\n",
      "epoch: 5 step: 500, loss is 0.44513851404190063\n",
      "epoch: 5 step: 501, loss is 0.2891233563423157\n",
      "epoch: 5 step: 502, loss is 0.23436294496059418\n",
      "epoch: 5 step: 503, loss is 0.11463604122400284\n",
      "epoch: 5 step: 504, loss is 0.5588828921318054\n",
      "epoch: 5 step: 505, loss is 0.27543771266937256\n",
      "epoch: 5 step: 506, loss is 0.1720459759235382\n",
      "epoch: 5 step: 507, loss is 0.20431923866271973\n",
      "epoch: 5 step: 508, loss is 0.4646601378917694\n",
      "epoch: 5 step: 509, loss is 0.21554023027420044\n",
      "epoch: 5 step: 510, loss is 0.1910346895456314\n",
      "epoch: 5 step: 511, loss is 0.30442339181900024\n",
      "epoch: 5 step: 512, loss is 0.28521060943603516\n",
      "epoch: 5 step: 513, loss is 0.3689180016517639\n",
      "epoch: 5 step: 514, loss is 0.33736926317214966\n",
      "epoch: 5 step: 515, loss is 0.33394041657447815\n",
      "epoch: 5 step: 516, loss is 0.4364165663719177\n",
      "epoch: 5 step: 517, loss is 0.2837963104248047\n",
      "epoch: 5 step: 518, loss is 0.3652424216270447\n",
      "epoch: 5 step: 519, loss is 0.3027057647705078\n",
      "epoch: 5 step: 520, loss is 0.21345390379428864\n",
      "epoch: 5 step: 521, loss is 0.18117482960224152\n",
      "epoch: 5 step: 522, loss is 0.4616190493106842\n",
      "epoch: 5 step: 523, loss is 0.24277649819850922\n",
      "epoch: 5 step: 524, loss is 0.18807673454284668\n",
      "epoch: 5 step: 525, loss is 0.4809038043022156\n",
      "epoch: 5 step: 526, loss is 0.32562676072120667\n",
      "epoch: 5 step: 527, loss is 0.1299675554037094\n",
      "epoch: 5 step: 528, loss is 0.32507556676864624\n",
      "epoch: 5 step: 529, loss is 0.21088755130767822\n",
      "epoch: 5 step: 530, loss is 0.2946185767650604\n",
      "epoch: 5 step: 531, loss is 0.2698690891265869\n",
      "epoch: 5 step: 532, loss is 0.2536667287349701\n",
      "epoch: 5 step: 533, loss is 0.2932812571525574\n",
      "epoch: 5 step: 534, loss is 0.3201126754283905\n",
      "epoch: 5 step: 535, loss is 0.19125115871429443\n",
      "epoch: 5 step: 536, loss is 0.31242918968200684\n",
      "epoch: 5 step: 537, loss is 0.4018808901309967\n",
      "epoch: 5 step: 538, loss is 0.25015363097190857\n",
      "epoch: 5 step: 539, loss is 0.29281482100486755\n",
      "epoch: 5 step: 540, loss is 0.18241697549819946\n",
      "epoch: 5 step: 541, loss is 0.34905150532722473\n",
      "epoch: 5 step: 542, loss is 0.27740010619163513\n",
      "epoch: 5 step: 543, loss is 0.23414942622184753\n",
      "epoch: 5 step: 544, loss is 0.505139172077179\n",
      "epoch: 5 step: 545, loss is 0.4609070420265198\n",
      "epoch: 5 step: 546, loss is 0.2892666459083557\n",
      "epoch: 5 step: 547, loss is 0.23920471966266632\n",
      "epoch: 5 step: 548, loss is 0.21543151140213013\n",
      "epoch: 5 step: 549, loss is 0.1952507346868515\n",
      "epoch: 5 step: 550, loss is 0.19458523392677307\n",
      "epoch: 5 step: 551, loss is 0.2966865301132202\n",
      "epoch: 5 step: 552, loss is 0.1179165467619896\n",
      "epoch: 5 step: 553, loss is 0.2747352123260498\n",
      "epoch: 5 step: 554, loss is 0.25801581144332886\n",
      "epoch: 5 step: 555, loss is 0.2184508591890335\n",
      "epoch: 5 step: 556, loss is 0.24001444876194\n",
      "epoch: 5 step: 557, loss is 0.4125033915042877\n",
      "epoch: 5 step: 558, loss is 0.316516250371933\n",
      "epoch: 5 step: 559, loss is 0.23879051208496094\n",
      "epoch: 5 step: 560, loss is 0.1497255265712738\n",
      "epoch: 5 step: 561, loss is 0.27320289611816406\n",
      "epoch: 5 step: 562, loss is 0.2731277048587799\n",
      "epoch: 5 step: 563, loss is 0.17496658861637115\n",
      "epoch: 5 step: 564, loss is 0.34169504046440125\n",
      "epoch: 5 step: 565, loss is 0.6584768295288086\n",
      "epoch: 5 step: 566, loss is 0.21041539311408997\n",
      "epoch: 5 step: 567, loss is 0.30069783329963684\n",
      "epoch: 5 step: 568, loss is 0.380905419588089\n",
      "epoch: 5 step: 569, loss is 0.12155658006668091\n",
      "epoch: 5 step: 570, loss is 0.1912999451160431\n",
      "epoch: 5 step: 571, loss is 0.3445821702480316\n",
      "epoch: 5 step: 572, loss is 0.22829607129096985\n",
      "epoch: 5 step: 573, loss is 0.46332570910453796\n",
      "epoch: 5 step: 574, loss is 0.14109784364700317\n",
      "epoch: 5 step: 575, loss is 0.16661572456359863\n",
      "epoch: 5 step: 576, loss is 0.2480505257844925\n",
      "epoch: 5 step: 577, loss is 0.26988184452056885\n",
      "epoch: 5 step: 578, loss is 0.4265131950378418\n",
      "epoch: 5 step: 579, loss is 0.2821800708770752\n",
      "epoch: 5 step: 580, loss is 0.25864866375923157\n",
      "epoch: 5 step: 581, loss is 0.26291966438293457\n",
      "epoch: 5 step: 582, loss is 0.45035168528556824\n",
      "epoch: 5 step: 583, loss is 0.1860378384590149\n",
      "epoch: 5 step: 584, loss is 0.35213062167167664\n",
      "epoch: 5 step: 585, loss is 0.19785696268081665\n",
      "epoch: 5 step: 586, loss is 0.45432645082473755\n",
      "epoch: 5 step: 587, loss is 0.30061638355255127\n",
      "epoch: 5 step: 588, loss is 0.22318163514137268\n",
      "epoch: 5 step: 589, loss is 0.28466519713401794\n",
      "epoch: 5 step: 590, loss is 0.22297395765781403\n",
      "epoch: 5 step: 591, loss is 0.30709874629974365\n",
      "epoch: 5 step: 592, loss is 0.28958654403686523\n",
      "epoch: 5 step: 593, loss is 0.28493738174438477\n",
      "epoch: 5 step: 594, loss is 0.2994689345359802\n",
      "epoch: 5 step: 595, loss is 0.33305683732032776\n",
      "epoch: 5 step: 596, loss is 0.25770777463912964\n",
      "epoch: 5 step: 597, loss is 0.24811778962612152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 598, loss is 0.33959445357322693\n",
      "epoch: 5 step: 599, loss is 0.20687371492385864\n",
      "epoch: 5 step: 600, loss is 0.33155858516693115\n",
      "epoch: 5 step: 601, loss is 0.42306557297706604\n",
      "epoch: 5 step: 602, loss is 0.26432549953460693\n",
      "epoch: 5 step: 603, loss is 0.2990751266479492\n",
      "epoch: 5 step: 604, loss is 0.23550447821617126\n",
      "epoch: 5 step: 605, loss is 0.19830197095870972\n",
      "epoch: 5 step: 606, loss is 0.2965407073497772\n",
      "epoch: 5 step: 607, loss is 0.2636435329914093\n",
      "epoch: 5 step: 608, loss is 0.29200172424316406\n",
      "epoch: 5 step: 609, loss is 0.2748754620552063\n",
      "epoch: 5 step: 610, loss is 0.3943950831890106\n",
      "epoch: 5 step: 611, loss is 0.38430193066596985\n",
      "epoch: 5 step: 612, loss is 0.35372689366340637\n",
      "epoch: 5 step: 613, loss is 0.2519541382789612\n",
      "epoch: 5 step: 614, loss is 0.3630121052265167\n",
      "epoch: 5 step: 615, loss is 0.21115778386592865\n",
      "epoch: 5 step: 616, loss is 0.43028339743614197\n",
      "epoch: 5 step: 617, loss is 0.30146050453186035\n",
      "epoch: 5 step: 618, loss is 0.30661988258361816\n",
      "epoch: 5 step: 619, loss is 0.22329996526241302\n",
      "epoch: 5 step: 620, loss is 0.47764521837234497\n",
      "epoch: 5 step: 621, loss is 0.613200843334198\n",
      "epoch: 5 step: 622, loss is 0.30468302965164185\n",
      "epoch: 5 step: 623, loss is 0.3112437427043915\n",
      "epoch: 5 step: 624, loss is 0.3240404725074768\n",
      "epoch: 5 step: 625, loss is 0.21359385550022125\n",
      "epoch: 5 step: 626, loss is 0.28521233797073364\n",
      "epoch: 5 step: 627, loss is 0.3579587936401367\n",
      "epoch: 5 step: 628, loss is 0.32960912585258484\n",
      "epoch: 5 step: 629, loss is 0.12516286969184875\n",
      "epoch: 5 step: 630, loss is 0.2859565019607544\n",
      "epoch: 5 step: 631, loss is 0.3917737305164337\n",
      "epoch: 5 step: 632, loss is 0.1894897073507309\n",
      "epoch: 5 step: 633, loss is 0.3440731167793274\n",
      "epoch: 5 step: 634, loss is 0.23418359458446503\n",
      "epoch: 5 step: 635, loss is 0.3364671766757965\n",
      "epoch: 5 step: 636, loss is 0.38940149545669556\n",
      "epoch: 5 step: 637, loss is 0.2709205448627472\n",
      "epoch: 5 step: 638, loss is 0.19388552010059357\n",
      "epoch: 5 step: 639, loss is 0.36808592081069946\n",
      "epoch: 5 step: 640, loss is 0.20972369611263275\n",
      "epoch: 5 step: 641, loss is 0.38948124647140503\n",
      "epoch: 5 step: 642, loss is 0.14484919607639313\n",
      "epoch: 5 step: 643, loss is 0.25742676854133606\n",
      "epoch: 5 step: 644, loss is 0.20024098455905914\n",
      "epoch: 5 step: 645, loss is 0.47722890973091125\n",
      "epoch: 5 step: 646, loss is 0.19313117861747742\n",
      "epoch: 5 step: 647, loss is 0.3789964020252228\n",
      "epoch: 5 step: 648, loss is 0.2625180184841156\n",
      "epoch: 5 step: 649, loss is 0.4745418131351471\n",
      "epoch: 5 step: 650, loss is 0.48059573769569397\n",
      "epoch: 5 step: 651, loss is 0.602005124092102\n",
      "epoch: 5 step: 652, loss is 0.34408652782440186\n",
      "epoch: 5 step: 653, loss is 0.3045417070388794\n",
      "epoch: 5 step: 654, loss is 0.20312489569187164\n",
      "epoch: 5 step: 655, loss is 0.2702488601207733\n",
      "epoch: 5 step: 656, loss is 0.305940717458725\n",
      "epoch: 5 step: 657, loss is 0.2754373848438263\n",
      "epoch: 5 step: 658, loss is 0.3654696047306061\n",
      "epoch: 5 step: 659, loss is 0.3035569190979004\n",
      "epoch: 5 step: 660, loss is 0.23412738740444183\n",
      "epoch: 5 step: 661, loss is 0.1800241619348526\n",
      "epoch: 5 step: 662, loss is 0.11645091325044632\n",
      "epoch: 5 step: 663, loss is 0.23814143240451813\n",
      "epoch: 5 step: 664, loss is 0.4817814528942108\n",
      "epoch: 5 step: 665, loss is 0.22755329310894012\n",
      "epoch: 5 step: 666, loss is 0.3166459798812866\n",
      "epoch: 5 step: 667, loss is 0.25822269916534424\n",
      "epoch: 5 step: 668, loss is 0.463320255279541\n",
      "epoch: 5 step: 669, loss is 0.5317131280899048\n",
      "epoch: 5 step: 670, loss is 0.2737906575202942\n",
      "epoch: 5 step: 671, loss is 0.18692055344581604\n",
      "epoch: 5 step: 672, loss is 0.2546262741088867\n",
      "epoch: 5 step: 673, loss is 0.18608242273330688\n",
      "epoch: 5 step: 674, loss is 0.3236488699913025\n",
      "epoch: 5 step: 675, loss is 0.2689114809036255\n",
      "epoch: 5 step: 676, loss is 0.19086429476737976\n",
      "epoch: 5 step: 677, loss is 0.14053358137607574\n",
      "epoch: 5 step: 678, loss is 0.46598783135414124\n",
      "epoch: 5 step: 679, loss is 0.3880976736545563\n",
      "epoch: 5 step: 680, loss is 0.19963960349559784\n",
      "epoch: 5 step: 681, loss is 0.2544477880001068\n",
      "epoch: 5 step: 682, loss is 0.44548630714416504\n",
      "epoch: 5 step: 683, loss is 0.12040560692548752\n",
      "epoch: 5 step: 684, loss is 0.14234793186187744\n",
      "epoch: 5 step: 685, loss is 0.3172513544559479\n",
      "epoch: 5 step: 686, loss is 0.23842068016529083\n",
      "epoch: 5 step: 687, loss is 0.46936461329460144\n",
      "epoch: 5 step: 688, loss is 0.3925231099128723\n",
      "epoch: 5 step: 689, loss is 0.32491713762283325\n",
      "epoch: 5 step: 690, loss is 0.20164711773395538\n",
      "epoch: 5 step: 691, loss is 0.3178284168243408\n",
      "epoch: 5 step: 692, loss is 0.3239324986934662\n",
      "epoch: 5 step: 693, loss is 0.24882175028324127\n",
      "epoch: 5 step: 694, loss is 0.2326986938714981\n",
      "epoch: 5 step: 695, loss is 0.23972876369953156\n",
      "epoch: 5 step: 696, loss is 0.3883483409881592\n",
      "epoch: 5 step: 697, loss is 0.29849499464035034\n",
      "epoch: 5 step: 698, loss is 0.3763231933116913\n",
      "epoch: 5 step: 699, loss is 0.12286943942308426\n",
      "epoch: 5 step: 700, loss is 0.2647150158882141\n",
      "epoch: 5 step: 701, loss is 0.28488191962242126\n",
      "epoch: 5 step: 702, loss is 0.2779842019081116\n",
      "epoch: 5 step: 703, loss is 0.1737036406993866\n",
      "epoch: 5 step: 704, loss is 0.1818418800830841\n",
      "epoch: 5 step: 705, loss is 0.22520847618579865\n",
      "epoch: 5 step: 706, loss is 0.19340190291404724\n",
      "epoch: 5 step: 707, loss is 0.20397944748401642\n",
      "epoch: 5 step: 708, loss is 0.33082231879234314\n",
      "epoch: 5 step: 709, loss is 0.21713854372501373\n",
      "epoch: 5 step: 710, loss is 0.3259758949279785\n",
      "epoch: 5 step: 711, loss is 0.322767972946167\n",
      "epoch: 5 step: 712, loss is 0.31193867325782776\n",
      "epoch: 5 step: 713, loss is 0.143871009349823\n",
      "epoch: 5 step: 714, loss is 0.2339412420988083\n",
      "epoch: 5 step: 715, loss is 0.32096317410469055\n",
      "epoch: 5 step: 716, loss is 0.28192639350891113\n",
      "epoch: 5 step: 717, loss is 0.29268309473991394\n",
      "epoch: 5 step: 718, loss is 0.42938411235809326\n",
      "epoch: 5 step: 719, loss is 0.32962504029273987\n",
      "epoch: 5 step: 720, loss is 0.2712564468383789\n",
      "epoch: 5 step: 721, loss is 0.2776671350002289\n",
      "epoch: 5 step: 722, loss is 0.37259456515312195\n",
      "epoch: 5 step: 723, loss is 0.25828617811203003\n",
      "epoch: 5 step: 724, loss is 0.5023866891860962\n",
      "epoch: 5 step: 725, loss is 0.1782778650522232\n",
      "epoch: 5 step: 726, loss is 0.36091771721839905\n",
      "epoch: 5 step: 727, loss is 0.15084202587604523\n",
      "epoch: 5 step: 728, loss is 0.21657465398311615\n",
      "epoch: 5 step: 729, loss is 0.2799329161643982\n",
      "epoch: 5 step: 730, loss is 0.35712704062461853\n",
      "epoch: 5 step: 731, loss is 0.28813278675079346\n",
      "epoch: 5 step: 732, loss is 0.3233754336833954\n",
      "epoch: 5 step: 733, loss is 0.14300097525119781\n",
      "epoch: 5 step: 734, loss is 0.49182870984077454\n",
      "epoch: 5 step: 735, loss is 0.147333025932312\n",
      "epoch: 5 step: 736, loss is 0.47291699051856995\n",
      "epoch: 5 step: 737, loss is 0.30519190430641174\n",
      "epoch: 5 step: 738, loss is 0.35419225692749023\n",
      "epoch: 5 step: 739, loss is 0.2477552592754364\n",
      "epoch: 5 step: 740, loss is 0.2886306047439575\n",
      "epoch: 5 step: 741, loss is 0.47494569420814514\n",
      "epoch: 5 step: 742, loss is 0.2873193919658661\n",
      "epoch: 5 step: 743, loss is 0.27737876772880554\n",
      "epoch: 5 step: 744, loss is 0.1602776199579239\n",
      "epoch: 5 step: 745, loss is 0.364399790763855\n",
      "epoch: 5 step: 746, loss is 0.25517040491104126\n",
      "epoch: 5 step: 747, loss is 0.2015894055366516\n",
      "epoch: 5 step: 748, loss is 0.4104273021221161\n",
      "epoch: 5 step: 749, loss is 0.348922461271286\n",
      "epoch: 5 step: 750, loss is 0.284404456615448\n",
      "epoch: 5 step: 751, loss is 0.29243892431259155\n",
      "epoch: 5 step: 752, loss is 0.2608581781387329\n",
      "epoch: 5 step: 753, loss is 0.4724346101284027\n",
      "epoch: 5 step: 754, loss is 0.20895613729953766\n",
      "epoch: 5 step: 755, loss is 0.17631766200065613\n",
      "epoch: 5 step: 756, loss is 0.19614680111408234\n",
      "epoch: 5 step: 757, loss is 0.3734077513217926\n",
      "epoch: 5 step: 758, loss is 0.33202093839645386\n",
      "epoch: 5 step: 759, loss is 0.4827975928783417\n",
      "epoch: 5 step: 760, loss is 0.3344871997833252\n",
      "epoch: 5 step: 761, loss is 0.39057260751724243\n",
      "epoch: 5 step: 762, loss is 0.2316674441099167\n",
      "epoch: 5 step: 763, loss is 0.2122269868850708\n",
      "epoch: 5 step: 764, loss is 0.4217350482940674\n",
      "epoch: 5 step: 765, loss is 0.18917036056518555\n",
      "epoch: 5 step: 766, loss is 0.29265791177749634\n",
      "epoch: 5 step: 767, loss is 0.23500880599021912\n",
      "epoch: 5 step: 768, loss is 0.24133841693401337\n",
      "epoch: 5 step: 769, loss is 0.3190802335739136\n",
      "epoch: 5 step: 770, loss is 0.3049046993255615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 771, loss is 0.2114519476890564\n",
      "epoch: 5 step: 772, loss is 0.4347813129425049\n",
      "epoch: 5 step: 773, loss is 0.5037981271743774\n",
      "epoch: 5 step: 774, loss is 0.3673665225505829\n",
      "epoch: 5 step: 775, loss is 0.22157591581344604\n",
      "epoch: 5 step: 776, loss is 0.3012271821498871\n",
      "epoch: 5 step: 777, loss is 0.3737848401069641\n",
      "epoch: 5 step: 778, loss is 0.2806227207183838\n",
      "epoch: 5 step: 779, loss is 0.3362438678741455\n",
      "epoch: 5 step: 780, loss is 0.2816333770751953\n",
      "epoch: 5 step: 781, loss is 0.3806457221508026\n",
      "epoch: 5 step: 782, loss is 0.3122747540473938\n",
      "epoch: 5 step: 783, loss is 0.2629421055316925\n",
      "epoch: 5 step: 784, loss is 0.2857827842235565\n",
      "epoch: 5 step: 785, loss is 0.1880592405796051\n",
      "epoch: 5 step: 786, loss is 0.34674930572509766\n",
      "epoch: 5 step: 787, loss is 0.33448755741119385\n",
      "epoch: 5 step: 788, loss is 0.3173184394836426\n",
      "epoch: 5 step: 789, loss is 0.3095746636390686\n",
      "epoch: 5 step: 790, loss is 0.31684255599975586\n",
      "epoch: 5 step: 791, loss is 0.26999691128730774\n",
      "epoch: 5 step: 792, loss is 0.2560937702655792\n",
      "epoch: 5 step: 793, loss is 0.26788395643234253\n",
      "epoch: 5 step: 794, loss is 0.361845999956131\n",
      "epoch: 5 step: 795, loss is 0.2756653428077698\n",
      "epoch: 5 step: 796, loss is 0.2692258954048157\n",
      "epoch: 5 step: 797, loss is 0.24680198729038239\n",
      "epoch: 5 step: 798, loss is 0.4066847264766693\n",
      "epoch: 5 step: 799, loss is 0.3367934226989746\n",
      "epoch: 5 step: 800, loss is 0.2417077273130417\n",
      "epoch: 5 step: 801, loss is 0.2483166605234146\n",
      "epoch: 5 step: 802, loss is 0.3339671194553375\n",
      "epoch: 5 step: 803, loss is 0.23292699456214905\n",
      "epoch: 5 step: 804, loss is 0.1789674162864685\n",
      "epoch: 5 step: 805, loss is 0.2624005973339081\n",
      "epoch: 5 step: 806, loss is 0.31688591837882996\n",
      "epoch: 5 step: 807, loss is 0.2635095417499542\n",
      "epoch: 5 step: 808, loss is 0.2612736225128174\n",
      "epoch: 5 step: 809, loss is 0.3300354480743408\n",
      "epoch: 5 step: 810, loss is 0.27810409665107727\n",
      "epoch: 5 step: 811, loss is 0.579105019569397\n",
      "epoch: 5 step: 812, loss is 0.27997979521751404\n",
      "epoch: 5 step: 813, loss is 0.22734522819519043\n",
      "epoch: 5 step: 814, loss is 0.3107735812664032\n",
      "epoch: 5 step: 815, loss is 0.23960645496845245\n",
      "epoch: 5 step: 816, loss is 0.2463788539171219\n",
      "epoch: 5 step: 817, loss is 0.35346120595932007\n",
      "epoch: 5 step: 818, loss is 0.2803463935852051\n",
      "epoch: 5 step: 819, loss is 0.30418920516967773\n",
      "epoch: 5 step: 820, loss is 0.17164252698421478\n",
      "epoch: 5 step: 821, loss is 0.2388104498386383\n",
      "epoch: 5 step: 822, loss is 0.2629312574863434\n",
      "epoch: 5 step: 823, loss is 0.2936776876449585\n",
      "epoch: 5 step: 824, loss is 0.2697404623031616\n",
      "epoch: 5 step: 825, loss is 0.23956021666526794\n",
      "epoch: 5 step: 826, loss is 0.2597601115703583\n",
      "epoch: 5 step: 827, loss is 0.18866612017154694\n",
      "epoch: 5 step: 828, loss is 0.2020275741815567\n",
      "epoch: 5 step: 829, loss is 0.16924810409545898\n",
      "epoch: 5 step: 830, loss is 0.1836969256401062\n",
      "epoch: 5 step: 831, loss is 0.345224529504776\n",
      "epoch: 5 step: 832, loss is 0.3956531584262848\n",
      "epoch: 5 step: 833, loss is 0.32801148295402527\n",
      "epoch: 5 step: 834, loss is 0.26703959703445435\n",
      "epoch: 5 step: 835, loss is 0.2526228129863739\n",
      "epoch: 5 step: 836, loss is 0.3784390985965729\n",
      "epoch: 5 step: 837, loss is 0.24822933971881866\n",
      "epoch: 5 step: 838, loss is 0.23611459136009216\n",
      "epoch: 5 step: 839, loss is 0.24976558983325958\n",
      "epoch: 5 step: 840, loss is 0.1365898847579956\n",
      "epoch: 5 step: 841, loss is 0.3840367794036865\n",
      "epoch: 5 step: 842, loss is 0.3808807134628296\n",
      "epoch: 5 step: 843, loss is 0.17662549018859863\n",
      "epoch: 5 step: 844, loss is 0.17711485922336578\n",
      "epoch: 5 step: 845, loss is 0.2978609502315521\n",
      "epoch: 5 step: 846, loss is 0.19207417964935303\n",
      "epoch: 5 step: 847, loss is 0.46932005882263184\n",
      "epoch: 5 step: 848, loss is 0.24003106355667114\n",
      "epoch: 5 step: 849, loss is 0.18392929434776306\n",
      "epoch: 5 step: 850, loss is 0.14953354001045227\n",
      "epoch: 5 step: 851, loss is 0.27014437317848206\n",
      "epoch: 5 step: 852, loss is 0.41684067249298096\n",
      "epoch: 5 step: 853, loss is 0.31337717175483704\n",
      "epoch: 5 step: 854, loss is 0.31852033734321594\n",
      "epoch: 5 step: 855, loss is 0.3584723472595215\n",
      "epoch: 5 step: 856, loss is 0.34631532430648804\n",
      "epoch: 5 step: 857, loss is 0.19193577766418457\n",
      "epoch: 5 step: 858, loss is 0.3823351263999939\n",
      "epoch: 5 step: 859, loss is 0.33473220467567444\n",
      "epoch: 5 step: 860, loss is 0.339306503534317\n",
      "epoch: 5 step: 861, loss is 0.22350138425827026\n",
      "epoch: 5 step: 862, loss is 0.3991107642650604\n",
      "epoch: 5 step: 863, loss is 0.3801470398902893\n",
      "epoch: 5 step: 864, loss is 0.3351636230945587\n",
      "epoch: 5 step: 865, loss is 0.21274137496948242\n",
      "epoch: 5 step: 866, loss is 0.26906949281692505\n",
      "epoch: 5 step: 867, loss is 0.2632107138633728\n",
      "epoch: 5 step: 868, loss is 0.24129082262516022\n",
      "epoch: 5 step: 869, loss is 0.21506862342357635\n",
      "epoch: 5 step: 870, loss is 0.23189574480056763\n",
      "epoch: 5 step: 871, loss is 0.22414666414260864\n",
      "epoch: 5 step: 872, loss is 0.466572642326355\n",
      "epoch: 5 step: 873, loss is 0.27282270789146423\n",
      "epoch: 5 step: 874, loss is 0.3107410669326782\n",
      "epoch: 5 step: 875, loss is 0.22811348736286163\n",
      "epoch: 5 step: 876, loss is 0.19356249272823334\n",
      "epoch: 5 step: 877, loss is 0.23030425608158112\n",
      "epoch: 5 step: 878, loss is 0.14201043546199799\n",
      "epoch: 5 step: 879, loss is 0.16568177938461304\n",
      "epoch: 5 step: 880, loss is 0.22889594733715057\n",
      "epoch: 5 step: 881, loss is 0.20215170085430145\n",
      "epoch: 5 step: 882, loss is 0.2357313185930252\n",
      "epoch: 5 step: 883, loss is 0.25621503591537476\n",
      "epoch: 5 step: 884, loss is 0.23369842767715454\n",
      "epoch: 5 step: 885, loss is 0.20977333188056946\n",
      "epoch: 5 step: 886, loss is 0.26787376403808594\n",
      "epoch: 5 step: 887, loss is 0.30839818716049194\n",
      "epoch: 5 step: 888, loss is 0.2457580268383026\n",
      "epoch: 5 step: 889, loss is 0.30491700768470764\n",
      "epoch: 5 step: 890, loss is 0.32404887676239014\n",
      "epoch: 5 step: 891, loss is 0.2697940170764923\n",
      "epoch: 5 step: 892, loss is 0.31580325961112976\n",
      "epoch: 5 step: 893, loss is 0.33792200684547424\n",
      "epoch: 5 step: 894, loss is 0.2873227894306183\n",
      "epoch: 5 step: 895, loss is 0.30896785855293274\n",
      "epoch: 5 step: 896, loss is 0.49986183643341064\n",
      "epoch: 5 step: 897, loss is 0.36504438519477844\n",
      "epoch: 5 step: 898, loss is 0.4615108370780945\n",
      "epoch: 5 step: 899, loss is 0.3769599497318268\n",
      "epoch: 5 step: 900, loss is 0.25694775581359863\n",
      "epoch: 5 step: 901, loss is 0.31613415479660034\n",
      "epoch: 5 step: 902, loss is 0.3368135094642639\n",
      "epoch: 5 step: 903, loss is 0.2826036512851715\n",
      "epoch: 5 step: 904, loss is 0.2638571262359619\n",
      "epoch: 5 step: 905, loss is 0.24972563982009888\n",
      "epoch: 5 step: 906, loss is 0.2885465621948242\n",
      "epoch: 5 step: 907, loss is 0.206683948636055\n",
      "epoch: 5 step: 908, loss is 0.36875274777412415\n",
      "epoch: 5 step: 909, loss is 0.37174999713897705\n",
      "epoch: 5 step: 910, loss is 0.3402560353279114\n",
      "epoch: 5 step: 911, loss is 0.38873356580734253\n",
      "epoch: 5 step: 912, loss is 0.34481415152549744\n",
      "epoch: 5 step: 913, loss is 0.3794601857662201\n",
      "epoch: 5 step: 914, loss is 0.5595855116844177\n",
      "epoch: 5 step: 915, loss is 0.5655708312988281\n",
      "epoch: 5 step: 916, loss is 0.29300376772880554\n",
      "epoch: 5 step: 917, loss is 0.32320263981819153\n",
      "epoch: 5 step: 918, loss is 0.2803053855895996\n",
      "epoch: 5 step: 919, loss is 0.20209279656410217\n",
      "epoch: 5 step: 920, loss is 0.25163373351097107\n",
      "epoch: 5 step: 921, loss is 0.18559780716896057\n",
      "epoch: 5 step: 922, loss is 0.24912555515766144\n",
      "epoch: 5 step: 923, loss is 0.19120103120803833\n",
      "epoch: 5 step: 924, loss is 0.30005645751953125\n",
      "epoch: 5 step: 925, loss is 0.3081396222114563\n",
      "epoch: 5 step: 926, loss is 0.3360905051231384\n",
      "epoch: 5 step: 927, loss is 0.2944777309894562\n",
      "epoch: 5 step: 928, loss is 0.35311442613601685\n",
      "epoch: 5 step: 929, loss is 0.12933234870433807\n",
      "epoch: 5 step: 930, loss is 0.23163318634033203\n",
      "epoch: 5 step: 931, loss is 0.1411394476890564\n",
      "epoch: 5 step: 932, loss is 0.23643264174461365\n",
      "epoch: 5 step: 933, loss is 0.37713462114334106\n",
      "epoch: 5 step: 934, loss is 0.29762351512908936\n",
      "epoch: 5 step: 935, loss is 0.2803013324737549\n",
      "epoch: 5 step: 936, loss is 0.2882556915283203\n",
      "epoch: 5 step: 937, loss is 0.20734953880310059\n",
      "epoch: 6 step: 1, loss is 0.2855195999145508\n",
      "epoch: 6 step: 2, loss is 0.24385198950767517\n",
      "epoch: 6 step: 3, loss is 0.1748887598514557\n",
      "epoch: 6 step: 4, loss is 0.24174843728542328\n",
      "epoch: 6 step: 5, loss is 0.5223996043205261\n",
      "epoch: 6 step: 6, loss is 0.18455058336257935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 7, loss is 0.19962432980537415\n",
      "epoch: 6 step: 8, loss is 0.41758766770362854\n",
      "epoch: 6 step: 9, loss is 0.24215951561927795\n",
      "epoch: 6 step: 10, loss is 0.4403962790966034\n",
      "epoch: 6 step: 11, loss is 0.34088319540023804\n",
      "epoch: 6 step: 12, loss is 0.231570765376091\n",
      "epoch: 6 step: 13, loss is 0.32540905475616455\n",
      "epoch: 6 step: 14, loss is 0.4368092119693756\n",
      "epoch: 6 step: 15, loss is 0.2966567277908325\n",
      "epoch: 6 step: 16, loss is 0.2991069257259369\n",
      "epoch: 6 step: 17, loss is 0.3249894380569458\n",
      "epoch: 6 step: 18, loss is 0.3304615318775177\n",
      "epoch: 6 step: 19, loss is 0.3546273112297058\n",
      "epoch: 6 step: 20, loss is 0.1967175304889679\n",
      "epoch: 6 step: 21, loss is 0.22686421871185303\n",
      "epoch: 6 step: 22, loss is 0.17669618129730225\n",
      "epoch: 6 step: 23, loss is 0.3006737232208252\n",
      "epoch: 6 step: 24, loss is 0.2556793987751007\n",
      "epoch: 6 step: 25, loss is 0.1500983089208603\n",
      "epoch: 6 step: 26, loss is 0.27708858251571655\n",
      "epoch: 6 step: 27, loss is 0.28465571999549866\n",
      "epoch: 6 step: 28, loss is 0.14284107089042664\n",
      "epoch: 6 step: 29, loss is 0.2984445095062256\n",
      "epoch: 6 step: 30, loss is 0.3392077684402466\n",
      "epoch: 6 step: 31, loss is 0.3443460464477539\n",
      "epoch: 6 step: 32, loss is 0.2390822023153305\n",
      "epoch: 6 step: 33, loss is 0.1884058266878128\n",
      "epoch: 6 step: 34, loss is 0.19121718406677246\n",
      "epoch: 6 step: 35, loss is 0.23240850865840912\n",
      "epoch: 6 step: 36, loss is 0.3784172236919403\n",
      "epoch: 6 step: 37, loss is 0.20386552810668945\n",
      "epoch: 6 step: 38, loss is 0.2966666519641876\n",
      "epoch: 6 step: 39, loss is 0.26735392212867737\n",
      "epoch: 6 step: 40, loss is 0.5848854780197144\n",
      "epoch: 6 step: 41, loss is 0.2853246033191681\n",
      "epoch: 6 step: 42, loss is 0.13903015851974487\n",
      "epoch: 6 step: 43, loss is 0.12202268093824387\n",
      "epoch: 6 step: 44, loss is 0.3248688280582428\n",
      "epoch: 6 step: 45, loss is 0.13169586658477783\n",
      "epoch: 6 step: 46, loss is 0.36281144618988037\n",
      "epoch: 6 step: 47, loss is 0.20554615557193756\n",
      "epoch: 6 step: 48, loss is 0.30276674032211304\n",
      "epoch: 6 step: 49, loss is 0.21133215725421906\n",
      "epoch: 6 step: 50, loss is 0.14761433005332947\n",
      "epoch: 6 step: 51, loss is 0.15382419526576996\n",
      "epoch: 6 step: 52, loss is 0.1200573593378067\n",
      "epoch: 6 step: 53, loss is 0.3336336016654968\n",
      "epoch: 6 step: 54, loss is 0.2646026313304901\n",
      "epoch: 6 step: 55, loss is 0.13654053211212158\n",
      "epoch: 6 step: 56, loss is 0.24379964172840118\n",
      "epoch: 6 step: 57, loss is 0.2513166666030884\n",
      "epoch: 6 step: 58, loss is 0.17724567651748657\n",
      "epoch: 6 step: 59, loss is 0.1938052475452423\n",
      "epoch: 6 step: 60, loss is 0.27721914649009705\n",
      "epoch: 6 step: 61, loss is 0.22886985540390015\n",
      "epoch: 6 step: 62, loss is 0.2982768416404724\n",
      "epoch: 6 step: 63, loss is 0.3771054744720459\n",
      "epoch: 6 step: 64, loss is 0.12824945151805878\n",
      "epoch: 6 step: 65, loss is 0.17988455295562744\n",
      "epoch: 6 step: 66, loss is 0.36755868792533875\n",
      "epoch: 6 step: 67, loss is 0.22697925567626953\n",
      "epoch: 6 step: 68, loss is 0.37425827980041504\n",
      "epoch: 6 step: 69, loss is 0.32855224609375\n",
      "epoch: 6 step: 70, loss is 0.2766039967536926\n",
      "epoch: 6 step: 71, loss is 0.16992762684822083\n",
      "epoch: 6 step: 72, loss is 0.33108144998550415\n",
      "epoch: 6 step: 73, loss is 0.3025011420249939\n",
      "epoch: 6 step: 74, loss is 0.24075594544410706\n",
      "epoch: 6 step: 75, loss is 0.21249224245548248\n",
      "epoch: 6 step: 76, loss is 0.2569928467273712\n",
      "epoch: 6 step: 77, loss is 0.22480763494968414\n",
      "epoch: 6 step: 78, loss is 0.17946265637874603\n",
      "epoch: 6 step: 79, loss is 0.20811395347118378\n",
      "epoch: 6 step: 80, loss is 0.2076326459646225\n",
      "epoch: 6 step: 81, loss is 0.29713624715805054\n",
      "epoch: 6 step: 82, loss is 0.25874415040016174\n",
      "epoch: 6 step: 83, loss is 0.25576403737068176\n",
      "epoch: 6 step: 84, loss is 0.21197067201137543\n",
      "epoch: 6 step: 85, loss is 0.406697154045105\n",
      "epoch: 6 step: 86, loss is 0.24585093557834625\n",
      "epoch: 6 step: 87, loss is 0.1847975254058838\n",
      "epoch: 6 step: 88, loss is 0.2726598381996155\n",
      "epoch: 6 step: 89, loss is 0.1634814590215683\n",
      "epoch: 6 step: 90, loss is 0.2350636124610901\n",
      "epoch: 6 step: 91, loss is 0.17502401769161224\n",
      "epoch: 6 step: 92, loss is 0.3346770107746124\n",
      "epoch: 6 step: 93, loss is 0.26022496819496155\n",
      "epoch: 6 step: 94, loss is 0.2590140402317047\n",
      "epoch: 6 step: 95, loss is 0.3735840320587158\n",
      "epoch: 6 step: 96, loss is 0.23460079729557037\n",
      "epoch: 6 step: 97, loss is 0.3706059753894806\n",
      "epoch: 6 step: 98, loss is 0.2905075252056122\n",
      "epoch: 6 step: 99, loss is 0.22008414566516876\n",
      "epoch: 6 step: 100, loss is 0.32563623785972595\n",
      "epoch: 6 step: 101, loss is 0.15762487053871155\n",
      "epoch: 6 step: 102, loss is 0.2988724410533905\n",
      "epoch: 6 step: 103, loss is 0.42818838357925415\n",
      "epoch: 6 step: 104, loss is 0.23455792665481567\n",
      "epoch: 6 step: 105, loss is 0.21253357827663422\n",
      "epoch: 6 step: 106, loss is 0.23526979982852936\n",
      "epoch: 6 step: 107, loss is 0.34154999256134033\n",
      "epoch: 6 step: 108, loss is 0.19137008488178253\n",
      "epoch: 6 step: 109, loss is 0.34367719292640686\n",
      "epoch: 6 step: 110, loss is 0.25490087270736694\n",
      "epoch: 6 step: 111, loss is 0.20840081572532654\n",
      "epoch: 6 step: 112, loss is 0.306496262550354\n",
      "epoch: 6 step: 113, loss is 0.32373926043510437\n",
      "epoch: 6 step: 114, loss is 0.4339844286441803\n",
      "epoch: 6 step: 115, loss is 0.45007434487342834\n",
      "epoch: 6 step: 116, loss is 0.2741756737232208\n",
      "epoch: 6 step: 117, loss is 0.12082569301128387\n",
      "epoch: 6 step: 118, loss is 0.30174770951271057\n",
      "epoch: 6 step: 119, loss is 0.20365354418754578\n",
      "epoch: 6 step: 120, loss is 0.3475983142852783\n",
      "epoch: 6 step: 121, loss is 0.3076953887939453\n",
      "epoch: 6 step: 122, loss is 0.23349156975746155\n",
      "epoch: 6 step: 123, loss is 0.266867071390152\n",
      "epoch: 6 step: 124, loss is 0.25019571185112\n",
      "epoch: 6 step: 125, loss is 0.19615088403224945\n",
      "epoch: 6 step: 126, loss is 0.514756977558136\n",
      "epoch: 6 step: 127, loss is 0.13485997915267944\n",
      "epoch: 6 step: 128, loss is 0.2619929313659668\n",
      "epoch: 6 step: 129, loss is 0.3216356039047241\n",
      "epoch: 6 step: 130, loss is 0.334966242313385\n",
      "epoch: 6 step: 131, loss is 0.19126524031162262\n",
      "epoch: 6 step: 132, loss is 0.16311071813106537\n",
      "epoch: 6 step: 133, loss is 0.3251800239086151\n",
      "epoch: 6 step: 134, loss is 0.27560967206954956\n",
      "epoch: 6 step: 135, loss is 0.5442888140678406\n",
      "epoch: 6 step: 136, loss is 0.21844549477100372\n",
      "epoch: 6 step: 137, loss is 0.21416246891021729\n",
      "epoch: 6 step: 138, loss is 0.17707616090774536\n",
      "epoch: 6 step: 139, loss is 0.31485986709594727\n",
      "epoch: 6 step: 140, loss is 0.16873817145824432\n",
      "epoch: 6 step: 141, loss is 0.3007570803165436\n",
      "epoch: 6 step: 142, loss is 0.35488221049308777\n",
      "epoch: 6 step: 143, loss is 0.2920073866844177\n",
      "epoch: 6 step: 144, loss is 0.28297945857048035\n",
      "epoch: 6 step: 145, loss is 0.42154762148857117\n",
      "epoch: 6 step: 146, loss is 0.26946941018104553\n",
      "epoch: 6 step: 147, loss is 0.33151766657829285\n",
      "epoch: 6 step: 148, loss is 0.28949618339538574\n",
      "epoch: 6 step: 149, loss is 0.22486662864685059\n",
      "epoch: 6 step: 150, loss is 0.28511348366737366\n",
      "epoch: 6 step: 151, loss is 0.42538291215896606\n",
      "epoch: 6 step: 152, loss is 0.31843075156211853\n",
      "epoch: 6 step: 153, loss is 0.20958192646503448\n",
      "epoch: 6 step: 154, loss is 0.3372727334499359\n",
      "epoch: 6 step: 155, loss is 0.20716401934623718\n",
      "epoch: 6 step: 156, loss is 0.22663378715515137\n",
      "epoch: 6 step: 157, loss is 0.22264014184474945\n",
      "epoch: 6 step: 158, loss is 0.24482038617134094\n",
      "epoch: 6 step: 159, loss is 0.13282091915607452\n",
      "epoch: 6 step: 160, loss is 0.21436309814453125\n",
      "epoch: 6 step: 161, loss is 0.32820212841033936\n",
      "epoch: 6 step: 162, loss is 0.37327468395233154\n",
      "epoch: 6 step: 163, loss is 0.2585786283016205\n",
      "epoch: 6 step: 164, loss is 0.3005770742893219\n",
      "epoch: 6 step: 165, loss is 0.35271507501602173\n",
      "epoch: 6 step: 166, loss is 0.21478132903575897\n",
      "epoch: 6 step: 167, loss is 0.4678821563720703\n",
      "epoch: 6 step: 168, loss is 0.2089071422815323\n",
      "epoch: 6 step: 169, loss is 0.46982696652412415\n",
      "epoch: 6 step: 170, loss is 0.22371429204940796\n",
      "epoch: 6 step: 171, loss is 0.21131117641925812\n",
      "epoch: 6 step: 172, loss is 0.35300251841545105\n",
      "epoch: 6 step: 173, loss is 0.22026872634887695\n",
      "epoch: 6 step: 174, loss is 0.2822190225124359\n",
      "epoch: 6 step: 175, loss is 0.20768626034259796\n",
      "epoch: 6 step: 176, loss is 0.38587474822998047\n",
      "epoch: 6 step: 177, loss is 0.2496771663427353\n",
      "epoch: 6 step: 178, loss is 0.19533111155033112\n",
      "epoch: 6 step: 179, loss is 0.17304116487503052\n",
      "epoch: 6 step: 180, loss is 0.17711803317070007\n",
      "epoch: 6 step: 181, loss is 0.19569550454616547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 182, loss is 0.19192944467067719\n",
      "epoch: 6 step: 183, loss is 0.2680366337299347\n",
      "epoch: 6 step: 184, loss is 0.16808244585990906\n",
      "epoch: 6 step: 185, loss is 0.2523323595523834\n",
      "epoch: 6 step: 186, loss is 0.18454599380493164\n",
      "epoch: 6 step: 187, loss is 0.39720532298088074\n",
      "epoch: 6 step: 188, loss is 0.25150376558303833\n",
      "epoch: 6 step: 189, loss is 0.23028455674648285\n",
      "epoch: 6 step: 190, loss is 0.24881181120872498\n",
      "epoch: 6 step: 191, loss is 0.1573181301355362\n",
      "epoch: 6 step: 192, loss is 0.24748586118221283\n",
      "epoch: 6 step: 193, loss is 0.2717606723308563\n",
      "epoch: 6 step: 194, loss is 0.4505194127559662\n",
      "epoch: 6 step: 195, loss is 0.20338988304138184\n",
      "epoch: 6 step: 196, loss is 0.4384313225746155\n",
      "epoch: 6 step: 197, loss is 0.3051632344722748\n",
      "epoch: 6 step: 198, loss is 0.20528264343738556\n",
      "epoch: 6 step: 199, loss is 0.28040632605552673\n",
      "epoch: 6 step: 200, loss is 0.22421404719352722\n",
      "epoch: 6 step: 201, loss is 0.15944796800613403\n",
      "epoch: 6 step: 202, loss is 0.3074195086956024\n",
      "epoch: 6 step: 203, loss is 0.27385246753692627\n",
      "epoch: 6 step: 204, loss is 0.3766495883464813\n",
      "epoch: 6 step: 205, loss is 0.27104803919792175\n",
      "epoch: 6 step: 206, loss is 0.26356592774391174\n",
      "epoch: 6 step: 207, loss is 0.31457585096359253\n",
      "epoch: 6 step: 208, loss is 0.3827984929084778\n",
      "epoch: 6 step: 209, loss is 0.30917102098464966\n",
      "epoch: 6 step: 210, loss is 0.1948137730360031\n",
      "epoch: 6 step: 211, loss is 0.28299680352211\n",
      "epoch: 6 step: 212, loss is 0.1438586711883545\n",
      "epoch: 6 step: 213, loss is 0.42230555415153503\n",
      "epoch: 6 step: 214, loss is 0.2756313383579254\n",
      "epoch: 6 step: 215, loss is 0.29017192125320435\n",
      "epoch: 6 step: 216, loss is 0.12943433225154877\n",
      "epoch: 6 step: 217, loss is 0.18497468531131744\n",
      "epoch: 6 step: 218, loss is 0.2790011167526245\n",
      "epoch: 6 step: 219, loss is 0.23471009731292725\n",
      "epoch: 6 step: 220, loss is 0.378967821598053\n",
      "epoch: 6 step: 221, loss is 0.3290564715862274\n",
      "epoch: 6 step: 222, loss is 0.35059404373168945\n",
      "epoch: 6 step: 223, loss is 0.230421781539917\n",
      "epoch: 6 step: 224, loss is 0.21349568665027618\n",
      "epoch: 6 step: 225, loss is 0.44451412558555603\n",
      "epoch: 6 step: 226, loss is 0.18997542560100555\n",
      "epoch: 6 step: 227, loss is 0.26458844542503357\n",
      "epoch: 6 step: 228, loss is 0.18539050221443176\n",
      "epoch: 6 step: 229, loss is 0.21259915828704834\n",
      "epoch: 6 step: 230, loss is 0.2176809459924698\n",
      "epoch: 6 step: 231, loss is 0.23704765737056732\n",
      "epoch: 6 step: 232, loss is 0.2706478536128998\n",
      "epoch: 6 step: 233, loss is 0.22623302042484283\n",
      "epoch: 6 step: 234, loss is 0.48106008768081665\n",
      "epoch: 6 step: 235, loss is 0.1517266035079956\n",
      "epoch: 6 step: 236, loss is 0.2529059648513794\n",
      "epoch: 6 step: 237, loss is 0.28942230343818665\n",
      "epoch: 6 step: 238, loss is 0.3395935893058777\n",
      "epoch: 6 step: 239, loss is 0.21317695081233978\n",
      "epoch: 6 step: 240, loss is 0.28372472524642944\n",
      "epoch: 6 step: 241, loss is 0.4084858000278473\n",
      "epoch: 6 step: 242, loss is 0.1955116242170334\n",
      "epoch: 6 step: 243, loss is 0.2984156608581543\n",
      "epoch: 6 step: 244, loss is 0.2924337387084961\n",
      "epoch: 6 step: 245, loss is 0.3819391131401062\n",
      "epoch: 6 step: 246, loss is 0.34310150146484375\n",
      "epoch: 6 step: 247, loss is 0.12891241908073425\n",
      "epoch: 6 step: 248, loss is 0.31797248125076294\n",
      "epoch: 6 step: 249, loss is 0.28964924812316895\n",
      "epoch: 6 step: 250, loss is 0.2715224623680115\n",
      "epoch: 6 step: 251, loss is 0.18621650338172913\n",
      "epoch: 6 step: 252, loss is 0.3334469199180603\n",
      "epoch: 6 step: 253, loss is 0.3793101906776428\n",
      "epoch: 6 step: 254, loss is 0.22817616164684296\n",
      "epoch: 6 step: 255, loss is 0.29291510581970215\n",
      "epoch: 6 step: 256, loss is 0.25888338685035706\n",
      "epoch: 6 step: 257, loss is 0.3164653182029724\n",
      "epoch: 6 step: 258, loss is 0.21318472921848297\n",
      "epoch: 6 step: 259, loss is 0.27629145979881287\n",
      "epoch: 6 step: 260, loss is 0.1694476306438446\n",
      "epoch: 6 step: 261, loss is 0.1887660026550293\n",
      "epoch: 6 step: 262, loss is 0.3813684284687042\n",
      "epoch: 6 step: 263, loss is 0.3803669810295105\n",
      "epoch: 6 step: 264, loss is 0.44708576798439026\n",
      "epoch: 6 step: 265, loss is 0.4708668887615204\n",
      "epoch: 6 step: 266, loss is 0.13111257553100586\n",
      "epoch: 6 step: 267, loss is 0.31690043210983276\n",
      "epoch: 6 step: 268, loss is 0.13508100807666779\n",
      "epoch: 6 step: 269, loss is 0.29702892899513245\n",
      "epoch: 6 step: 270, loss is 0.24998611211776733\n",
      "epoch: 6 step: 271, loss is 0.4549441635608673\n",
      "epoch: 6 step: 272, loss is 0.29257848858833313\n",
      "epoch: 6 step: 273, loss is 0.3184496760368347\n",
      "epoch: 6 step: 274, loss is 0.21115083992481232\n",
      "epoch: 6 step: 275, loss is 0.3974309265613556\n",
      "epoch: 6 step: 276, loss is 0.32729506492614746\n",
      "epoch: 6 step: 277, loss is 0.27078717947006226\n",
      "epoch: 6 step: 278, loss is 0.26405757665634155\n",
      "epoch: 6 step: 279, loss is 0.4478246569633484\n",
      "epoch: 6 step: 280, loss is 0.25323379039764404\n",
      "epoch: 6 step: 281, loss is 0.16261647641658783\n",
      "epoch: 6 step: 282, loss is 0.271719366312027\n",
      "epoch: 6 step: 283, loss is 0.19430777430534363\n",
      "epoch: 6 step: 284, loss is 0.31092017889022827\n",
      "epoch: 6 step: 285, loss is 0.42113441228866577\n",
      "epoch: 6 step: 286, loss is 0.3074228763580322\n",
      "epoch: 6 step: 287, loss is 0.319180965423584\n",
      "epoch: 6 step: 288, loss is 0.42615243792533875\n",
      "epoch: 6 step: 289, loss is 0.24898701906204224\n",
      "epoch: 6 step: 290, loss is 0.3425610363483429\n",
      "epoch: 6 step: 291, loss is 0.24993562698364258\n",
      "epoch: 6 step: 292, loss is 0.4290784001350403\n",
      "epoch: 6 step: 293, loss is 0.20085710287094116\n",
      "epoch: 6 step: 294, loss is 0.2714981734752655\n",
      "epoch: 6 step: 295, loss is 0.15265071392059326\n",
      "epoch: 6 step: 296, loss is 0.14597088098526\n",
      "epoch: 6 step: 297, loss is 0.46127474308013916\n",
      "epoch: 6 step: 298, loss is 0.45109185576438904\n",
      "epoch: 6 step: 299, loss is 0.20101580023765564\n",
      "epoch: 6 step: 300, loss is 0.2257309854030609\n",
      "epoch: 6 step: 301, loss is 0.40377917885780334\n",
      "epoch: 6 step: 302, loss is 0.26485493779182434\n",
      "epoch: 6 step: 303, loss is 0.46526002883911133\n",
      "epoch: 6 step: 304, loss is 0.2611830532550812\n",
      "epoch: 6 step: 305, loss is 0.36835604906082153\n",
      "epoch: 6 step: 306, loss is 0.2242209017276764\n",
      "epoch: 6 step: 307, loss is 0.30853959918022156\n",
      "epoch: 6 step: 308, loss is 0.13142642378807068\n",
      "epoch: 6 step: 309, loss is 0.3175417184829712\n",
      "epoch: 6 step: 310, loss is 0.22639010846614838\n",
      "epoch: 6 step: 311, loss is 0.23547187447547913\n",
      "epoch: 6 step: 312, loss is 0.36585792899131775\n",
      "epoch: 6 step: 313, loss is 0.2891820967197418\n",
      "epoch: 6 step: 314, loss is 0.2711820602416992\n",
      "epoch: 6 step: 315, loss is 0.2341148555278778\n",
      "epoch: 6 step: 316, loss is 0.14480793476104736\n",
      "epoch: 6 step: 317, loss is 0.32639607787132263\n",
      "epoch: 6 step: 318, loss is 0.10067755728960037\n",
      "epoch: 6 step: 319, loss is 0.28173238039016724\n",
      "epoch: 6 step: 320, loss is 0.3115122318267822\n",
      "epoch: 6 step: 321, loss is 0.19862276315689087\n",
      "epoch: 6 step: 322, loss is 0.27938419580459595\n",
      "epoch: 6 step: 323, loss is 0.3447378873825073\n",
      "epoch: 6 step: 324, loss is 0.44122540950775146\n",
      "epoch: 6 step: 325, loss is 0.22711044549942017\n",
      "epoch: 6 step: 326, loss is 0.2641740143299103\n",
      "epoch: 6 step: 327, loss is 0.3133833110332489\n",
      "epoch: 6 step: 328, loss is 0.3049283027648926\n",
      "epoch: 6 step: 329, loss is 0.16070155799388885\n",
      "epoch: 6 step: 330, loss is 0.2466137558221817\n",
      "epoch: 6 step: 331, loss is 0.3836052119731903\n",
      "epoch: 6 step: 332, loss is 0.24345415830612183\n",
      "epoch: 6 step: 333, loss is 0.15667644143104553\n",
      "epoch: 6 step: 334, loss is 0.4934557378292084\n",
      "epoch: 6 step: 335, loss is 0.32759010791778564\n",
      "epoch: 6 step: 336, loss is 0.47902098298072815\n",
      "epoch: 6 step: 337, loss is 0.2222774773836136\n",
      "epoch: 6 step: 338, loss is 0.21839116513729095\n",
      "epoch: 6 step: 339, loss is 0.1936173141002655\n",
      "epoch: 6 step: 340, loss is 0.3225827217102051\n",
      "epoch: 6 step: 341, loss is 0.2555723190307617\n",
      "epoch: 6 step: 342, loss is 0.32963770627975464\n",
      "epoch: 6 step: 343, loss is 0.2198912799358368\n",
      "epoch: 6 step: 344, loss is 0.33263713121414185\n",
      "epoch: 6 step: 345, loss is 0.2051883488893509\n",
      "epoch: 6 step: 346, loss is 0.17489419877529144\n",
      "epoch: 6 step: 347, loss is 0.17867031693458557\n",
      "epoch: 6 step: 348, loss is 0.2561793625354767\n",
      "epoch: 6 step: 349, loss is 0.300203412771225\n",
      "epoch: 6 step: 350, loss is 0.23227503895759583\n",
      "epoch: 6 step: 351, loss is 0.3941037654876709\n",
      "epoch: 6 step: 352, loss is 0.16092988848686218\n",
      "epoch: 6 step: 353, loss is 0.14564692974090576\n",
      "epoch: 6 step: 354, loss is 0.19229108095169067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 355, loss is 0.28397586941719055\n",
      "epoch: 6 step: 356, loss is 0.2847908139228821\n",
      "epoch: 6 step: 357, loss is 0.25499075651168823\n",
      "epoch: 6 step: 358, loss is 0.2305368036031723\n",
      "epoch: 6 step: 359, loss is 0.1756095290184021\n",
      "epoch: 6 step: 360, loss is 0.3948567509651184\n",
      "epoch: 6 step: 361, loss is 0.32200106978416443\n",
      "epoch: 6 step: 362, loss is 0.4561414122581482\n",
      "epoch: 6 step: 363, loss is 0.2258729785680771\n",
      "epoch: 6 step: 364, loss is 0.18764348328113556\n",
      "epoch: 6 step: 365, loss is 0.2942863702774048\n",
      "epoch: 6 step: 366, loss is 0.37029120326042175\n",
      "epoch: 6 step: 367, loss is 0.43741491436958313\n",
      "epoch: 6 step: 368, loss is 0.28761276602745056\n",
      "epoch: 6 step: 369, loss is 0.2182549387216568\n",
      "epoch: 6 step: 370, loss is 0.2048179656267166\n",
      "epoch: 6 step: 371, loss is 0.3556663393974304\n",
      "epoch: 6 step: 372, loss is 0.24149595201015472\n",
      "epoch: 6 step: 373, loss is 0.19380390644073486\n",
      "epoch: 6 step: 374, loss is 0.1492191106081009\n",
      "epoch: 6 step: 375, loss is 0.31677311658859253\n",
      "epoch: 6 step: 376, loss is 0.17601630091667175\n",
      "epoch: 6 step: 377, loss is 0.21293599903583527\n",
      "epoch: 6 step: 378, loss is 0.39933085441589355\n",
      "epoch: 6 step: 379, loss is 0.33538129925727844\n",
      "epoch: 6 step: 380, loss is 0.2269434779882431\n",
      "epoch: 6 step: 381, loss is 0.23353567719459534\n",
      "epoch: 6 step: 382, loss is 0.2836195230484009\n",
      "epoch: 6 step: 383, loss is 0.21125340461730957\n",
      "epoch: 6 step: 384, loss is 0.3148859143257141\n",
      "epoch: 6 step: 385, loss is 0.15386971831321716\n",
      "epoch: 6 step: 386, loss is 0.2593747079372406\n",
      "epoch: 6 step: 387, loss is 0.3041135370731354\n",
      "epoch: 6 step: 388, loss is 0.14623551070690155\n",
      "epoch: 6 step: 389, loss is 0.3200885057449341\n",
      "epoch: 6 step: 390, loss is 0.3177432119846344\n",
      "epoch: 6 step: 391, loss is 0.21428412199020386\n",
      "epoch: 6 step: 392, loss is 0.34383514523506165\n",
      "epoch: 6 step: 393, loss is 0.32799744606018066\n",
      "epoch: 6 step: 394, loss is 0.39134517312049866\n",
      "epoch: 6 step: 395, loss is 0.24147093296051025\n",
      "epoch: 6 step: 396, loss is 0.2626553773880005\n",
      "epoch: 6 step: 397, loss is 0.34327107667922974\n",
      "epoch: 6 step: 398, loss is 0.28108781576156616\n",
      "epoch: 6 step: 399, loss is 0.3504268527030945\n",
      "epoch: 6 step: 400, loss is 0.21751493215560913\n",
      "epoch: 6 step: 401, loss is 0.2779298722743988\n",
      "epoch: 6 step: 402, loss is 0.275870680809021\n",
      "epoch: 6 step: 403, loss is 0.21887502074241638\n",
      "epoch: 6 step: 404, loss is 0.22271989285945892\n",
      "epoch: 6 step: 405, loss is 0.2074110358953476\n",
      "epoch: 6 step: 406, loss is 0.27561429142951965\n",
      "epoch: 6 step: 407, loss is 0.23403090238571167\n",
      "epoch: 6 step: 408, loss is 0.26546719670295715\n",
      "epoch: 6 step: 409, loss is 0.24393263459205627\n",
      "epoch: 6 step: 410, loss is 0.35413461923599243\n",
      "epoch: 6 step: 411, loss is 0.18127299845218658\n",
      "epoch: 6 step: 412, loss is 0.391734778881073\n",
      "epoch: 6 step: 413, loss is 0.17802798748016357\n",
      "epoch: 6 step: 414, loss is 0.4877675771713257\n",
      "epoch: 6 step: 415, loss is 0.2386760413646698\n",
      "epoch: 6 step: 416, loss is 0.34960687160491943\n",
      "epoch: 6 step: 417, loss is 0.23138687014579773\n",
      "epoch: 6 step: 418, loss is 0.21522964537143707\n",
      "epoch: 6 step: 419, loss is 0.22824415564537048\n",
      "epoch: 6 step: 420, loss is 0.21930372714996338\n",
      "epoch: 6 step: 421, loss is 0.24948139488697052\n",
      "epoch: 6 step: 422, loss is 0.4259016811847687\n",
      "epoch: 6 step: 423, loss is 0.2647915780544281\n",
      "epoch: 6 step: 424, loss is 0.26504066586494446\n",
      "epoch: 6 step: 425, loss is 0.3556274175643921\n",
      "epoch: 6 step: 426, loss is 0.32076239585876465\n",
      "epoch: 6 step: 427, loss is 0.2668250501155853\n",
      "epoch: 6 step: 428, loss is 0.25273558497428894\n",
      "epoch: 6 step: 429, loss is 0.16155606508255005\n",
      "epoch: 6 step: 430, loss is 0.2568543553352356\n",
      "epoch: 6 step: 431, loss is 0.15000922977924347\n",
      "epoch: 6 step: 432, loss is 0.32300519943237305\n",
      "epoch: 6 step: 433, loss is 0.38424915075302124\n",
      "epoch: 6 step: 434, loss is 0.10294251143932343\n",
      "epoch: 6 step: 435, loss is 0.13244786858558655\n",
      "epoch: 6 step: 436, loss is 0.205010324716568\n",
      "epoch: 6 step: 437, loss is 0.3579324185848236\n",
      "epoch: 6 step: 438, loss is 0.18786941468715668\n",
      "epoch: 6 step: 439, loss is 0.40678712725639343\n",
      "epoch: 6 step: 440, loss is 0.2825538218021393\n",
      "epoch: 6 step: 441, loss is 0.1265680342912674\n",
      "epoch: 6 step: 442, loss is 0.40801694989204407\n",
      "epoch: 6 step: 443, loss is 0.18171322345733643\n",
      "epoch: 6 step: 444, loss is 0.3061494827270508\n",
      "epoch: 6 step: 445, loss is 0.2893056869506836\n",
      "epoch: 6 step: 446, loss is 0.19990253448486328\n",
      "epoch: 6 step: 447, loss is 0.34864792227745056\n",
      "epoch: 6 step: 448, loss is 0.3786576986312866\n",
      "epoch: 6 step: 449, loss is 0.5134295225143433\n",
      "epoch: 6 step: 450, loss is 0.3393256962299347\n",
      "epoch: 6 step: 451, loss is 0.23586708307266235\n",
      "epoch: 6 step: 452, loss is 0.3299727737903595\n",
      "epoch: 6 step: 453, loss is 0.30627891421318054\n",
      "epoch: 6 step: 454, loss is 0.2419806867837906\n",
      "epoch: 6 step: 455, loss is 0.1649821251630783\n",
      "epoch: 6 step: 456, loss is 0.41122689843177795\n",
      "epoch: 6 step: 457, loss is 0.17300277948379517\n",
      "epoch: 6 step: 458, loss is 0.2224324345588684\n",
      "epoch: 6 step: 459, loss is 0.34195050597190857\n",
      "epoch: 6 step: 460, loss is 0.15843680500984192\n",
      "epoch: 6 step: 461, loss is 0.3504960536956787\n",
      "epoch: 6 step: 462, loss is 0.2831379771232605\n",
      "epoch: 6 step: 463, loss is 0.21175986528396606\n",
      "epoch: 6 step: 464, loss is 0.34805452823638916\n",
      "epoch: 6 step: 465, loss is 0.19371220469474792\n",
      "epoch: 6 step: 466, loss is 0.23960012197494507\n",
      "epoch: 6 step: 467, loss is 0.32108649611473083\n",
      "epoch: 6 step: 468, loss is 0.24738311767578125\n",
      "epoch: 6 step: 469, loss is 0.254555344581604\n",
      "epoch: 6 step: 470, loss is 0.18471229076385498\n",
      "epoch: 6 step: 471, loss is 0.3787635564804077\n",
      "epoch: 6 step: 472, loss is 0.286324143409729\n",
      "epoch: 6 step: 473, loss is 0.24008065462112427\n",
      "epoch: 6 step: 474, loss is 0.2263275533914566\n",
      "epoch: 6 step: 475, loss is 0.3527107536792755\n",
      "epoch: 6 step: 476, loss is 0.2821350395679474\n",
      "epoch: 6 step: 477, loss is 0.17531545460224152\n",
      "epoch: 6 step: 478, loss is 0.33031347393989563\n",
      "epoch: 6 step: 479, loss is 0.2930864691734314\n",
      "epoch: 6 step: 480, loss is 0.24970774352550507\n",
      "epoch: 6 step: 481, loss is 0.15591846406459808\n",
      "epoch: 6 step: 482, loss is 0.19231398403644562\n",
      "epoch: 6 step: 483, loss is 0.17040878534317017\n",
      "epoch: 6 step: 484, loss is 0.3767654597759247\n",
      "epoch: 6 step: 485, loss is 0.1582762897014618\n",
      "epoch: 6 step: 486, loss is 0.20602713525295258\n",
      "epoch: 6 step: 487, loss is 0.15488794445991516\n",
      "epoch: 6 step: 488, loss is 0.1134430468082428\n",
      "epoch: 6 step: 489, loss is 0.19882583618164062\n",
      "epoch: 6 step: 490, loss is 0.2560381293296814\n",
      "epoch: 6 step: 491, loss is 0.3380188047885895\n",
      "epoch: 6 step: 492, loss is 0.19312942028045654\n",
      "epoch: 6 step: 493, loss is 0.365217387676239\n",
      "epoch: 6 step: 494, loss is 0.24063824117183685\n",
      "epoch: 6 step: 495, loss is 0.18792623281478882\n",
      "epoch: 6 step: 496, loss is 0.3641902506351471\n",
      "epoch: 6 step: 497, loss is 0.5475921034812927\n",
      "epoch: 6 step: 498, loss is 0.2435154914855957\n",
      "epoch: 6 step: 499, loss is 0.3711059093475342\n",
      "epoch: 6 step: 500, loss is 0.34518569707870483\n",
      "epoch: 6 step: 501, loss is 0.23279836773872375\n",
      "epoch: 6 step: 502, loss is 0.44272127747535706\n",
      "epoch: 6 step: 503, loss is 0.36010897159576416\n",
      "epoch: 6 step: 504, loss is 0.35526707768440247\n",
      "epoch: 6 step: 505, loss is 0.1326097995042801\n",
      "epoch: 6 step: 506, loss is 0.15292011201381683\n",
      "epoch: 6 step: 507, loss is 0.19436483085155487\n",
      "epoch: 6 step: 508, loss is 0.20133094489574432\n",
      "epoch: 6 step: 509, loss is 0.238876610994339\n",
      "epoch: 6 step: 510, loss is 0.2965483069419861\n",
      "epoch: 6 step: 511, loss is 0.25157618522644043\n",
      "epoch: 6 step: 512, loss is 0.2787957191467285\n",
      "epoch: 6 step: 513, loss is 0.19190773367881775\n",
      "epoch: 6 step: 514, loss is 0.2685367465019226\n",
      "epoch: 6 step: 515, loss is 0.28910988569259644\n",
      "epoch: 6 step: 516, loss is 0.26017364859580994\n",
      "epoch: 6 step: 517, loss is 0.31649351119995117\n",
      "epoch: 6 step: 518, loss is 0.16005219519138336\n",
      "epoch: 6 step: 519, loss is 0.44501107931137085\n",
      "epoch: 6 step: 520, loss is 0.23594364523887634\n",
      "epoch: 6 step: 521, loss is 0.28122055530548096\n",
      "epoch: 6 step: 522, loss is 0.25752076506614685\n",
      "epoch: 6 step: 523, loss is 0.22860486805438995\n",
      "epoch: 6 step: 524, loss is 0.33438926935195923\n",
      "epoch: 6 step: 525, loss is 0.2384810894727707\n",
      "epoch: 6 step: 526, loss is 0.23920737206935883\n",
      "epoch: 6 step: 527, loss is 0.3046657145023346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 528, loss is 0.20509257912635803\n",
      "epoch: 6 step: 529, loss is 0.1799536645412445\n",
      "epoch: 6 step: 530, loss is 0.1940285861492157\n",
      "epoch: 6 step: 531, loss is 0.22961381077766418\n",
      "epoch: 6 step: 532, loss is 0.3802911937236786\n",
      "epoch: 6 step: 533, loss is 0.27555933594703674\n",
      "epoch: 6 step: 534, loss is 0.20324240624904633\n",
      "epoch: 6 step: 535, loss is 0.30747976899147034\n",
      "epoch: 6 step: 536, loss is 0.4714473783969879\n",
      "epoch: 6 step: 537, loss is 0.43654704093933105\n",
      "epoch: 6 step: 538, loss is 0.29634830355644226\n",
      "epoch: 6 step: 539, loss is 0.33062946796417236\n",
      "epoch: 6 step: 540, loss is 0.16979101300239563\n",
      "epoch: 6 step: 541, loss is 0.2764090895652771\n",
      "epoch: 6 step: 542, loss is 0.2971978187561035\n",
      "epoch: 6 step: 543, loss is 0.23573040962219238\n",
      "epoch: 6 step: 544, loss is 0.19368934631347656\n",
      "epoch: 6 step: 545, loss is 0.28001680970191956\n",
      "epoch: 6 step: 546, loss is 0.2586539685726166\n",
      "epoch: 6 step: 547, loss is 0.16970984637737274\n",
      "epoch: 6 step: 548, loss is 0.252196729183197\n",
      "epoch: 6 step: 549, loss is 0.29979345202445984\n",
      "epoch: 6 step: 550, loss is 0.44713786244392395\n",
      "epoch: 6 step: 551, loss is 0.32260358333587646\n",
      "epoch: 6 step: 552, loss is 0.2902907431125641\n",
      "epoch: 6 step: 553, loss is 0.43389442563056946\n",
      "epoch: 6 step: 554, loss is 0.4760437607765198\n",
      "epoch: 6 step: 555, loss is 0.3384700119495392\n",
      "epoch: 6 step: 556, loss is 0.21100634336471558\n",
      "epoch: 6 step: 557, loss is 0.19363956153392792\n",
      "epoch: 6 step: 558, loss is 0.34497010707855225\n",
      "epoch: 6 step: 559, loss is 0.18314070999622345\n",
      "epoch: 6 step: 560, loss is 0.30677953362464905\n",
      "epoch: 6 step: 561, loss is 0.505622386932373\n",
      "epoch: 6 step: 562, loss is 0.19281551241874695\n",
      "epoch: 6 step: 563, loss is 0.24169117212295532\n",
      "epoch: 6 step: 564, loss is 0.2470349222421646\n",
      "epoch: 6 step: 565, loss is 0.16411788761615753\n",
      "epoch: 6 step: 566, loss is 0.24354791641235352\n",
      "epoch: 6 step: 567, loss is 0.3010053038597107\n",
      "epoch: 6 step: 568, loss is 0.18591712415218353\n",
      "epoch: 6 step: 569, loss is 0.2636951804161072\n",
      "epoch: 6 step: 570, loss is 0.23412871360778809\n",
      "epoch: 6 step: 571, loss is 0.21512991189956665\n",
      "epoch: 6 step: 572, loss is 0.2828223705291748\n",
      "epoch: 6 step: 573, loss is 0.2534213960170746\n",
      "epoch: 6 step: 574, loss is 0.2125910073518753\n",
      "epoch: 6 step: 575, loss is 0.283092737197876\n",
      "epoch: 6 step: 576, loss is 0.38040557503700256\n",
      "epoch: 6 step: 577, loss is 0.5105679035186768\n",
      "epoch: 6 step: 578, loss is 0.326195627450943\n",
      "epoch: 6 step: 579, loss is 0.19510968029499054\n",
      "epoch: 6 step: 580, loss is 0.21711468696594238\n",
      "epoch: 6 step: 581, loss is 0.21570108830928802\n",
      "epoch: 6 step: 582, loss is 0.21761539578437805\n",
      "epoch: 6 step: 583, loss is 0.4592573642730713\n",
      "epoch: 6 step: 584, loss is 0.27534157037734985\n",
      "epoch: 6 step: 585, loss is 0.3398350477218628\n",
      "epoch: 6 step: 586, loss is 0.34226810932159424\n",
      "epoch: 6 step: 587, loss is 0.17430856823921204\n",
      "epoch: 6 step: 588, loss is 0.22783984243869781\n",
      "epoch: 6 step: 589, loss is 0.12823301553726196\n",
      "epoch: 6 step: 590, loss is 0.3037721812725067\n",
      "epoch: 6 step: 591, loss is 0.37623295187950134\n",
      "epoch: 6 step: 592, loss is 0.2727970778942108\n",
      "epoch: 6 step: 593, loss is 0.29503363370895386\n",
      "epoch: 6 step: 594, loss is 0.352292001247406\n",
      "epoch: 6 step: 595, loss is 0.17597319185733795\n",
      "epoch: 6 step: 596, loss is 0.3348201513290405\n",
      "epoch: 6 step: 597, loss is 0.2691728174686432\n",
      "epoch: 6 step: 598, loss is 0.40293455123901367\n",
      "epoch: 6 step: 599, loss is 0.4289878308773041\n",
      "epoch: 6 step: 600, loss is 0.3731750547885895\n",
      "epoch: 6 step: 601, loss is 0.18998417258262634\n",
      "epoch: 6 step: 602, loss is 0.3359529674053192\n",
      "epoch: 6 step: 603, loss is 0.19976268708705902\n",
      "epoch: 6 step: 604, loss is 0.28035759925842285\n",
      "epoch: 6 step: 605, loss is 0.29832378029823303\n",
      "epoch: 6 step: 606, loss is 0.2974673807621002\n",
      "epoch: 6 step: 607, loss is 0.17996270954608917\n",
      "epoch: 6 step: 608, loss is 0.2529812455177307\n",
      "epoch: 6 step: 609, loss is 0.17434263229370117\n",
      "epoch: 6 step: 610, loss is 0.34191441535949707\n",
      "epoch: 6 step: 611, loss is 0.13725028932094574\n",
      "epoch: 6 step: 612, loss is 0.36784520745277405\n",
      "epoch: 6 step: 613, loss is 0.2263374626636505\n",
      "epoch: 6 step: 614, loss is 0.1950480341911316\n",
      "epoch: 6 step: 615, loss is 0.23033548891544342\n",
      "epoch: 6 step: 616, loss is 0.354459673166275\n",
      "epoch: 6 step: 617, loss is 0.3130654990673065\n",
      "epoch: 6 step: 618, loss is 0.41513770818710327\n",
      "epoch: 6 step: 619, loss is 0.18357177078723907\n",
      "epoch: 6 step: 620, loss is 0.30657798051834106\n",
      "epoch: 6 step: 621, loss is 0.27594584226608276\n",
      "epoch: 6 step: 622, loss is 0.359040230512619\n",
      "epoch: 6 step: 623, loss is 0.2998381555080414\n",
      "epoch: 6 step: 624, loss is 0.37485623359680176\n",
      "epoch: 6 step: 625, loss is 0.22576653957366943\n",
      "epoch: 6 step: 626, loss is 0.2863042950630188\n",
      "epoch: 6 step: 627, loss is 0.21877792477607727\n",
      "epoch: 6 step: 628, loss is 0.35853099822998047\n",
      "epoch: 6 step: 629, loss is 0.2938784956932068\n",
      "epoch: 6 step: 630, loss is 0.3536716401576996\n",
      "epoch: 6 step: 631, loss is 0.27940091490745544\n",
      "epoch: 6 step: 632, loss is 0.32125306129455566\n",
      "epoch: 6 step: 633, loss is 0.24789252877235413\n",
      "epoch: 6 step: 634, loss is 0.17477881908416748\n",
      "epoch: 6 step: 635, loss is 0.2162138819694519\n",
      "epoch: 6 step: 636, loss is 0.23802341520786285\n",
      "epoch: 6 step: 637, loss is 0.31189125776290894\n",
      "epoch: 6 step: 638, loss is 0.23556336760520935\n",
      "epoch: 6 step: 639, loss is 0.342360258102417\n",
      "epoch: 6 step: 640, loss is 0.24094195663928986\n",
      "epoch: 6 step: 641, loss is 0.28456351161003113\n",
      "epoch: 6 step: 642, loss is 0.24560387432575226\n",
      "epoch: 6 step: 643, loss is 0.4978993535041809\n",
      "epoch: 6 step: 644, loss is 0.3090045154094696\n",
      "epoch: 6 step: 645, loss is 0.2461792379617691\n",
      "epoch: 6 step: 646, loss is 0.3603973388671875\n",
      "epoch: 6 step: 647, loss is 0.486371785402298\n",
      "epoch: 6 step: 648, loss is 0.22034189105033875\n",
      "epoch: 6 step: 649, loss is 0.3522946238517761\n",
      "epoch: 6 step: 650, loss is 0.3164242208003998\n",
      "epoch: 6 step: 651, loss is 0.26946496963500977\n",
      "epoch: 6 step: 652, loss is 0.2965065836906433\n",
      "epoch: 6 step: 653, loss is 0.26727572083473206\n",
      "epoch: 6 step: 654, loss is 0.3839568793773651\n",
      "epoch: 6 step: 655, loss is 0.4026528298854828\n",
      "epoch: 6 step: 656, loss is 0.24246171116828918\n",
      "epoch: 6 step: 657, loss is 0.29712536931037903\n",
      "epoch: 6 step: 658, loss is 0.27433234453201294\n",
      "epoch: 6 step: 659, loss is 0.23427094519138336\n",
      "epoch: 6 step: 660, loss is 0.10739421099424362\n",
      "epoch: 6 step: 661, loss is 0.24235574901103973\n",
      "epoch: 6 step: 662, loss is 0.3060145378112793\n",
      "epoch: 6 step: 663, loss is 0.2831341624259949\n",
      "epoch: 6 step: 664, loss is 0.41752517223358154\n",
      "epoch: 6 step: 665, loss is 0.21479344367980957\n",
      "epoch: 6 step: 666, loss is 0.3139224052429199\n",
      "epoch: 6 step: 667, loss is 0.2470128983259201\n",
      "epoch: 6 step: 668, loss is 0.23559236526489258\n",
      "epoch: 6 step: 669, loss is 0.17541201412677765\n",
      "epoch: 6 step: 670, loss is 0.18846286833286285\n",
      "epoch: 6 step: 671, loss is 0.19803935289382935\n",
      "epoch: 6 step: 672, loss is 0.15378829836845398\n",
      "epoch: 6 step: 673, loss is 0.19159527122974396\n",
      "epoch: 6 step: 674, loss is 0.20121799409389496\n",
      "epoch: 6 step: 675, loss is 0.1856154352426529\n",
      "epoch: 6 step: 676, loss is 0.1764589101076126\n",
      "epoch: 6 step: 677, loss is 0.19808368384838104\n",
      "epoch: 6 step: 678, loss is 0.3272029757499695\n",
      "epoch: 6 step: 679, loss is 0.3160117268562317\n",
      "epoch: 6 step: 680, loss is 0.25817084312438965\n",
      "epoch: 6 step: 681, loss is 0.32864493131637573\n",
      "epoch: 6 step: 682, loss is 0.22368285059928894\n",
      "epoch: 6 step: 683, loss is 0.17368091642856598\n",
      "epoch: 6 step: 684, loss is 0.22261297702789307\n",
      "epoch: 6 step: 685, loss is 0.24158848822116852\n",
      "epoch: 6 step: 686, loss is 0.36445677280426025\n",
      "epoch: 6 step: 687, loss is 0.35431626439094543\n",
      "epoch: 6 step: 688, loss is 0.10400974005460739\n",
      "epoch: 6 step: 689, loss is 0.3167443871498108\n",
      "epoch: 6 step: 690, loss is 0.21375495195388794\n",
      "epoch: 6 step: 691, loss is 0.17110295593738556\n",
      "epoch: 6 step: 692, loss is 0.10263128578662872\n",
      "epoch: 6 step: 693, loss is 0.22777099907398224\n",
      "epoch: 6 step: 694, loss is 0.1646212637424469\n",
      "epoch: 6 step: 695, loss is 0.30356305837631226\n",
      "epoch: 6 step: 696, loss is 0.2229306697845459\n",
      "epoch: 6 step: 697, loss is 0.14219830930233002\n",
      "epoch: 6 step: 698, loss is 0.37803006172180176\n",
      "epoch: 6 step: 699, loss is 0.3937484323978424\n",
      "epoch: 6 step: 700, loss is 0.23835918307304382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 701, loss is 0.1568451225757599\n",
      "epoch: 6 step: 702, loss is 0.3112524151802063\n",
      "epoch: 6 step: 703, loss is 0.26457837224006653\n",
      "epoch: 6 step: 704, loss is 0.2881013751029968\n",
      "epoch: 6 step: 705, loss is 0.3454343378543854\n",
      "epoch: 6 step: 706, loss is 0.3321487605571747\n",
      "epoch: 6 step: 707, loss is 0.25411731004714966\n",
      "epoch: 6 step: 708, loss is 0.31773388385772705\n",
      "epoch: 6 step: 709, loss is 0.22968821227550507\n",
      "epoch: 6 step: 710, loss is 0.33325034379959106\n",
      "epoch: 6 step: 711, loss is 0.26871970295906067\n",
      "epoch: 6 step: 712, loss is 0.21730077266693115\n",
      "epoch: 6 step: 713, loss is 0.21536268293857574\n",
      "epoch: 6 step: 714, loss is 0.22685998678207397\n",
      "epoch: 6 step: 715, loss is 0.2760245203971863\n",
      "epoch: 6 step: 716, loss is 0.16210712492465973\n",
      "epoch: 6 step: 717, loss is 0.4092198312282562\n",
      "epoch: 6 step: 718, loss is 0.32478755712509155\n",
      "epoch: 6 step: 719, loss is 0.33604490756988525\n",
      "epoch: 6 step: 720, loss is 0.17555689811706543\n",
      "epoch: 6 step: 721, loss is 0.4811100959777832\n",
      "epoch: 6 step: 722, loss is 0.2746700346469879\n",
      "epoch: 6 step: 723, loss is 0.22082805633544922\n",
      "epoch: 6 step: 724, loss is 0.2709302604198456\n",
      "epoch: 6 step: 725, loss is 0.3387766182422638\n",
      "epoch: 6 step: 726, loss is 0.3701235055923462\n",
      "epoch: 6 step: 727, loss is 0.35027915239334106\n",
      "epoch: 6 step: 728, loss is 0.24170134961605072\n",
      "epoch: 6 step: 729, loss is 0.12133241444826126\n",
      "epoch: 6 step: 730, loss is 0.22162741422653198\n",
      "epoch: 6 step: 731, loss is 0.21909844875335693\n",
      "epoch: 6 step: 732, loss is 0.4292670786380768\n",
      "epoch: 6 step: 733, loss is 0.17226925492286682\n",
      "epoch: 6 step: 734, loss is 0.23099718987941742\n",
      "epoch: 6 step: 735, loss is 0.28033941984176636\n",
      "epoch: 6 step: 736, loss is 0.34666648507118225\n",
      "epoch: 6 step: 737, loss is 0.16477124392986298\n",
      "epoch: 6 step: 738, loss is 0.23458430171012878\n",
      "epoch: 6 step: 739, loss is 0.26272672414779663\n",
      "epoch: 6 step: 740, loss is 0.2280215322971344\n",
      "epoch: 6 step: 741, loss is 0.2644968330860138\n",
      "epoch: 6 step: 742, loss is 0.3428939878940582\n",
      "epoch: 6 step: 743, loss is 0.24671541154384613\n",
      "epoch: 6 step: 744, loss is 0.51650470495224\n",
      "epoch: 6 step: 745, loss is 0.41524961590766907\n",
      "epoch: 6 step: 746, loss is 0.2657090425491333\n",
      "epoch: 6 step: 747, loss is 0.1979348063468933\n",
      "epoch: 6 step: 748, loss is 0.29902157187461853\n",
      "epoch: 6 step: 749, loss is 0.26329344511032104\n",
      "epoch: 6 step: 750, loss is 0.29665547609329224\n",
      "epoch: 6 step: 751, loss is 0.27401575446128845\n",
      "epoch: 6 step: 752, loss is 0.3604632318019867\n",
      "epoch: 6 step: 753, loss is 0.21779261529445648\n",
      "epoch: 6 step: 754, loss is 0.2028629630804062\n",
      "epoch: 6 step: 755, loss is 0.4204181730747223\n",
      "epoch: 6 step: 756, loss is 0.33582815527915955\n",
      "epoch: 6 step: 757, loss is 0.2586110532283783\n",
      "epoch: 6 step: 758, loss is 0.16543419659137726\n",
      "epoch: 6 step: 759, loss is 0.3144606649875641\n",
      "epoch: 6 step: 760, loss is 0.12620529532432556\n",
      "epoch: 6 step: 761, loss is 0.33698877692222595\n",
      "epoch: 6 step: 762, loss is 0.3512306809425354\n",
      "epoch: 6 step: 763, loss is 0.41302168369293213\n",
      "epoch: 6 step: 764, loss is 0.3536960780620575\n",
      "epoch: 6 step: 765, loss is 0.27315568923950195\n",
      "epoch: 6 step: 766, loss is 0.24091659486293793\n",
      "epoch: 6 step: 767, loss is 0.44359707832336426\n",
      "epoch: 6 step: 768, loss is 0.32837817072868347\n",
      "epoch: 6 step: 769, loss is 0.20377017557621002\n",
      "epoch: 6 step: 770, loss is 0.24218793213367462\n",
      "epoch: 6 step: 771, loss is 0.2966838479042053\n",
      "epoch: 6 step: 772, loss is 0.29348841309547424\n",
      "epoch: 6 step: 773, loss is 0.20518858730793\n",
      "epoch: 6 step: 774, loss is 0.30024033784866333\n",
      "epoch: 6 step: 775, loss is 0.15930062532424927\n",
      "epoch: 6 step: 776, loss is 0.21832755208015442\n",
      "epoch: 6 step: 777, loss is 0.2282797396183014\n",
      "epoch: 6 step: 778, loss is 0.4328187108039856\n",
      "epoch: 6 step: 779, loss is 0.23930689692497253\n",
      "epoch: 6 step: 780, loss is 0.20201919972896576\n",
      "epoch: 6 step: 781, loss is 0.18950240314006805\n",
      "epoch: 6 step: 782, loss is 0.3578525483608246\n",
      "epoch: 6 step: 783, loss is 0.30709826946258545\n",
      "epoch: 6 step: 784, loss is 0.3634204566478729\n",
      "epoch: 6 step: 785, loss is 0.31659746170043945\n",
      "epoch: 6 step: 786, loss is 0.39782559871673584\n",
      "epoch: 6 step: 787, loss is 0.21286477148532867\n",
      "epoch: 6 step: 788, loss is 0.2628237009048462\n",
      "epoch: 6 step: 789, loss is 0.2852281928062439\n",
      "epoch: 6 step: 790, loss is 0.5526130795478821\n",
      "epoch: 6 step: 791, loss is 0.21507102251052856\n",
      "epoch: 6 step: 792, loss is 0.2774791717529297\n",
      "epoch: 6 step: 793, loss is 0.3940427005290985\n",
      "epoch: 6 step: 794, loss is 0.3077588677406311\n",
      "epoch: 6 step: 795, loss is 0.37381067872047424\n",
      "epoch: 6 step: 796, loss is 0.25605493783950806\n",
      "epoch: 6 step: 797, loss is 0.298027366399765\n",
      "epoch: 6 step: 798, loss is 0.22016511857509613\n",
      "epoch: 6 step: 799, loss is 0.4185267686843872\n",
      "epoch: 6 step: 800, loss is 0.24937422573566437\n",
      "epoch: 6 step: 801, loss is 0.126582071185112\n",
      "epoch: 6 step: 802, loss is 0.21927471458911896\n",
      "epoch: 6 step: 803, loss is 0.1904529631137848\n",
      "epoch: 6 step: 804, loss is 0.26145827770233154\n",
      "epoch: 6 step: 805, loss is 0.25147807598114014\n",
      "epoch: 6 step: 806, loss is 0.2028597593307495\n",
      "epoch: 6 step: 807, loss is 0.12821762263774872\n",
      "epoch: 6 step: 808, loss is 0.4152241349220276\n",
      "epoch: 6 step: 809, loss is 0.5537688136100769\n",
      "epoch: 6 step: 810, loss is 0.3620894253253937\n",
      "epoch: 6 step: 811, loss is 0.11747167259454727\n",
      "epoch: 6 step: 812, loss is 0.13278889656066895\n",
      "epoch: 6 step: 813, loss is 0.4446285367012024\n",
      "epoch: 6 step: 814, loss is 0.2818630337715149\n",
      "epoch: 6 step: 815, loss is 0.22442904114723206\n",
      "epoch: 6 step: 816, loss is 0.29981163144111633\n",
      "epoch: 6 step: 817, loss is 0.3296271562576294\n",
      "epoch: 6 step: 818, loss is 0.1682586967945099\n",
      "epoch: 6 step: 819, loss is 0.22335688769817352\n",
      "epoch: 6 step: 820, loss is 0.3238842487335205\n",
      "epoch: 6 step: 821, loss is 0.15048152208328247\n",
      "epoch: 6 step: 822, loss is 0.16541863977909088\n",
      "epoch: 6 step: 823, loss is 0.31343963742256165\n",
      "epoch: 6 step: 824, loss is 0.24233782291412354\n",
      "epoch: 6 step: 825, loss is 0.40356379747390747\n",
      "epoch: 6 step: 826, loss is 0.23907405138015747\n",
      "epoch: 6 step: 827, loss is 0.14695973694324493\n",
      "epoch: 6 step: 828, loss is 0.2415713369846344\n",
      "epoch: 6 step: 829, loss is 0.22347815334796906\n",
      "epoch: 6 step: 830, loss is 0.3549053966999054\n",
      "epoch: 6 step: 831, loss is 0.2703982889652252\n",
      "epoch: 6 step: 832, loss is 0.14810988306999207\n",
      "epoch: 6 step: 833, loss is 0.24795499444007874\n",
      "epoch: 6 step: 834, loss is 0.2380414456129074\n",
      "epoch: 6 step: 835, loss is 0.33377769589424133\n",
      "epoch: 6 step: 836, loss is 0.30655109882354736\n",
      "epoch: 6 step: 837, loss is 0.1427239030599594\n",
      "epoch: 6 step: 838, loss is 0.28388872742652893\n",
      "epoch: 6 step: 839, loss is 0.6102609038352966\n",
      "epoch: 6 step: 840, loss is 0.2541644275188446\n",
      "epoch: 6 step: 841, loss is 0.35923755168914795\n",
      "epoch: 6 step: 842, loss is 0.262935996055603\n",
      "epoch: 6 step: 843, loss is 0.19011381268501282\n",
      "epoch: 6 step: 844, loss is 0.31116917729377747\n",
      "epoch: 6 step: 845, loss is 0.25913751125335693\n",
      "epoch: 6 step: 846, loss is 0.3172641098499298\n",
      "epoch: 6 step: 847, loss is 0.31878045201301575\n",
      "epoch: 6 step: 848, loss is 0.2706362307071686\n",
      "epoch: 6 step: 849, loss is 0.4347192049026489\n",
      "epoch: 6 step: 850, loss is 0.3379423916339874\n",
      "epoch: 6 step: 851, loss is 0.2886291444301605\n",
      "epoch: 6 step: 852, loss is 0.15989047288894653\n",
      "epoch: 6 step: 853, loss is 0.3369423747062683\n",
      "epoch: 6 step: 854, loss is 0.27003684639930725\n",
      "epoch: 6 step: 855, loss is 0.12424582988023758\n",
      "epoch: 6 step: 856, loss is 0.17681002616882324\n",
      "epoch: 6 step: 857, loss is 0.21480104327201843\n",
      "epoch: 6 step: 858, loss is 0.25619176030158997\n",
      "epoch: 6 step: 859, loss is 0.2469196766614914\n",
      "epoch: 6 step: 860, loss is 0.2175104171037674\n",
      "epoch: 6 step: 861, loss is 0.18423889577388763\n",
      "epoch: 6 step: 862, loss is 0.3780861496925354\n",
      "epoch: 6 step: 863, loss is 0.2817372679710388\n",
      "epoch: 6 step: 864, loss is 0.358303427696228\n",
      "epoch: 6 step: 865, loss is 0.3187713623046875\n",
      "epoch: 6 step: 866, loss is 0.14635126292705536\n",
      "epoch: 6 step: 867, loss is 0.17221905291080475\n",
      "epoch: 6 step: 868, loss is 0.2671394348144531\n",
      "epoch: 6 step: 869, loss is 0.38040080666542053\n",
      "epoch: 6 step: 870, loss is 0.2172311246395111\n",
      "epoch: 6 step: 871, loss is 0.33103856444358826\n",
      "epoch: 6 step: 872, loss is 0.1641777604818344\n",
      "epoch: 6 step: 873, loss is 0.1679335981607437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 874, loss is 0.5018686056137085\n",
      "epoch: 6 step: 875, loss is 0.37974944710731506\n",
      "epoch: 6 step: 876, loss is 0.3135624825954437\n",
      "epoch: 6 step: 877, loss is 0.2851647138595581\n",
      "epoch: 6 step: 878, loss is 0.1874663382768631\n",
      "epoch: 6 step: 879, loss is 0.19883495569229126\n",
      "epoch: 6 step: 880, loss is 0.3059294521808624\n",
      "epoch: 6 step: 881, loss is 0.3130832612514496\n",
      "epoch: 6 step: 882, loss is 0.33354446291923523\n",
      "epoch: 6 step: 883, loss is 0.2930728793144226\n",
      "epoch: 6 step: 884, loss is 0.2080177366733551\n",
      "epoch: 6 step: 885, loss is 0.22399573028087616\n",
      "epoch: 6 step: 886, loss is 0.23268015682697296\n",
      "epoch: 6 step: 887, loss is 0.24507537484169006\n",
      "epoch: 6 step: 888, loss is 0.27036020159721375\n",
      "epoch: 6 step: 889, loss is 0.3379732370376587\n",
      "epoch: 6 step: 890, loss is 0.1460483968257904\n",
      "epoch: 6 step: 891, loss is 0.3167801797389984\n",
      "epoch: 6 step: 892, loss is 0.23326651751995087\n",
      "epoch: 6 step: 893, loss is 0.22018671035766602\n",
      "epoch: 6 step: 894, loss is 0.38161197304725647\n",
      "epoch: 6 step: 895, loss is 0.33382830023765564\n",
      "epoch: 6 step: 896, loss is 0.24739477038383484\n",
      "epoch: 6 step: 897, loss is 0.36991390585899353\n",
      "epoch: 6 step: 898, loss is 0.3678942024707794\n",
      "epoch: 6 step: 899, loss is 0.4627499282360077\n",
      "epoch: 6 step: 900, loss is 0.21446293592453003\n",
      "epoch: 6 step: 901, loss is 0.3518720865249634\n",
      "epoch: 6 step: 902, loss is 0.28547003865242004\n",
      "epoch: 6 step: 903, loss is 0.30805903673171997\n",
      "epoch: 6 step: 904, loss is 0.21380870044231415\n",
      "epoch: 6 step: 905, loss is 0.08389059454202652\n",
      "epoch: 6 step: 906, loss is 0.1940622329711914\n",
      "epoch: 6 step: 907, loss is 0.2581424415111542\n",
      "epoch: 6 step: 908, loss is 0.24565553665161133\n",
      "epoch: 6 step: 909, loss is 0.22035080194473267\n",
      "epoch: 6 step: 910, loss is 0.20563015341758728\n",
      "epoch: 6 step: 911, loss is 0.18766197562217712\n",
      "epoch: 6 step: 912, loss is 0.21335221827030182\n",
      "epoch: 6 step: 913, loss is 0.2969207763671875\n",
      "epoch: 6 step: 914, loss is 0.20133711397647858\n",
      "epoch: 6 step: 915, loss is 0.2267305552959442\n",
      "epoch: 6 step: 916, loss is 0.44394615292549133\n",
      "epoch: 6 step: 917, loss is 0.27349674701690674\n",
      "epoch: 6 step: 918, loss is 0.19446082413196564\n",
      "epoch: 6 step: 919, loss is 0.325383722782135\n",
      "epoch: 6 step: 920, loss is 0.2706507444381714\n",
      "epoch: 6 step: 921, loss is 0.43022391200065613\n",
      "epoch: 6 step: 922, loss is 0.17241524159908295\n",
      "epoch: 6 step: 923, loss is 0.19175995886325836\n",
      "epoch: 6 step: 924, loss is 0.16549496352672577\n",
      "epoch: 6 step: 925, loss is 0.2249504178762436\n",
      "epoch: 6 step: 926, loss is 0.4442054331302643\n",
      "epoch: 6 step: 927, loss is 0.17911331355571747\n",
      "epoch: 6 step: 928, loss is 0.49848198890686035\n",
      "epoch: 6 step: 929, loss is 0.23370850086212158\n",
      "epoch: 6 step: 930, loss is 0.13114015758037567\n",
      "epoch: 6 step: 931, loss is 0.1671859622001648\n",
      "epoch: 6 step: 932, loss is 0.35816776752471924\n",
      "epoch: 6 step: 933, loss is 0.28302815556526184\n",
      "epoch: 6 step: 934, loss is 0.2496902346611023\n",
      "epoch: 6 step: 935, loss is 0.17186152935028076\n",
      "epoch: 6 step: 936, loss is 0.2538089156150818\n",
      "epoch: 6 step: 937, loss is 0.42362627387046814\n",
      "epoch: 7 step: 1, loss is 0.2209915816783905\n",
      "epoch: 7 step: 2, loss is 0.2159392386674881\n",
      "epoch: 7 step: 3, loss is 0.21537500619888306\n",
      "epoch: 7 step: 4, loss is 0.12934432923793793\n",
      "epoch: 7 step: 5, loss is 0.26090389490127563\n",
      "epoch: 7 step: 6, loss is 0.24658189713954926\n",
      "epoch: 7 step: 7, loss is 0.1288834661245346\n",
      "epoch: 7 step: 8, loss is 0.3091491162776947\n",
      "epoch: 7 step: 9, loss is 0.21090692281723022\n",
      "epoch: 7 step: 10, loss is 0.27525249123573303\n",
      "epoch: 7 step: 11, loss is 0.1777801364660263\n",
      "epoch: 7 step: 12, loss is 0.4270011782646179\n",
      "epoch: 7 step: 13, loss is 0.31364235281944275\n",
      "epoch: 7 step: 14, loss is 0.31016239523887634\n",
      "epoch: 7 step: 15, loss is 0.16661906242370605\n",
      "epoch: 7 step: 16, loss is 0.22944863140583038\n",
      "epoch: 7 step: 17, loss is 0.14500996470451355\n",
      "epoch: 7 step: 18, loss is 0.16963081061840057\n",
      "epoch: 7 step: 19, loss is 0.1425936222076416\n",
      "epoch: 7 step: 20, loss is 0.28673309087753296\n",
      "epoch: 7 step: 21, loss is 0.13457144796848297\n",
      "epoch: 7 step: 22, loss is 0.20272105932235718\n",
      "epoch: 7 step: 23, loss is 0.2859155535697937\n",
      "epoch: 7 step: 24, loss is 0.21777765452861786\n",
      "epoch: 7 step: 25, loss is 0.3282332718372345\n",
      "epoch: 7 step: 26, loss is 0.3675404489040375\n",
      "epoch: 7 step: 27, loss is 0.21554292738437653\n",
      "epoch: 7 step: 28, loss is 0.2049037516117096\n",
      "epoch: 7 step: 29, loss is 0.18406416475772858\n",
      "epoch: 7 step: 30, loss is 0.4495219588279724\n",
      "epoch: 7 step: 31, loss is 0.27666670083999634\n",
      "epoch: 7 step: 32, loss is 0.2592567801475525\n",
      "epoch: 7 step: 33, loss is 0.11086168885231018\n",
      "epoch: 7 step: 34, loss is 0.18156659603118896\n",
      "epoch: 7 step: 35, loss is 0.26265132427215576\n",
      "epoch: 7 step: 36, loss is 0.2223069965839386\n",
      "epoch: 7 step: 37, loss is 0.2819708287715912\n",
      "epoch: 7 step: 38, loss is 0.18517033755779266\n",
      "epoch: 7 step: 39, loss is 0.19599837064743042\n",
      "epoch: 7 step: 40, loss is 0.4652237892150879\n",
      "epoch: 7 step: 41, loss is 0.1162465512752533\n",
      "epoch: 7 step: 42, loss is 0.19568219780921936\n",
      "epoch: 7 step: 43, loss is 0.13073299825191498\n",
      "epoch: 7 step: 44, loss is 0.4857862591743469\n",
      "epoch: 7 step: 45, loss is 0.19679568707942963\n",
      "epoch: 7 step: 46, loss is 0.17947474122047424\n",
      "epoch: 7 step: 47, loss is 0.18057407438755035\n",
      "epoch: 7 step: 48, loss is 0.26983770728111267\n",
      "epoch: 7 step: 49, loss is 0.35389402508735657\n",
      "epoch: 7 step: 50, loss is 0.15704387426376343\n",
      "epoch: 7 step: 51, loss is 0.25823676586151123\n",
      "epoch: 7 step: 52, loss is 0.2580656409263611\n",
      "epoch: 7 step: 53, loss is 0.3269933760166168\n",
      "epoch: 7 step: 54, loss is 0.3111976385116577\n",
      "epoch: 7 step: 55, loss is 0.20894192159175873\n",
      "epoch: 7 step: 56, loss is 0.40102624893188477\n",
      "epoch: 7 step: 57, loss is 0.3691173195838928\n",
      "epoch: 7 step: 58, loss is 0.15776607394218445\n",
      "epoch: 7 step: 59, loss is 0.17394188046455383\n",
      "epoch: 7 step: 60, loss is 0.3216841518878937\n",
      "epoch: 7 step: 61, loss is 0.2223970592021942\n",
      "epoch: 7 step: 62, loss is 0.19041194021701813\n",
      "epoch: 7 step: 63, loss is 0.20286080241203308\n",
      "epoch: 7 step: 64, loss is 0.24069209396839142\n",
      "epoch: 7 step: 65, loss is 0.21025164425373077\n",
      "epoch: 7 step: 66, loss is 0.3541581332683563\n",
      "epoch: 7 step: 67, loss is 0.2903352975845337\n",
      "epoch: 7 step: 68, loss is 0.3233804702758789\n",
      "epoch: 7 step: 69, loss is 0.08212795853614807\n",
      "epoch: 7 step: 70, loss is 0.2699373662471771\n",
      "epoch: 7 step: 71, loss is 0.3716907799243927\n",
      "epoch: 7 step: 72, loss is 0.2709795832633972\n",
      "epoch: 7 step: 73, loss is 0.18133358657360077\n",
      "epoch: 7 step: 74, loss is 0.29639118909835815\n",
      "epoch: 7 step: 75, loss is 0.23548761010169983\n",
      "epoch: 7 step: 76, loss is 0.4044531583786011\n",
      "epoch: 7 step: 77, loss is 0.19382911920547485\n",
      "epoch: 7 step: 78, loss is 0.34023115038871765\n",
      "epoch: 7 step: 79, loss is 0.23545891046524048\n",
      "epoch: 7 step: 80, loss is 0.2852545380592346\n",
      "epoch: 7 step: 81, loss is 0.20211665332317352\n",
      "epoch: 7 step: 82, loss is 0.2659238874912262\n",
      "epoch: 7 step: 83, loss is 0.31678903102874756\n",
      "epoch: 7 step: 84, loss is 0.26356253027915955\n",
      "epoch: 7 step: 85, loss is 0.163748100399971\n",
      "epoch: 7 step: 86, loss is 0.47603100538253784\n",
      "epoch: 7 step: 87, loss is 0.2020564079284668\n",
      "epoch: 7 step: 88, loss is 0.33433714509010315\n",
      "epoch: 7 step: 89, loss is 0.23955057561397552\n",
      "epoch: 7 step: 90, loss is 0.1710338443517685\n",
      "epoch: 7 step: 91, loss is 0.539654552936554\n",
      "epoch: 7 step: 92, loss is 0.2618280053138733\n",
      "epoch: 7 step: 93, loss is 0.11755837500095367\n",
      "epoch: 7 step: 94, loss is 0.24427230656147003\n",
      "epoch: 7 step: 95, loss is 0.16804370284080505\n",
      "epoch: 7 step: 96, loss is 0.451217919588089\n",
      "epoch: 7 step: 97, loss is 0.2028176635503769\n",
      "epoch: 7 step: 98, loss is 0.2928425967693329\n",
      "epoch: 7 step: 99, loss is 0.33529824018478394\n",
      "epoch: 7 step: 100, loss is 0.36321672797203064\n",
      "epoch: 7 step: 101, loss is 0.25567948818206787\n",
      "epoch: 7 step: 102, loss is 0.31966984272003174\n",
      "epoch: 7 step: 103, loss is 0.1509534865617752\n",
      "epoch: 7 step: 104, loss is 0.371734082698822\n",
      "epoch: 7 step: 105, loss is 0.25021156668663025\n",
      "epoch: 7 step: 106, loss is 0.24899247288703918\n",
      "epoch: 7 step: 107, loss is 0.35897374153137207\n",
      "epoch: 7 step: 108, loss is 0.46739429235458374\n",
      "epoch: 7 step: 109, loss is 0.17535075545310974\n",
      "epoch: 7 step: 110, loss is 0.2442326694726944\n",
      "epoch: 7 step: 111, loss is 0.45619451999664307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 112, loss is 0.36193642020225525\n",
      "epoch: 7 step: 113, loss is 0.306232213973999\n",
      "epoch: 7 step: 114, loss is 0.170233353972435\n",
      "epoch: 7 step: 115, loss is 0.17667505145072937\n",
      "epoch: 7 step: 116, loss is 0.31257733702659607\n",
      "epoch: 7 step: 117, loss is 0.1471719890832901\n",
      "epoch: 7 step: 118, loss is 0.28032809495925903\n",
      "epoch: 7 step: 119, loss is 0.23460371792316437\n",
      "epoch: 7 step: 120, loss is 0.18164129555225372\n",
      "epoch: 7 step: 121, loss is 0.2422477900981903\n",
      "epoch: 7 step: 122, loss is 0.20561492443084717\n",
      "epoch: 7 step: 123, loss is 0.19170475006103516\n",
      "epoch: 7 step: 124, loss is 0.25933095812797546\n",
      "epoch: 7 step: 125, loss is 0.20782029628753662\n",
      "epoch: 7 step: 126, loss is 0.17924636602401733\n",
      "epoch: 7 step: 127, loss is 0.284610390663147\n",
      "epoch: 7 step: 128, loss is 0.33747589588165283\n",
      "epoch: 7 step: 129, loss is 0.21583113074302673\n",
      "epoch: 7 step: 130, loss is 0.26974669098854065\n",
      "epoch: 7 step: 131, loss is 0.19318938255310059\n",
      "epoch: 7 step: 132, loss is 0.31026288866996765\n",
      "epoch: 7 step: 133, loss is 0.33328667283058167\n",
      "epoch: 7 step: 134, loss is 0.31097498536109924\n",
      "epoch: 7 step: 135, loss is 0.21509377658367157\n",
      "epoch: 7 step: 136, loss is 0.14791780710220337\n",
      "epoch: 7 step: 137, loss is 0.38593244552612305\n",
      "epoch: 7 step: 138, loss is 0.11565474420785904\n",
      "epoch: 7 step: 139, loss is 0.288764625787735\n",
      "epoch: 7 step: 140, loss is 0.3588413596153259\n",
      "epoch: 7 step: 141, loss is 0.2576126754283905\n",
      "epoch: 7 step: 142, loss is 0.3527134358882904\n",
      "epoch: 7 step: 143, loss is 0.2956239581108093\n",
      "epoch: 7 step: 144, loss is 0.33187684416770935\n",
      "epoch: 7 step: 145, loss is 0.2286801040172577\n",
      "epoch: 7 step: 146, loss is 0.3085486590862274\n",
      "epoch: 7 step: 147, loss is 0.2277253419160843\n",
      "epoch: 7 step: 148, loss is 0.38802090287208557\n",
      "epoch: 7 step: 149, loss is 0.2841187119483948\n",
      "epoch: 7 step: 150, loss is 0.21384131908416748\n",
      "epoch: 7 step: 151, loss is 0.25939711928367615\n",
      "epoch: 7 step: 152, loss is 0.19755268096923828\n",
      "epoch: 7 step: 153, loss is 0.2901780903339386\n",
      "epoch: 7 step: 154, loss is 0.23240463435649872\n",
      "epoch: 7 step: 155, loss is 0.27219632267951965\n",
      "epoch: 7 step: 156, loss is 0.34575557708740234\n",
      "epoch: 7 step: 157, loss is 0.22053079307079315\n",
      "epoch: 7 step: 158, loss is 0.11749821156263351\n",
      "epoch: 7 step: 159, loss is 0.2874567210674286\n",
      "epoch: 7 step: 160, loss is 0.2768135964870453\n",
      "epoch: 7 step: 161, loss is 0.24919185042381287\n",
      "epoch: 7 step: 162, loss is 0.2776636779308319\n",
      "epoch: 7 step: 163, loss is 0.29591232538223267\n",
      "epoch: 7 step: 164, loss is 0.22743842005729675\n",
      "epoch: 7 step: 165, loss is 0.24780935049057007\n",
      "epoch: 7 step: 166, loss is 0.40723052620887756\n",
      "epoch: 7 step: 167, loss is 0.3275863528251648\n",
      "epoch: 7 step: 168, loss is 0.23392502963542938\n",
      "epoch: 7 step: 169, loss is 0.3146052956581116\n",
      "epoch: 7 step: 170, loss is 0.17847594618797302\n",
      "epoch: 7 step: 171, loss is 0.15926049649715424\n",
      "epoch: 7 step: 172, loss is 0.23250535130500793\n",
      "epoch: 7 step: 173, loss is 0.312184602022171\n",
      "epoch: 7 step: 174, loss is 0.28581929206848145\n",
      "epoch: 7 step: 175, loss is 0.1837504804134369\n",
      "epoch: 7 step: 176, loss is 0.31355780363082886\n",
      "epoch: 7 step: 177, loss is 0.2362843155860901\n",
      "epoch: 7 step: 178, loss is 0.35277292132377625\n",
      "epoch: 7 step: 179, loss is 0.4101281762123108\n",
      "epoch: 7 step: 180, loss is 0.17181722819805145\n",
      "epoch: 7 step: 181, loss is 0.19289608299732208\n",
      "epoch: 7 step: 182, loss is 0.20374952256679535\n",
      "epoch: 7 step: 183, loss is 0.44144490361213684\n",
      "epoch: 7 step: 184, loss is 0.4374312460422516\n",
      "epoch: 7 step: 185, loss is 0.22522534430027008\n",
      "epoch: 7 step: 186, loss is 0.2776463031768799\n",
      "epoch: 7 step: 187, loss is 0.22519437968730927\n",
      "epoch: 7 step: 188, loss is 0.27690204977989197\n",
      "epoch: 7 step: 189, loss is 0.3193853497505188\n",
      "epoch: 7 step: 190, loss is 0.18912388384342194\n",
      "epoch: 7 step: 191, loss is 0.16148746013641357\n",
      "epoch: 7 step: 192, loss is 0.1605188548564911\n",
      "epoch: 7 step: 193, loss is 0.3138781487941742\n",
      "epoch: 7 step: 194, loss is 0.4469474256038666\n",
      "epoch: 7 step: 195, loss is 0.18448388576507568\n",
      "epoch: 7 step: 196, loss is 0.21426799893379211\n",
      "epoch: 7 step: 197, loss is 0.2159433364868164\n",
      "epoch: 7 step: 198, loss is 0.30518585443496704\n",
      "epoch: 7 step: 199, loss is 0.2867078185081482\n",
      "epoch: 7 step: 200, loss is 0.29445984959602356\n",
      "epoch: 7 step: 201, loss is 0.24011538922786713\n",
      "epoch: 7 step: 202, loss is 0.20729120075702667\n",
      "epoch: 7 step: 203, loss is 0.20270027220249176\n",
      "epoch: 7 step: 204, loss is 0.2904263138771057\n",
      "epoch: 7 step: 205, loss is 0.12039326876401901\n",
      "epoch: 7 step: 206, loss is 0.48712143301963806\n",
      "epoch: 7 step: 207, loss is 0.3077230453491211\n",
      "epoch: 7 step: 208, loss is 0.20831483602523804\n",
      "epoch: 7 step: 209, loss is 0.27218490839004517\n",
      "epoch: 7 step: 210, loss is 0.3582766354084015\n",
      "epoch: 7 step: 211, loss is 0.2303515076637268\n",
      "epoch: 7 step: 212, loss is 0.21194329857826233\n",
      "epoch: 7 step: 213, loss is 0.2363632619380951\n",
      "epoch: 7 step: 214, loss is 0.23393240571022034\n",
      "epoch: 7 step: 215, loss is 0.2812487781047821\n",
      "epoch: 7 step: 216, loss is 0.2397453933954239\n",
      "epoch: 7 step: 217, loss is 0.269425630569458\n",
      "epoch: 7 step: 218, loss is 0.2879336178302765\n",
      "epoch: 7 step: 219, loss is 0.14891374111175537\n",
      "epoch: 7 step: 220, loss is 0.17246894538402557\n",
      "epoch: 7 step: 221, loss is 0.20420601963996887\n",
      "epoch: 7 step: 222, loss is 0.1955176293849945\n",
      "epoch: 7 step: 223, loss is 0.2316747009754181\n",
      "epoch: 7 step: 224, loss is 0.3301507234573364\n",
      "epoch: 7 step: 225, loss is 0.1931217610836029\n",
      "epoch: 7 step: 226, loss is 0.2011772096157074\n",
      "epoch: 7 step: 227, loss is 0.3323763608932495\n",
      "epoch: 7 step: 228, loss is 0.13419382274150848\n",
      "epoch: 7 step: 229, loss is 0.25492843985557556\n",
      "epoch: 7 step: 230, loss is 0.21963617205619812\n",
      "epoch: 7 step: 231, loss is 0.18665319681167603\n",
      "epoch: 7 step: 232, loss is 0.37274932861328125\n",
      "epoch: 7 step: 233, loss is 0.18639510869979858\n",
      "epoch: 7 step: 234, loss is 0.31909987330436707\n",
      "epoch: 7 step: 235, loss is 0.19550162553787231\n",
      "epoch: 7 step: 236, loss is 0.29748573899269104\n",
      "epoch: 7 step: 237, loss is 0.28128430247306824\n",
      "epoch: 7 step: 238, loss is 0.16884957253932953\n",
      "epoch: 7 step: 239, loss is 0.35391896963119507\n",
      "epoch: 7 step: 240, loss is 0.5145733952522278\n",
      "epoch: 7 step: 241, loss is 0.2901071608066559\n",
      "epoch: 7 step: 242, loss is 0.2141837626695633\n",
      "epoch: 7 step: 243, loss is 0.3095666766166687\n",
      "epoch: 7 step: 244, loss is 0.361706405878067\n",
      "epoch: 7 step: 245, loss is 0.20008760690689087\n",
      "epoch: 7 step: 246, loss is 0.29896026849746704\n",
      "epoch: 7 step: 247, loss is 0.15626153349876404\n",
      "epoch: 7 step: 248, loss is 0.1621655523777008\n",
      "epoch: 7 step: 249, loss is 0.33245599269866943\n",
      "epoch: 7 step: 250, loss is 0.23564402759075165\n",
      "epoch: 7 step: 251, loss is 0.2668469250202179\n",
      "epoch: 7 step: 252, loss is 0.33415085077285767\n",
      "epoch: 7 step: 253, loss is 0.44011402130126953\n",
      "epoch: 7 step: 254, loss is 0.1902044117450714\n",
      "epoch: 7 step: 255, loss is 0.3087390959262848\n",
      "epoch: 7 step: 256, loss is 0.25315579771995544\n",
      "epoch: 7 step: 257, loss is 0.22925105690956116\n",
      "epoch: 7 step: 258, loss is 0.16873317956924438\n",
      "epoch: 7 step: 259, loss is 0.28386902809143066\n",
      "epoch: 7 step: 260, loss is 0.24043899774551392\n",
      "epoch: 7 step: 261, loss is 0.36414864659309387\n",
      "epoch: 7 step: 262, loss is 0.19980767369270325\n",
      "epoch: 7 step: 263, loss is 0.22731927037239075\n",
      "epoch: 7 step: 264, loss is 0.35886350274086\n",
      "epoch: 7 step: 265, loss is 0.21676786243915558\n",
      "epoch: 7 step: 266, loss is 0.4437931776046753\n",
      "epoch: 7 step: 267, loss is 0.15007255971431732\n",
      "epoch: 7 step: 268, loss is 0.13272343575954437\n",
      "epoch: 7 step: 269, loss is 0.2612673044204712\n",
      "epoch: 7 step: 270, loss is 0.2489437758922577\n",
      "epoch: 7 step: 271, loss is 0.17030514776706696\n",
      "epoch: 7 step: 272, loss is 0.1978788524866104\n",
      "epoch: 7 step: 273, loss is 0.20791466534137726\n",
      "epoch: 7 step: 274, loss is 0.3122716248035431\n",
      "epoch: 7 step: 275, loss is 0.379641056060791\n",
      "epoch: 7 step: 276, loss is 0.3947243094444275\n",
      "epoch: 7 step: 277, loss is 0.2409839928150177\n",
      "epoch: 7 step: 278, loss is 0.2066904902458191\n",
      "epoch: 7 step: 279, loss is 0.2383459061384201\n",
      "epoch: 7 step: 280, loss is 0.3020796775817871\n",
      "epoch: 7 step: 281, loss is 0.37762027978897095\n",
      "epoch: 7 step: 282, loss is 0.3145817518234253\n",
      "epoch: 7 step: 283, loss is 0.2552587687969208\n",
      "epoch: 7 step: 284, loss is 0.14516344666481018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 285, loss is 0.2932371497154236\n",
      "epoch: 7 step: 286, loss is 0.394745409488678\n",
      "epoch: 7 step: 287, loss is 0.3082491457462311\n",
      "epoch: 7 step: 288, loss is 0.09836651384830475\n",
      "epoch: 7 step: 289, loss is 0.18168330192565918\n",
      "epoch: 7 step: 290, loss is 0.13461828231811523\n",
      "epoch: 7 step: 291, loss is 0.26452720165252686\n",
      "epoch: 7 step: 292, loss is 0.31289687752723694\n",
      "epoch: 7 step: 293, loss is 0.25276365876197815\n",
      "epoch: 7 step: 294, loss is 0.3799450099468231\n",
      "epoch: 7 step: 295, loss is 0.38176968693733215\n",
      "epoch: 7 step: 296, loss is 0.37839725613594055\n",
      "epoch: 7 step: 297, loss is 0.4419184625148773\n",
      "epoch: 7 step: 298, loss is 0.13419778645038605\n",
      "epoch: 7 step: 299, loss is 0.26336944103240967\n",
      "epoch: 7 step: 300, loss is 0.27224868535995483\n",
      "epoch: 7 step: 301, loss is 0.16255512833595276\n",
      "epoch: 7 step: 302, loss is 0.28043848276138306\n",
      "epoch: 7 step: 303, loss is 0.25831764936447144\n",
      "epoch: 7 step: 304, loss is 0.29156294465065\n",
      "epoch: 7 step: 305, loss is 0.25102803111076355\n",
      "epoch: 7 step: 306, loss is 0.278552770614624\n",
      "epoch: 7 step: 307, loss is 0.37121498584747314\n",
      "epoch: 7 step: 308, loss is 0.16513410210609436\n",
      "epoch: 7 step: 309, loss is 0.22814834117889404\n",
      "epoch: 7 step: 310, loss is 0.4852147102355957\n",
      "epoch: 7 step: 311, loss is 0.09462283551692963\n",
      "epoch: 7 step: 312, loss is 0.2929137945175171\n",
      "epoch: 7 step: 313, loss is 0.41053688526153564\n",
      "epoch: 7 step: 314, loss is 0.18257223069667816\n",
      "epoch: 7 step: 315, loss is 0.2092367261648178\n",
      "epoch: 7 step: 316, loss is 0.2642517387866974\n",
      "epoch: 7 step: 317, loss is 0.3664799928665161\n",
      "epoch: 7 step: 318, loss is 0.1539185494184494\n",
      "epoch: 7 step: 319, loss is 0.14466463029384613\n",
      "epoch: 7 step: 320, loss is 0.2523803114891052\n",
      "epoch: 7 step: 321, loss is 0.2714787721633911\n",
      "epoch: 7 step: 322, loss is 0.1524319052696228\n",
      "epoch: 7 step: 323, loss is 0.5001711845397949\n",
      "epoch: 7 step: 324, loss is 0.24823155999183655\n",
      "epoch: 7 step: 325, loss is 0.2925574779510498\n",
      "epoch: 7 step: 326, loss is 0.23345240950584412\n",
      "epoch: 7 step: 327, loss is 0.16178838908672333\n",
      "epoch: 7 step: 328, loss is 0.31802013516426086\n",
      "epoch: 7 step: 329, loss is 0.23253586888313293\n",
      "epoch: 7 step: 330, loss is 0.2660766839981079\n",
      "epoch: 7 step: 331, loss is 0.3394660949707031\n",
      "epoch: 7 step: 332, loss is 0.2650245726108551\n",
      "epoch: 7 step: 333, loss is 0.3832044303417206\n",
      "epoch: 7 step: 334, loss is 0.25848376750946045\n",
      "epoch: 7 step: 335, loss is 0.4972144365310669\n",
      "epoch: 7 step: 336, loss is 0.26769566535949707\n",
      "epoch: 7 step: 337, loss is 0.1594638228416443\n",
      "epoch: 7 step: 338, loss is 0.28664839267730713\n",
      "epoch: 7 step: 339, loss is 0.2753044664859772\n",
      "epoch: 7 step: 340, loss is 0.34106096625328064\n",
      "epoch: 7 step: 341, loss is 0.12826742231845856\n",
      "epoch: 7 step: 342, loss is 0.24635924398899078\n",
      "epoch: 7 step: 343, loss is 0.15513403713703156\n",
      "epoch: 7 step: 344, loss is 0.1915980875492096\n",
      "epoch: 7 step: 345, loss is 0.31990644335746765\n",
      "epoch: 7 step: 346, loss is 0.34926584362983704\n",
      "epoch: 7 step: 347, loss is 0.3097814917564392\n",
      "epoch: 7 step: 348, loss is 0.184711292386055\n",
      "epoch: 7 step: 349, loss is 0.3148552179336548\n",
      "epoch: 7 step: 350, loss is 0.26994994282722473\n",
      "epoch: 7 step: 351, loss is 0.24025990068912506\n",
      "epoch: 7 step: 352, loss is 0.4716901183128357\n",
      "epoch: 7 step: 353, loss is 0.3688596487045288\n",
      "epoch: 7 step: 354, loss is 0.15897327661514282\n",
      "epoch: 7 step: 355, loss is 0.23153769969940186\n",
      "epoch: 7 step: 356, loss is 0.1697755753993988\n",
      "epoch: 7 step: 357, loss is 0.3180902898311615\n",
      "epoch: 7 step: 358, loss is 0.39466574788093567\n",
      "epoch: 7 step: 359, loss is 0.2668617069721222\n",
      "epoch: 7 step: 360, loss is 0.20590007305145264\n",
      "epoch: 7 step: 361, loss is 0.28575390577316284\n",
      "epoch: 7 step: 362, loss is 0.39469000697135925\n",
      "epoch: 7 step: 363, loss is 0.42637285590171814\n",
      "epoch: 7 step: 364, loss is 0.35760390758514404\n",
      "epoch: 7 step: 365, loss is 0.29433315992355347\n",
      "epoch: 7 step: 366, loss is 0.1643410325050354\n",
      "epoch: 7 step: 367, loss is 0.3370753228664398\n",
      "epoch: 7 step: 368, loss is 0.40780195593833923\n",
      "epoch: 7 step: 369, loss is 0.2520207464694977\n",
      "epoch: 7 step: 370, loss is 0.19535715878009796\n",
      "epoch: 7 step: 371, loss is 0.18594352900981903\n",
      "epoch: 7 step: 372, loss is 0.3446069359779358\n",
      "epoch: 7 step: 373, loss is 0.3867158889770508\n",
      "epoch: 7 step: 374, loss is 0.2927698791027069\n",
      "epoch: 7 step: 375, loss is 0.22521260380744934\n",
      "epoch: 7 step: 376, loss is 0.15380620956420898\n",
      "epoch: 7 step: 377, loss is 0.10156447440385818\n",
      "epoch: 7 step: 378, loss is 0.2778051197528839\n",
      "epoch: 7 step: 379, loss is 0.2336151897907257\n",
      "epoch: 7 step: 380, loss is 0.27708572149276733\n",
      "epoch: 7 step: 381, loss is 0.29243847727775574\n",
      "epoch: 7 step: 382, loss is 0.21501250565052032\n",
      "epoch: 7 step: 383, loss is 0.309678316116333\n",
      "epoch: 7 step: 384, loss is 0.2485462725162506\n",
      "epoch: 7 step: 385, loss is 0.37542134523391724\n",
      "epoch: 7 step: 386, loss is 0.2976905405521393\n",
      "epoch: 7 step: 387, loss is 0.1999247819185257\n",
      "epoch: 7 step: 388, loss is 0.21259288489818573\n",
      "epoch: 7 step: 389, loss is 0.2941840887069702\n",
      "epoch: 7 step: 390, loss is 0.15757587552070618\n",
      "epoch: 7 step: 391, loss is 0.14549247920513153\n",
      "epoch: 7 step: 392, loss is 0.14668984711170197\n",
      "epoch: 7 step: 393, loss is 0.24796687066555023\n",
      "epoch: 7 step: 394, loss is 0.1955861747264862\n",
      "epoch: 7 step: 395, loss is 0.2567654848098755\n",
      "epoch: 7 step: 396, loss is 0.27011480927467346\n",
      "epoch: 7 step: 397, loss is 0.2508937120437622\n",
      "epoch: 7 step: 398, loss is 0.18847590684890747\n",
      "epoch: 7 step: 399, loss is 0.35115015506744385\n",
      "epoch: 7 step: 400, loss is 0.2751730978488922\n",
      "epoch: 7 step: 401, loss is 0.37147122621536255\n",
      "epoch: 7 step: 402, loss is 0.11511413007974625\n",
      "epoch: 7 step: 403, loss is 0.16643396019935608\n",
      "epoch: 7 step: 404, loss is 0.22021101415157318\n",
      "epoch: 7 step: 405, loss is 0.3813634514808655\n",
      "epoch: 7 step: 406, loss is 0.20447689294815063\n",
      "epoch: 7 step: 407, loss is 0.38973525166511536\n",
      "epoch: 7 step: 408, loss is 0.3553224205970764\n",
      "epoch: 7 step: 409, loss is 0.42300117015838623\n",
      "epoch: 7 step: 410, loss is 0.1682695597410202\n",
      "epoch: 7 step: 411, loss is 0.2438534051179886\n",
      "epoch: 7 step: 412, loss is 0.22640226781368256\n",
      "epoch: 7 step: 413, loss is 0.08680326491594315\n",
      "epoch: 7 step: 414, loss is 0.39588648080825806\n",
      "epoch: 7 step: 415, loss is 0.3233431875705719\n",
      "epoch: 7 step: 416, loss is 0.34431612491607666\n",
      "epoch: 7 step: 417, loss is 0.37653255462646484\n",
      "epoch: 7 step: 418, loss is 0.2303493469953537\n",
      "epoch: 7 step: 419, loss is 0.20370224118232727\n",
      "epoch: 7 step: 420, loss is 0.1874934583902359\n",
      "epoch: 7 step: 421, loss is 0.360442578792572\n",
      "epoch: 7 step: 422, loss is 0.2357492595911026\n",
      "epoch: 7 step: 423, loss is 0.11084449291229248\n",
      "epoch: 7 step: 424, loss is 0.2490243762731552\n",
      "epoch: 7 step: 425, loss is 0.24529851973056793\n",
      "epoch: 7 step: 426, loss is 0.24892543256282806\n",
      "epoch: 7 step: 427, loss is 0.3177088499069214\n",
      "epoch: 7 step: 428, loss is 0.32137152552604675\n",
      "epoch: 7 step: 429, loss is 0.19590824842453003\n",
      "epoch: 7 step: 430, loss is 0.18158301711082458\n",
      "epoch: 7 step: 431, loss is 0.2191498577594757\n",
      "epoch: 7 step: 432, loss is 0.21161749958992004\n",
      "epoch: 7 step: 433, loss is 0.24897384643554688\n",
      "epoch: 7 step: 434, loss is 0.29212889075279236\n",
      "epoch: 7 step: 435, loss is 0.3060302734375\n",
      "epoch: 7 step: 436, loss is 0.22803817689418793\n",
      "epoch: 7 step: 437, loss is 0.27397388219833374\n",
      "epoch: 7 step: 438, loss is 0.3668835461139679\n",
      "epoch: 7 step: 439, loss is 0.204249769449234\n",
      "epoch: 7 step: 440, loss is 0.17545489966869354\n",
      "epoch: 7 step: 441, loss is 0.34884142875671387\n",
      "epoch: 7 step: 442, loss is 0.26703864336013794\n",
      "epoch: 7 step: 443, loss is 0.2186586856842041\n",
      "epoch: 7 step: 444, loss is 0.3733457326889038\n",
      "epoch: 7 step: 445, loss is 0.21479853987693787\n",
      "epoch: 7 step: 446, loss is 0.1222294420003891\n",
      "epoch: 7 step: 447, loss is 0.41052913665771484\n",
      "epoch: 7 step: 448, loss is 0.3526979386806488\n",
      "epoch: 7 step: 449, loss is 0.14999109506607056\n",
      "epoch: 7 step: 450, loss is 0.42912769317626953\n",
      "epoch: 7 step: 451, loss is 0.16457632184028625\n",
      "epoch: 7 step: 452, loss is 0.22174964845180511\n",
      "epoch: 7 step: 453, loss is 0.40083006024360657\n",
      "epoch: 7 step: 454, loss is 0.18961329758167267\n",
      "epoch: 7 step: 455, loss is 0.4120784401893616\n",
      "epoch: 7 step: 456, loss is 0.25422126054763794\n",
      "epoch: 7 step: 457, loss is 0.47931843996047974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 458, loss is 0.172139510512352\n",
      "epoch: 7 step: 459, loss is 0.2223537415266037\n",
      "epoch: 7 step: 460, loss is 0.3283699154853821\n",
      "epoch: 7 step: 461, loss is 0.14007337391376495\n",
      "epoch: 7 step: 462, loss is 0.43105876445770264\n",
      "epoch: 7 step: 463, loss is 0.23366111516952515\n",
      "epoch: 7 step: 464, loss is 0.24647128582000732\n",
      "epoch: 7 step: 465, loss is 0.3848806619644165\n",
      "epoch: 7 step: 466, loss is 0.2815723717212677\n",
      "epoch: 7 step: 467, loss is 0.2579341530799866\n",
      "epoch: 7 step: 468, loss is 0.5578879117965698\n",
      "epoch: 7 step: 469, loss is 0.1735679656267166\n",
      "epoch: 7 step: 470, loss is 0.1443067342042923\n",
      "epoch: 7 step: 471, loss is 0.16936367750167847\n",
      "epoch: 7 step: 472, loss is 0.36231642961502075\n",
      "epoch: 7 step: 473, loss is 0.1855449676513672\n",
      "epoch: 7 step: 474, loss is 0.19288776814937592\n",
      "epoch: 7 step: 475, loss is 0.30648142099380493\n",
      "epoch: 7 step: 476, loss is 0.32815229892730713\n",
      "epoch: 7 step: 477, loss is 0.3320271968841553\n",
      "epoch: 7 step: 478, loss is 0.317411333322525\n",
      "epoch: 7 step: 479, loss is 0.15334662795066833\n",
      "epoch: 7 step: 480, loss is 0.3195696473121643\n",
      "epoch: 7 step: 481, loss is 0.32937508821487427\n",
      "epoch: 7 step: 482, loss is 0.2030038982629776\n",
      "epoch: 7 step: 483, loss is 0.4649996757507324\n",
      "epoch: 7 step: 484, loss is 0.2526540458202362\n",
      "epoch: 7 step: 485, loss is 0.23408420383930206\n",
      "epoch: 7 step: 486, loss is 0.4226923882961273\n",
      "epoch: 7 step: 487, loss is 0.37587469816207886\n",
      "epoch: 7 step: 488, loss is 0.2180391103029251\n",
      "epoch: 7 step: 489, loss is 0.3389301300048828\n",
      "epoch: 7 step: 490, loss is 0.2993376553058624\n",
      "epoch: 7 step: 491, loss is 0.17716850340366364\n",
      "epoch: 7 step: 492, loss is 0.18848839402198792\n",
      "epoch: 7 step: 493, loss is 0.3288096487522125\n",
      "epoch: 7 step: 494, loss is 0.17947658896446228\n",
      "epoch: 7 step: 495, loss is 0.14941567182540894\n",
      "epoch: 7 step: 496, loss is 0.44893941283226013\n",
      "epoch: 7 step: 497, loss is 0.26413241028785706\n",
      "epoch: 7 step: 498, loss is 0.25893524289131165\n",
      "epoch: 7 step: 499, loss is 0.33677998185157776\n",
      "epoch: 7 step: 500, loss is 0.23811207711696625\n",
      "epoch: 7 step: 501, loss is 0.3039877712726593\n",
      "epoch: 7 step: 502, loss is 0.23009748756885529\n",
      "epoch: 7 step: 503, loss is 0.20095595717430115\n",
      "epoch: 7 step: 504, loss is 0.3546298146247864\n",
      "epoch: 7 step: 505, loss is 0.18461042642593384\n",
      "epoch: 7 step: 506, loss is 0.24440374970436096\n",
      "epoch: 7 step: 507, loss is 0.11260735243558884\n",
      "epoch: 7 step: 508, loss is 0.36229750514030457\n",
      "epoch: 7 step: 509, loss is 0.3188939392566681\n",
      "epoch: 7 step: 510, loss is 0.20939700305461884\n",
      "epoch: 7 step: 511, loss is 0.2145105004310608\n",
      "epoch: 7 step: 512, loss is 0.24577581882476807\n",
      "epoch: 7 step: 513, loss is 0.225311279296875\n",
      "epoch: 7 step: 514, loss is 0.2454446256160736\n",
      "epoch: 7 step: 515, loss is 0.25107553601264954\n",
      "epoch: 7 step: 516, loss is 0.36771920323371887\n",
      "epoch: 7 step: 517, loss is 0.4461270272731781\n",
      "epoch: 7 step: 518, loss is 0.17610891163349152\n",
      "epoch: 7 step: 519, loss is 0.39794304966926575\n",
      "epoch: 7 step: 520, loss is 0.13625837862491608\n",
      "epoch: 7 step: 521, loss is 0.2247847467660904\n",
      "epoch: 7 step: 522, loss is 0.4157244861125946\n",
      "epoch: 7 step: 523, loss is 0.4386886954307556\n",
      "epoch: 7 step: 524, loss is 0.1365693360567093\n",
      "epoch: 7 step: 525, loss is 0.20566102862358093\n",
      "epoch: 7 step: 526, loss is 0.22056667506694794\n",
      "epoch: 7 step: 527, loss is 0.3841172456741333\n",
      "epoch: 7 step: 528, loss is 0.2266770601272583\n",
      "epoch: 7 step: 529, loss is 0.2525261640548706\n",
      "epoch: 7 step: 530, loss is 0.2841452360153198\n",
      "epoch: 7 step: 531, loss is 0.28354036808013916\n",
      "epoch: 7 step: 532, loss is 0.23605413734912872\n",
      "epoch: 7 step: 533, loss is 0.18425887823104858\n",
      "epoch: 7 step: 534, loss is 0.46987631916999817\n",
      "epoch: 7 step: 535, loss is 0.2835906744003296\n",
      "epoch: 7 step: 536, loss is 0.22917748987674713\n",
      "epoch: 7 step: 537, loss is 0.3117258548736572\n",
      "epoch: 7 step: 538, loss is 0.29877951741218567\n",
      "epoch: 7 step: 539, loss is 0.26709139347076416\n",
      "epoch: 7 step: 540, loss is 0.19541198015213013\n",
      "epoch: 7 step: 541, loss is 0.20298857986927032\n",
      "epoch: 7 step: 542, loss is 0.24730664491653442\n",
      "epoch: 7 step: 543, loss is 0.19176866114139557\n",
      "epoch: 7 step: 544, loss is 0.21015900373458862\n",
      "epoch: 7 step: 545, loss is 0.20433160662651062\n",
      "epoch: 7 step: 546, loss is 0.38847726583480835\n",
      "epoch: 7 step: 547, loss is 0.21324902772903442\n",
      "epoch: 7 step: 548, loss is 0.24287712574005127\n",
      "epoch: 7 step: 549, loss is 0.6562466621398926\n",
      "epoch: 7 step: 550, loss is 0.25901973247528076\n",
      "epoch: 7 step: 551, loss is 0.26381635665893555\n",
      "epoch: 7 step: 552, loss is 0.3067967891693115\n",
      "epoch: 7 step: 553, loss is 0.39384016394615173\n",
      "epoch: 7 step: 554, loss is 0.21108102798461914\n",
      "epoch: 7 step: 555, loss is 0.35671553015708923\n",
      "epoch: 7 step: 556, loss is 0.29023706912994385\n",
      "epoch: 7 step: 557, loss is 0.49300622940063477\n",
      "epoch: 7 step: 558, loss is 0.21309791505336761\n",
      "epoch: 7 step: 559, loss is 0.17770975828170776\n",
      "epoch: 7 step: 560, loss is 0.3216058015823364\n",
      "epoch: 7 step: 561, loss is 0.26407700777053833\n",
      "epoch: 7 step: 562, loss is 0.3325137794017792\n",
      "epoch: 7 step: 563, loss is 0.27939000725746155\n",
      "epoch: 7 step: 564, loss is 0.17609965801239014\n",
      "epoch: 7 step: 565, loss is 0.19540981948375702\n",
      "epoch: 7 step: 566, loss is 0.23469366133213043\n",
      "epoch: 7 step: 567, loss is 0.2893480956554413\n",
      "epoch: 7 step: 568, loss is 0.33494263887405396\n",
      "epoch: 7 step: 569, loss is 0.3005794584751129\n",
      "epoch: 7 step: 570, loss is 0.35760003328323364\n",
      "epoch: 7 step: 571, loss is 0.23175691068172455\n",
      "epoch: 7 step: 572, loss is 0.21513749659061432\n",
      "epoch: 7 step: 573, loss is 0.23480156064033508\n",
      "epoch: 7 step: 574, loss is 0.24848437309265137\n",
      "epoch: 7 step: 575, loss is 0.1479390412569046\n",
      "epoch: 7 step: 576, loss is 0.37814128398895264\n",
      "epoch: 7 step: 577, loss is 0.17359167337417603\n",
      "epoch: 7 step: 578, loss is 0.29030841588974\n",
      "epoch: 7 step: 579, loss is 0.21488100290298462\n",
      "epoch: 7 step: 580, loss is 0.10832170397043228\n",
      "epoch: 7 step: 581, loss is 0.17600305378437042\n",
      "epoch: 7 step: 582, loss is 0.26642805337905884\n",
      "epoch: 7 step: 583, loss is 0.2540363669395447\n",
      "epoch: 7 step: 584, loss is 0.3254477083683014\n",
      "epoch: 7 step: 585, loss is 0.20725883543491364\n",
      "epoch: 7 step: 586, loss is 0.19639423489570618\n",
      "epoch: 7 step: 587, loss is 0.24252182245254517\n",
      "epoch: 7 step: 588, loss is 0.16985642910003662\n",
      "epoch: 7 step: 589, loss is 0.3844715356826782\n",
      "epoch: 7 step: 590, loss is 0.23732979595661163\n",
      "epoch: 7 step: 591, loss is 0.2582602798938751\n",
      "epoch: 7 step: 592, loss is 0.18545779585838318\n",
      "epoch: 7 step: 593, loss is 0.27668383717536926\n",
      "epoch: 7 step: 594, loss is 0.41521692276000977\n",
      "epoch: 7 step: 595, loss is 0.20065529644489288\n",
      "epoch: 7 step: 596, loss is 0.18325123190879822\n",
      "epoch: 7 step: 597, loss is 0.2550807297229767\n",
      "epoch: 7 step: 598, loss is 0.37161964178085327\n",
      "epoch: 7 step: 599, loss is 0.17896278202533722\n",
      "epoch: 7 step: 600, loss is 0.2758948504924774\n",
      "epoch: 7 step: 601, loss is 0.10561066120862961\n",
      "epoch: 7 step: 602, loss is 0.28385359048843384\n",
      "epoch: 7 step: 603, loss is 0.1655983328819275\n",
      "epoch: 7 step: 604, loss is 0.24938979744911194\n",
      "epoch: 7 step: 605, loss is 0.18086190521717072\n",
      "epoch: 7 step: 606, loss is 0.24682876467704773\n",
      "epoch: 7 step: 607, loss is 0.11506661027669907\n",
      "epoch: 7 step: 608, loss is 0.41215863823890686\n",
      "epoch: 7 step: 609, loss is 0.12466602772474289\n",
      "epoch: 7 step: 610, loss is 0.27664873003959656\n",
      "epoch: 7 step: 611, loss is 0.26711633801460266\n",
      "epoch: 7 step: 612, loss is 0.28272196650505066\n",
      "epoch: 7 step: 613, loss is 0.29033252596855164\n",
      "epoch: 7 step: 614, loss is 0.15787598490715027\n",
      "epoch: 7 step: 615, loss is 0.24235014617443085\n",
      "epoch: 7 step: 616, loss is 0.26219332218170166\n",
      "epoch: 7 step: 617, loss is 0.3587214946746826\n",
      "epoch: 7 step: 618, loss is 0.21453776955604553\n",
      "epoch: 7 step: 619, loss is 0.278156042098999\n",
      "epoch: 7 step: 620, loss is 0.1724126935005188\n",
      "epoch: 7 step: 621, loss is 0.28074443340301514\n",
      "epoch: 7 step: 622, loss is 0.21156062185764313\n",
      "epoch: 7 step: 623, loss is 0.16284207999706268\n",
      "epoch: 7 step: 624, loss is 0.22655433416366577\n",
      "epoch: 7 step: 625, loss is 0.2763848602771759\n",
      "epoch: 7 step: 626, loss is 0.15189194679260254\n",
      "epoch: 7 step: 627, loss is 0.23896068334579468\n",
      "epoch: 7 step: 628, loss is 0.34219786524772644\n",
      "epoch: 7 step: 629, loss is 0.22934100031852722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 630, loss is 0.29810863733291626\n",
      "epoch: 7 step: 631, loss is 0.31681129336357117\n",
      "epoch: 7 step: 632, loss is 0.19844481348991394\n",
      "epoch: 7 step: 633, loss is 0.27870696783065796\n",
      "epoch: 7 step: 634, loss is 0.29864513874053955\n",
      "epoch: 7 step: 635, loss is 0.2436162531375885\n",
      "epoch: 7 step: 636, loss is 0.19633755087852478\n",
      "epoch: 7 step: 637, loss is 0.29648876190185547\n",
      "epoch: 7 step: 638, loss is 0.29983004927635193\n",
      "epoch: 7 step: 639, loss is 0.31562092900276184\n",
      "epoch: 7 step: 640, loss is 0.3270750343799591\n",
      "epoch: 7 step: 641, loss is 0.24801184237003326\n",
      "epoch: 7 step: 642, loss is 0.2712160646915436\n",
      "epoch: 7 step: 643, loss is 0.44847455620765686\n",
      "epoch: 7 step: 644, loss is 0.30710458755493164\n",
      "epoch: 7 step: 645, loss is 0.33030006289482117\n",
      "epoch: 7 step: 646, loss is 0.37364599108695984\n",
      "epoch: 7 step: 647, loss is 0.3194163143634796\n",
      "epoch: 7 step: 648, loss is 0.37947505712509155\n",
      "epoch: 7 step: 649, loss is 0.16179491579532623\n",
      "epoch: 7 step: 650, loss is 0.11840885132551193\n",
      "epoch: 7 step: 651, loss is 0.24003668129444122\n",
      "epoch: 7 step: 652, loss is 0.16392827033996582\n",
      "epoch: 7 step: 653, loss is 0.26466062664985657\n",
      "epoch: 7 step: 654, loss is 0.3199838697910309\n",
      "epoch: 7 step: 655, loss is 0.19524505734443665\n",
      "epoch: 7 step: 656, loss is 0.2007024884223938\n",
      "epoch: 7 step: 657, loss is 0.16756029427051544\n",
      "epoch: 7 step: 658, loss is 0.18299171328544617\n",
      "epoch: 7 step: 659, loss is 0.16904567182064056\n",
      "epoch: 7 step: 660, loss is 0.2650502026081085\n",
      "epoch: 7 step: 661, loss is 0.2923249304294586\n",
      "epoch: 7 step: 662, loss is 0.30262091755867004\n",
      "epoch: 7 step: 663, loss is 0.2692945897579193\n",
      "epoch: 7 step: 664, loss is 0.20904596149921417\n",
      "epoch: 7 step: 665, loss is 0.303134948015213\n",
      "epoch: 7 step: 666, loss is 0.2352634072303772\n",
      "epoch: 7 step: 667, loss is 0.3932919204235077\n",
      "epoch: 7 step: 668, loss is 0.13386313617229462\n",
      "epoch: 7 step: 669, loss is 0.36396971344947815\n",
      "epoch: 7 step: 670, loss is 0.22975195944309235\n",
      "epoch: 7 step: 671, loss is 0.09110195189714432\n",
      "epoch: 7 step: 672, loss is 0.2767014503479004\n",
      "epoch: 7 step: 673, loss is 0.45202088356018066\n",
      "epoch: 7 step: 674, loss is 0.28173351287841797\n",
      "epoch: 7 step: 675, loss is 0.20458315312862396\n",
      "epoch: 7 step: 676, loss is 0.4084964394569397\n",
      "epoch: 7 step: 677, loss is 0.21595601737499237\n",
      "epoch: 7 step: 678, loss is 0.3006439805030823\n",
      "epoch: 7 step: 679, loss is 0.2618362605571747\n",
      "epoch: 7 step: 680, loss is 0.1294742226600647\n",
      "epoch: 7 step: 681, loss is 0.3096558451652527\n",
      "epoch: 7 step: 682, loss is 0.2457941472530365\n",
      "epoch: 7 step: 683, loss is 0.320090115070343\n",
      "epoch: 7 step: 684, loss is 0.23919416964054108\n",
      "epoch: 7 step: 685, loss is 0.16301009058952332\n",
      "epoch: 7 step: 686, loss is 0.23812203109264374\n",
      "epoch: 7 step: 687, loss is 0.29798775911331177\n",
      "epoch: 7 step: 688, loss is 0.27755847573280334\n",
      "epoch: 7 step: 689, loss is 0.07082363218069077\n",
      "epoch: 7 step: 690, loss is 0.14466282725334167\n",
      "epoch: 7 step: 691, loss is 0.2231253832578659\n",
      "epoch: 7 step: 692, loss is 0.42090049386024475\n",
      "epoch: 7 step: 693, loss is 0.3022865653038025\n",
      "epoch: 7 step: 694, loss is 0.36855238676071167\n",
      "epoch: 7 step: 695, loss is 0.09687376022338867\n",
      "epoch: 7 step: 696, loss is 0.2577877342700958\n",
      "epoch: 7 step: 697, loss is 0.2108704149723053\n",
      "epoch: 7 step: 698, loss is 0.3918209671974182\n",
      "epoch: 7 step: 699, loss is 0.33715423941612244\n",
      "epoch: 7 step: 700, loss is 0.40542054176330566\n",
      "epoch: 7 step: 701, loss is 0.1270497739315033\n",
      "epoch: 7 step: 702, loss is 0.3414742946624756\n",
      "epoch: 7 step: 703, loss is 0.29629409313201904\n",
      "epoch: 7 step: 704, loss is 0.3175200819969177\n",
      "epoch: 7 step: 705, loss is 0.36438581347465515\n",
      "epoch: 7 step: 706, loss is 0.2096434235572815\n",
      "epoch: 7 step: 707, loss is 0.38953691720962524\n",
      "epoch: 7 step: 708, loss is 0.2225807160139084\n",
      "epoch: 7 step: 709, loss is 0.12518249452114105\n",
      "epoch: 7 step: 710, loss is 0.35342973470687866\n",
      "epoch: 7 step: 711, loss is 0.31984058022499084\n",
      "epoch: 7 step: 712, loss is 0.08196669816970825\n",
      "epoch: 7 step: 713, loss is 0.41018834710121155\n",
      "epoch: 7 step: 714, loss is 0.2407817393541336\n",
      "epoch: 7 step: 715, loss is 0.2668292820453644\n",
      "epoch: 7 step: 716, loss is 0.22551989555358887\n",
      "epoch: 7 step: 717, loss is 0.1967662125825882\n",
      "epoch: 7 step: 718, loss is 0.290565550327301\n",
      "epoch: 7 step: 719, loss is 0.29121461510658264\n",
      "epoch: 7 step: 720, loss is 0.48271143436431885\n",
      "epoch: 7 step: 721, loss is 0.19274809956550598\n",
      "epoch: 7 step: 722, loss is 0.2564109265804291\n",
      "epoch: 7 step: 723, loss is 0.31178516149520874\n",
      "epoch: 7 step: 724, loss is 0.2514157295227051\n",
      "epoch: 7 step: 725, loss is 0.34960079193115234\n",
      "epoch: 7 step: 726, loss is 0.334162175655365\n",
      "epoch: 7 step: 727, loss is 0.19013290107250214\n",
      "epoch: 7 step: 728, loss is 0.3391415774822235\n",
      "epoch: 7 step: 729, loss is 0.23523201048374176\n",
      "epoch: 7 step: 730, loss is 0.23521031439304352\n",
      "epoch: 7 step: 731, loss is 0.13517752289772034\n",
      "epoch: 7 step: 732, loss is 0.3533633351325989\n",
      "epoch: 7 step: 733, loss is 0.31815624237060547\n",
      "epoch: 7 step: 734, loss is 0.2804885506629944\n",
      "epoch: 7 step: 735, loss is 0.3294717073440552\n",
      "epoch: 7 step: 736, loss is 0.16306617856025696\n",
      "epoch: 7 step: 737, loss is 0.2775227129459381\n",
      "epoch: 7 step: 738, loss is 0.33333200216293335\n",
      "epoch: 7 step: 739, loss is 0.31972354650497437\n",
      "epoch: 7 step: 740, loss is 0.3583718538284302\n",
      "epoch: 7 step: 741, loss is 0.20461590588092804\n",
      "epoch: 7 step: 742, loss is 0.17243002355098724\n",
      "epoch: 7 step: 743, loss is 0.29063814878463745\n",
      "epoch: 7 step: 744, loss is 0.28441715240478516\n",
      "epoch: 7 step: 745, loss is 0.29966601729393005\n",
      "epoch: 7 step: 746, loss is 0.24350465834140778\n",
      "epoch: 7 step: 747, loss is 0.29947417974472046\n",
      "epoch: 7 step: 748, loss is 0.2469806671142578\n",
      "epoch: 7 step: 749, loss is 0.30012622475624084\n",
      "epoch: 7 step: 750, loss is 0.16882406175136566\n",
      "epoch: 7 step: 751, loss is 0.2986561357975006\n",
      "epoch: 7 step: 752, loss is 0.27420300245285034\n",
      "epoch: 7 step: 753, loss is 0.25102734565734863\n",
      "epoch: 7 step: 754, loss is 0.29120051860809326\n",
      "epoch: 7 step: 755, loss is 0.24236059188842773\n",
      "epoch: 7 step: 756, loss is 0.25484922528266907\n",
      "epoch: 7 step: 757, loss is 0.3781643807888031\n",
      "epoch: 7 step: 758, loss is 0.3484466075897217\n",
      "epoch: 7 step: 759, loss is 0.235973060131073\n",
      "epoch: 7 step: 760, loss is 0.2731863260269165\n",
      "epoch: 7 step: 761, loss is 0.5683415532112122\n",
      "epoch: 7 step: 762, loss is 0.23513247072696686\n",
      "epoch: 7 step: 763, loss is 0.41733023524284363\n",
      "epoch: 7 step: 764, loss is 0.1532607525587082\n",
      "epoch: 7 step: 765, loss is 0.2048483043909073\n",
      "epoch: 7 step: 766, loss is 0.35260236263275146\n",
      "epoch: 7 step: 767, loss is 0.15988098084926605\n",
      "epoch: 7 step: 768, loss is 0.2722463309764862\n",
      "epoch: 7 step: 769, loss is 0.3887166976928711\n",
      "epoch: 7 step: 770, loss is 0.1476890742778778\n",
      "epoch: 7 step: 771, loss is 0.17295143008232117\n",
      "epoch: 7 step: 772, loss is 0.3094295263290405\n",
      "epoch: 7 step: 773, loss is 0.3011479377746582\n",
      "epoch: 7 step: 774, loss is 0.24022138118743896\n",
      "epoch: 7 step: 775, loss is 0.20769667625427246\n",
      "epoch: 7 step: 776, loss is 0.305637925863266\n",
      "epoch: 7 step: 777, loss is 0.2579813301563263\n",
      "epoch: 7 step: 778, loss is 0.22147294878959656\n",
      "epoch: 7 step: 779, loss is 0.3504377007484436\n",
      "epoch: 7 step: 780, loss is 0.338530033826828\n",
      "epoch: 7 step: 781, loss is 0.15168389678001404\n",
      "epoch: 7 step: 782, loss is 0.3333418667316437\n",
      "epoch: 7 step: 783, loss is 0.31551966071128845\n",
      "epoch: 7 step: 784, loss is 0.2193099856376648\n",
      "epoch: 7 step: 785, loss is 0.15256783366203308\n",
      "epoch: 7 step: 786, loss is 0.22727732360363007\n",
      "epoch: 7 step: 787, loss is 0.5213127732276917\n",
      "epoch: 7 step: 788, loss is 0.1114400178194046\n",
      "epoch: 7 step: 789, loss is 0.27467671036720276\n",
      "epoch: 7 step: 790, loss is 0.35441043972969055\n",
      "epoch: 7 step: 791, loss is 0.17044411599636078\n",
      "epoch: 7 step: 792, loss is 0.3162473142147064\n",
      "epoch: 7 step: 793, loss is 0.1487443894147873\n",
      "epoch: 7 step: 794, loss is 0.15603432059288025\n",
      "epoch: 7 step: 795, loss is 0.3211853802204132\n",
      "epoch: 7 step: 796, loss is 0.4065625071525574\n",
      "epoch: 7 step: 797, loss is 0.40488705039024353\n",
      "epoch: 7 step: 798, loss is 0.21882708370685577\n",
      "epoch: 7 step: 799, loss is 0.30860960483551025\n",
      "epoch: 7 step: 800, loss is 0.3527100086212158\n",
      "epoch: 7 step: 801, loss is 0.24313123524188995\n",
      "epoch: 7 step: 802, loss is 0.22402538359165192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 803, loss is 0.22601477801799774\n",
      "epoch: 7 step: 804, loss is 0.14380167424678802\n",
      "epoch: 7 step: 805, loss is 0.2030664086341858\n",
      "epoch: 7 step: 806, loss is 0.1348385363817215\n",
      "epoch: 7 step: 807, loss is 0.19807834923267365\n",
      "epoch: 7 step: 808, loss is 0.2591035068035126\n",
      "epoch: 7 step: 809, loss is 0.20244364440441132\n",
      "epoch: 7 step: 810, loss is 0.15904586017131805\n",
      "epoch: 7 step: 811, loss is 0.24038594961166382\n",
      "epoch: 7 step: 812, loss is 0.1923745572566986\n",
      "epoch: 7 step: 813, loss is 0.2532673478126526\n",
      "epoch: 7 step: 814, loss is 0.23210273683071136\n",
      "epoch: 7 step: 815, loss is 0.22946366667747498\n",
      "epoch: 7 step: 816, loss is 0.12748214602470398\n",
      "epoch: 7 step: 817, loss is 0.1483374834060669\n",
      "epoch: 7 step: 818, loss is 0.22136801481246948\n",
      "epoch: 7 step: 819, loss is 0.19790217280387878\n",
      "epoch: 7 step: 820, loss is 0.2935441732406616\n",
      "epoch: 7 step: 821, loss is 0.21409612894058228\n",
      "epoch: 7 step: 822, loss is 0.286017507314682\n",
      "epoch: 7 step: 823, loss is 0.2391103357076645\n",
      "epoch: 7 step: 824, loss is 0.1439448893070221\n",
      "epoch: 7 step: 825, loss is 0.2713402807712555\n",
      "epoch: 7 step: 826, loss is 0.22213001549243927\n",
      "epoch: 7 step: 827, loss is 0.18389911949634552\n",
      "epoch: 7 step: 828, loss is 0.270517498254776\n",
      "epoch: 7 step: 829, loss is 0.2529379725456238\n",
      "epoch: 7 step: 830, loss is 0.2699657678604126\n",
      "epoch: 7 step: 831, loss is 0.24060243368148804\n",
      "epoch: 7 step: 832, loss is 0.16630564630031586\n",
      "epoch: 7 step: 833, loss is 0.201264888048172\n",
      "epoch: 7 step: 834, loss is 0.22867631912231445\n",
      "epoch: 7 step: 835, loss is 0.30315759778022766\n",
      "epoch: 7 step: 836, loss is 0.21664512157440186\n",
      "epoch: 7 step: 837, loss is 0.39166152477264404\n",
      "epoch: 7 step: 838, loss is 0.31186944246292114\n",
      "epoch: 7 step: 839, loss is 0.32052695751190186\n",
      "epoch: 7 step: 840, loss is 0.2533802092075348\n",
      "epoch: 7 step: 841, loss is 0.2184402495622635\n",
      "epoch: 7 step: 842, loss is 0.28749746084213257\n",
      "epoch: 7 step: 843, loss is 0.25005996227264404\n",
      "epoch: 7 step: 844, loss is 0.18431106209754944\n",
      "epoch: 7 step: 845, loss is 0.3346917927265167\n",
      "epoch: 7 step: 846, loss is 0.36613407731056213\n",
      "epoch: 7 step: 847, loss is 0.31947430968284607\n",
      "epoch: 7 step: 848, loss is 0.33719685673713684\n",
      "epoch: 7 step: 849, loss is 0.27391791343688965\n",
      "epoch: 7 step: 850, loss is 0.2805471122264862\n",
      "epoch: 7 step: 851, loss is 0.46336302161216736\n",
      "epoch: 7 step: 852, loss is 0.21331237256526947\n",
      "epoch: 7 step: 853, loss is 0.28742915391921997\n",
      "epoch: 7 step: 854, loss is 0.43512779474258423\n",
      "epoch: 7 step: 855, loss is 0.1440444439649582\n",
      "epoch: 7 step: 856, loss is 0.2729998230934143\n",
      "epoch: 7 step: 857, loss is 0.19752168655395508\n",
      "epoch: 7 step: 858, loss is 0.16285671293735504\n",
      "epoch: 7 step: 859, loss is 0.2603785991668701\n",
      "epoch: 7 step: 860, loss is 0.4666352868080139\n",
      "epoch: 7 step: 861, loss is 0.14228807389736176\n",
      "epoch: 7 step: 862, loss is 0.3022657036781311\n",
      "epoch: 7 step: 863, loss is 0.10673042386770248\n",
      "epoch: 7 step: 864, loss is 0.2183012068271637\n",
      "epoch: 7 step: 865, loss is 0.37228864431381226\n",
      "epoch: 7 step: 866, loss is 0.13410936295986176\n",
      "epoch: 7 step: 867, loss is 0.412738561630249\n",
      "epoch: 7 step: 868, loss is 0.2061477154493332\n",
      "epoch: 7 step: 869, loss is 0.4481084942817688\n",
      "epoch: 7 step: 870, loss is 0.27851051092147827\n",
      "epoch: 7 step: 871, loss is 0.1966930627822876\n",
      "epoch: 7 step: 872, loss is 0.26831790804862976\n",
      "epoch: 7 step: 873, loss is 0.30086398124694824\n",
      "epoch: 7 step: 874, loss is 0.2917986810207367\n",
      "epoch: 7 step: 875, loss is 0.2891550362110138\n",
      "epoch: 7 step: 876, loss is 0.20012545585632324\n",
      "epoch: 7 step: 877, loss is 0.22737248241901398\n",
      "epoch: 7 step: 878, loss is 0.21436342597007751\n",
      "epoch: 7 step: 879, loss is 0.20050445199012756\n",
      "epoch: 7 step: 880, loss is 0.17397412657737732\n",
      "epoch: 7 step: 881, loss is 0.1981000155210495\n",
      "epoch: 7 step: 882, loss is 0.18239721655845642\n",
      "epoch: 7 step: 883, loss is 0.17221346497535706\n",
      "epoch: 7 step: 884, loss is 0.35436946153640747\n",
      "epoch: 7 step: 885, loss is 0.2869601845741272\n",
      "epoch: 7 step: 886, loss is 0.4119168519973755\n",
      "epoch: 7 step: 887, loss is 0.40777602791786194\n",
      "epoch: 7 step: 888, loss is 0.27800315618515015\n",
      "epoch: 7 step: 889, loss is 0.382778137922287\n",
      "epoch: 7 step: 890, loss is 0.1763075888156891\n",
      "epoch: 7 step: 891, loss is 0.21763281524181366\n",
      "epoch: 7 step: 892, loss is 0.2161267101764679\n",
      "epoch: 7 step: 893, loss is 0.393572598695755\n",
      "epoch: 7 step: 894, loss is 0.30395567417144775\n",
      "epoch: 7 step: 895, loss is 0.26167458295822144\n",
      "epoch: 7 step: 896, loss is 0.17878052592277527\n",
      "epoch: 7 step: 897, loss is 0.19596673548221588\n",
      "epoch: 7 step: 898, loss is 0.37443283200263977\n",
      "epoch: 7 step: 899, loss is 0.2169407606124878\n",
      "epoch: 7 step: 900, loss is 0.24264365434646606\n",
      "epoch: 7 step: 901, loss is 0.29602792859077454\n",
      "epoch: 7 step: 902, loss is 0.08832726627588272\n",
      "epoch: 7 step: 903, loss is 0.18204839527606964\n",
      "epoch: 7 step: 904, loss is 0.24256294965744019\n",
      "epoch: 7 step: 905, loss is 0.4259797930717468\n",
      "epoch: 7 step: 906, loss is 0.20035237073898315\n",
      "epoch: 7 step: 907, loss is 0.21067926287651062\n",
      "epoch: 7 step: 908, loss is 0.3805266320705414\n",
      "epoch: 7 step: 909, loss is 0.28935128450393677\n",
      "epoch: 7 step: 910, loss is 0.4856593906879425\n",
      "epoch: 7 step: 911, loss is 0.19822657108306885\n",
      "epoch: 7 step: 912, loss is 0.4046204388141632\n",
      "epoch: 7 step: 913, loss is 0.2877442240715027\n",
      "epoch: 7 step: 914, loss is 0.2979011535644531\n",
      "epoch: 7 step: 915, loss is 0.18343664705753326\n",
      "epoch: 7 step: 916, loss is 0.24316130578517914\n",
      "epoch: 7 step: 917, loss is 0.22067198157310486\n",
      "epoch: 7 step: 918, loss is 0.3349238634109497\n",
      "epoch: 7 step: 919, loss is 0.15967796742916107\n",
      "epoch: 7 step: 920, loss is 0.22306954860687256\n",
      "epoch: 7 step: 921, loss is 0.36576077342033386\n",
      "epoch: 7 step: 922, loss is 0.22028176486492157\n",
      "epoch: 7 step: 923, loss is 0.18803533911705017\n",
      "epoch: 7 step: 924, loss is 0.2033160924911499\n",
      "epoch: 7 step: 925, loss is 0.225852832198143\n",
      "epoch: 7 step: 926, loss is 0.39817842841148376\n",
      "epoch: 7 step: 927, loss is 0.4154979884624481\n",
      "epoch: 7 step: 928, loss is 0.27333950996398926\n",
      "epoch: 7 step: 929, loss is 0.2226438820362091\n",
      "epoch: 7 step: 930, loss is 0.2516031563282013\n",
      "epoch: 7 step: 931, loss is 0.1521414965391159\n",
      "epoch: 7 step: 932, loss is 0.38052305579185486\n",
      "epoch: 7 step: 933, loss is 0.1757012903690338\n",
      "epoch: 7 step: 934, loss is 0.45053771138191223\n",
      "epoch: 7 step: 935, loss is 0.26235678791999817\n",
      "epoch: 7 step: 936, loss is 0.47756025195121765\n",
      "epoch: 7 step: 937, loss is 0.17615853250026703\n",
      "epoch: 8 step: 1, loss is 0.20691236853599548\n",
      "epoch: 8 step: 2, loss is 0.20750562846660614\n",
      "epoch: 8 step: 3, loss is 0.24403759837150574\n",
      "epoch: 8 step: 4, loss is 0.16925175487995148\n",
      "epoch: 8 step: 5, loss is 0.22049564123153687\n",
      "epoch: 8 step: 6, loss is 0.2917570173740387\n",
      "epoch: 8 step: 7, loss is 0.26811280846595764\n",
      "epoch: 8 step: 8, loss is 0.27331241965293884\n",
      "epoch: 8 step: 9, loss is 0.22478176653385162\n",
      "epoch: 8 step: 10, loss is 0.18191629648208618\n",
      "epoch: 8 step: 11, loss is 0.21029777824878693\n",
      "epoch: 8 step: 12, loss is 0.11041458696126938\n",
      "epoch: 8 step: 13, loss is 0.32885149121284485\n",
      "epoch: 8 step: 14, loss is 0.2422366738319397\n",
      "epoch: 8 step: 15, loss is 0.27338314056396484\n",
      "epoch: 8 step: 16, loss is 0.282370388507843\n",
      "epoch: 8 step: 17, loss is 0.16820722818374634\n",
      "epoch: 8 step: 18, loss is 0.5068241953849792\n",
      "epoch: 8 step: 19, loss is 0.21519893407821655\n",
      "epoch: 8 step: 20, loss is 0.32594311237335205\n",
      "epoch: 8 step: 21, loss is 0.29196417331695557\n",
      "epoch: 8 step: 22, loss is 0.2833503782749176\n",
      "epoch: 8 step: 23, loss is 0.18419350683689117\n",
      "epoch: 8 step: 24, loss is 0.197724848985672\n",
      "epoch: 8 step: 25, loss is 0.15976929664611816\n",
      "epoch: 8 step: 26, loss is 0.34556835889816284\n",
      "epoch: 8 step: 27, loss is 0.1745835244655609\n",
      "epoch: 8 step: 28, loss is 0.2151133269071579\n",
      "epoch: 8 step: 29, loss is 0.242767333984375\n",
      "epoch: 8 step: 30, loss is 0.49055996537208557\n",
      "epoch: 8 step: 31, loss is 0.2792453169822693\n",
      "epoch: 8 step: 32, loss is 0.15142373740673065\n",
      "epoch: 8 step: 33, loss is 0.3221606910228729\n",
      "epoch: 8 step: 34, loss is 0.2167179137468338\n",
      "epoch: 8 step: 35, loss is 0.15543536841869354\n",
      "epoch: 8 step: 36, loss is 0.42686522006988525\n",
      "epoch: 8 step: 37, loss is 0.34912925958633423\n",
      "epoch: 8 step: 38, loss is 0.29695358872413635\n",
      "epoch: 8 step: 39, loss is 0.24086163938045502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 40, loss is 0.19696681201457977\n",
      "epoch: 8 step: 41, loss is 0.2244076281785965\n",
      "epoch: 8 step: 42, loss is 0.258249431848526\n",
      "epoch: 8 step: 43, loss is 0.25601500272750854\n",
      "epoch: 8 step: 44, loss is 0.23416215181350708\n",
      "epoch: 8 step: 45, loss is 0.16399557888507843\n",
      "epoch: 8 step: 46, loss is 0.2973495423793793\n",
      "epoch: 8 step: 47, loss is 0.4184816777706146\n",
      "epoch: 8 step: 48, loss is 0.05319974198937416\n",
      "epoch: 8 step: 49, loss is 0.36156514286994934\n",
      "epoch: 8 step: 50, loss is 0.25038784742355347\n",
      "epoch: 8 step: 51, loss is 0.14887458086013794\n",
      "epoch: 8 step: 52, loss is 0.23753362894058228\n",
      "epoch: 8 step: 53, loss is 0.393351286649704\n",
      "epoch: 8 step: 54, loss is 0.2768442928791046\n",
      "epoch: 8 step: 55, loss is 0.21098531782627106\n",
      "epoch: 8 step: 56, loss is 0.3437165319919586\n",
      "epoch: 8 step: 57, loss is 0.2301282286643982\n",
      "epoch: 8 step: 58, loss is 0.333539217710495\n",
      "epoch: 8 step: 59, loss is 0.17501848936080933\n",
      "epoch: 8 step: 60, loss is 0.23370739817619324\n",
      "epoch: 8 step: 61, loss is 0.20885053277015686\n",
      "epoch: 8 step: 62, loss is 0.1876830905675888\n",
      "epoch: 8 step: 63, loss is 0.4377726912498474\n",
      "epoch: 8 step: 64, loss is 0.19576314091682434\n",
      "epoch: 8 step: 65, loss is 0.18018704652786255\n",
      "epoch: 8 step: 66, loss is 0.12313089519739151\n",
      "epoch: 8 step: 67, loss is 0.22813871502876282\n",
      "epoch: 8 step: 68, loss is 0.18869264423847198\n",
      "epoch: 8 step: 69, loss is 0.2617781460285187\n",
      "epoch: 8 step: 70, loss is 0.23854997754096985\n",
      "epoch: 8 step: 71, loss is 0.37048235535621643\n",
      "epoch: 8 step: 72, loss is 0.28181353211402893\n",
      "epoch: 8 step: 73, loss is 0.14886367321014404\n",
      "epoch: 8 step: 74, loss is 0.16881611943244934\n",
      "epoch: 8 step: 75, loss is 0.41692307591438293\n",
      "epoch: 8 step: 76, loss is 0.18314515054225922\n",
      "epoch: 8 step: 77, loss is 0.12084181606769562\n",
      "epoch: 8 step: 78, loss is 0.3646191656589508\n",
      "epoch: 8 step: 79, loss is 0.4110356271266937\n",
      "epoch: 8 step: 80, loss is 0.3819780945777893\n",
      "epoch: 8 step: 81, loss is 0.2863540053367615\n",
      "epoch: 8 step: 82, loss is 0.1239871084690094\n",
      "epoch: 8 step: 83, loss is 0.11521382629871368\n",
      "epoch: 8 step: 84, loss is 0.29205915331840515\n",
      "epoch: 8 step: 85, loss is 0.261181503534317\n",
      "epoch: 8 step: 86, loss is 0.2303933948278427\n",
      "epoch: 8 step: 87, loss is 0.1749880164861679\n",
      "epoch: 8 step: 88, loss is 0.14634618163108826\n",
      "epoch: 8 step: 89, loss is 0.3335535526275635\n",
      "epoch: 8 step: 90, loss is 0.32761651277542114\n",
      "epoch: 8 step: 91, loss is 0.3367658853530884\n",
      "epoch: 8 step: 92, loss is 0.23777024447917938\n",
      "epoch: 8 step: 93, loss is 0.11877197027206421\n",
      "epoch: 8 step: 94, loss is 0.28242000937461853\n",
      "epoch: 8 step: 95, loss is 0.2229394167661667\n",
      "epoch: 8 step: 96, loss is 0.19575288891792297\n",
      "epoch: 8 step: 97, loss is 0.26869773864746094\n",
      "epoch: 8 step: 98, loss is 0.3023368716239929\n",
      "epoch: 8 step: 99, loss is 0.2740812301635742\n",
      "epoch: 8 step: 100, loss is 0.1600375473499298\n",
      "epoch: 8 step: 101, loss is 0.18738669157028198\n",
      "epoch: 8 step: 102, loss is 0.11387678980827332\n",
      "epoch: 8 step: 103, loss is 0.46252599358558655\n",
      "epoch: 8 step: 104, loss is 0.29528674483299255\n",
      "epoch: 8 step: 105, loss is 0.1730760931968689\n",
      "epoch: 8 step: 106, loss is 0.16275762021541595\n",
      "epoch: 8 step: 107, loss is 0.35077181458473206\n",
      "epoch: 8 step: 108, loss is 0.21793803572654724\n",
      "epoch: 8 step: 109, loss is 0.2654518783092499\n",
      "epoch: 8 step: 110, loss is 0.2251831740140915\n",
      "epoch: 8 step: 111, loss is 0.43229830265045166\n",
      "epoch: 8 step: 112, loss is 0.326359361410141\n",
      "epoch: 8 step: 113, loss is 0.26362210512161255\n",
      "epoch: 8 step: 114, loss is 0.35629144310951233\n",
      "epoch: 8 step: 115, loss is 0.18910737335681915\n",
      "epoch: 8 step: 116, loss is 0.11632762849330902\n",
      "epoch: 8 step: 117, loss is 0.2918187081813812\n",
      "epoch: 8 step: 118, loss is 0.29695749282836914\n",
      "epoch: 8 step: 119, loss is 0.3328875005245209\n",
      "epoch: 8 step: 120, loss is 0.24266786873340607\n",
      "epoch: 8 step: 121, loss is 0.3371555209159851\n",
      "epoch: 8 step: 122, loss is 0.25789016485214233\n",
      "epoch: 8 step: 123, loss is 0.23188625276088715\n",
      "epoch: 8 step: 124, loss is 0.2480088621377945\n",
      "epoch: 8 step: 125, loss is 0.2254512757062912\n",
      "epoch: 8 step: 126, loss is 0.21817658841609955\n",
      "epoch: 8 step: 127, loss is 0.18887025117874146\n",
      "epoch: 8 step: 128, loss is 0.22537082433700562\n",
      "epoch: 8 step: 129, loss is 0.33064207434654236\n",
      "epoch: 8 step: 130, loss is 0.2432238608598709\n",
      "epoch: 8 step: 131, loss is 0.33488914370536804\n",
      "epoch: 8 step: 132, loss is 0.3306696116924286\n",
      "epoch: 8 step: 133, loss is 0.16713017225265503\n",
      "epoch: 8 step: 134, loss is 0.33504897356033325\n",
      "epoch: 8 step: 135, loss is 0.43131735920906067\n",
      "epoch: 8 step: 136, loss is 0.13798688352108002\n",
      "epoch: 8 step: 137, loss is 0.41127440333366394\n",
      "epoch: 8 step: 138, loss is 0.4898878037929535\n",
      "epoch: 8 step: 139, loss is 0.0948433130979538\n",
      "epoch: 8 step: 140, loss is 0.18246819078922272\n",
      "epoch: 8 step: 141, loss is 0.3869888186454773\n",
      "epoch: 8 step: 142, loss is 0.2488061487674713\n",
      "epoch: 8 step: 143, loss is 0.19327867031097412\n",
      "epoch: 8 step: 144, loss is 0.1881522536277771\n",
      "epoch: 8 step: 145, loss is 0.2767191231250763\n",
      "epoch: 8 step: 146, loss is 0.22975555062294006\n",
      "epoch: 8 step: 147, loss is 0.13731089234352112\n",
      "epoch: 8 step: 148, loss is 0.3941786289215088\n",
      "epoch: 8 step: 149, loss is 0.14066165685653687\n",
      "epoch: 8 step: 150, loss is 0.20469242334365845\n",
      "epoch: 8 step: 151, loss is 0.16676093637943268\n",
      "epoch: 8 step: 152, loss is 0.23440997302532196\n",
      "epoch: 8 step: 153, loss is 0.23376360535621643\n",
      "epoch: 8 step: 154, loss is 0.15596817433834076\n",
      "epoch: 8 step: 155, loss is 0.2769647240638733\n",
      "epoch: 8 step: 156, loss is 0.3635936379432678\n",
      "epoch: 8 step: 157, loss is 0.193318709731102\n",
      "epoch: 8 step: 158, loss is 0.1631697118282318\n",
      "epoch: 8 step: 159, loss is 0.37649524211883545\n",
      "epoch: 8 step: 160, loss is 0.1711452454328537\n",
      "epoch: 8 step: 161, loss is 0.1566804200410843\n",
      "epoch: 8 step: 162, loss is 0.2600834369659424\n",
      "epoch: 8 step: 163, loss is 0.08697675913572311\n",
      "epoch: 8 step: 164, loss is 0.15649458765983582\n",
      "epoch: 8 step: 165, loss is 0.1779169887304306\n",
      "epoch: 8 step: 166, loss is 0.1445685625076294\n",
      "epoch: 8 step: 167, loss is 0.25494709610939026\n",
      "epoch: 8 step: 168, loss is 0.1639453023672104\n",
      "epoch: 8 step: 169, loss is 0.3158937394618988\n",
      "epoch: 8 step: 170, loss is 0.25500521063804626\n",
      "epoch: 8 step: 171, loss is 0.29609939455986023\n",
      "epoch: 8 step: 172, loss is 0.23628441989421844\n",
      "epoch: 8 step: 173, loss is 0.23931248486042023\n",
      "epoch: 8 step: 174, loss is 0.267180472612381\n",
      "epoch: 8 step: 175, loss is 0.20868846774101257\n",
      "epoch: 8 step: 176, loss is 0.1770055592060089\n",
      "epoch: 8 step: 177, loss is 0.24890965223312378\n",
      "epoch: 8 step: 178, loss is 0.16985587775707245\n",
      "epoch: 8 step: 179, loss is 0.3288690447807312\n",
      "epoch: 8 step: 180, loss is 0.1699386090040207\n",
      "epoch: 8 step: 181, loss is 0.19380789995193481\n",
      "epoch: 8 step: 182, loss is 0.2255469262599945\n",
      "epoch: 8 step: 183, loss is 0.12537993490695953\n",
      "epoch: 8 step: 184, loss is 0.36808207631111145\n",
      "epoch: 8 step: 185, loss is 0.3261118531227112\n",
      "epoch: 8 step: 186, loss is 0.22165203094482422\n",
      "epoch: 8 step: 187, loss is 0.3624398708343506\n",
      "epoch: 8 step: 188, loss is 0.36442676186561584\n",
      "epoch: 8 step: 189, loss is 0.28328096866607666\n",
      "epoch: 8 step: 190, loss is 0.2725825905799866\n",
      "epoch: 8 step: 191, loss is 0.1876325011253357\n",
      "epoch: 8 step: 192, loss is 0.24890339374542236\n",
      "epoch: 8 step: 193, loss is 0.35894256830215454\n",
      "epoch: 8 step: 194, loss is 0.35886144638061523\n",
      "epoch: 8 step: 195, loss is 0.1321282982826233\n",
      "epoch: 8 step: 196, loss is 0.28029462695121765\n",
      "epoch: 8 step: 197, loss is 0.1554923802614212\n",
      "epoch: 8 step: 198, loss is 0.20814812183380127\n",
      "epoch: 8 step: 199, loss is 0.1686464548110962\n",
      "epoch: 8 step: 200, loss is 0.26341405510902405\n",
      "epoch: 8 step: 201, loss is 0.15858963131904602\n",
      "epoch: 8 step: 202, loss is 0.46515923738479614\n",
      "epoch: 8 step: 203, loss is 0.3730086386203766\n",
      "epoch: 8 step: 204, loss is 0.22045959532260895\n",
      "epoch: 8 step: 205, loss is 0.1976664662361145\n",
      "epoch: 8 step: 206, loss is 0.2417479157447815\n",
      "epoch: 8 step: 207, loss is 0.3345060646533966\n",
      "epoch: 8 step: 208, loss is 0.15186282992362976\n",
      "epoch: 8 step: 209, loss is 0.21713313460350037\n",
      "epoch: 8 step: 210, loss is 0.20791754126548767\n",
      "epoch: 8 step: 211, loss is 0.2673775255680084\n",
      "epoch: 8 step: 212, loss is 0.38793256878852844\n",
      "epoch: 8 step: 213, loss is 0.3859117329120636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 214, loss is 0.15355564653873444\n",
      "epoch: 8 step: 215, loss is 0.15298807621002197\n",
      "epoch: 8 step: 216, loss is 0.3157864809036255\n",
      "epoch: 8 step: 217, loss is 0.26799893379211426\n",
      "epoch: 8 step: 218, loss is 0.42821699380874634\n",
      "epoch: 8 step: 219, loss is 0.23522934317588806\n",
      "epoch: 8 step: 220, loss is 0.2234382927417755\n",
      "epoch: 8 step: 221, loss is 0.21824082732200623\n",
      "epoch: 8 step: 222, loss is 0.12020068615674973\n",
      "epoch: 8 step: 223, loss is 0.25686249136924744\n",
      "epoch: 8 step: 224, loss is 0.31396251916885376\n",
      "epoch: 8 step: 225, loss is 0.17454901337623596\n",
      "epoch: 8 step: 226, loss is 0.26593512296676636\n",
      "epoch: 8 step: 227, loss is 0.24651499092578888\n",
      "epoch: 8 step: 228, loss is 0.17510530352592468\n",
      "epoch: 8 step: 229, loss is 0.2631870210170746\n",
      "epoch: 8 step: 230, loss is 0.3544836640357971\n",
      "epoch: 8 step: 231, loss is 0.28910157084465027\n",
      "epoch: 8 step: 232, loss is 0.1449057012796402\n",
      "epoch: 8 step: 233, loss is 0.34730568528175354\n",
      "epoch: 8 step: 234, loss is 0.2151719033718109\n",
      "epoch: 8 step: 235, loss is 0.3431304097175598\n",
      "epoch: 8 step: 236, loss is 0.27763277292251587\n",
      "epoch: 8 step: 237, loss is 0.22388336062431335\n",
      "epoch: 8 step: 238, loss is 0.10728130489587784\n",
      "epoch: 8 step: 239, loss is 0.16699276864528656\n",
      "epoch: 8 step: 240, loss is 0.21866241097450256\n",
      "epoch: 8 step: 241, loss is 0.15709373354911804\n",
      "epoch: 8 step: 242, loss is 0.37104588747024536\n",
      "epoch: 8 step: 243, loss is 0.41998380422592163\n",
      "epoch: 8 step: 244, loss is 0.20759081840515137\n",
      "epoch: 8 step: 245, loss is 0.14118342101573944\n",
      "epoch: 8 step: 246, loss is 0.1899442970752716\n",
      "epoch: 8 step: 247, loss is 0.3868720531463623\n",
      "epoch: 8 step: 248, loss is 0.15309007465839386\n",
      "epoch: 8 step: 249, loss is 0.09700523316860199\n",
      "epoch: 8 step: 250, loss is 0.2101200819015503\n",
      "epoch: 8 step: 251, loss is 0.24032115936279297\n",
      "epoch: 8 step: 252, loss is 0.18808497488498688\n",
      "epoch: 8 step: 253, loss is 0.1777983158826828\n",
      "epoch: 8 step: 254, loss is 0.3242921233177185\n",
      "epoch: 8 step: 255, loss is 0.36702343821525574\n",
      "epoch: 8 step: 256, loss is 0.190666064620018\n",
      "epoch: 8 step: 257, loss is 0.27130019664764404\n",
      "epoch: 8 step: 258, loss is 0.16807152330875397\n",
      "epoch: 8 step: 259, loss is 0.27757611870765686\n",
      "epoch: 8 step: 260, loss is 0.18747620284557343\n",
      "epoch: 8 step: 261, loss is 0.14009486138820648\n",
      "epoch: 8 step: 262, loss is 0.27747711539268494\n",
      "epoch: 8 step: 263, loss is 0.1437705159187317\n",
      "epoch: 8 step: 264, loss is 0.24612382054328918\n",
      "epoch: 8 step: 265, loss is 0.16082483530044556\n",
      "epoch: 8 step: 266, loss is 0.35066837072372437\n",
      "epoch: 8 step: 267, loss is 0.19297915697097778\n",
      "epoch: 8 step: 268, loss is 0.23365001380443573\n",
      "epoch: 8 step: 269, loss is 0.30453941226005554\n",
      "epoch: 8 step: 270, loss is 0.2709941565990448\n",
      "epoch: 8 step: 271, loss is 0.3556206226348877\n",
      "epoch: 8 step: 272, loss is 0.27013474702835083\n",
      "epoch: 8 step: 273, loss is 0.42119091749191284\n",
      "epoch: 8 step: 274, loss is 0.29738858342170715\n",
      "epoch: 8 step: 275, loss is 0.3554578423500061\n",
      "epoch: 8 step: 276, loss is 0.1930253952741623\n",
      "epoch: 8 step: 277, loss is 0.39291441440582275\n",
      "epoch: 8 step: 278, loss is 0.26941928267478943\n",
      "epoch: 8 step: 279, loss is 0.1796855330467224\n",
      "epoch: 8 step: 280, loss is 0.3509640097618103\n",
      "epoch: 8 step: 281, loss is 0.42853066325187683\n",
      "epoch: 8 step: 282, loss is 0.2245635986328125\n",
      "epoch: 8 step: 283, loss is 0.33994752168655396\n",
      "epoch: 8 step: 284, loss is 0.3461149036884308\n",
      "epoch: 8 step: 285, loss is 0.15239794552326202\n",
      "epoch: 8 step: 286, loss is 0.4253234267234802\n",
      "epoch: 8 step: 287, loss is 0.36143940687179565\n",
      "epoch: 8 step: 288, loss is 0.32341790199279785\n",
      "epoch: 8 step: 289, loss is 0.26182687282562256\n",
      "epoch: 8 step: 290, loss is 0.1366153508424759\n",
      "epoch: 8 step: 291, loss is 0.19304104149341583\n",
      "epoch: 8 step: 292, loss is 0.13760027289390564\n",
      "epoch: 8 step: 293, loss is 0.1317836195230484\n",
      "epoch: 8 step: 294, loss is 0.31474757194519043\n",
      "epoch: 8 step: 295, loss is 0.27040573954582214\n",
      "epoch: 8 step: 296, loss is 0.26770445704460144\n",
      "epoch: 8 step: 297, loss is 0.24427734315395355\n",
      "epoch: 8 step: 298, loss is 0.2725968360900879\n",
      "epoch: 8 step: 299, loss is 0.1929917335510254\n",
      "epoch: 8 step: 300, loss is 0.27939438819885254\n",
      "epoch: 8 step: 301, loss is 0.20056775212287903\n",
      "epoch: 8 step: 302, loss is 0.3494788110256195\n",
      "epoch: 8 step: 303, loss is 0.36272624135017395\n",
      "epoch: 8 step: 304, loss is 0.42232125997543335\n",
      "epoch: 8 step: 305, loss is 0.23469223082065582\n",
      "epoch: 8 step: 306, loss is 0.3151499330997467\n",
      "epoch: 8 step: 307, loss is 0.21051083505153656\n",
      "epoch: 8 step: 308, loss is 0.24705316126346588\n",
      "epoch: 8 step: 309, loss is 0.19926688075065613\n",
      "epoch: 8 step: 310, loss is 0.1237582266330719\n",
      "epoch: 8 step: 311, loss is 0.18839752674102783\n",
      "epoch: 8 step: 312, loss is 0.17330197989940643\n",
      "epoch: 8 step: 313, loss is 0.29571256041526794\n",
      "epoch: 8 step: 314, loss is 0.25122153759002686\n",
      "epoch: 8 step: 315, loss is 0.3645454943180084\n",
      "epoch: 8 step: 316, loss is 0.18836933374404907\n",
      "epoch: 8 step: 317, loss is 0.18828140199184418\n",
      "epoch: 8 step: 318, loss is 0.3027793765068054\n",
      "epoch: 8 step: 319, loss is 0.13293682038784027\n",
      "epoch: 8 step: 320, loss is 0.19224973022937775\n",
      "epoch: 8 step: 321, loss is 0.3161364197731018\n",
      "epoch: 8 step: 322, loss is 0.2839937210083008\n",
      "epoch: 8 step: 323, loss is 0.28538912534713745\n",
      "epoch: 8 step: 324, loss is 0.25688284635543823\n",
      "epoch: 8 step: 325, loss is 0.17009025812149048\n",
      "epoch: 8 step: 326, loss is 0.23296406865119934\n",
      "epoch: 8 step: 327, loss is 0.27675947546958923\n",
      "epoch: 8 step: 328, loss is 0.14283043146133423\n",
      "epoch: 8 step: 329, loss is 0.30405762791633606\n",
      "epoch: 8 step: 330, loss is 0.2110593169927597\n",
      "epoch: 8 step: 331, loss is 0.49210864305496216\n",
      "epoch: 8 step: 332, loss is 0.3349096477031708\n",
      "epoch: 8 step: 333, loss is 0.15915977954864502\n",
      "epoch: 8 step: 334, loss is 0.22183048725128174\n",
      "epoch: 8 step: 335, loss is 0.26047849655151367\n",
      "epoch: 8 step: 336, loss is 0.25725114345550537\n",
      "epoch: 8 step: 337, loss is 0.26554015278816223\n",
      "epoch: 8 step: 338, loss is 0.09704964607954025\n",
      "epoch: 8 step: 339, loss is 0.24637503921985626\n",
      "epoch: 8 step: 340, loss is 0.4516961872577667\n",
      "epoch: 8 step: 341, loss is 0.26229047775268555\n",
      "epoch: 8 step: 342, loss is 0.19968438148498535\n",
      "epoch: 8 step: 343, loss is 0.3797752559185028\n",
      "epoch: 8 step: 344, loss is 0.3996767997741699\n",
      "epoch: 8 step: 345, loss is 0.3183053135871887\n",
      "epoch: 8 step: 346, loss is 0.2882494628429413\n",
      "epoch: 8 step: 347, loss is 0.31498590111732483\n",
      "epoch: 8 step: 348, loss is 0.26857322454452515\n",
      "epoch: 8 step: 349, loss is 0.14461278915405273\n",
      "epoch: 8 step: 350, loss is 0.28597408533096313\n",
      "epoch: 8 step: 351, loss is 0.19584114849567413\n",
      "epoch: 8 step: 352, loss is 0.17355501651763916\n",
      "epoch: 8 step: 353, loss is 0.2870553731918335\n",
      "epoch: 8 step: 354, loss is 0.3132329285144806\n",
      "epoch: 8 step: 355, loss is 0.23600554466247559\n",
      "epoch: 8 step: 356, loss is 0.169566348195076\n",
      "epoch: 8 step: 357, loss is 0.2138388305902481\n",
      "epoch: 8 step: 358, loss is 0.31808459758758545\n",
      "epoch: 8 step: 359, loss is 0.28352224826812744\n",
      "epoch: 8 step: 360, loss is 0.18130427598953247\n",
      "epoch: 8 step: 361, loss is 0.2239808738231659\n",
      "epoch: 8 step: 362, loss is 0.27576199173927307\n",
      "epoch: 8 step: 363, loss is 0.28021782636642456\n",
      "epoch: 8 step: 364, loss is 0.13598321378231049\n",
      "epoch: 8 step: 365, loss is 0.5359365940093994\n",
      "epoch: 8 step: 366, loss is 0.2714523673057556\n",
      "epoch: 8 step: 367, loss is 0.3710845410823822\n",
      "epoch: 8 step: 368, loss is 0.25664573907852173\n",
      "epoch: 8 step: 369, loss is 0.28563717007637024\n",
      "epoch: 8 step: 370, loss is 0.21193541586399078\n",
      "epoch: 8 step: 371, loss is 0.37227872014045715\n",
      "epoch: 8 step: 372, loss is 0.2333286702632904\n",
      "epoch: 8 step: 373, loss is 0.19609881937503815\n",
      "epoch: 8 step: 374, loss is 0.263915091753006\n",
      "epoch: 8 step: 375, loss is 0.2766735553741455\n",
      "epoch: 8 step: 376, loss is 0.5705816149711609\n",
      "epoch: 8 step: 377, loss is 0.16258831322193146\n",
      "epoch: 8 step: 378, loss is 0.3590877056121826\n",
      "epoch: 8 step: 379, loss is 0.15445534884929657\n",
      "epoch: 8 step: 380, loss is 0.1844639778137207\n",
      "epoch: 8 step: 381, loss is 0.5065443515777588\n",
      "epoch: 8 step: 382, loss is 0.17450208961963654\n",
      "epoch: 8 step: 383, loss is 0.27209538221359253\n",
      "epoch: 8 step: 384, loss is 0.20541736483573914\n",
      "epoch: 8 step: 385, loss is 0.3443879187107086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 386, loss is 0.23914749920368195\n",
      "epoch: 8 step: 387, loss is 0.18025527894496918\n",
      "epoch: 8 step: 388, loss is 0.3520638644695282\n",
      "epoch: 8 step: 389, loss is 0.2903960049152374\n",
      "epoch: 8 step: 390, loss is 0.40496668219566345\n",
      "epoch: 8 step: 391, loss is 0.2306620329618454\n",
      "epoch: 8 step: 392, loss is 0.39562609791755676\n",
      "epoch: 8 step: 393, loss is 0.16623681783676147\n",
      "epoch: 8 step: 394, loss is 0.23223133385181427\n",
      "epoch: 8 step: 395, loss is 0.2386358082294464\n",
      "epoch: 8 step: 396, loss is 0.13498607277870178\n",
      "epoch: 8 step: 397, loss is 0.18425728380680084\n",
      "epoch: 8 step: 398, loss is 0.36590540409088135\n",
      "epoch: 8 step: 399, loss is 0.24311710894107819\n",
      "epoch: 8 step: 400, loss is 0.22207872569561005\n",
      "epoch: 8 step: 401, loss is 0.3796074092388153\n",
      "epoch: 8 step: 402, loss is 0.20473045110702515\n",
      "epoch: 8 step: 403, loss is 0.18984541296958923\n",
      "epoch: 8 step: 404, loss is 0.11942987143993378\n",
      "epoch: 8 step: 405, loss is 0.2481568604707718\n",
      "epoch: 8 step: 406, loss is 0.22141338884830475\n",
      "epoch: 8 step: 407, loss is 0.21877409517765045\n",
      "epoch: 8 step: 408, loss is 0.31243154406547546\n",
      "epoch: 8 step: 409, loss is 0.2924072742462158\n",
      "epoch: 8 step: 410, loss is 0.3917969763278961\n",
      "epoch: 8 step: 411, loss is 0.18895885348320007\n",
      "epoch: 8 step: 412, loss is 0.18846163153648376\n",
      "epoch: 8 step: 413, loss is 0.18429571390151978\n",
      "epoch: 8 step: 414, loss is 0.2547384202480316\n",
      "epoch: 8 step: 415, loss is 0.1307070404291153\n",
      "epoch: 8 step: 416, loss is 0.28037720918655396\n",
      "epoch: 8 step: 417, loss is 0.11139269918203354\n",
      "epoch: 8 step: 418, loss is 0.3475668430328369\n",
      "epoch: 8 step: 419, loss is 0.3274702727794647\n",
      "epoch: 8 step: 420, loss is 0.2917037308216095\n",
      "epoch: 8 step: 421, loss is 0.22620704770088196\n",
      "epoch: 8 step: 422, loss is 0.21280089020729065\n",
      "epoch: 8 step: 423, loss is 0.2674943506717682\n",
      "epoch: 8 step: 424, loss is 0.19078227877616882\n",
      "epoch: 8 step: 425, loss is 0.35897940397262573\n",
      "epoch: 8 step: 426, loss is 0.13116231560707092\n",
      "epoch: 8 step: 427, loss is 0.34202802181243896\n",
      "epoch: 8 step: 428, loss is 0.2192428559064865\n",
      "epoch: 8 step: 429, loss is 0.3497503697872162\n",
      "epoch: 8 step: 430, loss is 0.22979171574115753\n",
      "epoch: 8 step: 431, loss is 0.2097652405500412\n",
      "epoch: 8 step: 432, loss is 0.3548542559146881\n",
      "epoch: 8 step: 433, loss is 0.1905270367860794\n",
      "epoch: 8 step: 434, loss is 0.347589910030365\n",
      "epoch: 8 step: 435, loss is 0.16961407661437988\n",
      "epoch: 8 step: 436, loss is 0.3128809928894043\n",
      "epoch: 8 step: 437, loss is 0.32623791694641113\n",
      "epoch: 8 step: 438, loss is 0.32685086131095886\n",
      "epoch: 8 step: 439, loss is 0.23351162672042847\n",
      "epoch: 8 step: 440, loss is 0.3337292969226837\n",
      "epoch: 8 step: 441, loss is 0.2813636064529419\n",
      "epoch: 8 step: 442, loss is 0.28472989797592163\n",
      "epoch: 8 step: 443, loss is 0.28064680099487305\n",
      "epoch: 8 step: 444, loss is 0.26510244607925415\n",
      "epoch: 8 step: 445, loss is 0.364134818315506\n",
      "epoch: 8 step: 446, loss is 0.11314551532268524\n",
      "epoch: 8 step: 447, loss is 0.19258283078670502\n",
      "epoch: 8 step: 448, loss is 0.29190388321876526\n",
      "epoch: 8 step: 449, loss is 0.10627665370702744\n",
      "epoch: 8 step: 450, loss is 0.275767982006073\n",
      "epoch: 8 step: 451, loss is 0.0760190486907959\n",
      "epoch: 8 step: 452, loss is 0.3471662998199463\n",
      "epoch: 8 step: 453, loss is 0.2208579182624817\n",
      "epoch: 8 step: 454, loss is 0.39124488830566406\n",
      "epoch: 8 step: 455, loss is 0.3127584457397461\n",
      "epoch: 8 step: 456, loss is 0.14360174536705017\n",
      "epoch: 8 step: 457, loss is 0.2611912488937378\n",
      "epoch: 8 step: 458, loss is 0.1371532529592514\n",
      "epoch: 8 step: 459, loss is 0.2409055531024933\n",
      "epoch: 8 step: 460, loss is 0.158450648188591\n",
      "epoch: 8 step: 461, loss is 0.3043188154697418\n",
      "epoch: 8 step: 462, loss is 0.29736262559890747\n",
      "epoch: 8 step: 463, loss is 0.29264044761657715\n",
      "epoch: 8 step: 464, loss is 0.13690735399723053\n",
      "epoch: 8 step: 465, loss is 0.2779647409915924\n",
      "epoch: 8 step: 466, loss is 0.46102049946784973\n",
      "epoch: 8 step: 467, loss is 0.3217260241508484\n",
      "epoch: 8 step: 468, loss is 0.3375788629055023\n",
      "epoch: 8 step: 469, loss is 0.23452919721603394\n",
      "epoch: 8 step: 470, loss is 0.22937169671058655\n",
      "epoch: 8 step: 471, loss is 0.3998754024505615\n",
      "epoch: 8 step: 472, loss is 0.3615873157978058\n",
      "epoch: 8 step: 473, loss is 0.3873808979988098\n",
      "epoch: 8 step: 474, loss is 0.2948548197746277\n",
      "epoch: 8 step: 475, loss is 0.19172106683254242\n",
      "epoch: 8 step: 476, loss is 0.4955292344093323\n",
      "epoch: 8 step: 477, loss is 0.4311111867427826\n",
      "epoch: 8 step: 478, loss is 0.23292526602745056\n",
      "epoch: 8 step: 479, loss is 0.2601032257080078\n",
      "epoch: 8 step: 480, loss is 0.2116905003786087\n",
      "epoch: 8 step: 481, loss is 0.33037298917770386\n",
      "epoch: 8 step: 482, loss is 0.23949670791625977\n",
      "epoch: 8 step: 483, loss is 0.18332740664482117\n",
      "epoch: 8 step: 484, loss is 0.3194940388202667\n",
      "epoch: 8 step: 485, loss is 0.25840190052986145\n",
      "epoch: 8 step: 486, loss is 0.22344772517681122\n",
      "epoch: 8 step: 487, loss is 0.41150447726249695\n",
      "epoch: 8 step: 488, loss is 0.2382587045431137\n",
      "epoch: 8 step: 489, loss is 0.2411152720451355\n",
      "epoch: 8 step: 490, loss is 0.26744866371154785\n",
      "epoch: 8 step: 491, loss is 0.3382726013660431\n",
      "epoch: 8 step: 492, loss is 0.2935446798801422\n",
      "epoch: 8 step: 493, loss is 0.302291601896286\n",
      "epoch: 8 step: 494, loss is 0.13333718478679657\n",
      "epoch: 8 step: 495, loss is 0.20640477538108826\n",
      "epoch: 8 step: 496, loss is 0.23146900534629822\n",
      "epoch: 8 step: 497, loss is 0.2923901081085205\n",
      "epoch: 8 step: 498, loss is 0.25086596608161926\n",
      "epoch: 8 step: 499, loss is 0.18194769322872162\n",
      "epoch: 8 step: 500, loss is 0.2414814680814743\n",
      "epoch: 8 step: 501, loss is 0.38504260778427124\n",
      "epoch: 8 step: 502, loss is 0.25326675176620483\n",
      "epoch: 8 step: 503, loss is 0.27376362681388855\n",
      "epoch: 8 step: 504, loss is 0.28453630208969116\n",
      "epoch: 8 step: 505, loss is 0.19582459330558777\n",
      "epoch: 8 step: 506, loss is 0.09528244286775589\n",
      "epoch: 8 step: 507, loss is 0.2339189052581787\n",
      "epoch: 8 step: 508, loss is 0.2350039929151535\n",
      "epoch: 8 step: 509, loss is 0.10145896673202515\n",
      "epoch: 8 step: 510, loss is 0.33428797125816345\n",
      "epoch: 8 step: 511, loss is 0.17940394580364227\n",
      "epoch: 8 step: 512, loss is 0.15189898014068604\n",
      "epoch: 8 step: 513, loss is 0.1110009178519249\n",
      "epoch: 8 step: 514, loss is 0.27231261134147644\n",
      "epoch: 8 step: 515, loss is 0.1286429613828659\n",
      "epoch: 8 step: 516, loss is 0.16985151171684265\n",
      "epoch: 8 step: 517, loss is 0.1815468668937683\n",
      "epoch: 8 step: 518, loss is 0.2316272258758545\n",
      "epoch: 8 step: 519, loss is 0.2156158834695816\n",
      "epoch: 8 step: 520, loss is 0.3568154275417328\n",
      "epoch: 8 step: 521, loss is 0.17388609051704407\n",
      "epoch: 8 step: 522, loss is 0.2555365264415741\n",
      "epoch: 8 step: 523, loss is 0.2527119517326355\n",
      "epoch: 8 step: 524, loss is 0.31545108556747437\n",
      "epoch: 8 step: 525, loss is 0.22356581687927246\n",
      "epoch: 8 step: 526, loss is 0.24482271075248718\n",
      "epoch: 8 step: 527, loss is 0.12135569751262665\n",
      "epoch: 8 step: 528, loss is 0.4359624981880188\n",
      "epoch: 8 step: 529, loss is 0.274982750415802\n",
      "epoch: 8 step: 530, loss is 0.28163942694664\n",
      "epoch: 8 step: 531, loss is 0.34961938858032227\n",
      "epoch: 8 step: 532, loss is 0.2486080378293991\n",
      "epoch: 8 step: 533, loss is 0.2531804144382477\n",
      "epoch: 8 step: 534, loss is 0.2687549293041229\n",
      "epoch: 8 step: 535, loss is 0.24917930364608765\n",
      "epoch: 8 step: 536, loss is 0.2289169430732727\n",
      "epoch: 8 step: 537, loss is 0.2071099579334259\n",
      "epoch: 8 step: 538, loss is 0.3688417375087738\n",
      "epoch: 8 step: 539, loss is 0.20287787914276123\n",
      "epoch: 8 step: 540, loss is 0.1730964183807373\n",
      "epoch: 8 step: 541, loss is 0.13077475130558014\n",
      "epoch: 8 step: 542, loss is 0.15828481316566467\n",
      "epoch: 8 step: 543, loss is 0.19116349518299103\n",
      "epoch: 8 step: 544, loss is 0.23314842581748962\n",
      "epoch: 8 step: 545, loss is 0.3379954695701599\n",
      "epoch: 8 step: 546, loss is 0.22348085045814514\n",
      "epoch: 8 step: 547, loss is 0.2788392901420593\n",
      "epoch: 8 step: 548, loss is 0.2811967730522156\n",
      "epoch: 8 step: 549, loss is 0.18184253573417664\n",
      "epoch: 8 step: 550, loss is 0.20170629024505615\n",
      "epoch: 8 step: 551, loss is 0.4155506491661072\n",
      "epoch: 8 step: 552, loss is 0.2910057306289673\n",
      "epoch: 8 step: 553, loss is 0.3670342266559601\n",
      "epoch: 8 step: 554, loss is 0.08416423946619034\n",
      "epoch: 8 step: 555, loss is 0.3491145670413971\n",
      "epoch: 8 step: 556, loss is 0.2785753905773163\n",
      "epoch: 8 step: 557, loss is 0.34311771392822266\n",
      "epoch: 8 step: 558, loss is 0.2794399857521057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 559, loss is 0.1464182585477829\n",
      "epoch: 8 step: 560, loss is 0.32616615295410156\n",
      "epoch: 8 step: 561, loss is 0.2061329483985901\n",
      "epoch: 8 step: 562, loss is 0.3337840735912323\n",
      "epoch: 8 step: 563, loss is 0.1998959183692932\n",
      "epoch: 8 step: 564, loss is 0.18489763140678406\n",
      "epoch: 8 step: 565, loss is 0.15053719282150269\n",
      "epoch: 8 step: 566, loss is 0.12711165845394135\n",
      "epoch: 8 step: 567, loss is 0.20243939757347107\n",
      "epoch: 8 step: 568, loss is 0.3137407898902893\n",
      "epoch: 8 step: 569, loss is 0.21526551246643066\n",
      "epoch: 8 step: 570, loss is 0.17533960938453674\n",
      "epoch: 8 step: 571, loss is 0.29780107736587524\n",
      "epoch: 8 step: 572, loss is 0.27765288949012756\n",
      "epoch: 8 step: 573, loss is 0.25717654824256897\n",
      "epoch: 8 step: 574, loss is 0.41180506348609924\n",
      "epoch: 8 step: 575, loss is 0.21326477825641632\n",
      "epoch: 8 step: 576, loss is 0.1365118771791458\n",
      "epoch: 8 step: 577, loss is 0.42122429609298706\n",
      "epoch: 8 step: 578, loss is 0.22546131908893585\n",
      "epoch: 8 step: 579, loss is 0.23881042003631592\n",
      "epoch: 8 step: 580, loss is 0.19650813937187195\n",
      "epoch: 8 step: 581, loss is 0.2346489131450653\n",
      "epoch: 8 step: 582, loss is 0.21776817739009857\n",
      "epoch: 8 step: 583, loss is 0.24409662187099457\n",
      "epoch: 8 step: 584, loss is 0.2760583758354187\n",
      "epoch: 8 step: 585, loss is 0.32220906019210815\n",
      "epoch: 8 step: 586, loss is 0.326930969953537\n",
      "epoch: 8 step: 587, loss is 0.35078662633895874\n",
      "epoch: 8 step: 588, loss is 0.19318461418151855\n",
      "epoch: 8 step: 589, loss is 0.2095952183008194\n",
      "epoch: 8 step: 590, loss is 0.07955282926559448\n",
      "epoch: 8 step: 591, loss is 0.16830012202262878\n",
      "epoch: 8 step: 592, loss is 0.18358473479747772\n",
      "epoch: 8 step: 593, loss is 0.20323751866817474\n",
      "epoch: 8 step: 594, loss is 0.2509081959724426\n",
      "epoch: 8 step: 595, loss is 0.26882490515708923\n",
      "epoch: 8 step: 596, loss is 0.45549988746643066\n",
      "epoch: 8 step: 597, loss is 0.24499669671058655\n",
      "epoch: 8 step: 598, loss is 0.45167088508605957\n",
      "epoch: 8 step: 599, loss is 0.14245176315307617\n",
      "epoch: 8 step: 600, loss is 0.26980146765708923\n",
      "epoch: 8 step: 601, loss is 0.17610037326812744\n",
      "epoch: 8 step: 602, loss is 0.20230180025100708\n",
      "epoch: 8 step: 603, loss is 0.23413042724132538\n",
      "epoch: 8 step: 604, loss is 0.2500409483909607\n",
      "epoch: 8 step: 605, loss is 0.12639257311820984\n",
      "epoch: 8 step: 606, loss is 0.1620182991027832\n",
      "epoch: 8 step: 607, loss is 0.38172805309295654\n",
      "epoch: 8 step: 608, loss is 0.17053359746932983\n",
      "epoch: 8 step: 609, loss is 0.276799738407135\n",
      "epoch: 8 step: 610, loss is 0.17124296724796295\n",
      "epoch: 8 step: 611, loss is 0.12839257717132568\n",
      "epoch: 8 step: 612, loss is 0.2234918624162674\n",
      "epoch: 8 step: 613, loss is 0.31521889567375183\n",
      "epoch: 8 step: 614, loss is 0.1420290470123291\n",
      "epoch: 8 step: 615, loss is 0.28636133670806885\n",
      "epoch: 8 step: 616, loss is 0.1839861422777176\n",
      "epoch: 8 step: 617, loss is 0.3347678780555725\n",
      "epoch: 8 step: 618, loss is 0.25999554991722107\n",
      "epoch: 8 step: 619, loss is 0.20840047299861908\n",
      "epoch: 8 step: 620, loss is 0.2092645913362503\n",
      "epoch: 8 step: 621, loss is 0.1918162852525711\n",
      "epoch: 8 step: 622, loss is 0.43867284059524536\n",
      "epoch: 8 step: 623, loss is 0.2644619643688202\n",
      "epoch: 8 step: 624, loss is 0.23573800921440125\n",
      "epoch: 8 step: 625, loss is 0.33916863799095154\n",
      "epoch: 8 step: 626, loss is 0.3089667856693268\n",
      "epoch: 8 step: 627, loss is 0.2365766316652298\n",
      "epoch: 8 step: 628, loss is 0.23930113017559052\n",
      "epoch: 8 step: 629, loss is 0.15128661692142487\n",
      "epoch: 8 step: 630, loss is 0.1143207848072052\n",
      "epoch: 8 step: 631, loss is 0.23700283467769623\n",
      "epoch: 8 step: 632, loss is 0.2641756534576416\n",
      "epoch: 8 step: 633, loss is 0.24408386647701263\n",
      "epoch: 8 step: 634, loss is 0.18805114924907684\n",
      "epoch: 8 step: 635, loss is 0.25297126173973083\n",
      "epoch: 8 step: 636, loss is 0.16388756036758423\n",
      "epoch: 8 step: 637, loss is 0.09350639581680298\n",
      "epoch: 8 step: 638, loss is 0.24047715961933136\n",
      "epoch: 8 step: 639, loss is 0.23345042765140533\n",
      "epoch: 8 step: 640, loss is 0.21140265464782715\n",
      "epoch: 8 step: 641, loss is 0.11645814776420593\n",
      "epoch: 8 step: 642, loss is 0.2774946987628937\n",
      "epoch: 8 step: 643, loss is 0.15017658472061157\n",
      "epoch: 8 step: 644, loss is 0.3425240218639374\n",
      "epoch: 8 step: 645, loss is 0.20371831953525543\n",
      "epoch: 8 step: 646, loss is 0.28411129117012024\n",
      "epoch: 8 step: 647, loss is 0.24390870332717896\n",
      "epoch: 8 step: 648, loss is 0.35500577092170715\n",
      "epoch: 8 step: 649, loss is 0.2599509656429291\n",
      "epoch: 8 step: 650, loss is 0.2962900400161743\n",
      "epoch: 8 step: 651, loss is 0.2583051025867462\n",
      "epoch: 8 step: 652, loss is 0.216032475233078\n",
      "epoch: 8 step: 653, loss is 0.2059246003627777\n",
      "epoch: 8 step: 654, loss is 0.4101361632347107\n",
      "epoch: 8 step: 655, loss is 0.22383303940296173\n",
      "epoch: 8 step: 656, loss is 0.4071939289569855\n",
      "epoch: 8 step: 657, loss is 0.1243346780538559\n",
      "epoch: 8 step: 658, loss is 0.30810123682022095\n",
      "epoch: 8 step: 659, loss is 0.17756296694278717\n",
      "epoch: 8 step: 660, loss is 0.2683890163898468\n",
      "epoch: 8 step: 661, loss is 0.10167639702558517\n",
      "epoch: 8 step: 662, loss is 0.2796746492385864\n",
      "epoch: 8 step: 663, loss is 0.4757155478000641\n",
      "epoch: 8 step: 664, loss is 0.399618536233902\n",
      "epoch: 8 step: 665, loss is 0.34323734045028687\n",
      "epoch: 8 step: 666, loss is 0.24113872647285461\n",
      "epoch: 8 step: 667, loss is 0.27142953872680664\n",
      "epoch: 8 step: 668, loss is 0.30274057388305664\n",
      "epoch: 8 step: 669, loss is 0.17598992586135864\n",
      "epoch: 8 step: 670, loss is 0.17948366701602936\n",
      "epoch: 8 step: 671, loss is 0.29554226994514465\n",
      "epoch: 8 step: 672, loss is 0.4476216435432434\n",
      "epoch: 8 step: 673, loss is 0.21497061848640442\n",
      "epoch: 8 step: 674, loss is 0.20364710688591003\n",
      "epoch: 8 step: 675, loss is 0.22842739522457123\n",
      "epoch: 8 step: 676, loss is 0.2444140762090683\n",
      "epoch: 8 step: 677, loss is 0.23244468867778778\n",
      "epoch: 8 step: 678, loss is 0.13476309180259705\n",
      "epoch: 8 step: 679, loss is 0.290208101272583\n",
      "epoch: 8 step: 680, loss is 0.2834237217903137\n",
      "epoch: 8 step: 681, loss is 0.1742614209651947\n",
      "epoch: 8 step: 682, loss is 0.21584074199199677\n",
      "epoch: 8 step: 683, loss is 0.15297017991542816\n",
      "epoch: 8 step: 684, loss is 0.24603411555290222\n",
      "epoch: 8 step: 685, loss is 0.11636680364608765\n",
      "epoch: 8 step: 686, loss is 0.2569454312324524\n",
      "epoch: 8 step: 687, loss is 0.2101355344057083\n",
      "epoch: 8 step: 688, loss is 0.2859908938407898\n",
      "epoch: 8 step: 689, loss is 0.20802301168441772\n",
      "epoch: 8 step: 690, loss is 0.23085947334766388\n",
      "epoch: 8 step: 691, loss is 0.37740427255630493\n",
      "epoch: 8 step: 692, loss is 0.21524274349212646\n",
      "epoch: 8 step: 693, loss is 0.25869619846343994\n",
      "epoch: 8 step: 694, loss is 0.3227009177207947\n",
      "epoch: 8 step: 695, loss is 0.17292769253253937\n",
      "epoch: 8 step: 696, loss is 0.14212431013584137\n",
      "epoch: 8 step: 697, loss is 0.16577081382274628\n",
      "epoch: 8 step: 698, loss is 0.23557406663894653\n",
      "epoch: 8 step: 699, loss is 0.27432557940483093\n",
      "epoch: 8 step: 700, loss is 0.1725451946258545\n",
      "epoch: 8 step: 701, loss is 0.16461296379566193\n",
      "epoch: 8 step: 702, loss is 0.25217777490615845\n",
      "epoch: 8 step: 703, loss is 0.23408031463623047\n",
      "epoch: 8 step: 704, loss is 0.1998043805360794\n",
      "epoch: 8 step: 705, loss is 0.40303489565849304\n",
      "epoch: 8 step: 706, loss is 0.15527930855751038\n",
      "epoch: 8 step: 707, loss is 0.3015921711921692\n",
      "epoch: 8 step: 708, loss is 0.3560451567173004\n",
      "epoch: 8 step: 709, loss is 0.26615428924560547\n",
      "epoch: 8 step: 710, loss is 0.2674437165260315\n",
      "epoch: 8 step: 711, loss is 0.34322646260261536\n",
      "epoch: 8 step: 712, loss is 0.35290518403053284\n",
      "epoch: 8 step: 713, loss is 0.14712569117546082\n",
      "epoch: 8 step: 714, loss is 0.16109202802181244\n",
      "epoch: 8 step: 715, loss is 0.15052074193954468\n",
      "epoch: 8 step: 716, loss is 0.3437975347042084\n",
      "epoch: 8 step: 717, loss is 0.32909223437309265\n",
      "epoch: 8 step: 718, loss is 0.35939735174179077\n",
      "epoch: 8 step: 719, loss is 0.17118369042873383\n",
      "epoch: 8 step: 720, loss is 0.27532994747161865\n",
      "epoch: 8 step: 721, loss is 0.35072922706604004\n",
      "epoch: 8 step: 722, loss is 0.19457978010177612\n",
      "epoch: 8 step: 723, loss is 0.24579130113124847\n",
      "epoch: 8 step: 724, loss is 0.24429050087928772\n",
      "epoch: 8 step: 725, loss is 0.17534752190113068\n",
      "epoch: 8 step: 726, loss is 0.317996621131897\n",
      "epoch: 8 step: 727, loss is 0.3470432162284851\n",
      "epoch: 8 step: 728, loss is 0.4624171257019043\n",
      "epoch: 8 step: 729, loss is 0.22488319873809814\n",
      "epoch: 8 step: 730, loss is 0.304885596036911\n",
      "epoch: 8 step: 731, loss is 0.3792818486690521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 732, loss is 0.22117938101291656\n",
      "epoch: 8 step: 733, loss is 0.2779478132724762\n",
      "epoch: 8 step: 734, loss is 0.14940659701824188\n",
      "epoch: 8 step: 735, loss is 0.46119916439056396\n",
      "epoch: 8 step: 736, loss is 0.1619080752134323\n",
      "epoch: 8 step: 737, loss is 0.2569669783115387\n",
      "epoch: 8 step: 738, loss is 0.19989429414272308\n",
      "epoch: 8 step: 739, loss is 0.30284497141838074\n",
      "epoch: 8 step: 740, loss is 0.45311883091926575\n",
      "epoch: 8 step: 741, loss is 0.30811357498168945\n",
      "epoch: 8 step: 742, loss is 0.25388848781585693\n",
      "epoch: 8 step: 743, loss is 0.18135438859462738\n",
      "epoch: 8 step: 744, loss is 0.7014870643615723\n",
      "epoch: 8 step: 745, loss is 0.28889375925064087\n",
      "epoch: 8 step: 746, loss is 0.2904600501060486\n",
      "epoch: 8 step: 747, loss is 0.2827965319156647\n",
      "epoch: 8 step: 748, loss is 0.26619645953178406\n",
      "epoch: 8 step: 749, loss is 0.23235076665878296\n",
      "epoch: 8 step: 750, loss is 0.2698807716369629\n",
      "epoch: 8 step: 751, loss is 0.487506240606308\n",
      "epoch: 8 step: 752, loss is 0.3857971131801605\n",
      "epoch: 8 step: 753, loss is 0.30034592747688293\n",
      "epoch: 8 step: 754, loss is 0.2549287676811218\n",
      "epoch: 8 step: 755, loss is 0.1897232085466385\n",
      "epoch: 8 step: 756, loss is 0.29565268754959106\n",
      "epoch: 8 step: 757, loss is 0.1633814126253128\n",
      "epoch: 8 step: 758, loss is 0.17198680341243744\n",
      "epoch: 8 step: 759, loss is 0.36162105202674866\n",
      "epoch: 8 step: 760, loss is 0.22766342759132385\n",
      "epoch: 8 step: 761, loss is 0.2234952449798584\n",
      "epoch: 8 step: 762, loss is 0.2894684970378876\n",
      "epoch: 8 step: 763, loss is 0.23231031000614166\n",
      "epoch: 8 step: 764, loss is 0.151570126414299\n",
      "epoch: 8 step: 765, loss is 0.24471162259578705\n",
      "epoch: 8 step: 766, loss is 0.3103930950164795\n",
      "epoch: 8 step: 767, loss is 0.4513593316078186\n",
      "epoch: 8 step: 768, loss is 0.22935479879379272\n",
      "epoch: 8 step: 769, loss is 0.15281887352466583\n",
      "epoch: 8 step: 770, loss is 0.19891871511936188\n",
      "epoch: 8 step: 771, loss is 0.17152364552021027\n",
      "epoch: 8 step: 772, loss is 0.1869184970855713\n",
      "epoch: 8 step: 773, loss is 0.33653759956359863\n",
      "epoch: 8 step: 774, loss is 0.2652628719806671\n",
      "epoch: 8 step: 775, loss is 0.2534187436103821\n",
      "epoch: 8 step: 776, loss is 0.11995664238929749\n",
      "epoch: 8 step: 777, loss is 0.2105964571237564\n",
      "epoch: 8 step: 778, loss is 0.38231298327445984\n",
      "epoch: 8 step: 779, loss is 0.31301578879356384\n",
      "epoch: 8 step: 780, loss is 0.41887709498405457\n",
      "epoch: 8 step: 781, loss is 0.29944637417793274\n",
      "epoch: 8 step: 782, loss is 0.22946946322917938\n",
      "epoch: 8 step: 783, loss is 0.24442678689956665\n",
      "epoch: 8 step: 784, loss is 0.2186342179775238\n",
      "epoch: 8 step: 785, loss is 0.39840051531791687\n",
      "epoch: 8 step: 786, loss is 0.2406994253396988\n",
      "epoch: 8 step: 787, loss is 0.2510049641132355\n",
      "epoch: 8 step: 788, loss is 0.3144122064113617\n",
      "epoch: 8 step: 789, loss is 0.19345048069953918\n",
      "epoch: 8 step: 790, loss is 0.20252227783203125\n",
      "epoch: 8 step: 791, loss is 0.25857245922088623\n",
      "epoch: 8 step: 792, loss is 0.17341482639312744\n",
      "epoch: 8 step: 793, loss is 0.2475549578666687\n",
      "epoch: 8 step: 794, loss is 0.2749203145503998\n",
      "epoch: 8 step: 795, loss is 0.1945115029811859\n",
      "epoch: 8 step: 796, loss is 0.2147008329629898\n",
      "epoch: 8 step: 797, loss is 0.25454747676849365\n",
      "epoch: 8 step: 798, loss is 0.47121939063072205\n",
      "epoch: 8 step: 799, loss is 0.11128078401088715\n",
      "epoch: 8 step: 800, loss is 0.1844777911901474\n",
      "epoch: 8 step: 801, loss is 0.3036988079547882\n",
      "epoch: 8 step: 802, loss is 0.13496211171150208\n",
      "epoch: 8 step: 803, loss is 0.20234522223472595\n",
      "epoch: 8 step: 804, loss is 0.34891700744628906\n",
      "epoch: 8 step: 805, loss is 0.23537345230579376\n",
      "epoch: 8 step: 806, loss is 0.2636435031890869\n",
      "epoch: 8 step: 807, loss is 0.42902591824531555\n",
      "epoch: 8 step: 808, loss is 0.1959829032421112\n",
      "epoch: 8 step: 809, loss is 0.13918276131153107\n",
      "epoch: 8 step: 810, loss is 0.48449790477752686\n",
      "epoch: 8 step: 811, loss is 0.18045532703399658\n",
      "epoch: 8 step: 812, loss is 0.29079172015190125\n",
      "epoch: 8 step: 813, loss is 0.21764907240867615\n",
      "epoch: 8 step: 814, loss is 0.20568853616714478\n",
      "epoch: 8 step: 815, loss is 0.13515977561473846\n",
      "epoch: 8 step: 816, loss is 0.3782269060611725\n",
      "epoch: 8 step: 817, loss is 0.14953529834747314\n",
      "epoch: 8 step: 818, loss is 0.1925068199634552\n",
      "epoch: 8 step: 819, loss is 0.36198481917381287\n",
      "epoch: 8 step: 820, loss is 0.23592564463615417\n",
      "epoch: 8 step: 821, loss is 0.242497056722641\n",
      "epoch: 8 step: 822, loss is 0.2734428942203522\n",
      "epoch: 8 step: 823, loss is 0.29118525981903076\n",
      "epoch: 8 step: 824, loss is 0.2760481536388397\n",
      "epoch: 8 step: 825, loss is 0.3707803189754486\n",
      "epoch: 8 step: 826, loss is 0.1514165699481964\n",
      "epoch: 8 step: 827, loss is 0.12769323587417603\n",
      "epoch: 8 step: 828, loss is 0.3745315372943878\n",
      "epoch: 8 step: 829, loss is 0.28090929985046387\n",
      "epoch: 8 step: 830, loss is 0.3343009650707245\n",
      "epoch: 8 step: 831, loss is 0.25476503372192383\n",
      "epoch: 8 step: 832, loss is 0.27560341358184814\n",
      "epoch: 8 step: 833, loss is 0.18134969472885132\n",
      "epoch: 8 step: 834, loss is 0.11987246572971344\n",
      "epoch: 8 step: 835, loss is 0.34239670634269714\n",
      "epoch: 8 step: 836, loss is 0.2680213749408722\n",
      "epoch: 8 step: 837, loss is 0.3116855025291443\n",
      "epoch: 8 step: 838, loss is 0.34401339292526245\n",
      "epoch: 8 step: 839, loss is 0.3176160752773285\n",
      "epoch: 8 step: 840, loss is 0.24491707980632782\n",
      "epoch: 8 step: 841, loss is 0.47208139300346375\n",
      "epoch: 8 step: 842, loss is 0.34990817308425903\n",
      "epoch: 8 step: 843, loss is 0.35494428873062134\n",
      "epoch: 8 step: 844, loss is 0.3026270568370819\n",
      "epoch: 8 step: 845, loss is 0.28604188561439514\n",
      "epoch: 8 step: 846, loss is 0.29910776019096375\n",
      "epoch: 8 step: 847, loss is 0.33853965997695923\n",
      "epoch: 8 step: 848, loss is 0.36063456535339355\n",
      "epoch: 8 step: 849, loss is 0.3369675278663635\n",
      "epoch: 8 step: 850, loss is 0.23960445821285248\n",
      "epoch: 8 step: 851, loss is 0.20804233849048615\n",
      "epoch: 8 step: 852, loss is 0.34102651476860046\n",
      "epoch: 8 step: 853, loss is 0.269910603761673\n",
      "epoch: 8 step: 854, loss is 0.1926763504743576\n",
      "epoch: 8 step: 855, loss is 0.41396835446357727\n",
      "epoch: 8 step: 856, loss is 0.1862819343805313\n",
      "epoch: 8 step: 857, loss is 0.408033162355423\n",
      "epoch: 8 step: 858, loss is 0.211416557431221\n",
      "epoch: 8 step: 859, loss is 0.15039828419685364\n",
      "epoch: 8 step: 860, loss is 0.17014268040657043\n",
      "epoch: 8 step: 861, loss is 0.394637793302536\n",
      "epoch: 8 step: 862, loss is 0.4681142568588257\n",
      "epoch: 8 step: 863, loss is 0.17516571283340454\n",
      "epoch: 8 step: 864, loss is 0.20599354803562164\n",
      "epoch: 8 step: 865, loss is 0.34240084886550903\n",
      "epoch: 8 step: 866, loss is 0.32516998052597046\n",
      "epoch: 8 step: 867, loss is 0.4067981541156769\n",
      "epoch: 8 step: 868, loss is 0.21245631575584412\n",
      "epoch: 8 step: 869, loss is 0.18838956952095032\n",
      "epoch: 8 step: 870, loss is 0.3551993668079376\n",
      "epoch: 8 step: 871, loss is 0.3142251968383789\n",
      "epoch: 8 step: 872, loss is 0.23249298334121704\n",
      "epoch: 8 step: 873, loss is 0.24965117871761322\n",
      "epoch: 8 step: 874, loss is 0.3128221333026886\n",
      "epoch: 8 step: 875, loss is 0.256060391664505\n",
      "epoch: 8 step: 876, loss is 0.1926829069852829\n",
      "epoch: 8 step: 877, loss is 0.305078387260437\n",
      "epoch: 8 step: 878, loss is 0.24445144832134247\n",
      "epoch: 8 step: 879, loss is 0.2736440598964691\n",
      "epoch: 8 step: 880, loss is 0.13431787490844727\n",
      "epoch: 8 step: 881, loss is 0.3765619993209839\n",
      "epoch: 8 step: 882, loss is 0.23924797773361206\n",
      "epoch: 8 step: 883, loss is 0.11754478514194489\n",
      "epoch: 8 step: 884, loss is 0.2142866849899292\n",
      "epoch: 8 step: 885, loss is 0.4443369209766388\n",
      "epoch: 8 step: 886, loss is 0.3242014944553375\n",
      "epoch: 8 step: 887, loss is 0.1863960325717926\n",
      "epoch: 8 step: 888, loss is 0.3706950843334198\n",
      "epoch: 8 step: 889, loss is 0.20913025736808777\n",
      "epoch: 8 step: 890, loss is 0.3228956162929535\n",
      "epoch: 8 step: 891, loss is 0.24853959679603577\n",
      "epoch: 8 step: 892, loss is 0.27993839979171753\n",
      "epoch: 8 step: 893, loss is 0.466916561126709\n",
      "epoch: 8 step: 894, loss is 0.335897296667099\n",
      "epoch: 8 step: 895, loss is 0.16147208213806152\n",
      "epoch: 8 step: 896, loss is 0.24438723921775818\n",
      "epoch: 8 step: 897, loss is 0.1681453436613083\n",
      "epoch: 8 step: 898, loss is 0.28721046447753906\n",
      "epoch: 8 step: 899, loss is 0.2917807102203369\n",
      "epoch: 8 step: 900, loss is 0.24877449870109558\n",
      "epoch: 8 step: 901, loss is 0.2102276086807251\n",
      "epoch: 8 step: 902, loss is 0.2976575493812561\n",
      "epoch: 8 step: 903, loss is 0.21555274724960327\n",
      "epoch: 8 step: 904, loss is 0.08317913860082626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 905, loss is 0.2030855119228363\n",
      "epoch: 8 step: 906, loss is 0.16944265365600586\n",
      "epoch: 8 step: 907, loss is 0.24276308715343475\n",
      "epoch: 8 step: 908, loss is 0.17030097544193268\n",
      "epoch: 8 step: 909, loss is 0.26797521114349365\n",
      "epoch: 8 step: 910, loss is 0.1365896314382553\n",
      "epoch: 8 step: 911, loss is 0.15507279336452484\n",
      "epoch: 8 step: 912, loss is 0.31782326102256775\n",
      "epoch: 8 step: 913, loss is 0.16945485770702362\n",
      "epoch: 8 step: 914, loss is 0.3875490725040436\n",
      "epoch: 8 step: 915, loss is 0.1185343936085701\n",
      "epoch: 8 step: 916, loss is 0.38542497158050537\n",
      "epoch: 8 step: 917, loss is 0.19322822988033295\n",
      "epoch: 8 step: 918, loss is 0.17417781054973602\n",
      "epoch: 8 step: 919, loss is 0.26682940125465393\n",
      "epoch: 8 step: 920, loss is 0.15852971374988556\n",
      "epoch: 8 step: 921, loss is 0.21772845089435577\n",
      "epoch: 8 step: 922, loss is 0.2732081115245819\n",
      "epoch: 8 step: 923, loss is 0.2104831337928772\n",
      "epoch: 8 step: 924, loss is 0.2346148043870926\n",
      "epoch: 8 step: 925, loss is 0.4112517237663269\n",
      "epoch: 8 step: 926, loss is 0.26980724930763245\n",
      "epoch: 8 step: 927, loss is 0.41319888830184937\n",
      "epoch: 8 step: 928, loss is 0.25909918546676636\n",
      "epoch: 8 step: 929, loss is 0.164925679564476\n",
      "epoch: 8 step: 930, loss is 0.29803821444511414\n",
      "epoch: 8 step: 931, loss is 0.29150936007499695\n",
      "epoch: 8 step: 932, loss is 0.25040510296821594\n",
      "epoch: 8 step: 933, loss is 0.17806552350521088\n",
      "epoch: 8 step: 934, loss is 0.2958296537399292\n",
      "epoch: 8 step: 935, loss is 0.2641913890838623\n",
      "epoch: 8 step: 936, loss is 0.18825703859329224\n",
      "epoch: 8 step: 937, loss is 0.22961635887622833\n",
      "epoch: 9 step: 1, loss is 0.2405575066804886\n",
      "epoch: 9 step: 2, loss is 0.3555956780910492\n",
      "epoch: 9 step: 3, loss is 0.09569500386714935\n",
      "epoch: 9 step: 4, loss is 0.31579017639160156\n",
      "epoch: 9 step: 5, loss is 0.1937035620212555\n",
      "epoch: 9 step: 6, loss is 0.29200923442840576\n",
      "epoch: 9 step: 7, loss is 0.24632759392261505\n",
      "epoch: 9 step: 8, loss is 0.31023621559143066\n",
      "epoch: 9 step: 9, loss is 0.20213069021701813\n",
      "epoch: 9 step: 10, loss is 0.21418221294879913\n",
      "epoch: 9 step: 11, loss is 0.32859957218170166\n",
      "epoch: 9 step: 12, loss is 0.23967117071151733\n",
      "epoch: 9 step: 13, loss is 0.4068335294723511\n",
      "epoch: 9 step: 14, loss is 0.22496682405471802\n",
      "epoch: 9 step: 15, loss is 0.18635733425617218\n",
      "epoch: 9 step: 16, loss is 0.3376145362854004\n",
      "epoch: 9 step: 17, loss is 0.17171506583690643\n",
      "epoch: 9 step: 18, loss is 0.1871694028377533\n",
      "epoch: 9 step: 19, loss is 0.2171659618616104\n",
      "epoch: 9 step: 20, loss is 0.24888944625854492\n",
      "epoch: 9 step: 21, loss is 0.2623739242553711\n",
      "epoch: 9 step: 22, loss is 0.12068035453557968\n",
      "epoch: 9 step: 23, loss is 0.2522417902946472\n",
      "epoch: 9 step: 24, loss is 0.2538951337337494\n",
      "epoch: 9 step: 25, loss is 0.2082051783800125\n",
      "epoch: 9 step: 26, loss is 0.44747135043144226\n",
      "epoch: 9 step: 27, loss is 0.1066591888666153\n",
      "epoch: 9 step: 28, loss is 0.28234028816223145\n",
      "epoch: 9 step: 29, loss is 0.3599175214767456\n",
      "epoch: 9 step: 30, loss is 0.29246780276298523\n",
      "epoch: 9 step: 31, loss is 0.20400795340538025\n",
      "epoch: 9 step: 32, loss is 0.12066280841827393\n",
      "epoch: 9 step: 33, loss is 0.28956419229507446\n",
      "epoch: 9 step: 34, loss is 0.3446328938007355\n",
      "epoch: 9 step: 35, loss is 0.32393303513526917\n",
      "epoch: 9 step: 36, loss is 0.23018284142017365\n",
      "epoch: 9 step: 37, loss is 0.2915358543395996\n",
      "epoch: 9 step: 38, loss is 0.15561255812644958\n",
      "epoch: 9 step: 39, loss is 0.21837401390075684\n",
      "epoch: 9 step: 40, loss is 0.2702537775039673\n",
      "epoch: 9 step: 41, loss is 0.22954750061035156\n",
      "epoch: 9 step: 42, loss is 0.24848535656929016\n",
      "epoch: 9 step: 43, loss is 0.27060654759407043\n",
      "epoch: 9 step: 44, loss is 0.3190191686153412\n",
      "epoch: 9 step: 45, loss is 0.17284439504146576\n",
      "epoch: 9 step: 46, loss is 0.3050869107246399\n",
      "epoch: 9 step: 47, loss is 0.4738346040248871\n",
      "epoch: 9 step: 48, loss is 0.25933632254600525\n",
      "epoch: 9 step: 49, loss is 0.14269647002220154\n",
      "epoch: 9 step: 50, loss is 0.26190146803855896\n",
      "epoch: 9 step: 51, loss is 0.2731826603412628\n",
      "epoch: 9 step: 52, loss is 0.24987204372882843\n",
      "epoch: 9 step: 53, loss is 0.2799426019191742\n",
      "epoch: 9 step: 54, loss is 0.26029130816459656\n",
      "epoch: 9 step: 55, loss is 0.29070964455604553\n",
      "epoch: 9 step: 56, loss is 0.40220358967781067\n",
      "epoch: 9 step: 57, loss is 0.35779568552970886\n",
      "epoch: 9 step: 58, loss is 0.3714708983898163\n",
      "epoch: 9 step: 59, loss is 0.15663392841815948\n",
      "epoch: 9 step: 60, loss is 0.15834608674049377\n",
      "epoch: 9 step: 61, loss is 0.291490763425827\n",
      "epoch: 9 step: 62, loss is 0.13680486381053925\n",
      "epoch: 9 step: 63, loss is 0.19675752520561218\n",
      "epoch: 9 step: 64, loss is 0.23846472799777985\n",
      "epoch: 9 step: 65, loss is 0.2930651009082794\n",
      "epoch: 9 step: 66, loss is 0.19141536951065063\n",
      "epoch: 9 step: 67, loss is 0.2455141693353653\n",
      "epoch: 9 step: 68, loss is 0.3009472191333771\n",
      "epoch: 9 step: 69, loss is 0.2467823028564453\n",
      "epoch: 9 step: 70, loss is 0.20572450757026672\n",
      "epoch: 9 step: 71, loss is 0.3293914794921875\n",
      "epoch: 9 step: 72, loss is 0.3552311956882477\n",
      "epoch: 9 step: 73, loss is 0.2237018346786499\n",
      "epoch: 9 step: 74, loss is 0.29006585478782654\n",
      "epoch: 9 step: 75, loss is 0.3221304416656494\n",
      "epoch: 9 step: 76, loss is 0.432086318731308\n",
      "epoch: 9 step: 77, loss is 0.21678544580936432\n",
      "epoch: 9 step: 78, loss is 0.10309643298387527\n",
      "epoch: 9 step: 79, loss is 0.28375256061553955\n",
      "epoch: 9 step: 80, loss is 0.11817900836467743\n",
      "epoch: 9 step: 81, loss is 0.13915042579174042\n",
      "epoch: 9 step: 82, loss is 0.3553546369075775\n",
      "epoch: 9 step: 83, loss is 0.27096590399742126\n",
      "epoch: 9 step: 84, loss is 0.2351371794939041\n",
      "epoch: 9 step: 85, loss is 0.20405213534832\n",
      "epoch: 9 step: 86, loss is 0.268706738948822\n",
      "epoch: 9 step: 87, loss is 0.16618339717388153\n",
      "epoch: 9 step: 88, loss is 0.19707942008972168\n",
      "epoch: 9 step: 89, loss is 0.23687811195850372\n",
      "epoch: 9 step: 90, loss is 0.3355688452720642\n",
      "epoch: 9 step: 91, loss is 0.22597530484199524\n",
      "epoch: 9 step: 92, loss is 0.39689457416534424\n",
      "epoch: 9 step: 93, loss is 0.2583135962486267\n",
      "epoch: 9 step: 94, loss is 0.3124404847621918\n",
      "epoch: 9 step: 95, loss is 0.28605854511260986\n",
      "epoch: 9 step: 96, loss is 0.14325374364852905\n",
      "epoch: 9 step: 97, loss is 0.22722309827804565\n",
      "epoch: 9 step: 98, loss is 0.15450157225131989\n",
      "epoch: 9 step: 99, loss is 0.2059761881828308\n",
      "epoch: 9 step: 100, loss is 0.19374558329582214\n",
      "epoch: 9 step: 101, loss is 0.18822810053825378\n",
      "epoch: 9 step: 102, loss is 0.338764488697052\n",
      "epoch: 9 step: 103, loss is 0.2852473556995392\n",
      "epoch: 9 step: 104, loss is 0.342151939868927\n",
      "epoch: 9 step: 105, loss is 0.1538529396057129\n",
      "epoch: 9 step: 106, loss is 0.1478969305753708\n",
      "epoch: 9 step: 107, loss is 0.1444857269525528\n",
      "epoch: 9 step: 108, loss is 0.16917386651039124\n",
      "epoch: 9 step: 109, loss is 0.35640162229537964\n",
      "epoch: 9 step: 110, loss is 0.22071176767349243\n",
      "epoch: 9 step: 111, loss is 0.19885262846946716\n",
      "epoch: 9 step: 112, loss is 0.33354824781417847\n",
      "epoch: 9 step: 113, loss is 0.3683885633945465\n",
      "epoch: 9 step: 114, loss is 0.1510961502790451\n",
      "epoch: 9 step: 115, loss is 0.19268764555454254\n",
      "epoch: 9 step: 116, loss is 0.4519215524196625\n",
      "epoch: 9 step: 117, loss is 0.39482229948043823\n",
      "epoch: 9 step: 118, loss is 0.19432833790779114\n",
      "epoch: 9 step: 119, loss is 0.3525811731815338\n",
      "epoch: 9 step: 120, loss is 0.30694863200187683\n",
      "epoch: 9 step: 121, loss is 0.30578964948654175\n",
      "epoch: 9 step: 122, loss is 0.427388072013855\n",
      "epoch: 9 step: 123, loss is 0.20261281728744507\n",
      "epoch: 9 step: 124, loss is 0.33742088079452515\n",
      "epoch: 9 step: 125, loss is 0.21152469515800476\n",
      "epoch: 9 step: 126, loss is 0.1628793179988861\n",
      "epoch: 9 step: 127, loss is 0.2423303872346878\n",
      "epoch: 9 step: 128, loss is 0.3334033787250519\n",
      "epoch: 9 step: 129, loss is 0.23404709994792938\n",
      "epoch: 9 step: 130, loss is 0.40909862518310547\n",
      "epoch: 9 step: 131, loss is 0.29396310448646545\n",
      "epoch: 9 step: 132, loss is 0.24844089150428772\n",
      "epoch: 9 step: 133, loss is 0.22613409161567688\n",
      "epoch: 9 step: 134, loss is 0.14592890441417694\n",
      "epoch: 9 step: 135, loss is 0.16285106539726257\n",
      "epoch: 9 step: 136, loss is 0.22402219474315643\n",
      "epoch: 9 step: 137, loss is 0.1868165284395218\n",
      "epoch: 9 step: 138, loss is 0.21799471974372864\n",
      "epoch: 9 step: 139, loss is 0.27650657296180725\n",
      "epoch: 9 step: 140, loss is 0.2119283825159073\n",
      "epoch: 9 step: 141, loss is 0.18347369134426117\n",
      "epoch: 9 step: 142, loss is 0.09602752327919006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 143, loss is 0.1862039864063263\n",
      "epoch: 9 step: 144, loss is 0.2684853672981262\n",
      "epoch: 9 step: 145, loss is 0.443285197019577\n",
      "epoch: 9 step: 146, loss is 0.3002888262271881\n",
      "epoch: 9 step: 147, loss is 0.3445533812046051\n",
      "epoch: 9 step: 148, loss is 0.48199039697647095\n",
      "epoch: 9 step: 149, loss is 0.37548089027404785\n",
      "epoch: 9 step: 150, loss is 0.2453869879245758\n",
      "epoch: 9 step: 151, loss is 0.218944251537323\n",
      "epoch: 9 step: 152, loss is 0.2849881947040558\n",
      "epoch: 9 step: 153, loss is 0.22624637186527252\n",
      "epoch: 9 step: 154, loss is 0.1683015674352646\n",
      "epoch: 9 step: 155, loss is 0.16892041265964508\n",
      "epoch: 9 step: 156, loss is 0.2459648698568344\n",
      "epoch: 9 step: 157, loss is 0.19793406128883362\n",
      "epoch: 9 step: 158, loss is 0.2744006812572479\n",
      "epoch: 9 step: 159, loss is 0.25447240471839905\n",
      "epoch: 9 step: 160, loss is 0.20595945417881012\n",
      "epoch: 9 step: 161, loss is 0.23872019350528717\n",
      "epoch: 9 step: 162, loss is 0.14247292280197144\n",
      "epoch: 9 step: 163, loss is 0.34387707710266113\n",
      "epoch: 9 step: 164, loss is 0.3516457676887512\n",
      "epoch: 9 step: 165, loss is 0.27449893951416016\n",
      "epoch: 9 step: 166, loss is 0.2752743661403656\n",
      "epoch: 9 step: 167, loss is 0.22793418169021606\n",
      "epoch: 9 step: 168, loss is 0.37406784296035767\n",
      "epoch: 9 step: 169, loss is 0.2467213273048401\n",
      "epoch: 9 step: 170, loss is 0.286721408367157\n",
      "epoch: 9 step: 171, loss is 0.29901060461997986\n",
      "epoch: 9 step: 172, loss is 0.31264135241508484\n",
      "epoch: 9 step: 173, loss is 0.20071306824684143\n",
      "epoch: 9 step: 174, loss is 0.18437445163726807\n",
      "epoch: 9 step: 175, loss is 0.18259958922863007\n",
      "epoch: 9 step: 176, loss is 0.33894026279449463\n",
      "epoch: 9 step: 177, loss is 0.26510605216026306\n",
      "epoch: 9 step: 178, loss is 0.19114483892917633\n",
      "epoch: 9 step: 179, loss is 0.2598457336425781\n",
      "epoch: 9 step: 180, loss is 0.16899916529655457\n",
      "epoch: 9 step: 181, loss is 0.1631910502910614\n",
      "epoch: 9 step: 182, loss is 0.2175561636686325\n",
      "epoch: 9 step: 183, loss is 0.20892737805843353\n",
      "epoch: 9 step: 184, loss is 0.18354304134845734\n",
      "epoch: 9 step: 185, loss is 0.20714826881885529\n",
      "epoch: 9 step: 186, loss is 0.15057896077632904\n",
      "epoch: 9 step: 187, loss is 0.2821846008300781\n",
      "epoch: 9 step: 188, loss is 0.2371833771467209\n",
      "epoch: 9 step: 189, loss is 0.16154015064239502\n",
      "epoch: 9 step: 190, loss is 0.2614458501338959\n",
      "epoch: 9 step: 191, loss is 0.25179749727249146\n",
      "epoch: 9 step: 192, loss is 0.20968501269817352\n",
      "epoch: 9 step: 193, loss is 0.36469680070877075\n",
      "epoch: 9 step: 194, loss is 0.34400680661201477\n",
      "epoch: 9 step: 195, loss is 0.34272176027297974\n",
      "epoch: 9 step: 196, loss is 0.39282578229904175\n",
      "epoch: 9 step: 197, loss is 0.19567589461803436\n",
      "epoch: 9 step: 198, loss is 0.2715052366256714\n",
      "epoch: 9 step: 199, loss is 0.3924310505390167\n",
      "epoch: 9 step: 200, loss is 0.16012074053287506\n",
      "epoch: 9 step: 201, loss is 0.30717960000038147\n",
      "epoch: 9 step: 202, loss is 0.2624158561229706\n",
      "epoch: 9 step: 203, loss is 0.17539435625076294\n",
      "epoch: 9 step: 204, loss is 0.21020115911960602\n",
      "epoch: 9 step: 205, loss is 0.37316325306892395\n",
      "epoch: 9 step: 206, loss is 0.12855468690395355\n",
      "epoch: 9 step: 207, loss is 0.2456551343202591\n",
      "epoch: 9 step: 208, loss is 0.24350804090499878\n",
      "epoch: 9 step: 209, loss is 0.16831201314926147\n",
      "epoch: 9 step: 210, loss is 0.2812160551548004\n",
      "epoch: 9 step: 211, loss is 0.13084164261817932\n",
      "epoch: 9 step: 212, loss is 0.2008269727230072\n",
      "epoch: 9 step: 213, loss is 0.20693692564964294\n",
      "epoch: 9 step: 214, loss is 0.19981960952281952\n",
      "epoch: 9 step: 215, loss is 0.2805231511592865\n",
      "epoch: 9 step: 216, loss is 0.2516840696334839\n",
      "epoch: 9 step: 217, loss is 0.1675621122121811\n",
      "epoch: 9 step: 218, loss is 0.17469972372055054\n",
      "epoch: 9 step: 219, loss is 0.3473961353302002\n",
      "epoch: 9 step: 220, loss is 0.19243520498275757\n",
      "epoch: 9 step: 221, loss is 0.2871539890766144\n",
      "epoch: 9 step: 222, loss is 0.142412468791008\n",
      "epoch: 9 step: 223, loss is 0.19380436837673187\n",
      "epoch: 9 step: 224, loss is 0.44234928488731384\n",
      "epoch: 9 step: 225, loss is 0.1606340855360031\n",
      "epoch: 9 step: 226, loss is 0.42578408122062683\n",
      "epoch: 9 step: 227, loss is 0.267257958650589\n",
      "epoch: 9 step: 228, loss is 0.19031327962875366\n",
      "epoch: 9 step: 229, loss is 0.23682934045791626\n",
      "epoch: 9 step: 230, loss is 0.2380324751138687\n",
      "epoch: 9 step: 231, loss is 0.25087636709213257\n",
      "epoch: 9 step: 232, loss is 0.32262319326400757\n",
      "epoch: 9 step: 233, loss is 0.24775590002536774\n",
      "epoch: 9 step: 234, loss is 0.24075423181056976\n",
      "epoch: 9 step: 235, loss is 0.27620652318000793\n",
      "epoch: 9 step: 236, loss is 0.16926223039627075\n",
      "epoch: 9 step: 237, loss is 0.35128921270370483\n",
      "epoch: 9 step: 238, loss is 0.2685263156890869\n",
      "epoch: 9 step: 239, loss is 0.21295082569122314\n",
      "epoch: 9 step: 240, loss is 0.2829658091068268\n",
      "epoch: 9 step: 241, loss is 0.16647683084011078\n",
      "epoch: 9 step: 242, loss is 0.3322378396987915\n",
      "epoch: 9 step: 243, loss is 0.2500017583370209\n",
      "epoch: 9 step: 244, loss is 0.2203913927078247\n",
      "epoch: 9 step: 245, loss is 0.10698495805263519\n",
      "epoch: 9 step: 246, loss is 0.3062223792076111\n",
      "epoch: 9 step: 247, loss is 0.4275971055030823\n",
      "epoch: 9 step: 248, loss is 0.3478507697582245\n",
      "epoch: 9 step: 249, loss is 0.22012099623680115\n",
      "epoch: 9 step: 250, loss is 0.22494593262672424\n",
      "epoch: 9 step: 251, loss is 0.309813529253006\n",
      "epoch: 9 step: 252, loss is 0.2092336267232895\n",
      "epoch: 9 step: 253, loss is 0.15136735141277313\n",
      "epoch: 9 step: 254, loss is 0.2194880098104477\n",
      "epoch: 9 step: 255, loss is 0.21109412610530853\n",
      "epoch: 9 step: 256, loss is 0.2585780620574951\n",
      "epoch: 9 step: 257, loss is 0.23814129829406738\n",
      "epoch: 9 step: 258, loss is 0.12590132653713226\n",
      "epoch: 9 step: 259, loss is 0.21322903037071228\n",
      "epoch: 9 step: 260, loss is 0.35815802216529846\n",
      "epoch: 9 step: 261, loss is 0.22697438299655914\n",
      "epoch: 9 step: 262, loss is 0.2740694284439087\n",
      "epoch: 9 step: 263, loss is 0.3155488669872284\n",
      "epoch: 9 step: 264, loss is 0.11628986150026321\n",
      "epoch: 9 step: 265, loss is 0.3658545911312103\n",
      "epoch: 9 step: 266, loss is 0.2798404097557068\n",
      "epoch: 9 step: 267, loss is 0.21940335631370544\n",
      "epoch: 9 step: 268, loss is 0.3126465380191803\n",
      "epoch: 9 step: 269, loss is 0.2335645854473114\n",
      "epoch: 9 step: 270, loss is 0.049369439482688904\n",
      "epoch: 9 step: 271, loss is 0.20624783635139465\n",
      "epoch: 9 step: 272, loss is 0.30514729022979736\n",
      "epoch: 9 step: 273, loss is 0.11462698131799698\n",
      "epoch: 9 step: 274, loss is 0.26728567481040955\n",
      "epoch: 9 step: 275, loss is 0.28965628147125244\n",
      "epoch: 9 step: 276, loss is 0.20961299538612366\n",
      "epoch: 9 step: 277, loss is 0.1705617904663086\n",
      "epoch: 9 step: 278, loss is 0.2172395884990692\n",
      "epoch: 9 step: 279, loss is 0.2805374562740326\n",
      "epoch: 9 step: 280, loss is 0.3097737729549408\n",
      "epoch: 9 step: 281, loss is 0.20304657518863678\n",
      "epoch: 9 step: 282, loss is 0.06449735909700394\n",
      "epoch: 9 step: 283, loss is 0.16107890009880066\n",
      "epoch: 9 step: 284, loss is 0.11166587471961975\n",
      "epoch: 9 step: 285, loss is 0.08294295519590378\n",
      "epoch: 9 step: 286, loss is 0.1833268105983734\n",
      "epoch: 9 step: 287, loss is 0.19851750135421753\n",
      "epoch: 9 step: 288, loss is 0.5269824266433716\n",
      "epoch: 9 step: 289, loss is 0.3351131081581116\n",
      "epoch: 9 step: 290, loss is 0.23147481679916382\n",
      "epoch: 9 step: 291, loss is 0.09672607481479645\n",
      "epoch: 9 step: 292, loss is 0.1818477213382721\n",
      "epoch: 9 step: 293, loss is 0.3435838520526886\n",
      "epoch: 9 step: 294, loss is 0.3081626892089844\n",
      "epoch: 9 step: 295, loss is 0.294910728931427\n",
      "epoch: 9 step: 296, loss is 0.25959935784339905\n",
      "epoch: 9 step: 297, loss is 0.15831591188907623\n",
      "epoch: 9 step: 298, loss is 0.23970794677734375\n",
      "epoch: 9 step: 299, loss is 0.28141266107559204\n",
      "epoch: 9 step: 300, loss is 0.3741969168186188\n",
      "epoch: 9 step: 301, loss is 0.38658607006073\n",
      "epoch: 9 step: 302, loss is 0.09374859184026718\n",
      "epoch: 9 step: 303, loss is 0.15050983428955078\n",
      "epoch: 9 step: 304, loss is 0.1936718374490738\n",
      "epoch: 9 step: 305, loss is 0.21948450803756714\n",
      "epoch: 9 step: 306, loss is 0.2358134686946869\n",
      "epoch: 9 step: 307, loss is 0.2560959458351135\n",
      "epoch: 9 step: 308, loss is 0.24232029914855957\n",
      "epoch: 9 step: 309, loss is 0.25016647577285767\n",
      "epoch: 9 step: 310, loss is 0.3276388347148895\n",
      "epoch: 9 step: 311, loss is 0.4220350980758667\n",
      "epoch: 9 step: 312, loss is 0.23301364481449127\n",
      "epoch: 9 step: 313, loss is 0.18712446093559265\n",
      "epoch: 9 step: 314, loss is 0.1657993197441101\n",
      "epoch: 9 step: 315, loss is 0.2777252495288849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 316, loss is 0.29139938950538635\n",
      "epoch: 9 step: 317, loss is 0.27535510063171387\n",
      "epoch: 9 step: 318, loss is 0.135218545794487\n",
      "epoch: 9 step: 319, loss is 0.2196638286113739\n",
      "epoch: 9 step: 320, loss is 0.2718560993671417\n",
      "epoch: 9 step: 321, loss is 0.38959774374961853\n",
      "epoch: 9 step: 322, loss is 0.30796921253204346\n",
      "epoch: 9 step: 323, loss is 0.27074939012527466\n",
      "epoch: 9 step: 324, loss is 0.3317231833934784\n",
      "epoch: 9 step: 325, loss is 0.3304101824760437\n",
      "epoch: 9 step: 326, loss is 0.20538178086280823\n",
      "epoch: 9 step: 327, loss is 0.2045964002609253\n",
      "epoch: 9 step: 328, loss is 0.11660339683294296\n",
      "epoch: 9 step: 329, loss is 0.1702445149421692\n",
      "epoch: 9 step: 330, loss is 0.207632377743721\n",
      "epoch: 9 step: 331, loss is 0.3394051790237427\n",
      "epoch: 9 step: 332, loss is 0.3196006417274475\n",
      "epoch: 9 step: 333, loss is 0.3542557954788208\n",
      "epoch: 9 step: 334, loss is 0.21461039781570435\n",
      "epoch: 9 step: 335, loss is 0.14395642280578613\n",
      "epoch: 9 step: 336, loss is 0.25446245074272156\n",
      "epoch: 9 step: 337, loss is 0.42322564125061035\n",
      "epoch: 9 step: 338, loss is 0.2501388192176819\n",
      "epoch: 9 step: 339, loss is 0.24320261180400848\n",
      "epoch: 9 step: 340, loss is 0.3198489844799042\n",
      "epoch: 9 step: 341, loss is 0.21373797953128815\n",
      "epoch: 9 step: 342, loss is 0.18182122707366943\n",
      "epoch: 9 step: 343, loss is 0.15131527185440063\n",
      "epoch: 9 step: 344, loss is 0.21492157876491547\n",
      "epoch: 9 step: 345, loss is 0.23784297704696655\n",
      "epoch: 9 step: 346, loss is 0.27504777908325195\n",
      "epoch: 9 step: 347, loss is 0.41401126980781555\n",
      "epoch: 9 step: 348, loss is 0.2326078712940216\n",
      "epoch: 9 step: 349, loss is 0.27086156606674194\n",
      "epoch: 9 step: 350, loss is 0.2558634579181671\n",
      "epoch: 9 step: 351, loss is 0.23858202993869781\n",
      "epoch: 9 step: 352, loss is 0.3815717399120331\n",
      "epoch: 9 step: 353, loss is 0.23418951034545898\n",
      "epoch: 9 step: 354, loss is 0.1300085484981537\n",
      "epoch: 9 step: 355, loss is 0.20392467081546783\n",
      "epoch: 9 step: 356, loss is 0.23623807728290558\n",
      "epoch: 9 step: 357, loss is 0.39072149991989136\n",
      "epoch: 9 step: 358, loss is 0.11539647728204727\n",
      "epoch: 9 step: 359, loss is 0.28146880865097046\n",
      "epoch: 9 step: 360, loss is 0.2539081275463104\n",
      "epoch: 9 step: 361, loss is 0.14587700366973877\n",
      "epoch: 9 step: 362, loss is 0.14573781192302704\n",
      "epoch: 9 step: 363, loss is 0.27939116954803467\n",
      "epoch: 9 step: 364, loss is 0.30771228671073914\n",
      "epoch: 9 step: 365, loss is 0.29923564195632935\n",
      "epoch: 9 step: 366, loss is 0.22337079048156738\n",
      "epoch: 9 step: 367, loss is 0.2871306538581848\n",
      "epoch: 9 step: 368, loss is 0.18558336794376373\n",
      "epoch: 9 step: 369, loss is 0.3197978436946869\n",
      "epoch: 9 step: 370, loss is 0.19751006364822388\n",
      "epoch: 9 step: 371, loss is 0.14172367751598358\n",
      "epoch: 9 step: 372, loss is 0.31987252831459045\n",
      "epoch: 9 step: 373, loss is 0.15936428308486938\n",
      "epoch: 9 step: 374, loss is 0.19112372398376465\n",
      "epoch: 9 step: 375, loss is 0.30784523487091064\n",
      "epoch: 9 step: 376, loss is 0.2566927671432495\n",
      "epoch: 9 step: 377, loss is 0.238599494099617\n",
      "epoch: 9 step: 378, loss is 0.3390340209007263\n",
      "epoch: 9 step: 379, loss is 0.20214766263961792\n",
      "epoch: 9 step: 380, loss is 0.33533385396003723\n",
      "epoch: 9 step: 381, loss is 0.28451162576675415\n",
      "epoch: 9 step: 382, loss is 0.21380092203617096\n",
      "epoch: 9 step: 383, loss is 0.22133415937423706\n",
      "epoch: 9 step: 384, loss is 0.39391040802001953\n",
      "epoch: 9 step: 385, loss is 0.16026391088962555\n",
      "epoch: 9 step: 386, loss is 0.33795592188835144\n",
      "epoch: 9 step: 387, loss is 0.2747654318809509\n",
      "epoch: 9 step: 388, loss is 0.29692623019218445\n",
      "epoch: 9 step: 389, loss is 0.2417396903038025\n",
      "epoch: 9 step: 390, loss is 0.2939890921115875\n",
      "epoch: 9 step: 391, loss is 0.2917451858520508\n",
      "epoch: 9 step: 392, loss is 0.2771860659122467\n",
      "epoch: 9 step: 393, loss is 0.20155486464500427\n",
      "epoch: 9 step: 394, loss is 0.34060239791870117\n",
      "epoch: 9 step: 395, loss is 0.10047737509012222\n",
      "epoch: 9 step: 396, loss is 0.249752476811409\n",
      "epoch: 9 step: 397, loss is 0.16136938333511353\n",
      "epoch: 9 step: 398, loss is 0.29339906573295593\n",
      "epoch: 9 step: 399, loss is 0.2434593141078949\n",
      "epoch: 9 step: 400, loss is 0.17560476064682007\n",
      "epoch: 9 step: 401, loss is 0.16097091138362885\n",
      "epoch: 9 step: 402, loss is 0.2555707097053528\n",
      "epoch: 9 step: 403, loss is 0.16937825083732605\n",
      "epoch: 9 step: 404, loss is 0.31269460916519165\n",
      "epoch: 9 step: 405, loss is 0.1779857724905014\n",
      "epoch: 9 step: 406, loss is 0.2031114548444748\n",
      "epoch: 9 step: 407, loss is 0.19303394854068756\n",
      "epoch: 9 step: 408, loss is 0.463357537984848\n",
      "epoch: 9 step: 409, loss is 0.27612757682800293\n",
      "epoch: 9 step: 410, loss is 0.18620121479034424\n",
      "epoch: 9 step: 411, loss is 0.2514193058013916\n",
      "epoch: 9 step: 412, loss is 0.18447037041187286\n",
      "epoch: 9 step: 413, loss is 0.3228587210178375\n",
      "epoch: 9 step: 414, loss is 0.09342558681964874\n",
      "epoch: 9 step: 415, loss is 0.3510308265686035\n",
      "epoch: 9 step: 416, loss is 0.2474508285522461\n",
      "epoch: 9 step: 417, loss is 0.3808717429637909\n",
      "epoch: 9 step: 418, loss is 0.245988667011261\n",
      "epoch: 9 step: 419, loss is 0.26941120624542236\n",
      "epoch: 9 step: 420, loss is 0.36970600485801697\n",
      "epoch: 9 step: 421, loss is 0.310465931892395\n",
      "epoch: 9 step: 422, loss is 0.2914959490299225\n",
      "epoch: 9 step: 423, loss is 0.1309845894575119\n",
      "epoch: 9 step: 424, loss is 0.19550438225269318\n",
      "epoch: 9 step: 425, loss is 0.247419074177742\n",
      "epoch: 9 step: 426, loss is 0.3912239670753479\n",
      "epoch: 9 step: 427, loss is 0.2255730926990509\n",
      "epoch: 9 step: 428, loss is 0.2279330939054489\n",
      "epoch: 9 step: 429, loss is 0.3821043074131012\n",
      "epoch: 9 step: 430, loss is 0.26979172229766846\n",
      "epoch: 9 step: 431, loss is 0.2557918429374695\n",
      "epoch: 9 step: 432, loss is 0.2027624398469925\n",
      "epoch: 9 step: 433, loss is 0.259762167930603\n",
      "epoch: 9 step: 434, loss is 0.18431465327739716\n",
      "epoch: 9 step: 435, loss is 0.23173801600933075\n",
      "epoch: 9 step: 436, loss is 0.2593340575695038\n",
      "epoch: 9 step: 437, loss is 0.24449390172958374\n",
      "epoch: 9 step: 438, loss is 0.29055270552635193\n",
      "epoch: 9 step: 439, loss is 0.2606227695941925\n",
      "epoch: 9 step: 440, loss is 0.19149012863636017\n",
      "epoch: 9 step: 441, loss is 0.20700350403785706\n",
      "epoch: 9 step: 442, loss is 0.35637661814689636\n",
      "epoch: 9 step: 443, loss is 0.2548445463180542\n",
      "epoch: 9 step: 444, loss is 0.2259875237941742\n",
      "epoch: 9 step: 445, loss is 0.17973971366882324\n",
      "epoch: 9 step: 446, loss is 0.21028642356395721\n",
      "epoch: 9 step: 447, loss is 0.20187948644161224\n",
      "epoch: 9 step: 448, loss is 0.18500424921512604\n",
      "epoch: 9 step: 449, loss is 0.3189466893672943\n",
      "epoch: 9 step: 450, loss is 0.15191687643527985\n",
      "epoch: 9 step: 451, loss is 0.24926352500915527\n",
      "epoch: 9 step: 452, loss is 0.2609676420688629\n",
      "epoch: 9 step: 453, loss is 0.37555837631225586\n",
      "epoch: 9 step: 454, loss is 0.30009570717811584\n",
      "epoch: 9 step: 455, loss is 0.1582401543855667\n",
      "epoch: 9 step: 456, loss is 0.3324388563632965\n",
      "epoch: 9 step: 457, loss is 0.25960442423820496\n",
      "epoch: 9 step: 458, loss is 0.15759064257144928\n",
      "epoch: 9 step: 459, loss is 0.16704218089580536\n",
      "epoch: 9 step: 460, loss is 0.28323861956596375\n",
      "epoch: 9 step: 461, loss is 0.49347546696662903\n",
      "epoch: 9 step: 462, loss is 0.23004594445228577\n",
      "epoch: 9 step: 463, loss is 0.24071410298347473\n",
      "epoch: 9 step: 464, loss is 0.17912527918815613\n",
      "epoch: 9 step: 465, loss is 0.23664049804210663\n",
      "epoch: 9 step: 466, loss is 0.22651976346969604\n",
      "epoch: 9 step: 467, loss is 0.059444181621074677\n",
      "epoch: 9 step: 468, loss is 0.1620735079050064\n",
      "epoch: 9 step: 469, loss is 0.15023744106292725\n",
      "epoch: 9 step: 470, loss is 0.21058478951454163\n",
      "epoch: 9 step: 471, loss is 0.29440292716026306\n",
      "epoch: 9 step: 472, loss is 0.1407272219657898\n",
      "epoch: 9 step: 473, loss is 0.21122121810913086\n",
      "epoch: 9 step: 474, loss is 0.2168562412261963\n",
      "epoch: 9 step: 475, loss is 0.3054471015930176\n",
      "epoch: 9 step: 476, loss is 0.2064109593629837\n",
      "epoch: 9 step: 477, loss is 0.2163502424955368\n",
      "epoch: 9 step: 478, loss is 0.29477953910827637\n",
      "epoch: 9 step: 479, loss is 0.17181241512298584\n",
      "epoch: 9 step: 480, loss is 0.1708858162164688\n",
      "epoch: 9 step: 481, loss is 0.36508825421333313\n",
      "epoch: 9 step: 482, loss is 0.185482919216156\n",
      "epoch: 9 step: 483, loss is 0.41885295510292053\n",
      "epoch: 9 step: 484, loss is 0.13520172238349915\n",
      "epoch: 9 step: 485, loss is 0.20876088738441467\n",
      "epoch: 9 step: 486, loss is 0.19047069549560547\n",
      "epoch: 9 step: 487, loss is 0.20828208327293396\n",
      "epoch: 9 step: 488, loss is 0.14168739318847656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 489, loss is 0.3192078471183777\n",
      "epoch: 9 step: 490, loss is 0.303997278213501\n",
      "epoch: 9 step: 491, loss is 0.4537597894668579\n",
      "epoch: 9 step: 492, loss is 0.39608797430992126\n",
      "epoch: 9 step: 493, loss is 0.2191459983587265\n",
      "epoch: 9 step: 494, loss is 0.28333520889282227\n",
      "epoch: 9 step: 495, loss is 0.23544463515281677\n",
      "epoch: 9 step: 496, loss is 0.28385090827941895\n",
      "epoch: 9 step: 497, loss is 0.11860369890928268\n",
      "epoch: 9 step: 498, loss is 0.1679431051015854\n",
      "epoch: 9 step: 499, loss is 0.219611257314682\n",
      "epoch: 9 step: 500, loss is 0.33561116456985474\n",
      "epoch: 9 step: 501, loss is 0.24287386238574982\n",
      "epoch: 9 step: 502, loss is 0.3683142364025116\n",
      "epoch: 9 step: 503, loss is 0.3033781349658966\n",
      "epoch: 9 step: 504, loss is 0.13525600731372833\n",
      "epoch: 9 step: 505, loss is 0.375522643327713\n",
      "epoch: 9 step: 506, loss is 0.5172765851020813\n",
      "epoch: 9 step: 507, loss is 0.17576928436756134\n",
      "epoch: 9 step: 508, loss is 0.5093207955360413\n",
      "epoch: 9 step: 509, loss is 0.28538089990615845\n",
      "epoch: 9 step: 510, loss is 0.1926027536392212\n",
      "epoch: 9 step: 511, loss is 0.1863679736852646\n",
      "epoch: 9 step: 512, loss is 0.36469733715057373\n",
      "epoch: 9 step: 513, loss is 0.1764514297246933\n",
      "epoch: 9 step: 514, loss is 0.22498421370983124\n",
      "epoch: 9 step: 515, loss is 0.3243391513824463\n",
      "epoch: 9 step: 516, loss is 0.3870878517627716\n",
      "epoch: 9 step: 517, loss is 0.20045846700668335\n",
      "epoch: 9 step: 518, loss is 0.26827067136764526\n",
      "epoch: 9 step: 519, loss is 0.2905047535896301\n",
      "epoch: 9 step: 520, loss is 0.17135219275951385\n",
      "epoch: 9 step: 521, loss is 0.40902337431907654\n",
      "epoch: 9 step: 522, loss is 0.18842460215091705\n",
      "epoch: 9 step: 523, loss is 0.10546581447124481\n",
      "epoch: 9 step: 524, loss is 0.1816556751728058\n",
      "epoch: 9 step: 525, loss is 0.15230780839920044\n",
      "epoch: 9 step: 526, loss is 0.3034147024154663\n",
      "epoch: 9 step: 527, loss is 0.18049173057079315\n",
      "epoch: 9 step: 528, loss is 0.17987631261348724\n",
      "epoch: 9 step: 529, loss is 0.2331809103488922\n",
      "epoch: 9 step: 530, loss is 0.4813438951969147\n",
      "epoch: 9 step: 531, loss is 0.22870519757270813\n",
      "epoch: 9 step: 532, loss is 0.17512144148349762\n",
      "epoch: 9 step: 533, loss is 0.34888991713523865\n",
      "epoch: 9 step: 534, loss is 0.24497953057289124\n",
      "epoch: 9 step: 535, loss is 0.21118322014808655\n",
      "epoch: 9 step: 536, loss is 0.20304128527641296\n",
      "epoch: 9 step: 537, loss is 0.2099199742078781\n",
      "epoch: 9 step: 538, loss is 0.30963996052742004\n",
      "epoch: 9 step: 539, loss is 0.12282006442546844\n",
      "epoch: 9 step: 540, loss is 0.15674038231372833\n",
      "epoch: 9 step: 541, loss is 0.11472022533416748\n",
      "epoch: 9 step: 542, loss is 0.1735495924949646\n",
      "epoch: 9 step: 543, loss is 0.14452660083770752\n",
      "epoch: 9 step: 544, loss is 0.1113332062959671\n",
      "epoch: 9 step: 545, loss is 0.43048030138015747\n",
      "epoch: 9 step: 546, loss is 0.19799017906188965\n",
      "epoch: 9 step: 547, loss is 0.17068475484848022\n",
      "epoch: 9 step: 548, loss is 0.42036551237106323\n",
      "epoch: 9 step: 549, loss is 0.28619882464408875\n",
      "epoch: 9 step: 550, loss is 0.12318515032529831\n",
      "epoch: 9 step: 551, loss is 0.17626920342445374\n",
      "epoch: 9 step: 552, loss is 0.2600027918815613\n",
      "epoch: 9 step: 553, loss is 0.2118939757347107\n",
      "epoch: 9 step: 554, loss is 0.2317350059747696\n",
      "epoch: 9 step: 555, loss is 0.1786632388830185\n",
      "epoch: 9 step: 556, loss is 0.3219086825847626\n",
      "epoch: 9 step: 557, loss is 0.27284106612205505\n",
      "epoch: 9 step: 558, loss is 0.18996480107307434\n",
      "epoch: 9 step: 559, loss is 0.13241040706634521\n",
      "epoch: 9 step: 560, loss is 0.3527470529079437\n",
      "epoch: 9 step: 561, loss is 0.3189821243286133\n",
      "epoch: 9 step: 562, loss is 0.24501413106918335\n",
      "epoch: 9 step: 563, loss is 0.22799494862556458\n",
      "epoch: 9 step: 564, loss is 0.2220139503479004\n",
      "epoch: 9 step: 565, loss is 0.2549000382423401\n",
      "epoch: 9 step: 566, loss is 0.16454724967479706\n",
      "epoch: 9 step: 567, loss is 0.27999815344810486\n",
      "epoch: 9 step: 568, loss is 0.31942054629325867\n",
      "epoch: 9 step: 569, loss is 0.28056350350379944\n",
      "epoch: 9 step: 570, loss is 0.14997999370098114\n",
      "epoch: 9 step: 571, loss is 0.22575654089450836\n",
      "epoch: 9 step: 572, loss is 0.27524861693382263\n",
      "epoch: 9 step: 573, loss is 0.24292676150798798\n",
      "epoch: 9 step: 574, loss is 0.38113197684288025\n",
      "epoch: 9 step: 575, loss is 0.23431892693042755\n",
      "epoch: 9 step: 576, loss is 0.3230286240577698\n",
      "epoch: 9 step: 577, loss is 0.22564108669757843\n",
      "epoch: 9 step: 578, loss is 0.33117973804473877\n",
      "epoch: 9 step: 579, loss is 0.24075265228748322\n",
      "epoch: 9 step: 580, loss is 0.32523828744888306\n",
      "epoch: 9 step: 581, loss is 0.3831225335597992\n",
      "epoch: 9 step: 582, loss is 0.1462743580341339\n",
      "epoch: 9 step: 583, loss is 0.22681523859500885\n",
      "epoch: 9 step: 584, loss is 0.21922050416469574\n",
      "epoch: 9 step: 585, loss is 0.1758117377758026\n",
      "epoch: 9 step: 586, loss is 0.23593701422214508\n",
      "epoch: 9 step: 587, loss is 0.22267666459083557\n",
      "epoch: 9 step: 588, loss is 0.19207389652729034\n",
      "epoch: 9 step: 589, loss is 0.33792829513549805\n",
      "epoch: 9 step: 590, loss is 0.13597223162651062\n",
      "epoch: 9 step: 591, loss is 0.18805678188800812\n",
      "epoch: 9 step: 592, loss is 0.2712435722351074\n",
      "epoch: 9 step: 593, loss is 0.26851093769073486\n",
      "epoch: 9 step: 594, loss is 0.16147355735301971\n",
      "epoch: 9 step: 595, loss is 0.25474172830581665\n",
      "epoch: 9 step: 596, loss is 0.1736089140176773\n",
      "epoch: 9 step: 597, loss is 0.3857593238353729\n",
      "epoch: 9 step: 598, loss is 0.08633435517549515\n",
      "epoch: 9 step: 599, loss is 0.11401607096195221\n",
      "epoch: 9 step: 600, loss is 0.263053297996521\n",
      "epoch: 9 step: 601, loss is 0.36400294303894043\n",
      "epoch: 9 step: 602, loss is 0.19950266182422638\n",
      "epoch: 9 step: 603, loss is 0.22079125046730042\n",
      "epoch: 9 step: 604, loss is 0.3479282259941101\n",
      "epoch: 9 step: 605, loss is 0.20661889016628265\n",
      "epoch: 9 step: 606, loss is 0.35437244176864624\n",
      "epoch: 9 step: 607, loss is 0.254196435213089\n",
      "epoch: 9 step: 608, loss is 0.17632418870925903\n",
      "epoch: 9 step: 609, loss is 0.2505492866039276\n",
      "epoch: 9 step: 610, loss is 0.18826033174991608\n",
      "epoch: 9 step: 611, loss is 0.21690866351127625\n",
      "epoch: 9 step: 612, loss is 0.18292786180973053\n",
      "epoch: 9 step: 613, loss is 0.21814483404159546\n",
      "epoch: 9 step: 614, loss is 0.16362148523330688\n",
      "epoch: 9 step: 615, loss is 0.2614052891731262\n",
      "epoch: 9 step: 616, loss is 0.2631932497024536\n",
      "epoch: 9 step: 617, loss is 0.2979312241077423\n",
      "epoch: 9 step: 618, loss is 0.40902966260910034\n",
      "epoch: 9 step: 619, loss is 0.233582004904747\n",
      "epoch: 9 step: 620, loss is 0.35991495847702026\n",
      "epoch: 9 step: 621, loss is 0.2558639645576477\n",
      "epoch: 9 step: 622, loss is 0.22908079624176025\n",
      "epoch: 9 step: 623, loss is 0.2383141964673996\n",
      "epoch: 9 step: 624, loss is 0.15901650488376617\n",
      "epoch: 9 step: 625, loss is 0.17346422374248505\n",
      "epoch: 9 step: 626, loss is 0.16937966644763947\n",
      "epoch: 9 step: 627, loss is 0.09386412054300308\n",
      "epoch: 9 step: 628, loss is 0.21023325622081757\n",
      "epoch: 9 step: 629, loss is 0.09507529437541962\n",
      "epoch: 9 step: 630, loss is 0.23870131373405457\n",
      "epoch: 9 step: 631, loss is 0.36039113998413086\n",
      "epoch: 9 step: 632, loss is 0.251189261674881\n",
      "epoch: 9 step: 633, loss is 0.12853381037712097\n",
      "epoch: 9 step: 634, loss is 0.20103392004966736\n",
      "epoch: 9 step: 635, loss is 0.28014877438545227\n",
      "epoch: 9 step: 636, loss is 0.1940092146396637\n",
      "epoch: 9 step: 637, loss is 0.14323243498802185\n",
      "epoch: 9 step: 638, loss is 0.25847893953323364\n",
      "epoch: 9 step: 639, loss is 0.08559856563806534\n",
      "epoch: 9 step: 640, loss is 0.3781895935535431\n",
      "epoch: 9 step: 641, loss is 0.43701598048210144\n",
      "epoch: 9 step: 642, loss is 0.16341452300548553\n",
      "epoch: 9 step: 643, loss is 0.2551354765892029\n",
      "epoch: 9 step: 644, loss is 0.4850735366344452\n",
      "epoch: 9 step: 645, loss is 0.224745512008667\n",
      "epoch: 9 step: 646, loss is 0.2599651515483856\n",
      "epoch: 9 step: 647, loss is 0.34241241216659546\n",
      "epoch: 9 step: 648, loss is 0.11522550135850906\n",
      "epoch: 9 step: 649, loss is 0.21498210728168488\n",
      "epoch: 9 step: 650, loss is 0.30887556076049805\n",
      "epoch: 9 step: 651, loss is 0.16248662769794464\n",
      "epoch: 9 step: 652, loss is 0.4117737114429474\n",
      "epoch: 9 step: 653, loss is 0.3007684648036957\n",
      "epoch: 9 step: 654, loss is 0.20961512625217438\n",
      "epoch: 9 step: 655, loss is 0.30912163853645325\n",
      "epoch: 9 step: 656, loss is 0.24189060926437378\n",
      "epoch: 9 step: 657, loss is 0.3008272051811218\n",
      "epoch: 9 step: 658, loss is 0.24008701741695404\n",
      "epoch: 9 step: 659, loss is 0.2902393043041229\n",
      "epoch: 9 step: 660, loss is 0.18252497911453247\n",
      "epoch: 9 step: 661, loss is 0.2683465778827667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 662, loss is 0.20520669221878052\n",
      "epoch: 9 step: 663, loss is 0.32153868675231934\n",
      "epoch: 9 step: 664, loss is 0.24024370312690735\n",
      "epoch: 9 step: 665, loss is 0.36948078870773315\n",
      "epoch: 9 step: 666, loss is 0.22225545346736908\n",
      "epoch: 9 step: 667, loss is 0.23596706986427307\n",
      "epoch: 9 step: 668, loss is 0.3285062909126282\n",
      "epoch: 9 step: 669, loss is 0.20377789437770844\n",
      "epoch: 9 step: 670, loss is 0.18480125069618225\n",
      "epoch: 9 step: 671, loss is 0.27260836958885193\n",
      "epoch: 9 step: 672, loss is 0.10981456935405731\n",
      "epoch: 9 step: 673, loss is 0.28774121403694153\n",
      "epoch: 9 step: 674, loss is 0.2549000382423401\n",
      "epoch: 9 step: 675, loss is 0.20620878040790558\n",
      "epoch: 9 step: 676, loss is 0.18951626121997833\n",
      "epoch: 9 step: 677, loss is 0.09809902310371399\n",
      "epoch: 9 step: 678, loss is 0.3048485815525055\n",
      "epoch: 9 step: 679, loss is 0.26001185178756714\n",
      "epoch: 9 step: 680, loss is 0.38615819811820984\n",
      "epoch: 9 step: 681, loss is 0.22473689913749695\n",
      "epoch: 9 step: 682, loss is 0.149276003241539\n",
      "epoch: 9 step: 683, loss is 0.23898793756961823\n",
      "epoch: 9 step: 684, loss is 0.24082385003566742\n",
      "epoch: 9 step: 685, loss is 0.2537723481655121\n",
      "epoch: 9 step: 686, loss is 0.30701324343681335\n",
      "epoch: 9 step: 687, loss is 0.3474215269088745\n",
      "epoch: 9 step: 688, loss is 0.16630879044532776\n",
      "epoch: 9 step: 689, loss is 0.41437023878097534\n",
      "epoch: 9 step: 690, loss is 0.13703785836696625\n",
      "epoch: 9 step: 691, loss is 0.28025734424591064\n",
      "epoch: 9 step: 692, loss is 0.23097120225429535\n",
      "epoch: 9 step: 693, loss is 0.29910966753959656\n",
      "epoch: 9 step: 694, loss is 0.19473719596862793\n",
      "epoch: 9 step: 695, loss is 0.33236590027809143\n",
      "epoch: 9 step: 696, loss is 0.25673502683639526\n",
      "epoch: 9 step: 697, loss is 0.21009595692157745\n",
      "epoch: 9 step: 698, loss is 0.2673967480659485\n",
      "epoch: 9 step: 699, loss is 0.21682171523571014\n",
      "epoch: 9 step: 700, loss is 0.19343289732933044\n",
      "epoch: 9 step: 701, loss is 0.3318677246570587\n",
      "epoch: 9 step: 702, loss is 0.32905611395835876\n",
      "epoch: 9 step: 703, loss is 0.20356231927871704\n",
      "epoch: 9 step: 704, loss is 0.16474822163581848\n",
      "epoch: 9 step: 705, loss is 0.23888453841209412\n",
      "epoch: 9 step: 706, loss is 0.39605265855789185\n",
      "epoch: 9 step: 707, loss is 0.21579639613628387\n",
      "epoch: 9 step: 708, loss is 0.1602741926908493\n",
      "epoch: 9 step: 709, loss is 0.19138331711292267\n",
      "epoch: 9 step: 710, loss is 0.3679356276988983\n",
      "epoch: 9 step: 711, loss is 0.24814169108867645\n",
      "epoch: 9 step: 712, loss is 0.27254804968833923\n",
      "epoch: 9 step: 713, loss is 0.4941791296005249\n",
      "epoch: 9 step: 714, loss is 0.2901191711425781\n",
      "epoch: 9 step: 715, loss is 0.21906761825084686\n",
      "epoch: 9 step: 716, loss is 0.12477327883243561\n",
      "epoch: 9 step: 717, loss is 0.12915334105491638\n",
      "epoch: 9 step: 718, loss is 0.35705146193504333\n",
      "epoch: 9 step: 719, loss is 0.22755810618400574\n",
      "epoch: 9 step: 720, loss is 0.14599962532520294\n",
      "epoch: 9 step: 721, loss is 0.11027854681015015\n",
      "epoch: 9 step: 722, loss is 0.21130239963531494\n",
      "epoch: 9 step: 723, loss is 0.2168528139591217\n",
      "epoch: 9 step: 724, loss is 0.2701232135295868\n",
      "epoch: 9 step: 725, loss is 0.40939807891845703\n",
      "epoch: 9 step: 726, loss is 0.2775079905986786\n",
      "epoch: 9 step: 727, loss is 0.16385518014431\n",
      "epoch: 9 step: 728, loss is 0.4002668261528015\n",
      "epoch: 9 step: 729, loss is 0.33151859045028687\n",
      "epoch: 9 step: 730, loss is 0.3673516511917114\n",
      "epoch: 9 step: 731, loss is 0.1654430478811264\n",
      "epoch: 9 step: 732, loss is 0.08058471232652664\n",
      "epoch: 9 step: 733, loss is 0.24469734728336334\n",
      "epoch: 9 step: 734, loss is 0.5285567045211792\n",
      "epoch: 9 step: 735, loss is 0.2946866452693939\n",
      "epoch: 9 step: 736, loss is 0.42415156960487366\n",
      "epoch: 9 step: 737, loss is 0.381197065114975\n",
      "epoch: 9 step: 738, loss is 0.2613258361816406\n",
      "epoch: 9 step: 739, loss is 0.18202610313892365\n",
      "epoch: 9 step: 740, loss is 0.44275328516960144\n",
      "epoch: 9 step: 741, loss is 0.13910144567489624\n",
      "epoch: 9 step: 742, loss is 0.4041900336742401\n",
      "epoch: 9 step: 743, loss is 0.2727668583393097\n",
      "epoch: 9 step: 744, loss is 0.17962439358234406\n",
      "epoch: 9 step: 745, loss is 0.36712589859962463\n",
      "epoch: 9 step: 746, loss is 0.3195800483226776\n",
      "epoch: 9 step: 747, loss is 0.23301976919174194\n",
      "epoch: 9 step: 748, loss is 0.3131776750087738\n",
      "epoch: 9 step: 749, loss is 0.21714001893997192\n",
      "epoch: 9 step: 750, loss is 0.25277355313301086\n",
      "epoch: 9 step: 751, loss is 0.22095675766468048\n",
      "epoch: 9 step: 752, loss is 0.1777874082326889\n",
      "epoch: 9 step: 753, loss is 0.3598880469799042\n",
      "epoch: 9 step: 754, loss is 0.1397809535264969\n",
      "epoch: 9 step: 755, loss is 0.18816564977169037\n",
      "epoch: 9 step: 756, loss is 0.17988504469394684\n",
      "epoch: 9 step: 757, loss is 0.11524681001901627\n",
      "epoch: 9 step: 758, loss is 0.2581373453140259\n",
      "epoch: 9 step: 759, loss is 0.29105710983276367\n",
      "epoch: 9 step: 760, loss is 0.2859257459640503\n",
      "epoch: 9 step: 761, loss is 0.2205556035041809\n",
      "epoch: 9 step: 762, loss is 0.19614994525909424\n",
      "epoch: 9 step: 763, loss is 0.25383642315864563\n",
      "epoch: 9 step: 764, loss is 0.2822287678718567\n",
      "epoch: 9 step: 765, loss is 0.22519458830356598\n",
      "epoch: 9 step: 766, loss is 0.22599796950817108\n",
      "epoch: 9 step: 767, loss is 0.1698407679796219\n",
      "epoch: 9 step: 768, loss is 0.4074196517467499\n",
      "epoch: 9 step: 769, loss is 0.14664003252983093\n",
      "epoch: 9 step: 770, loss is 0.3319099247455597\n",
      "epoch: 9 step: 771, loss is 0.2233705371618271\n",
      "epoch: 9 step: 772, loss is 0.2265794575214386\n",
      "epoch: 9 step: 773, loss is 0.17932240664958954\n",
      "epoch: 9 step: 774, loss is 0.24683676660060883\n",
      "epoch: 9 step: 775, loss is 0.22639864683151245\n",
      "epoch: 9 step: 776, loss is 0.1960241198539734\n",
      "epoch: 9 step: 777, loss is 0.09393159300088882\n",
      "epoch: 9 step: 778, loss is 0.20176668465137482\n",
      "epoch: 9 step: 779, loss is 0.3257928192615509\n",
      "epoch: 9 step: 780, loss is 0.14289571344852448\n",
      "epoch: 9 step: 781, loss is 0.11863301694393158\n",
      "epoch: 9 step: 782, loss is 0.24804997444152832\n",
      "epoch: 9 step: 783, loss is 0.2677779495716095\n",
      "epoch: 9 step: 784, loss is 0.1916780322790146\n",
      "epoch: 9 step: 785, loss is 0.12446283549070358\n",
      "epoch: 9 step: 786, loss is 0.3170696198940277\n",
      "epoch: 9 step: 787, loss is 0.21724197268486023\n",
      "epoch: 9 step: 788, loss is 0.22428502142429352\n",
      "epoch: 9 step: 789, loss is 0.28513050079345703\n",
      "epoch: 9 step: 790, loss is 0.43550604581832886\n",
      "epoch: 9 step: 791, loss is 0.21878667175769806\n",
      "epoch: 9 step: 792, loss is 0.18632882833480835\n",
      "epoch: 9 step: 793, loss is 0.17838267982006073\n",
      "epoch: 9 step: 794, loss is 0.21078850328922272\n",
      "epoch: 9 step: 795, loss is 0.1036798357963562\n",
      "epoch: 9 step: 796, loss is 0.221490740776062\n",
      "epoch: 9 step: 797, loss is 0.34002888202667236\n",
      "epoch: 9 step: 798, loss is 0.4523698091506958\n",
      "epoch: 9 step: 799, loss is 0.29904258251190186\n",
      "epoch: 9 step: 800, loss is 0.12914317846298218\n",
      "epoch: 9 step: 801, loss is 0.4387054443359375\n",
      "epoch: 9 step: 802, loss is 0.37034526467323303\n",
      "epoch: 9 step: 803, loss is 0.27512526512145996\n",
      "epoch: 9 step: 804, loss is 0.2892954647541046\n",
      "epoch: 9 step: 805, loss is 0.18794658780097961\n",
      "epoch: 9 step: 806, loss is 0.30075928568840027\n",
      "epoch: 9 step: 807, loss is 0.32358241081237793\n",
      "epoch: 9 step: 808, loss is 0.13193921744823456\n",
      "epoch: 9 step: 809, loss is 0.3536635935306549\n",
      "epoch: 9 step: 810, loss is 0.18036550283432007\n",
      "epoch: 9 step: 811, loss is 0.3259629011154175\n",
      "epoch: 9 step: 812, loss is 0.33017438650131226\n",
      "epoch: 9 step: 813, loss is 0.283414363861084\n",
      "epoch: 9 step: 814, loss is 0.1443534642457962\n",
      "epoch: 9 step: 815, loss is 0.14833225309848785\n",
      "epoch: 9 step: 816, loss is 0.19152429699897766\n",
      "epoch: 9 step: 817, loss is 0.2843647599220276\n",
      "epoch: 9 step: 818, loss is 0.2291945517063141\n",
      "epoch: 9 step: 819, loss is 0.18961836397647858\n",
      "epoch: 9 step: 820, loss is 0.24383069574832916\n",
      "epoch: 9 step: 821, loss is 0.13677093386650085\n",
      "epoch: 9 step: 822, loss is 0.19583344459533691\n",
      "epoch: 9 step: 823, loss is 0.3125813603401184\n",
      "epoch: 9 step: 824, loss is 0.1614837795495987\n",
      "epoch: 9 step: 825, loss is 0.2439449280500412\n",
      "epoch: 9 step: 826, loss is 0.2508213222026825\n",
      "epoch: 9 step: 827, loss is 0.2510741651058197\n",
      "epoch: 9 step: 828, loss is 0.1939426064491272\n",
      "epoch: 9 step: 829, loss is 0.3455037772655487\n",
      "epoch: 9 step: 830, loss is 0.22186751663684845\n",
      "epoch: 9 step: 831, loss is 0.23347041010856628\n",
      "epoch: 9 step: 832, loss is 0.29256483912467957\n",
      "epoch: 9 step: 833, loss is 0.2102532535791397\n",
      "epoch: 9 step: 834, loss is 0.2644263505935669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 835, loss is 0.3513236343860626\n",
      "epoch: 9 step: 836, loss is 0.2857147753238678\n",
      "epoch: 9 step: 837, loss is 0.2664714753627777\n",
      "epoch: 9 step: 838, loss is 0.14050692319869995\n",
      "epoch: 9 step: 839, loss is 0.12721717357635498\n",
      "epoch: 9 step: 840, loss is 0.15199483931064606\n",
      "epoch: 9 step: 841, loss is 0.33360961079597473\n",
      "epoch: 9 step: 842, loss is 0.29177048802375793\n",
      "epoch: 9 step: 843, loss is 0.43748709559440613\n",
      "epoch: 9 step: 844, loss is 0.4104095697402954\n",
      "epoch: 9 step: 845, loss is 0.23897099494934082\n",
      "epoch: 9 step: 846, loss is 0.27677008509635925\n",
      "epoch: 9 step: 847, loss is 0.28598615527153015\n",
      "epoch: 9 step: 848, loss is 0.1462894082069397\n",
      "epoch: 9 step: 849, loss is 0.2137603610754013\n",
      "epoch: 9 step: 850, loss is 0.3018180727958679\n",
      "epoch: 9 step: 851, loss is 0.26346439123153687\n",
      "epoch: 9 step: 852, loss is 0.11069026589393616\n",
      "epoch: 9 step: 853, loss is 0.1361929178237915\n",
      "epoch: 9 step: 854, loss is 0.17561492323875427\n",
      "epoch: 9 step: 855, loss is 0.18259482085704803\n",
      "epoch: 9 step: 856, loss is 0.250485360622406\n",
      "epoch: 9 step: 857, loss is 0.4600805640220642\n",
      "epoch: 9 step: 858, loss is 0.3829766809940338\n",
      "epoch: 9 step: 859, loss is 0.2416275590658188\n",
      "epoch: 9 step: 860, loss is 0.17779196798801422\n",
      "epoch: 9 step: 861, loss is 0.2252023071050644\n",
      "epoch: 9 step: 862, loss is 0.2240152508020401\n",
      "epoch: 9 step: 863, loss is 0.41174376010894775\n",
      "epoch: 9 step: 864, loss is 0.16290591657161713\n",
      "epoch: 9 step: 865, loss is 0.2994009852409363\n",
      "epoch: 9 step: 866, loss is 0.22257906198501587\n",
      "epoch: 9 step: 867, loss is 0.21039259433746338\n",
      "epoch: 9 step: 868, loss is 0.23079076409339905\n",
      "epoch: 9 step: 869, loss is 0.26302796602249146\n",
      "epoch: 9 step: 870, loss is 0.250984787940979\n",
      "epoch: 9 step: 871, loss is 0.17666083574295044\n",
      "epoch: 9 step: 872, loss is 0.10574278980493546\n",
      "epoch: 9 step: 873, loss is 0.20234842598438263\n",
      "epoch: 9 step: 874, loss is 0.15075801312923431\n",
      "epoch: 9 step: 875, loss is 0.3813435435295105\n",
      "epoch: 9 step: 876, loss is 0.12278599292039871\n",
      "epoch: 9 step: 877, loss is 0.2220156490802765\n",
      "epoch: 9 step: 878, loss is 0.2699984014034271\n",
      "epoch: 9 step: 879, loss is 0.23780710995197296\n",
      "epoch: 9 step: 880, loss is 0.13025537133216858\n",
      "epoch: 9 step: 881, loss is 0.18045629560947418\n",
      "epoch: 9 step: 882, loss is 0.14052505791187286\n",
      "epoch: 9 step: 883, loss is 0.22972719371318817\n",
      "epoch: 9 step: 884, loss is 0.5238398313522339\n",
      "epoch: 9 step: 885, loss is 0.2767586410045624\n",
      "epoch: 9 step: 886, loss is 0.2618359327316284\n",
      "epoch: 9 step: 887, loss is 0.3698885440826416\n",
      "epoch: 9 step: 888, loss is 0.36278945207595825\n",
      "epoch: 9 step: 889, loss is 0.2774438261985779\n",
      "epoch: 9 step: 890, loss is 0.19016438722610474\n",
      "epoch: 9 step: 891, loss is 0.20079028606414795\n",
      "epoch: 9 step: 892, loss is 0.21657700836658478\n",
      "epoch: 9 step: 893, loss is 0.18579083681106567\n",
      "epoch: 9 step: 894, loss is 0.27544671297073364\n",
      "epoch: 9 step: 895, loss is 0.30505454540252686\n",
      "epoch: 9 step: 896, loss is 0.21590691804885864\n",
      "epoch: 9 step: 897, loss is 0.25138112902641296\n",
      "epoch: 9 step: 898, loss is 0.16167308390140533\n",
      "epoch: 9 step: 899, loss is 0.23068782687187195\n",
      "epoch: 9 step: 900, loss is 0.18237589299678802\n",
      "epoch: 9 step: 901, loss is 0.19268837571144104\n",
      "epoch: 9 step: 902, loss is 0.32787591218948364\n",
      "epoch: 9 step: 903, loss is 0.2763918340206146\n",
      "epoch: 9 step: 904, loss is 0.1785605251789093\n",
      "epoch: 9 step: 905, loss is 0.18414926528930664\n",
      "epoch: 9 step: 906, loss is 0.21354155242443085\n",
      "epoch: 9 step: 907, loss is 0.2256241887807846\n",
      "epoch: 9 step: 908, loss is 0.20908640325069427\n",
      "epoch: 9 step: 909, loss is 0.2486303448677063\n",
      "epoch: 9 step: 910, loss is 0.32836201786994934\n",
      "epoch: 9 step: 911, loss is 0.3319123089313507\n",
      "epoch: 9 step: 912, loss is 0.271342396736145\n",
      "epoch: 9 step: 913, loss is 0.2519514858722687\n",
      "epoch: 9 step: 914, loss is 0.10279318690299988\n",
      "epoch: 9 step: 915, loss is 0.3665268123149872\n",
      "epoch: 9 step: 916, loss is 0.26622241735458374\n",
      "epoch: 9 step: 917, loss is 0.284084290266037\n",
      "epoch: 9 step: 918, loss is 0.450883686542511\n",
      "epoch: 9 step: 919, loss is 0.33349093794822693\n",
      "epoch: 9 step: 920, loss is 0.24809885025024414\n",
      "epoch: 9 step: 921, loss is 0.433540940284729\n",
      "epoch: 9 step: 922, loss is 0.48489323258399963\n",
      "epoch: 9 step: 923, loss is 0.1597261279821396\n",
      "epoch: 9 step: 924, loss is 0.3030971884727478\n",
      "epoch: 9 step: 925, loss is 0.25390955805778503\n",
      "epoch: 9 step: 926, loss is 0.3737289011478424\n",
      "epoch: 9 step: 927, loss is 0.24791733920574188\n",
      "epoch: 9 step: 928, loss is 0.14954625070095062\n",
      "epoch: 9 step: 929, loss is 0.22149460017681122\n",
      "epoch: 9 step: 930, loss is 0.1656610071659088\n",
      "epoch: 9 step: 931, loss is 0.5080524682998657\n",
      "epoch: 9 step: 932, loss is 0.3436714708805084\n",
      "epoch: 9 step: 933, loss is 0.1994415819644928\n",
      "epoch: 9 step: 934, loss is 0.3185853064060211\n",
      "epoch: 9 step: 935, loss is 0.27919405698776245\n",
      "epoch: 9 step: 936, loss is 0.29717040061950684\n",
      "epoch: 9 step: 937, loss is 0.23189334571361542\n",
      "epoch: 10 step: 1, loss is 0.2812158465385437\n",
      "epoch: 10 step: 2, loss is 0.2639046609401703\n",
      "epoch: 10 step: 3, loss is 0.17973268032073975\n",
      "epoch: 10 step: 4, loss is 0.3571934103965759\n",
      "epoch: 10 step: 5, loss is 0.27175799012184143\n",
      "epoch: 10 step: 6, loss is 0.29952096939086914\n",
      "epoch: 10 step: 7, loss is 0.27952075004577637\n",
      "epoch: 10 step: 8, loss is 0.2329511195421219\n",
      "epoch: 10 step: 9, loss is 0.2664642035961151\n",
      "epoch: 10 step: 10, loss is 0.1477022022008896\n",
      "epoch: 10 step: 11, loss is 0.20361511409282684\n",
      "epoch: 10 step: 12, loss is 0.2615988850593567\n",
      "epoch: 10 step: 13, loss is 0.2670349180698395\n",
      "epoch: 10 step: 14, loss is 0.20148305594921112\n",
      "epoch: 10 step: 15, loss is 0.24717222154140472\n",
      "epoch: 10 step: 16, loss is 0.22036829590797424\n",
      "epoch: 10 step: 17, loss is 0.14929041266441345\n",
      "epoch: 10 step: 18, loss is 0.30664345622062683\n",
      "epoch: 10 step: 19, loss is 0.3972626030445099\n",
      "epoch: 10 step: 20, loss is 0.2697073817253113\n",
      "epoch: 10 step: 21, loss is 0.27416983246803284\n",
      "epoch: 10 step: 22, loss is 0.2597309947013855\n",
      "epoch: 10 step: 23, loss is 0.3081035017967224\n",
      "epoch: 10 step: 24, loss is 0.27602481842041016\n",
      "epoch: 10 step: 25, loss is 0.08854316920042038\n",
      "epoch: 10 step: 26, loss is 0.4444639980792999\n",
      "epoch: 10 step: 27, loss is 0.3048887848854065\n",
      "epoch: 10 step: 28, loss is 0.18756379187107086\n",
      "epoch: 10 step: 29, loss is 0.13917206227779388\n",
      "epoch: 10 step: 30, loss is 0.18003003299236298\n",
      "epoch: 10 step: 31, loss is 0.10328087955713272\n",
      "epoch: 10 step: 32, loss is 0.18707893788814545\n",
      "epoch: 10 step: 33, loss is 0.2280033677816391\n",
      "epoch: 10 step: 34, loss is 0.16592557728290558\n",
      "epoch: 10 step: 35, loss is 0.2997550368309021\n",
      "epoch: 10 step: 36, loss is 0.21343623101711273\n",
      "epoch: 10 step: 37, loss is 0.18679529428482056\n",
      "epoch: 10 step: 38, loss is 0.3145018219947815\n",
      "epoch: 10 step: 39, loss is 0.2593923807144165\n",
      "epoch: 10 step: 40, loss is 0.23678334057331085\n",
      "epoch: 10 step: 41, loss is 0.21988873183727264\n",
      "epoch: 10 step: 42, loss is 0.414280503988266\n",
      "epoch: 10 step: 43, loss is 0.3115854263305664\n",
      "epoch: 10 step: 44, loss is 0.22515030205249786\n",
      "epoch: 10 step: 45, loss is 0.26121973991394043\n",
      "epoch: 10 step: 46, loss is 0.4529419541358948\n",
      "epoch: 10 step: 47, loss is 0.17629565298557281\n",
      "epoch: 10 step: 48, loss is 0.19740243256092072\n",
      "epoch: 10 step: 49, loss is 0.2940806448459625\n",
      "epoch: 10 step: 50, loss is 0.3250339925289154\n",
      "epoch: 10 step: 51, loss is 0.532060444355011\n",
      "epoch: 10 step: 52, loss is 0.3092680871486664\n",
      "epoch: 10 step: 53, loss is 0.33428049087524414\n",
      "epoch: 10 step: 54, loss is 0.4650033712387085\n",
      "epoch: 10 step: 55, loss is 0.1334676444530487\n",
      "epoch: 10 step: 56, loss is 0.3721020519733429\n",
      "epoch: 10 step: 57, loss is 0.197503462433815\n",
      "epoch: 10 step: 58, loss is 0.2425096482038498\n",
      "epoch: 10 step: 59, loss is 0.126919224858284\n",
      "epoch: 10 step: 60, loss is 0.18660378456115723\n",
      "epoch: 10 step: 61, loss is 0.1502123475074768\n",
      "epoch: 10 step: 62, loss is 0.3332429826259613\n",
      "epoch: 10 step: 63, loss is 0.2611371576786041\n",
      "epoch: 10 step: 64, loss is 0.35042765736579895\n",
      "epoch: 10 step: 65, loss is 0.22936469316482544\n",
      "epoch: 10 step: 66, loss is 0.1702888011932373\n",
      "epoch: 10 step: 67, loss is 0.3486610949039459\n",
      "epoch: 10 step: 68, loss is 0.21996313333511353\n",
      "epoch: 10 step: 69, loss is 0.3725634813308716\n",
      "epoch: 10 step: 70, loss is 0.21809306740760803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 71, loss is 0.25063973665237427\n",
      "epoch: 10 step: 72, loss is 0.24296002089977264\n",
      "epoch: 10 step: 73, loss is 0.10300888121128082\n",
      "epoch: 10 step: 74, loss is 0.3207348585128784\n",
      "epoch: 10 step: 75, loss is 0.33996856212615967\n",
      "epoch: 10 step: 76, loss is 0.17150309681892395\n",
      "epoch: 10 step: 77, loss is 0.07491655647754669\n",
      "epoch: 10 step: 78, loss is 0.22631169855594635\n",
      "epoch: 10 step: 79, loss is 0.3942926824092865\n",
      "epoch: 10 step: 80, loss is 0.14336010813713074\n",
      "epoch: 10 step: 81, loss is 0.16827519237995148\n",
      "epoch: 10 step: 82, loss is 0.2909912168979645\n",
      "epoch: 10 step: 83, loss is 0.3243221044540405\n",
      "epoch: 10 step: 84, loss is 0.21841071546077728\n",
      "epoch: 10 step: 85, loss is 0.11570320278406143\n",
      "epoch: 10 step: 86, loss is 0.4693261682987213\n",
      "epoch: 10 step: 87, loss is 0.15519529581069946\n",
      "epoch: 10 step: 88, loss is 0.32569149136543274\n",
      "epoch: 10 step: 89, loss is 0.22993876039981842\n",
      "epoch: 10 step: 90, loss is 0.19193758070468903\n",
      "epoch: 10 step: 91, loss is 0.16877666115760803\n",
      "epoch: 10 step: 92, loss is 0.12567855417728424\n",
      "epoch: 10 step: 93, loss is 0.1858147233724594\n",
      "epoch: 10 step: 94, loss is 0.23323854804039001\n",
      "epoch: 10 step: 95, loss is 0.1487770974636078\n",
      "epoch: 10 step: 96, loss is 0.30984941124916077\n",
      "epoch: 10 step: 97, loss is 0.43216854333877563\n",
      "epoch: 10 step: 98, loss is 0.2259611189365387\n",
      "epoch: 10 step: 99, loss is 0.3563201129436493\n",
      "epoch: 10 step: 100, loss is 0.2547767460346222\n",
      "epoch: 10 step: 101, loss is 0.3323502242565155\n",
      "epoch: 10 step: 102, loss is 0.19632168114185333\n",
      "epoch: 10 step: 103, loss is 0.22031627595424652\n",
      "epoch: 10 step: 104, loss is 0.33625468611717224\n",
      "epoch: 10 step: 105, loss is 0.23861168324947357\n",
      "epoch: 10 step: 106, loss is 0.22435392439365387\n",
      "epoch: 10 step: 107, loss is 0.2025068700313568\n",
      "epoch: 10 step: 108, loss is 0.18393462896347046\n",
      "epoch: 10 step: 109, loss is 0.19438929855823517\n",
      "epoch: 10 step: 110, loss is 0.1414014846086502\n",
      "epoch: 10 step: 111, loss is 0.30930083990097046\n",
      "epoch: 10 step: 112, loss is 0.3043210506439209\n",
      "epoch: 10 step: 113, loss is 0.20370100438594818\n",
      "epoch: 10 step: 114, loss is 0.2661549150943756\n",
      "epoch: 10 step: 115, loss is 0.17024335265159607\n",
      "epoch: 10 step: 116, loss is 0.2818085253238678\n",
      "epoch: 10 step: 117, loss is 0.3073691725730896\n",
      "epoch: 10 step: 118, loss is 0.3393098711967468\n",
      "epoch: 10 step: 119, loss is 0.19898656010627747\n",
      "epoch: 10 step: 120, loss is 0.25040778517723083\n",
      "epoch: 10 step: 121, loss is 0.17476589977741241\n",
      "epoch: 10 step: 122, loss is 0.3420529365539551\n",
      "epoch: 10 step: 123, loss is 0.20817828178405762\n",
      "epoch: 10 step: 124, loss is 0.1794600933790207\n",
      "epoch: 10 step: 125, loss is 0.30921244621276855\n",
      "epoch: 10 step: 126, loss is 0.2433435022830963\n",
      "epoch: 10 step: 127, loss is 0.08554418385028839\n",
      "epoch: 10 step: 128, loss is 0.32875731587409973\n",
      "epoch: 10 step: 129, loss is 0.28247717022895813\n",
      "epoch: 10 step: 130, loss is 0.24528662860393524\n",
      "epoch: 10 step: 131, loss is 0.18863411247730255\n",
      "epoch: 10 step: 132, loss is 0.20680421590805054\n",
      "epoch: 10 step: 133, loss is 0.38361233472824097\n",
      "epoch: 10 step: 134, loss is 0.18413934111595154\n",
      "epoch: 10 step: 135, loss is 0.19646233320236206\n",
      "epoch: 10 step: 136, loss is 0.23850499093532562\n",
      "epoch: 10 step: 137, loss is 0.31940957903862\n",
      "epoch: 10 step: 138, loss is 0.153758242726326\n",
      "epoch: 10 step: 139, loss is 0.2619045078754425\n",
      "epoch: 10 step: 140, loss is 0.2272731065750122\n",
      "epoch: 10 step: 141, loss is 0.11321552097797394\n",
      "epoch: 10 step: 142, loss is 0.22762341797351837\n",
      "epoch: 10 step: 143, loss is 0.19373509287834167\n",
      "epoch: 10 step: 144, loss is 0.139080211520195\n",
      "epoch: 10 step: 145, loss is 0.1546548455953598\n",
      "epoch: 10 step: 146, loss is 0.1034010648727417\n",
      "epoch: 10 step: 147, loss is 0.25932949781417847\n",
      "epoch: 10 step: 148, loss is 0.1135554164648056\n",
      "epoch: 10 step: 149, loss is 0.1516191065311432\n",
      "epoch: 10 step: 150, loss is 0.15391835570335388\n",
      "epoch: 10 step: 151, loss is 0.2597205638885498\n",
      "epoch: 10 step: 152, loss is 0.26327595114707947\n",
      "epoch: 10 step: 153, loss is 0.3394003212451935\n",
      "epoch: 10 step: 154, loss is 0.11561675369739532\n",
      "epoch: 10 step: 155, loss is 0.4179030656814575\n",
      "epoch: 10 step: 156, loss is 0.08743727207183838\n",
      "epoch: 10 step: 157, loss is 0.2601131200790405\n",
      "epoch: 10 step: 158, loss is 0.11457463353872299\n",
      "epoch: 10 step: 159, loss is 0.3197768032550812\n",
      "epoch: 10 step: 160, loss is 0.17644518613815308\n",
      "epoch: 10 step: 161, loss is 0.30217114090919495\n",
      "epoch: 10 step: 162, loss is 0.3814166784286499\n",
      "epoch: 10 step: 163, loss is 0.3552626073360443\n",
      "epoch: 10 step: 164, loss is 0.17977958917617798\n",
      "epoch: 10 step: 165, loss is 0.20179855823516846\n",
      "epoch: 10 step: 166, loss is 0.32026591897010803\n",
      "epoch: 10 step: 167, loss is 0.15948228538036346\n",
      "epoch: 10 step: 168, loss is 0.1765502691268921\n",
      "epoch: 10 step: 169, loss is 0.3567325472831726\n",
      "epoch: 10 step: 170, loss is 0.2884664833545685\n",
      "epoch: 10 step: 171, loss is 0.20404520630836487\n",
      "epoch: 10 step: 172, loss is 0.18673503398895264\n",
      "epoch: 10 step: 173, loss is 0.17479810118675232\n",
      "epoch: 10 step: 174, loss is 0.2357136607170105\n",
      "epoch: 10 step: 175, loss is 0.20110858976840973\n",
      "epoch: 10 step: 176, loss is 0.2025655210018158\n",
      "epoch: 10 step: 177, loss is 0.3751718997955322\n",
      "epoch: 10 step: 178, loss is 0.27657395601272583\n",
      "epoch: 10 step: 179, loss is 0.2613632082939148\n",
      "epoch: 10 step: 180, loss is 0.22877955436706543\n",
      "epoch: 10 step: 181, loss is 0.23959283530712128\n",
      "epoch: 10 step: 182, loss is 0.262779176235199\n",
      "epoch: 10 step: 183, loss is 0.21756356954574585\n",
      "epoch: 10 step: 184, loss is 0.3253120481967926\n",
      "epoch: 10 step: 185, loss is 0.2038760930299759\n",
      "epoch: 10 step: 186, loss is 0.27139246463775635\n",
      "epoch: 10 step: 187, loss is 0.2848558723926544\n",
      "epoch: 10 step: 188, loss is 0.16790418326854706\n",
      "epoch: 10 step: 189, loss is 0.16011396050453186\n",
      "epoch: 10 step: 190, loss is 0.2969319522380829\n",
      "epoch: 10 step: 191, loss is 0.13993366062641144\n",
      "epoch: 10 step: 192, loss is 0.31421446800231934\n",
      "epoch: 10 step: 193, loss is 0.3112668991088867\n",
      "epoch: 10 step: 194, loss is 0.1193397268652916\n",
      "epoch: 10 step: 195, loss is 0.25658881664276123\n",
      "epoch: 10 step: 196, loss is 0.14077164232730865\n",
      "epoch: 10 step: 197, loss is 0.2391459345817566\n",
      "epoch: 10 step: 198, loss is 0.3236929476261139\n",
      "epoch: 10 step: 199, loss is 0.1702670305967331\n",
      "epoch: 10 step: 200, loss is 0.20911408960819244\n",
      "epoch: 10 step: 201, loss is 0.23895776271820068\n",
      "epoch: 10 step: 202, loss is 0.22592346370220184\n",
      "epoch: 10 step: 203, loss is 0.1423342525959015\n",
      "epoch: 10 step: 204, loss is 0.23958975076675415\n",
      "epoch: 10 step: 205, loss is 0.1163662001490593\n",
      "epoch: 10 step: 206, loss is 0.11351490020751953\n",
      "epoch: 10 step: 207, loss is 0.30733832716941833\n",
      "epoch: 10 step: 208, loss is 0.15207870304584503\n",
      "epoch: 10 step: 209, loss is 0.1998234987258911\n",
      "epoch: 10 step: 210, loss is 0.23430508375167847\n",
      "epoch: 10 step: 211, loss is 0.2244519591331482\n",
      "epoch: 10 step: 212, loss is 0.2811908423900604\n",
      "epoch: 10 step: 213, loss is 0.17217402160167694\n",
      "epoch: 10 step: 214, loss is 0.10841144621372223\n",
      "epoch: 10 step: 215, loss is 0.16093850135803223\n",
      "epoch: 10 step: 216, loss is 0.26596909761428833\n",
      "epoch: 10 step: 217, loss is 0.12038447707891464\n",
      "epoch: 10 step: 218, loss is 0.25553813576698303\n",
      "epoch: 10 step: 219, loss is 0.08939693868160248\n",
      "epoch: 10 step: 220, loss is 0.2061162292957306\n",
      "epoch: 10 step: 221, loss is 0.32161930203437805\n",
      "epoch: 10 step: 222, loss is 0.20007044076919556\n",
      "epoch: 10 step: 223, loss is 0.35443076491355896\n",
      "epoch: 10 step: 224, loss is 0.3020670413970947\n",
      "epoch: 10 step: 225, loss is 0.17874261736869812\n",
      "epoch: 10 step: 226, loss is 0.18966785073280334\n",
      "epoch: 10 step: 227, loss is 0.21443036198616028\n",
      "epoch: 10 step: 228, loss is 0.27876025438308716\n",
      "epoch: 10 step: 229, loss is 0.1562589406967163\n",
      "epoch: 10 step: 230, loss is 0.2563018202781677\n",
      "epoch: 10 step: 231, loss is 0.16717742383480072\n",
      "epoch: 10 step: 232, loss is 0.4383445382118225\n",
      "epoch: 10 step: 233, loss is 0.1437956988811493\n",
      "epoch: 10 step: 234, loss is 0.1406208574771881\n",
      "epoch: 10 step: 235, loss is 0.09468287974596024\n",
      "epoch: 10 step: 236, loss is 0.12056640535593033\n",
      "epoch: 10 step: 237, loss is 0.1068778857588768\n",
      "epoch: 10 step: 238, loss is 0.2447969764471054\n",
      "epoch: 10 step: 239, loss is 0.3443455100059509\n",
      "epoch: 10 step: 240, loss is 0.16522440314292908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 241, loss is 0.3244207501411438\n",
      "epoch: 10 step: 242, loss is 0.25061866641044617\n",
      "epoch: 10 step: 243, loss is 0.29347342252731323\n",
      "epoch: 10 step: 244, loss is 0.32425442337989807\n",
      "epoch: 10 step: 245, loss is 0.2255689948797226\n",
      "epoch: 10 step: 246, loss is 0.2709229588508606\n",
      "epoch: 10 step: 247, loss is 0.23205134272575378\n",
      "epoch: 10 step: 248, loss is 0.14276346564292908\n",
      "epoch: 10 step: 249, loss is 0.3968142867088318\n",
      "epoch: 10 step: 250, loss is 0.21134060621261597\n",
      "epoch: 10 step: 251, loss is 0.2618772089481354\n",
      "epoch: 10 step: 252, loss is 0.16801699995994568\n",
      "epoch: 10 step: 253, loss is 0.15831121802330017\n",
      "epoch: 10 step: 254, loss is 0.40692827105522156\n",
      "epoch: 10 step: 255, loss is 0.24424852430820465\n",
      "epoch: 10 step: 256, loss is 0.4436035752296448\n",
      "epoch: 10 step: 257, loss is 0.1898389309644699\n",
      "epoch: 10 step: 258, loss is 0.32347026467323303\n",
      "epoch: 10 step: 259, loss is 0.18304955959320068\n",
      "epoch: 10 step: 260, loss is 0.32455363869667053\n",
      "epoch: 10 step: 261, loss is 0.1808941811323166\n",
      "epoch: 10 step: 262, loss is 0.48824530839920044\n",
      "epoch: 10 step: 263, loss is 0.24840858578681946\n",
      "epoch: 10 step: 264, loss is 0.14680525660514832\n",
      "epoch: 10 step: 265, loss is 0.2656341791152954\n",
      "epoch: 10 step: 266, loss is 0.2581837475299835\n",
      "epoch: 10 step: 267, loss is 0.22445198893547058\n",
      "epoch: 10 step: 268, loss is 0.13972312211990356\n",
      "epoch: 10 step: 269, loss is 0.11335039138793945\n",
      "epoch: 10 step: 270, loss is 0.40460795164108276\n",
      "epoch: 10 step: 271, loss is 0.18316888809204102\n",
      "epoch: 10 step: 272, loss is 0.26682665944099426\n",
      "epoch: 10 step: 273, loss is 0.20508091151714325\n",
      "epoch: 10 step: 274, loss is 0.23282574117183685\n",
      "epoch: 10 step: 275, loss is 0.13550761342048645\n",
      "epoch: 10 step: 276, loss is 0.31565988063812256\n",
      "epoch: 10 step: 277, loss is 0.1775842010974884\n",
      "epoch: 10 step: 278, loss is 0.20425181090831757\n",
      "epoch: 10 step: 279, loss is 0.2646799087524414\n",
      "epoch: 10 step: 280, loss is 0.23454803228378296\n",
      "epoch: 10 step: 281, loss is 0.21298089623451233\n",
      "epoch: 10 step: 282, loss is 0.49406230449676514\n",
      "epoch: 10 step: 283, loss is 0.19224540889263153\n",
      "epoch: 10 step: 284, loss is 0.1053243950009346\n",
      "epoch: 10 step: 285, loss is 0.15384835004806519\n",
      "epoch: 10 step: 286, loss is 0.2722225487232208\n",
      "epoch: 10 step: 287, loss is 0.20659932494163513\n",
      "epoch: 10 step: 288, loss is 0.14518596231937408\n",
      "epoch: 10 step: 289, loss is 0.20230764150619507\n",
      "epoch: 10 step: 290, loss is 0.34629756212234497\n",
      "epoch: 10 step: 291, loss is 0.3591066896915436\n",
      "epoch: 10 step: 292, loss is 0.23483924567699432\n",
      "epoch: 10 step: 293, loss is 0.25241875648498535\n",
      "epoch: 10 step: 294, loss is 0.19787496328353882\n",
      "epoch: 10 step: 295, loss is 0.14396536350250244\n",
      "epoch: 10 step: 296, loss is 0.14889724552631378\n",
      "epoch: 10 step: 297, loss is 0.24507364630699158\n",
      "epoch: 10 step: 298, loss is 0.30806785821914673\n",
      "epoch: 10 step: 299, loss is 0.24882040917873383\n",
      "epoch: 10 step: 300, loss is 0.2258666306734085\n",
      "epoch: 10 step: 301, loss is 0.2283412218093872\n",
      "epoch: 10 step: 302, loss is 0.4071364104747772\n",
      "epoch: 10 step: 303, loss is 0.10801386833190918\n",
      "epoch: 10 step: 304, loss is 0.3009442090988159\n",
      "epoch: 10 step: 305, loss is 0.21779000759124756\n",
      "epoch: 10 step: 306, loss is 0.12197986990213394\n",
      "epoch: 10 step: 307, loss is 0.23324836790561676\n",
      "epoch: 10 step: 308, loss is 0.25863102078437805\n",
      "epoch: 10 step: 309, loss is 0.3332495093345642\n",
      "epoch: 10 step: 310, loss is 0.16961292922496796\n",
      "epoch: 10 step: 311, loss is 0.19379690289497375\n",
      "epoch: 10 step: 312, loss is 0.27331289649009705\n",
      "epoch: 10 step: 313, loss is 0.36907559633255005\n",
      "epoch: 10 step: 314, loss is 0.31843456625938416\n",
      "epoch: 10 step: 315, loss is 0.24247047305107117\n",
      "epoch: 10 step: 316, loss is 0.1718265861272812\n",
      "epoch: 10 step: 317, loss is 0.2017989307641983\n",
      "epoch: 10 step: 318, loss is 0.28856635093688965\n",
      "epoch: 10 step: 319, loss is 0.2286091148853302\n",
      "epoch: 10 step: 320, loss is 0.22199565172195435\n",
      "epoch: 10 step: 321, loss is 0.1458655595779419\n",
      "epoch: 10 step: 322, loss is 0.13046208024024963\n",
      "epoch: 10 step: 323, loss is 0.2788545489311218\n",
      "epoch: 10 step: 324, loss is 0.2315826416015625\n",
      "epoch: 10 step: 325, loss is 0.3107058107852936\n",
      "epoch: 10 step: 326, loss is 0.35114243626594543\n",
      "epoch: 10 step: 327, loss is 0.30682122707366943\n",
      "epoch: 10 step: 328, loss is 0.23688673973083496\n",
      "epoch: 10 step: 329, loss is 0.5654235482215881\n",
      "epoch: 10 step: 330, loss is 0.19653883576393127\n",
      "epoch: 10 step: 331, loss is 0.25485551357269287\n",
      "epoch: 10 step: 332, loss is 0.32405194640159607\n",
      "epoch: 10 step: 333, loss is 0.2882719933986664\n",
      "epoch: 10 step: 334, loss is 0.14673884212970734\n",
      "epoch: 10 step: 335, loss is 0.2969275116920471\n",
      "epoch: 10 step: 336, loss is 0.22420313954353333\n",
      "epoch: 10 step: 337, loss is 0.23668408393859863\n",
      "epoch: 10 step: 338, loss is 0.21531304717063904\n",
      "epoch: 10 step: 339, loss is 0.05497739464044571\n",
      "epoch: 10 step: 340, loss is 0.19484500586986542\n",
      "epoch: 10 step: 341, loss is 0.18758028745651245\n",
      "epoch: 10 step: 342, loss is 0.19755545258522034\n",
      "epoch: 10 step: 343, loss is 0.4710564911365509\n",
      "epoch: 10 step: 344, loss is 0.2181401401758194\n",
      "epoch: 10 step: 345, loss is 0.1246018260717392\n",
      "epoch: 10 step: 346, loss is 0.2374689280986786\n",
      "epoch: 10 step: 347, loss is 0.17498844861984253\n",
      "epoch: 10 step: 348, loss is 0.15664154291152954\n",
      "epoch: 10 step: 349, loss is 0.38431650400161743\n",
      "epoch: 10 step: 350, loss is 0.15148386359214783\n",
      "epoch: 10 step: 351, loss is 0.26354706287384033\n",
      "epoch: 10 step: 352, loss is 0.3119228184223175\n",
      "epoch: 10 step: 353, loss is 0.07640501111745834\n",
      "epoch: 10 step: 354, loss is 0.30091890692710876\n",
      "epoch: 10 step: 355, loss is 0.19465109705924988\n",
      "epoch: 10 step: 356, loss is 0.37089523673057556\n",
      "epoch: 10 step: 357, loss is 0.2630254626274109\n",
      "epoch: 10 step: 358, loss is 0.2480558604001999\n",
      "epoch: 10 step: 359, loss is 0.45873942971229553\n",
      "epoch: 10 step: 360, loss is 0.2636408507823944\n",
      "epoch: 10 step: 361, loss is 0.3058030307292938\n",
      "epoch: 10 step: 362, loss is 0.21902942657470703\n",
      "epoch: 10 step: 363, loss is 0.22711558640003204\n",
      "epoch: 10 step: 364, loss is 0.2175423502922058\n",
      "epoch: 10 step: 365, loss is 0.1952042579650879\n",
      "epoch: 10 step: 366, loss is 0.2800326645374298\n",
      "epoch: 10 step: 367, loss is 0.2625497281551361\n",
      "epoch: 10 step: 368, loss is 0.18742218613624573\n",
      "epoch: 10 step: 369, loss is 0.32843294739723206\n",
      "epoch: 10 step: 370, loss is 0.2939804196357727\n",
      "epoch: 10 step: 371, loss is 0.49714913964271545\n",
      "epoch: 10 step: 372, loss is 0.36153003573417664\n",
      "epoch: 10 step: 373, loss is 0.30310800671577454\n",
      "epoch: 10 step: 374, loss is 0.19229114055633545\n",
      "epoch: 10 step: 375, loss is 0.30235734581947327\n",
      "epoch: 10 step: 376, loss is 0.1846732497215271\n",
      "epoch: 10 step: 377, loss is 0.212374746799469\n",
      "epoch: 10 step: 378, loss is 0.13850051164627075\n",
      "epoch: 10 step: 379, loss is 0.2982349991798401\n",
      "epoch: 10 step: 380, loss is 0.2587636411190033\n",
      "epoch: 10 step: 381, loss is 0.1470419317483902\n",
      "epoch: 10 step: 382, loss is 0.44771599769592285\n",
      "epoch: 10 step: 383, loss is 0.14995630085468292\n",
      "epoch: 10 step: 384, loss is 0.17868338525295258\n",
      "epoch: 10 step: 385, loss is 0.2980318069458008\n",
      "epoch: 10 step: 386, loss is 0.1818821132183075\n",
      "epoch: 10 step: 387, loss is 0.4684934914112091\n",
      "epoch: 10 step: 388, loss is 0.28237172961235046\n",
      "epoch: 10 step: 389, loss is 0.158906027674675\n",
      "epoch: 10 step: 390, loss is 0.30916786193847656\n",
      "epoch: 10 step: 391, loss is 0.08466120064258575\n",
      "epoch: 10 step: 392, loss is 0.2153117060661316\n",
      "epoch: 10 step: 393, loss is 0.3157568573951721\n",
      "epoch: 10 step: 394, loss is 0.2956620156764984\n",
      "epoch: 10 step: 395, loss is 0.27213117480278015\n",
      "epoch: 10 step: 396, loss is 0.2062639743089676\n",
      "epoch: 10 step: 397, loss is 0.3309360444545746\n",
      "epoch: 10 step: 398, loss is 0.11646772921085358\n",
      "epoch: 10 step: 399, loss is 0.26531311869621277\n",
      "epoch: 10 step: 400, loss is 0.263690710067749\n",
      "epoch: 10 step: 401, loss is 0.23016686737537384\n",
      "epoch: 10 step: 402, loss is 0.4428962767124176\n",
      "epoch: 10 step: 403, loss is 0.21776655316352844\n",
      "epoch: 10 step: 404, loss is 0.24716326594352722\n",
      "epoch: 10 step: 405, loss is 0.1429513841867447\n",
      "epoch: 10 step: 406, loss is 0.261464387178421\n",
      "epoch: 10 step: 407, loss is 0.21294356882572174\n",
      "epoch: 10 step: 408, loss is 0.21639569103717804\n",
      "epoch: 10 step: 409, loss is 0.23790331184864044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 410, loss is 0.16335564851760864\n",
      "epoch: 10 step: 411, loss is 0.22141551971435547\n",
      "epoch: 10 step: 412, loss is 0.20829692482948303\n",
      "epoch: 10 step: 413, loss is 0.30295515060424805\n",
      "epoch: 10 step: 414, loss is 0.25119540095329285\n",
      "epoch: 10 step: 415, loss is 0.22026054561138153\n",
      "epoch: 10 step: 416, loss is 0.1361279934644699\n",
      "epoch: 10 step: 417, loss is 0.19730201363563538\n",
      "epoch: 10 step: 418, loss is 0.19860537350177765\n",
      "epoch: 10 step: 419, loss is 0.1996711790561676\n",
      "epoch: 10 step: 420, loss is 0.28413644433021545\n",
      "epoch: 10 step: 421, loss is 0.2791549563407898\n",
      "epoch: 10 step: 422, loss is 0.2343595325946808\n",
      "epoch: 10 step: 423, loss is 0.17719729244709015\n",
      "epoch: 10 step: 424, loss is 0.21295660734176636\n",
      "epoch: 10 step: 425, loss is 0.1235504224896431\n",
      "epoch: 10 step: 426, loss is 0.33709031343460083\n",
      "epoch: 10 step: 427, loss is 0.15880389511585236\n",
      "epoch: 10 step: 428, loss is 0.08935346454381943\n",
      "epoch: 10 step: 429, loss is 0.16019394993782043\n",
      "epoch: 10 step: 430, loss is 0.3198617994785309\n",
      "epoch: 10 step: 431, loss is 0.2702729105949402\n",
      "epoch: 10 step: 432, loss is 0.1903744488954544\n",
      "epoch: 10 step: 433, loss is 0.2071182131767273\n",
      "epoch: 10 step: 434, loss is 0.18417279422283173\n",
      "epoch: 10 step: 435, loss is 0.12938101589679718\n",
      "epoch: 10 step: 436, loss is 0.2260211706161499\n",
      "epoch: 10 step: 437, loss is 0.2339244931936264\n",
      "epoch: 10 step: 438, loss is 0.3408520817756653\n",
      "epoch: 10 step: 439, loss is 0.2345767468214035\n",
      "epoch: 10 step: 440, loss is 0.21022990345954895\n",
      "epoch: 10 step: 441, loss is 0.2770857810974121\n",
      "epoch: 10 step: 442, loss is 0.14272211492061615\n",
      "epoch: 10 step: 443, loss is 0.3249298632144928\n",
      "epoch: 10 step: 444, loss is 0.24155591428279877\n",
      "epoch: 10 step: 445, loss is 0.100696861743927\n",
      "epoch: 10 step: 446, loss is 0.28607726097106934\n",
      "epoch: 10 step: 447, loss is 0.2425483912229538\n",
      "epoch: 10 step: 448, loss is 0.1861661672592163\n",
      "epoch: 10 step: 449, loss is 0.15901978313922882\n",
      "epoch: 10 step: 450, loss is 0.2552497088909149\n",
      "epoch: 10 step: 451, loss is 0.1474522054195404\n",
      "epoch: 10 step: 452, loss is 0.361822247505188\n",
      "epoch: 10 step: 453, loss is 0.22415246069431305\n",
      "epoch: 10 step: 454, loss is 0.1995154768228531\n",
      "epoch: 10 step: 455, loss is 0.2046053558588028\n",
      "epoch: 10 step: 456, loss is 0.2750314474105835\n",
      "epoch: 10 step: 457, loss is 0.2954971492290497\n",
      "epoch: 10 step: 458, loss is 0.1749573051929474\n",
      "epoch: 10 step: 459, loss is 0.19252821803092957\n",
      "epoch: 10 step: 460, loss is 0.20119553804397583\n",
      "epoch: 10 step: 461, loss is 0.3017452359199524\n",
      "epoch: 10 step: 462, loss is 0.10737305879592896\n",
      "epoch: 10 step: 463, loss is 0.16484671831130981\n",
      "epoch: 10 step: 464, loss is 0.15645979344844818\n",
      "epoch: 10 step: 465, loss is 0.1956019103527069\n",
      "epoch: 10 step: 466, loss is 0.22319364547729492\n",
      "epoch: 10 step: 467, loss is 0.1402304619550705\n",
      "epoch: 10 step: 468, loss is 0.3206266760826111\n",
      "epoch: 10 step: 469, loss is 0.16813509166240692\n",
      "epoch: 10 step: 470, loss is 0.29057174921035767\n",
      "epoch: 10 step: 471, loss is 0.17640791833400726\n",
      "epoch: 10 step: 472, loss is 0.2558691203594208\n",
      "epoch: 10 step: 473, loss is 0.36220988631248474\n",
      "epoch: 10 step: 474, loss is 0.24583084881305695\n",
      "epoch: 10 step: 475, loss is 0.23066411912441254\n",
      "epoch: 10 step: 476, loss is 0.3593391478061676\n",
      "epoch: 10 step: 477, loss is 0.15911799669265747\n",
      "epoch: 10 step: 478, loss is 0.2615812122821808\n",
      "epoch: 10 step: 479, loss is 0.12291958183050156\n",
      "epoch: 10 step: 480, loss is 0.2903405725955963\n",
      "epoch: 10 step: 481, loss is 0.4032595753669739\n",
      "epoch: 10 step: 482, loss is 0.3146606683731079\n",
      "epoch: 10 step: 483, loss is 0.16908837854862213\n",
      "epoch: 10 step: 484, loss is 0.2128230780363083\n",
      "epoch: 10 step: 485, loss is 0.29297903180122375\n",
      "epoch: 10 step: 486, loss is 0.16515867412090302\n",
      "epoch: 10 step: 487, loss is 0.3074522912502289\n",
      "epoch: 10 step: 488, loss is 0.19885052740573883\n",
      "epoch: 10 step: 489, loss is 0.34041011333465576\n",
      "epoch: 10 step: 490, loss is 0.39083585143089294\n",
      "epoch: 10 step: 491, loss is 0.33893612027168274\n",
      "epoch: 10 step: 492, loss is 0.3797614872455597\n",
      "epoch: 10 step: 493, loss is 0.3570597767829895\n",
      "epoch: 10 step: 494, loss is 0.11632683873176575\n",
      "epoch: 10 step: 495, loss is 0.15829169750213623\n",
      "epoch: 10 step: 496, loss is 0.4150860607624054\n",
      "epoch: 10 step: 497, loss is 0.13385090231895447\n",
      "epoch: 10 step: 498, loss is 0.23201489448547363\n",
      "epoch: 10 step: 499, loss is 0.2566360533237457\n",
      "epoch: 10 step: 500, loss is 0.28758078813552856\n",
      "epoch: 10 step: 501, loss is 0.21690908074378967\n",
      "epoch: 10 step: 502, loss is 0.21232451498508453\n",
      "epoch: 10 step: 503, loss is 0.1883591264486313\n",
      "epoch: 10 step: 504, loss is 0.38205617666244507\n",
      "epoch: 10 step: 505, loss is 0.39485275745391846\n",
      "epoch: 10 step: 506, loss is 0.3341694474220276\n",
      "epoch: 10 step: 507, loss is 0.24935171008110046\n",
      "epoch: 10 step: 508, loss is 0.1769442856311798\n",
      "epoch: 10 step: 509, loss is 0.29955270886421204\n",
      "epoch: 10 step: 510, loss is 0.20235109329223633\n",
      "epoch: 10 step: 511, loss is 0.15152060985565186\n",
      "epoch: 10 step: 512, loss is 0.2518066465854645\n",
      "epoch: 10 step: 513, loss is 0.2893213927745819\n",
      "epoch: 10 step: 514, loss is 0.3063585162162781\n",
      "epoch: 10 step: 515, loss is 0.29401904344558716\n",
      "epoch: 10 step: 516, loss is 0.262102335691452\n",
      "epoch: 10 step: 517, loss is 0.3051707446575165\n",
      "epoch: 10 step: 518, loss is 0.27081096172332764\n",
      "epoch: 10 step: 519, loss is 0.2579808831214905\n",
      "epoch: 10 step: 520, loss is 0.2110644280910492\n",
      "epoch: 10 step: 521, loss is 0.524750292301178\n",
      "epoch: 10 step: 522, loss is 0.26780006289482117\n",
      "epoch: 10 step: 523, loss is 0.18052437901496887\n",
      "epoch: 10 step: 524, loss is 0.21440978348255157\n",
      "epoch: 10 step: 525, loss is 0.2413852959871292\n",
      "epoch: 10 step: 526, loss is 0.23962834477424622\n",
      "epoch: 10 step: 527, loss is 0.32242676615715027\n",
      "epoch: 10 step: 528, loss is 0.25771477818489075\n",
      "epoch: 10 step: 529, loss is 0.18821781873703003\n",
      "epoch: 10 step: 530, loss is 0.19685843586921692\n",
      "epoch: 10 step: 531, loss is 0.14942824840545654\n",
      "epoch: 10 step: 532, loss is 0.24821247160434723\n",
      "epoch: 10 step: 533, loss is 0.205203577876091\n",
      "epoch: 10 step: 534, loss is 0.21950705349445343\n",
      "epoch: 10 step: 535, loss is 0.29018908739089966\n",
      "epoch: 10 step: 536, loss is 0.2006944864988327\n",
      "epoch: 10 step: 537, loss is 0.25468719005584717\n",
      "epoch: 10 step: 538, loss is 0.2063145488500595\n",
      "epoch: 10 step: 539, loss is 0.342830628156662\n",
      "epoch: 10 step: 540, loss is 0.17697227001190186\n",
      "epoch: 10 step: 541, loss is 0.22320270538330078\n",
      "epoch: 10 step: 542, loss is 0.30566614866256714\n",
      "epoch: 10 step: 543, loss is 0.3791601061820984\n",
      "epoch: 10 step: 544, loss is 0.13311493396759033\n",
      "epoch: 10 step: 545, loss is 0.2157100886106491\n",
      "epoch: 10 step: 546, loss is 0.272644966840744\n",
      "epoch: 10 step: 547, loss is 0.13650387525558472\n",
      "epoch: 10 step: 548, loss is 0.15674647688865662\n",
      "epoch: 10 step: 549, loss is 0.25112923979759216\n",
      "epoch: 10 step: 550, loss is 0.18501822650432587\n",
      "epoch: 10 step: 551, loss is 0.3285444378852844\n",
      "epoch: 10 step: 552, loss is 0.26517820358276367\n",
      "epoch: 10 step: 553, loss is 0.23614653944969177\n",
      "epoch: 10 step: 554, loss is 0.3013138473033905\n",
      "epoch: 10 step: 555, loss is 0.23145686089992523\n",
      "epoch: 10 step: 556, loss is 0.3314148485660553\n",
      "epoch: 10 step: 557, loss is 0.21636754274368286\n",
      "epoch: 10 step: 558, loss is 0.1633083075284958\n",
      "epoch: 10 step: 559, loss is 0.23202940821647644\n",
      "epoch: 10 step: 560, loss is 0.12412897497415543\n",
      "epoch: 10 step: 561, loss is 0.1626404970884323\n",
      "epoch: 10 step: 562, loss is 0.23171062767505646\n",
      "epoch: 10 step: 563, loss is 0.29336437582969666\n",
      "epoch: 10 step: 564, loss is 0.2398425191640854\n",
      "epoch: 10 step: 565, loss is 0.2631639540195465\n",
      "epoch: 10 step: 566, loss is 0.36820825934410095\n",
      "epoch: 10 step: 567, loss is 0.1895536482334137\n",
      "epoch: 10 step: 568, loss is 0.36670196056365967\n",
      "epoch: 10 step: 569, loss is 0.20478717982769012\n",
      "epoch: 10 step: 570, loss is 0.16915853321552277\n",
      "epoch: 10 step: 571, loss is 0.14639762043952942\n",
      "epoch: 10 step: 572, loss is 0.37479662895202637\n",
      "epoch: 10 step: 573, loss is 0.2516080141067505\n",
      "epoch: 10 step: 574, loss is 0.21181942522525787\n",
      "epoch: 10 step: 575, loss is 0.11747542768716812\n",
      "epoch: 10 step: 576, loss is 0.34482109546661377\n",
      "epoch: 10 step: 577, loss is 0.2480342835187912\n",
      "epoch: 10 step: 578, loss is 0.23216496407985687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 579, loss is 0.24633167684078217\n",
      "epoch: 10 step: 580, loss is 0.3238714039325714\n",
      "epoch: 10 step: 581, loss is 0.2592178285121918\n",
      "epoch: 10 step: 582, loss is 0.21626339852809906\n",
      "epoch: 10 step: 583, loss is 0.20984356105327606\n",
      "epoch: 10 step: 584, loss is 0.4120357036590576\n",
      "epoch: 10 step: 585, loss is 0.15417194366455078\n",
      "epoch: 10 step: 586, loss is 0.29564905166625977\n",
      "epoch: 10 step: 587, loss is 0.4471205174922943\n",
      "epoch: 10 step: 588, loss is 0.20980511605739594\n",
      "epoch: 10 step: 589, loss is 0.1827305257320404\n",
      "epoch: 10 step: 590, loss is 0.35740426182746887\n",
      "epoch: 10 step: 591, loss is 0.3861498236656189\n",
      "epoch: 10 step: 592, loss is 0.33015137910842896\n",
      "epoch: 10 step: 593, loss is 0.2656235694885254\n",
      "epoch: 10 step: 594, loss is 0.2826642692089081\n",
      "epoch: 10 step: 595, loss is 0.23537060618400574\n",
      "epoch: 10 step: 596, loss is 0.25388622283935547\n",
      "epoch: 10 step: 597, loss is 0.14185336232185364\n",
      "epoch: 10 step: 598, loss is 0.17595632374286652\n",
      "epoch: 10 step: 599, loss is 0.21489454805850983\n",
      "epoch: 10 step: 600, loss is 0.09612627327442169\n",
      "epoch: 10 step: 601, loss is 0.411807119846344\n",
      "epoch: 10 step: 602, loss is 0.2809252440929413\n",
      "epoch: 10 step: 603, loss is 0.262117862701416\n",
      "epoch: 10 step: 604, loss is 0.19685621559619904\n",
      "epoch: 10 step: 605, loss is 0.17586971819400787\n",
      "epoch: 10 step: 606, loss is 0.3529312312602997\n",
      "epoch: 10 step: 607, loss is 0.3325900435447693\n",
      "epoch: 10 step: 608, loss is 0.2311858981847763\n",
      "epoch: 10 step: 609, loss is 0.3299388885498047\n",
      "epoch: 10 step: 610, loss is 0.2944939434528351\n",
      "epoch: 10 step: 611, loss is 0.13652223348617554\n",
      "epoch: 10 step: 612, loss is 0.267421692609787\n",
      "epoch: 10 step: 613, loss is 0.34631863236427307\n",
      "epoch: 10 step: 614, loss is 0.1291693150997162\n",
      "epoch: 10 step: 615, loss is 0.2995469570159912\n",
      "epoch: 10 step: 616, loss is 0.18747664988040924\n",
      "epoch: 10 step: 617, loss is 0.22032171487808228\n",
      "epoch: 10 step: 618, loss is 0.2861578166484833\n",
      "epoch: 10 step: 619, loss is 0.27491068840026855\n",
      "epoch: 10 step: 620, loss is 0.3241589665412903\n",
      "epoch: 10 step: 621, loss is 0.21622072160243988\n",
      "epoch: 10 step: 622, loss is 0.2751108705997467\n",
      "epoch: 10 step: 623, loss is 0.2666035294532776\n",
      "epoch: 10 step: 624, loss is 0.33514219522476196\n",
      "epoch: 10 step: 625, loss is 0.18630273640155792\n",
      "epoch: 10 step: 626, loss is 0.22930149734020233\n",
      "epoch: 10 step: 627, loss is 0.24840253591537476\n",
      "epoch: 10 step: 628, loss is 0.12230534851551056\n",
      "epoch: 10 step: 629, loss is 0.12484265118837357\n",
      "epoch: 10 step: 630, loss is 0.2010645568370819\n",
      "epoch: 10 step: 631, loss is 0.2245609015226364\n",
      "epoch: 10 step: 632, loss is 0.40047723054885864\n",
      "epoch: 10 step: 633, loss is 0.2405577152967453\n",
      "epoch: 10 step: 634, loss is 0.14841489493846893\n",
      "epoch: 10 step: 635, loss is 0.16594499349594116\n",
      "epoch: 10 step: 636, loss is 0.3112555742263794\n",
      "epoch: 10 step: 637, loss is 0.15609069168567657\n",
      "epoch: 10 step: 638, loss is 0.27309873700141907\n",
      "epoch: 10 step: 639, loss is 0.41717803478240967\n",
      "epoch: 10 step: 640, loss is 0.20100808143615723\n",
      "epoch: 10 step: 641, loss is 0.2251891940832138\n",
      "epoch: 10 step: 642, loss is 0.21815329790115356\n",
      "epoch: 10 step: 643, loss is 0.15175439417362213\n",
      "epoch: 10 step: 644, loss is 0.18716789782047272\n",
      "epoch: 10 step: 645, loss is 0.2555897831916809\n",
      "epoch: 10 step: 646, loss is 0.38264036178588867\n",
      "epoch: 10 step: 647, loss is 0.1346997171640396\n",
      "epoch: 10 step: 648, loss is 0.19710099697113037\n",
      "epoch: 10 step: 649, loss is 0.13703610002994537\n",
      "epoch: 10 step: 650, loss is 0.11778581887483597\n",
      "epoch: 10 step: 651, loss is 0.4892580509185791\n",
      "epoch: 10 step: 652, loss is 0.22491654753684998\n",
      "epoch: 10 step: 653, loss is 0.34182336926460266\n",
      "epoch: 10 step: 654, loss is 0.09548147767782211\n",
      "epoch: 10 step: 655, loss is 0.3299559950828552\n",
      "epoch: 10 step: 656, loss is 0.2630593776702881\n",
      "epoch: 10 step: 657, loss is 0.2462354600429535\n",
      "epoch: 10 step: 658, loss is 0.17779213190078735\n",
      "epoch: 10 step: 659, loss is 0.25995540618896484\n",
      "epoch: 10 step: 660, loss is 0.24650973081588745\n",
      "epoch: 10 step: 661, loss is 0.19700729846954346\n",
      "epoch: 10 step: 662, loss is 0.1771489530801773\n",
      "epoch: 10 step: 663, loss is 0.14892074465751648\n",
      "epoch: 10 step: 664, loss is 0.1294468194246292\n",
      "epoch: 10 step: 665, loss is 0.07357048988342285\n",
      "epoch: 10 step: 666, loss is 0.09412404149770737\n",
      "epoch: 10 step: 667, loss is 0.1768229454755783\n",
      "epoch: 10 step: 668, loss is 0.250110387802124\n",
      "epoch: 10 step: 669, loss is 0.15815560519695282\n",
      "epoch: 10 step: 670, loss is 0.15508458018302917\n",
      "epoch: 10 step: 671, loss is 0.2464568018913269\n",
      "epoch: 10 step: 672, loss is 0.2526722848415375\n",
      "epoch: 10 step: 673, loss is 0.18078485131263733\n",
      "epoch: 10 step: 674, loss is 0.10703727602958679\n",
      "epoch: 10 step: 675, loss is 0.49223592877388\n",
      "epoch: 10 step: 676, loss is 0.3322142958641052\n",
      "epoch: 10 step: 677, loss is 0.16024883091449738\n",
      "epoch: 10 step: 678, loss is 0.09648483246564865\n",
      "epoch: 10 step: 679, loss is 0.27593475580215454\n",
      "epoch: 10 step: 680, loss is 0.2345055490732193\n",
      "epoch: 10 step: 681, loss is 0.20073530077934265\n",
      "epoch: 10 step: 682, loss is 0.28782713413238525\n",
      "epoch: 10 step: 683, loss is 0.17339476943016052\n",
      "epoch: 10 step: 684, loss is 0.1341596096754074\n",
      "epoch: 10 step: 685, loss is 0.34226399660110474\n",
      "epoch: 10 step: 686, loss is 0.25005602836608887\n",
      "epoch: 10 step: 687, loss is 0.17462386190891266\n",
      "epoch: 10 step: 688, loss is 0.22321751713752747\n",
      "epoch: 10 step: 689, loss is 0.25999903678894043\n",
      "epoch: 10 step: 690, loss is 0.24166381359100342\n",
      "epoch: 10 step: 691, loss is 0.2231856882572174\n",
      "epoch: 10 step: 692, loss is 0.21657055616378784\n",
      "epoch: 10 step: 693, loss is 0.5285525918006897\n",
      "epoch: 10 step: 694, loss is 0.25261226296424866\n",
      "epoch: 10 step: 695, loss is 0.3000638484954834\n",
      "epoch: 10 step: 696, loss is 0.2646304666996002\n",
      "epoch: 10 step: 697, loss is 0.28730228543281555\n",
      "epoch: 10 step: 698, loss is 0.3482850193977356\n",
      "epoch: 10 step: 699, loss is 0.1153649389743805\n",
      "epoch: 10 step: 700, loss is 0.21991832554340363\n",
      "epoch: 10 step: 701, loss is 0.28778377175331116\n",
      "epoch: 10 step: 702, loss is 0.19316768646240234\n",
      "epoch: 10 step: 703, loss is 0.23583269119262695\n",
      "epoch: 10 step: 704, loss is 0.266099750995636\n",
      "epoch: 10 step: 705, loss is 0.41686564683914185\n",
      "epoch: 10 step: 706, loss is 0.2543083131313324\n",
      "epoch: 10 step: 707, loss is 0.21705399453639984\n",
      "epoch: 10 step: 708, loss is 0.1823473572731018\n",
      "epoch: 10 step: 709, loss is 0.41829144954681396\n",
      "epoch: 10 step: 710, loss is 0.2809838354587555\n",
      "epoch: 10 step: 711, loss is 0.20507360994815826\n",
      "epoch: 10 step: 712, loss is 0.16104425489902496\n",
      "epoch: 10 step: 713, loss is 0.30644136667251587\n",
      "epoch: 10 step: 714, loss is 0.08257623761892319\n",
      "epoch: 10 step: 715, loss is 0.20691967010498047\n",
      "epoch: 10 step: 716, loss is 0.13672514259815216\n",
      "epoch: 10 step: 717, loss is 0.18086093664169312\n",
      "epoch: 10 step: 718, loss is 0.1402754932641983\n",
      "epoch: 10 step: 719, loss is 0.22490115463733673\n",
      "epoch: 10 step: 720, loss is 0.19650857150554657\n",
      "epoch: 10 step: 721, loss is 0.16965267062187195\n",
      "epoch: 10 step: 722, loss is 0.2046547383069992\n",
      "epoch: 10 step: 723, loss is 0.16105249524116516\n",
      "epoch: 10 step: 724, loss is 0.15662948787212372\n",
      "epoch: 10 step: 725, loss is 0.25593897700309753\n",
      "epoch: 10 step: 726, loss is 0.215834379196167\n",
      "epoch: 10 step: 727, loss is 0.20306947827339172\n",
      "epoch: 10 step: 728, loss is 0.3703905940055847\n",
      "epoch: 10 step: 729, loss is 0.22495032846927643\n",
      "epoch: 10 step: 730, loss is 0.15027065575122833\n",
      "epoch: 10 step: 731, loss is 0.2785260081291199\n",
      "epoch: 10 step: 732, loss is 0.16945341229438782\n",
      "epoch: 10 step: 733, loss is 0.28550365567207336\n",
      "epoch: 10 step: 734, loss is 0.401104599237442\n",
      "epoch: 10 step: 735, loss is 0.12052863836288452\n",
      "epoch: 10 step: 736, loss is 0.13666538894176483\n",
      "epoch: 10 step: 737, loss is 0.25237923860549927\n",
      "epoch: 10 step: 738, loss is 0.3967227041721344\n",
      "epoch: 10 step: 739, loss is 0.2505069971084595\n",
      "epoch: 10 step: 740, loss is 0.23781411349773407\n",
      "epoch: 10 step: 741, loss is 0.3394061326980591\n",
      "epoch: 10 step: 742, loss is 0.15473242104053497\n",
      "epoch: 10 step: 743, loss is 0.19414116442203522\n",
      "epoch: 10 step: 744, loss is 0.1889411062002182\n",
      "epoch: 10 step: 745, loss is 0.24749302864074707\n",
      "epoch: 10 step: 746, loss is 0.2600712180137634\n",
      "epoch: 10 step: 747, loss is 0.14063020050525665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 748, loss is 0.2434123307466507\n",
      "epoch: 10 step: 749, loss is 0.14854592084884644\n",
      "epoch: 10 step: 750, loss is 0.29855355620384216\n",
      "epoch: 10 step: 751, loss is 0.21553707122802734\n",
      "epoch: 10 step: 752, loss is 0.3171338140964508\n",
      "epoch: 10 step: 753, loss is 0.24405771493911743\n",
      "epoch: 10 step: 754, loss is 0.34424522519111633\n",
      "epoch: 10 step: 755, loss is 0.26552194356918335\n",
      "epoch: 10 step: 756, loss is 0.20684470236301422\n",
      "epoch: 10 step: 757, loss is 0.1798526644706726\n",
      "epoch: 10 step: 758, loss is 0.15037226676940918\n",
      "epoch: 10 step: 759, loss is 0.2634177803993225\n",
      "epoch: 10 step: 760, loss is 0.3651840090751648\n",
      "epoch: 10 step: 761, loss is 0.1698567271232605\n",
      "epoch: 10 step: 762, loss is 0.15299198031425476\n",
      "epoch: 10 step: 763, loss is 0.20430339872837067\n",
      "epoch: 10 step: 764, loss is 0.32207638025283813\n",
      "epoch: 10 step: 765, loss is 0.07334572076797485\n",
      "epoch: 10 step: 766, loss is 0.16630427539348602\n",
      "epoch: 10 step: 767, loss is 0.24606484174728394\n",
      "epoch: 10 step: 768, loss is 0.3441234529018402\n",
      "epoch: 10 step: 769, loss is 0.11799166351556778\n",
      "epoch: 10 step: 770, loss is 0.20911142230033875\n",
      "epoch: 10 step: 771, loss is 0.21042227745056152\n",
      "epoch: 10 step: 772, loss is 0.1653052121400833\n",
      "epoch: 10 step: 773, loss is 0.33813703060150146\n",
      "epoch: 10 step: 774, loss is 0.2005976289510727\n",
      "epoch: 10 step: 775, loss is 0.1429162472486496\n",
      "epoch: 10 step: 776, loss is 0.2956277132034302\n",
      "epoch: 10 step: 777, loss is 0.12318417429924011\n",
      "epoch: 10 step: 778, loss is 0.43384695053100586\n",
      "epoch: 10 step: 779, loss is 0.33331984281539917\n",
      "epoch: 10 step: 780, loss is 0.3842659294605255\n",
      "epoch: 10 step: 781, loss is 0.28866469860076904\n",
      "epoch: 10 step: 782, loss is 0.2786943018436432\n",
      "epoch: 10 step: 783, loss is 0.1181880310177803\n",
      "epoch: 10 step: 784, loss is 0.3623444437980652\n",
      "epoch: 10 step: 785, loss is 0.2988484799861908\n",
      "epoch: 10 step: 786, loss is 0.12235432863235474\n",
      "epoch: 10 step: 787, loss is 0.4405556321144104\n",
      "epoch: 10 step: 788, loss is 0.1835208684206009\n",
      "epoch: 10 step: 789, loss is 0.32710927724838257\n",
      "epoch: 10 step: 790, loss is 0.19611559808254242\n",
      "epoch: 10 step: 791, loss is 0.23219120502471924\n",
      "epoch: 10 step: 792, loss is 0.34552180767059326\n",
      "epoch: 10 step: 793, loss is 0.13345156610012054\n",
      "epoch: 10 step: 794, loss is 0.23637588322162628\n",
      "epoch: 10 step: 795, loss is 0.13050884008407593\n",
      "epoch: 10 step: 796, loss is 0.21264110505580902\n",
      "epoch: 10 step: 797, loss is 0.1786249577999115\n",
      "epoch: 10 step: 798, loss is 0.2654842138290405\n",
      "epoch: 10 step: 799, loss is 0.21369706094264984\n",
      "epoch: 10 step: 800, loss is 0.3078949451446533\n",
      "epoch: 10 step: 801, loss is 0.17855830490589142\n",
      "epoch: 10 step: 802, loss is 0.44231775403022766\n",
      "epoch: 10 step: 803, loss is 0.28015393018722534\n",
      "epoch: 10 step: 804, loss is 0.3517836928367615\n",
      "epoch: 10 step: 805, loss is 0.40764912962913513\n",
      "epoch: 10 step: 806, loss is 0.15449072420597076\n",
      "epoch: 10 step: 807, loss is 0.5717987418174744\n",
      "epoch: 10 step: 808, loss is 0.4038105607032776\n",
      "epoch: 10 step: 809, loss is 0.33724483847618103\n",
      "epoch: 10 step: 810, loss is 0.23175491392612457\n",
      "epoch: 10 step: 811, loss is 0.29527103900909424\n",
      "epoch: 10 step: 812, loss is 0.3556300699710846\n",
      "epoch: 10 step: 813, loss is 0.28933995962142944\n",
      "epoch: 10 step: 814, loss is 0.2942657470703125\n",
      "epoch: 10 step: 815, loss is 0.1892334222793579\n",
      "epoch: 10 step: 816, loss is 0.348321795463562\n",
      "epoch: 10 step: 817, loss is 0.2706255614757538\n",
      "epoch: 10 step: 818, loss is 0.277620404958725\n",
      "epoch: 10 step: 819, loss is 0.19913579523563385\n",
      "epoch: 10 step: 820, loss is 0.17109444737434387\n",
      "epoch: 10 step: 821, loss is 0.23026718199253082\n",
      "epoch: 10 step: 822, loss is 0.43825531005859375\n",
      "epoch: 10 step: 823, loss is 0.3626912534236908\n",
      "epoch: 10 step: 824, loss is 0.17202986776828766\n",
      "epoch: 10 step: 825, loss is 0.1799401193857193\n",
      "epoch: 10 step: 826, loss is 0.3454835116863251\n",
      "epoch: 10 step: 827, loss is 0.4550896883010864\n",
      "epoch: 10 step: 828, loss is 0.23364517092704773\n",
      "epoch: 10 step: 829, loss is 0.2467319220304489\n",
      "epoch: 10 step: 830, loss is 0.417533814907074\n",
      "epoch: 10 step: 831, loss is 0.23965206742286682\n",
      "epoch: 10 step: 832, loss is 0.40229907631874084\n",
      "epoch: 10 step: 833, loss is 0.380741685628891\n",
      "epoch: 10 step: 834, loss is 0.3697322607040405\n",
      "epoch: 10 step: 835, loss is 0.25527095794677734\n",
      "epoch: 10 step: 836, loss is 0.22753961384296417\n",
      "epoch: 10 step: 837, loss is 0.21627722680568695\n",
      "epoch: 10 step: 838, loss is 0.3657193183898926\n",
      "epoch: 10 step: 839, loss is 0.3430291712284088\n",
      "epoch: 10 step: 840, loss is 0.2602146863937378\n",
      "epoch: 10 step: 841, loss is 0.19879277050495148\n",
      "epoch: 10 step: 842, loss is 0.17195485532283783\n",
      "epoch: 10 step: 843, loss is 0.27498507499694824\n",
      "epoch: 10 step: 844, loss is 0.2973378002643585\n",
      "epoch: 10 step: 845, loss is 0.32081878185272217\n",
      "epoch: 10 step: 846, loss is 0.14377345144748688\n",
      "epoch: 10 step: 847, loss is 0.30900272727012634\n",
      "epoch: 10 step: 848, loss is 0.17725630104541779\n",
      "epoch: 10 step: 849, loss is 0.2906215786933899\n",
      "epoch: 10 step: 850, loss is 0.2589854598045349\n",
      "epoch: 10 step: 851, loss is 0.20720729231834412\n",
      "epoch: 10 step: 852, loss is 0.11890336871147156\n",
      "epoch: 10 step: 853, loss is 0.4363279640674591\n",
      "epoch: 10 step: 854, loss is 0.43968260288238525\n",
      "epoch: 10 step: 855, loss is 0.2193813920021057\n",
      "epoch: 10 step: 856, loss is 0.26137471199035645\n",
      "epoch: 10 step: 857, loss is 0.18715663254261017\n",
      "epoch: 10 step: 858, loss is 0.19798481464385986\n",
      "epoch: 10 step: 859, loss is 0.22383660078048706\n",
      "epoch: 10 step: 860, loss is 0.2583012282848358\n",
      "epoch: 10 step: 861, loss is 0.13342906534671783\n",
      "epoch: 10 step: 862, loss is 0.1939711570739746\n",
      "epoch: 10 step: 863, loss is 0.18896842002868652\n",
      "epoch: 10 step: 864, loss is 0.2611978054046631\n",
      "epoch: 10 step: 865, loss is 0.11169188469648361\n",
      "epoch: 10 step: 866, loss is 0.2325727343559265\n",
      "epoch: 10 step: 867, loss is 0.2652709186077118\n",
      "epoch: 10 step: 868, loss is 0.1974487602710724\n",
      "epoch: 10 step: 869, loss is 0.20766852796077728\n",
      "epoch: 10 step: 870, loss is 0.18838544189929962\n",
      "epoch: 10 step: 871, loss is 0.1689755618572235\n",
      "epoch: 10 step: 872, loss is 0.2819439768791199\n",
      "epoch: 10 step: 873, loss is 0.3032029867172241\n",
      "epoch: 10 step: 874, loss is 0.4326428771018982\n",
      "epoch: 10 step: 875, loss is 0.3778160512447357\n",
      "epoch: 10 step: 876, loss is 0.27469223737716675\n",
      "epoch: 10 step: 877, loss is 0.27038347721099854\n",
      "epoch: 10 step: 878, loss is 0.21764621138572693\n",
      "epoch: 10 step: 879, loss is 0.34394460916519165\n",
      "epoch: 10 step: 880, loss is 0.23548921942710876\n",
      "epoch: 10 step: 881, loss is 0.18788185715675354\n",
      "epoch: 10 step: 882, loss is 0.31174236536026\n",
      "epoch: 10 step: 883, loss is 0.17568179965019226\n",
      "epoch: 10 step: 884, loss is 0.1980891227722168\n",
      "epoch: 10 step: 885, loss is 0.29574716091156006\n",
      "epoch: 10 step: 886, loss is 0.20182134211063385\n",
      "epoch: 10 step: 887, loss is 0.17718857526779175\n",
      "epoch: 10 step: 888, loss is 0.16340112686157227\n",
      "epoch: 10 step: 889, loss is 0.234137162566185\n",
      "epoch: 10 step: 890, loss is 0.3275343179702759\n",
      "epoch: 10 step: 891, loss is 0.2502385675907135\n",
      "epoch: 10 step: 892, loss is 0.24449892342090607\n",
      "epoch: 10 step: 893, loss is 0.21257668733596802\n",
      "epoch: 10 step: 894, loss is 0.2637418508529663\n",
      "epoch: 10 step: 895, loss is 0.16027146577835083\n",
      "epoch: 10 step: 896, loss is 0.22429612278938293\n",
      "epoch: 10 step: 897, loss is 0.2439531534910202\n",
      "epoch: 10 step: 898, loss is 0.21889646351337433\n",
      "epoch: 10 step: 899, loss is 0.25801387429237366\n",
      "epoch: 10 step: 900, loss is 0.17967824637889862\n",
      "epoch: 10 step: 901, loss is 0.3430492579936981\n",
      "epoch: 10 step: 902, loss is 0.18115884065628052\n",
      "epoch: 10 step: 903, loss is 0.27009156346321106\n",
      "epoch: 10 step: 904, loss is 0.10665775090456009\n",
      "epoch: 10 step: 905, loss is 0.22101664543151855\n",
      "epoch: 10 step: 906, loss is 0.34475013613700867\n",
      "epoch: 10 step: 907, loss is 0.1463691145181656\n",
      "epoch: 10 step: 908, loss is 0.14809051156044006\n",
      "epoch: 10 step: 909, loss is 0.16122907400131226\n",
      "epoch: 10 step: 910, loss is 0.38071438670158386\n",
      "epoch: 10 step: 911, loss is 0.18290504813194275\n",
      "epoch: 10 step: 912, loss is 0.3954361379146576\n",
      "epoch: 10 step: 913, loss is 0.1867649406194687\n",
      "epoch: 10 step: 914, loss is 0.10650165379047394\n",
      "epoch: 10 step: 915, loss is 0.5131569504737854\n",
      "epoch: 10 step: 916, loss is 0.2558750808238983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 917, loss is 0.2907615900039673\n",
      "epoch: 10 step: 918, loss is 0.3063797354698181\n",
      "epoch: 10 step: 919, loss is 0.1911890208721161\n",
      "epoch: 10 step: 920, loss is 0.1821565181016922\n",
      "epoch: 10 step: 921, loss is 0.21848315000534058\n",
      "epoch: 10 step: 922, loss is 0.146575927734375\n",
      "epoch: 10 step: 923, loss is 0.4841615557670593\n",
      "epoch: 10 step: 924, loss is 0.2965717017650604\n",
      "epoch: 10 step: 925, loss is 0.12778306007385254\n",
      "epoch: 10 step: 926, loss is 0.20887650549411774\n",
      "epoch: 10 step: 927, loss is 0.308943510055542\n",
      "epoch: 10 step: 928, loss is 0.2847990095615387\n",
      "epoch: 10 step: 929, loss is 0.3112887442111969\n",
      "epoch: 10 step: 930, loss is 0.11922211199998856\n",
      "epoch: 10 step: 931, loss is 0.16137591004371643\n",
      "epoch: 10 step: 932, loss is 0.1359390765428543\n",
      "epoch: 10 step: 933, loss is 0.15726780891418457\n",
      "epoch: 10 step: 934, loss is 0.26958155632019043\n",
      "epoch: 10 step: 935, loss is 0.3804449737071991\n",
      "epoch: 10 step: 936, loss is 0.2129313200712204\n",
      "epoch: 10 step: 937, loss is 0.27164971828460693\n",
      "epoch: 11 step: 1, loss is 0.4387103319168091\n",
      "epoch: 11 step: 2, loss is 0.16959789395332336\n",
      "epoch: 11 step: 3, loss is 0.2834583520889282\n",
      "epoch: 11 step: 4, loss is 0.23338456451892853\n",
      "epoch: 11 step: 5, loss is 0.2801665663719177\n",
      "epoch: 11 step: 6, loss is 0.23040670156478882\n",
      "epoch: 11 step: 7, loss is 0.2668331563472748\n",
      "epoch: 11 step: 8, loss is 0.13686241209506989\n",
      "epoch: 11 step: 9, loss is 0.2924881875514984\n",
      "epoch: 11 step: 10, loss is 0.28080514073371887\n",
      "epoch: 11 step: 11, loss is 0.2829901874065399\n",
      "epoch: 11 step: 12, loss is 0.24781444668769836\n",
      "epoch: 11 step: 13, loss is 0.33016008138656616\n",
      "epoch: 11 step: 14, loss is 0.35321366786956787\n",
      "epoch: 11 step: 15, loss is 0.18319745361804962\n",
      "epoch: 11 step: 16, loss is 0.229130819439888\n",
      "epoch: 11 step: 17, loss is 0.13790328800678253\n",
      "epoch: 11 step: 18, loss is 0.282978355884552\n",
      "epoch: 11 step: 19, loss is 0.21193808317184448\n",
      "epoch: 11 step: 20, loss is 0.31366997957229614\n",
      "epoch: 11 step: 21, loss is 0.22869491577148438\n",
      "epoch: 11 step: 22, loss is 0.24479958415031433\n",
      "epoch: 11 step: 23, loss is 0.24579650163650513\n",
      "epoch: 11 step: 24, loss is 0.2504751980304718\n",
      "epoch: 11 step: 25, loss is 0.2935768961906433\n",
      "epoch: 11 step: 26, loss is 0.21218900382518768\n",
      "epoch: 11 step: 27, loss is 0.16471116244792938\n",
      "epoch: 11 step: 28, loss is 0.17780283093452454\n",
      "epoch: 11 step: 29, loss is 0.22385461628437042\n",
      "epoch: 11 step: 30, loss is 0.10269621014595032\n",
      "epoch: 11 step: 31, loss is 0.2046201229095459\n",
      "epoch: 11 step: 32, loss is 0.20915691554546356\n",
      "epoch: 11 step: 33, loss is 0.2678917348384857\n",
      "epoch: 11 step: 34, loss is 0.27091723680496216\n",
      "epoch: 11 step: 35, loss is 0.25991377234458923\n",
      "epoch: 11 step: 36, loss is 0.26756563782691956\n",
      "epoch: 11 step: 37, loss is 0.16327950358390808\n",
      "epoch: 11 step: 38, loss is 0.23753812909126282\n",
      "epoch: 11 step: 39, loss is 0.17503371834754944\n",
      "epoch: 11 step: 40, loss is 0.11421672999858856\n",
      "epoch: 11 step: 41, loss is 0.19912418723106384\n",
      "epoch: 11 step: 42, loss is 0.21575301885604858\n",
      "epoch: 11 step: 43, loss is 0.15439645946025848\n",
      "epoch: 11 step: 44, loss is 0.20478513836860657\n",
      "epoch: 11 step: 45, loss is 0.2714453637599945\n",
      "epoch: 11 step: 46, loss is 0.3205414116382599\n",
      "epoch: 11 step: 47, loss is 0.1949584186077118\n",
      "epoch: 11 step: 48, loss is 0.13137488067150116\n",
      "epoch: 11 step: 49, loss is 0.11502639204263687\n",
      "epoch: 11 step: 50, loss is 0.3226160407066345\n",
      "epoch: 11 step: 51, loss is 0.19110459089279175\n",
      "epoch: 11 step: 52, loss is 0.26041361689567566\n",
      "epoch: 11 step: 53, loss is 0.1802005171775818\n",
      "epoch: 11 step: 54, loss is 0.15773725509643555\n",
      "epoch: 11 step: 55, loss is 0.19823195040225983\n",
      "epoch: 11 step: 56, loss is 0.3173612058162689\n",
      "epoch: 11 step: 57, loss is 0.1611330807209015\n",
      "epoch: 11 step: 58, loss is 0.37917444109916687\n",
      "epoch: 11 step: 59, loss is 0.36809980869293213\n",
      "epoch: 11 step: 60, loss is 0.19706670939922333\n",
      "epoch: 11 step: 61, loss is 0.32363682985305786\n",
      "epoch: 11 step: 62, loss is 0.20793239772319794\n",
      "epoch: 11 step: 63, loss is 0.13180728256702423\n",
      "epoch: 11 step: 64, loss is 0.26468637585639954\n",
      "epoch: 11 step: 65, loss is 0.18580788373947144\n",
      "epoch: 11 step: 66, loss is 0.28681275248527527\n",
      "epoch: 11 step: 67, loss is 0.20695383846759796\n",
      "epoch: 11 step: 68, loss is 0.26396045088768005\n",
      "epoch: 11 step: 69, loss is 0.18054726719856262\n",
      "epoch: 11 step: 70, loss is 0.189383402466774\n",
      "epoch: 11 step: 71, loss is 0.1915239840745926\n",
      "epoch: 11 step: 72, loss is 0.25950464606285095\n",
      "epoch: 11 step: 73, loss is 0.2407965511083603\n",
      "epoch: 11 step: 74, loss is 0.30151161551475525\n",
      "epoch: 11 step: 75, loss is 0.12250049412250519\n",
      "epoch: 11 step: 76, loss is 0.19651372730731964\n",
      "epoch: 11 step: 77, loss is 0.21669629216194153\n",
      "epoch: 11 step: 78, loss is 0.4864911437034607\n",
      "epoch: 11 step: 79, loss is 0.1413242369890213\n",
      "epoch: 11 step: 80, loss is 0.15397416055202484\n",
      "epoch: 11 step: 81, loss is 0.23020771145820618\n",
      "epoch: 11 step: 82, loss is 0.23078013956546783\n",
      "epoch: 11 step: 83, loss is 0.3896613121032715\n",
      "epoch: 11 step: 84, loss is 0.2348160296678543\n",
      "epoch: 11 step: 85, loss is 0.24191172420978546\n",
      "epoch: 11 step: 86, loss is 0.3397238254547119\n",
      "epoch: 11 step: 87, loss is 0.3198893368244171\n",
      "epoch: 11 step: 88, loss is 0.21675272285938263\n",
      "epoch: 11 step: 89, loss is 0.28000888228416443\n",
      "epoch: 11 step: 90, loss is 0.1299147754907608\n",
      "epoch: 11 step: 91, loss is 0.18111960589885712\n",
      "epoch: 11 step: 92, loss is 0.26596421003341675\n",
      "epoch: 11 step: 93, loss is 0.26934537291526794\n",
      "epoch: 11 step: 94, loss is 0.2975488305091858\n",
      "epoch: 11 step: 95, loss is 0.2684057354927063\n",
      "epoch: 11 step: 96, loss is 0.2245086133480072\n",
      "epoch: 11 step: 97, loss is 0.19096341729164124\n",
      "epoch: 11 step: 98, loss is 0.1298261433839798\n",
      "epoch: 11 step: 99, loss is 0.3723597824573517\n",
      "epoch: 11 step: 100, loss is 0.17062053084373474\n",
      "epoch: 11 step: 101, loss is 0.38269874453544617\n",
      "epoch: 11 step: 102, loss is 0.15806250274181366\n",
      "epoch: 11 step: 103, loss is 0.15113984048366547\n",
      "epoch: 11 step: 104, loss is 0.15592330694198608\n",
      "epoch: 11 step: 105, loss is 0.17453093826770782\n",
      "epoch: 11 step: 106, loss is 0.26606351137161255\n",
      "epoch: 11 step: 107, loss is 0.18646952509880066\n",
      "epoch: 11 step: 108, loss is 0.23662076890468597\n",
      "epoch: 11 step: 109, loss is 0.3268771767616272\n",
      "epoch: 11 step: 110, loss is 0.27452030777931213\n",
      "epoch: 11 step: 111, loss is 0.2882364094257355\n",
      "epoch: 11 step: 112, loss is 0.18520905077457428\n",
      "epoch: 11 step: 113, loss is 0.13385142385959625\n",
      "epoch: 11 step: 114, loss is 0.2571156322956085\n",
      "epoch: 11 step: 115, loss is 0.18343666195869446\n",
      "epoch: 11 step: 116, loss is 0.19531193375587463\n",
      "epoch: 11 step: 117, loss is 0.14927074313163757\n",
      "epoch: 11 step: 118, loss is 0.2857017517089844\n",
      "epoch: 11 step: 119, loss is 0.09029264748096466\n",
      "epoch: 11 step: 120, loss is 0.15138773620128632\n",
      "epoch: 11 step: 121, loss is 0.18138769268989563\n",
      "epoch: 11 step: 122, loss is 0.25345027446746826\n",
      "epoch: 11 step: 123, loss is 0.1881004273891449\n",
      "epoch: 11 step: 124, loss is 0.11897497624158859\n",
      "epoch: 11 step: 125, loss is 0.3119469881057739\n",
      "epoch: 11 step: 126, loss is 0.2091178297996521\n",
      "epoch: 11 step: 127, loss is 0.1877116709947586\n",
      "epoch: 11 step: 128, loss is 0.2755798101425171\n",
      "epoch: 11 step: 129, loss is 0.24869924783706665\n",
      "epoch: 11 step: 130, loss is 0.27694886922836304\n",
      "epoch: 11 step: 131, loss is 0.314344584941864\n",
      "epoch: 11 step: 132, loss is 0.17774154245853424\n",
      "epoch: 11 step: 133, loss is 0.16975948214530945\n",
      "epoch: 11 step: 134, loss is 0.23082873225212097\n",
      "epoch: 11 step: 135, loss is 0.1795368492603302\n",
      "epoch: 11 step: 136, loss is 0.2616714537143707\n",
      "epoch: 11 step: 137, loss is 0.238247349858284\n",
      "epoch: 11 step: 138, loss is 0.2340344786643982\n",
      "epoch: 11 step: 139, loss is 0.10501451790332794\n",
      "epoch: 11 step: 140, loss is 0.18705454468727112\n",
      "epoch: 11 step: 141, loss is 0.3213392496109009\n",
      "epoch: 11 step: 142, loss is 0.23612141609191895\n",
      "epoch: 11 step: 143, loss is 0.24940435588359833\n",
      "epoch: 11 step: 144, loss is 0.2789015769958496\n",
      "epoch: 11 step: 145, loss is 0.32546868920326233\n",
      "epoch: 11 step: 146, loss is 0.3135879635810852\n",
      "epoch: 11 step: 147, loss is 0.30558907985687256\n",
      "epoch: 11 step: 148, loss is 0.30949100852012634\n",
      "epoch: 11 step: 149, loss is 0.2666793763637543\n",
      "epoch: 11 step: 150, loss is 0.3229188621044159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 151, loss is 0.20298680663108826\n",
      "epoch: 11 step: 152, loss is 0.23534582555294037\n",
      "epoch: 11 step: 153, loss is 0.2356501817703247\n",
      "epoch: 11 step: 154, loss is 0.16495709121227264\n",
      "epoch: 11 step: 155, loss is 0.21007372438907623\n",
      "epoch: 11 step: 156, loss is 0.2314024567604065\n",
      "epoch: 11 step: 157, loss is 0.29093876481056213\n",
      "epoch: 11 step: 158, loss is 0.26816150546073914\n",
      "epoch: 11 step: 159, loss is 0.2352868914604187\n",
      "epoch: 11 step: 160, loss is 0.37987208366394043\n",
      "epoch: 11 step: 161, loss is 0.25635015964508057\n",
      "epoch: 11 step: 162, loss is 0.10151740163564682\n",
      "epoch: 11 step: 163, loss is 0.23683586716651917\n",
      "epoch: 11 step: 164, loss is 0.3363671898841858\n",
      "epoch: 11 step: 165, loss is 0.1868164986371994\n",
      "epoch: 11 step: 166, loss is 0.24825170636177063\n",
      "epoch: 11 step: 167, loss is 0.32723432779312134\n",
      "epoch: 11 step: 168, loss is 0.30885955691337585\n",
      "epoch: 11 step: 169, loss is 0.2745840549468994\n",
      "epoch: 11 step: 170, loss is 0.23416218161582947\n",
      "epoch: 11 step: 171, loss is 0.253448486328125\n",
      "epoch: 11 step: 172, loss is 0.2537532150745392\n",
      "epoch: 11 step: 173, loss is 0.16979095339775085\n",
      "epoch: 11 step: 174, loss is 0.24278096854686737\n",
      "epoch: 11 step: 175, loss is 0.2783156633377075\n",
      "epoch: 11 step: 176, loss is 0.12340893596410751\n",
      "epoch: 11 step: 177, loss is 0.350386381149292\n",
      "epoch: 11 step: 178, loss is 0.19581593573093414\n",
      "epoch: 11 step: 179, loss is 0.38853615522384644\n",
      "epoch: 11 step: 180, loss is 0.18712589144706726\n",
      "epoch: 11 step: 181, loss is 0.2583927810192108\n",
      "epoch: 11 step: 182, loss is 0.12340713292360306\n",
      "epoch: 11 step: 183, loss is 0.2725081443786621\n",
      "epoch: 11 step: 184, loss is 0.1759524792432785\n",
      "epoch: 11 step: 185, loss is 0.25417980551719666\n",
      "epoch: 11 step: 186, loss is 0.23335987329483032\n",
      "epoch: 11 step: 187, loss is 0.11132290214300156\n",
      "epoch: 11 step: 188, loss is 0.19391821324825287\n",
      "epoch: 11 step: 189, loss is 0.3080309331417084\n",
      "epoch: 11 step: 190, loss is 0.28517818450927734\n",
      "epoch: 11 step: 191, loss is 0.4262937009334564\n",
      "epoch: 11 step: 192, loss is 0.25162839889526367\n",
      "epoch: 11 step: 193, loss is 0.2159559279680252\n",
      "epoch: 11 step: 194, loss is 0.13992100954055786\n",
      "epoch: 11 step: 195, loss is 0.13691838085651398\n",
      "epoch: 11 step: 196, loss is 0.13608919084072113\n",
      "epoch: 11 step: 197, loss is 0.17056156694889069\n",
      "epoch: 11 step: 198, loss is 0.20972830057144165\n",
      "epoch: 11 step: 199, loss is 0.22927118837833405\n",
      "epoch: 11 step: 200, loss is 0.13780885934829712\n",
      "epoch: 11 step: 201, loss is 0.28913614153862\n",
      "epoch: 11 step: 202, loss is 0.13743086159229279\n",
      "epoch: 11 step: 203, loss is 0.28209856152534485\n",
      "epoch: 11 step: 204, loss is 0.1659397929906845\n",
      "epoch: 11 step: 205, loss is 0.2943747341632843\n",
      "epoch: 11 step: 206, loss is 0.2755201458930969\n",
      "epoch: 11 step: 207, loss is 0.13241608440876007\n",
      "epoch: 11 step: 208, loss is 0.2510131001472473\n",
      "epoch: 11 step: 209, loss is 0.1261524260044098\n",
      "epoch: 11 step: 210, loss is 0.3872170150279999\n",
      "epoch: 11 step: 211, loss is 0.2350923717021942\n",
      "epoch: 11 step: 212, loss is 0.1701532006263733\n",
      "epoch: 11 step: 213, loss is 0.1757148802280426\n",
      "epoch: 11 step: 214, loss is 0.22448599338531494\n",
      "epoch: 11 step: 215, loss is 0.19250747561454773\n",
      "epoch: 11 step: 216, loss is 0.3116878271102905\n",
      "epoch: 11 step: 217, loss is 0.15120036900043488\n",
      "epoch: 11 step: 218, loss is 0.22416384518146515\n",
      "epoch: 11 step: 219, loss is 0.2044631540775299\n",
      "epoch: 11 step: 220, loss is 0.10919319838285446\n",
      "epoch: 11 step: 221, loss is 0.18774625658988953\n",
      "epoch: 11 step: 222, loss is 0.22790035605430603\n",
      "epoch: 11 step: 223, loss is 0.1120593249797821\n",
      "epoch: 11 step: 224, loss is 0.3108678162097931\n",
      "epoch: 11 step: 225, loss is 0.17692293226718903\n",
      "epoch: 11 step: 226, loss is 0.14465083181858063\n",
      "epoch: 11 step: 227, loss is 0.23485249280929565\n",
      "epoch: 11 step: 228, loss is 0.2559872269630432\n",
      "epoch: 11 step: 229, loss is 0.45352429151535034\n",
      "epoch: 11 step: 230, loss is 0.1899750530719757\n",
      "epoch: 11 step: 231, loss is 0.2829296886920929\n",
      "epoch: 11 step: 232, loss is 0.4459535777568817\n",
      "epoch: 11 step: 233, loss is 0.2126844972372055\n",
      "epoch: 11 step: 234, loss is 0.30128511786460876\n",
      "epoch: 11 step: 235, loss is 0.2972191274166107\n",
      "epoch: 11 step: 236, loss is 0.2675826847553253\n",
      "epoch: 11 step: 237, loss is 0.13605348765850067\n",
      "epoch: 11 step: 238, loss is 0.19931253790855408\n",
      "epoch: 11 step: 239, loss is 0.11998457461595535\n",
      "epoch: 11 step: 240, loss is 0.1299261450767517\n",
      "epoch: 11 step: 241, loss is 0.12761540710926056\n",
      "epoch: 11 step: 242, loss is 0.15238338708877563\n",
      "epoch: 11 step: 243, loss is 0.0839012861251831\n",
      "epoch: 11 step: 244, loss is 0.15855452418327332\n",
      "epoch: 11 step: 245, loss is 0.21528957784175873\n",
      "epoch: 11 step: 246, loss is 0.3138989508152008\n",
      "epoch: 11 step: 247, loss is 0.2521226406097412\n",
      "epoch: 11 step: 248, loss is 0.17058950662612915\n",
      "epoch: 11 step: 249, loss is 0.21766658127307892\n",
      "epoch: 11 step: 250, loss is 0.15541526675224304\n",
      "epoch: 11 step: 251, loss is 0.20896735787391663\n",
      "epoch: 11 step: 252, loss is 0.21491028368473053\n",
      "epoch: 11 step: 253, loss is 0.1557442545890808\n",
      "epoch: 11 step: 254, loss is 0.18421252071857452\n",
      "epoch: 11 step: 255, loss is 0.3226200044155121\n",
      "epoch: 11 step: 256, loss is 0.16541072726249695\n",
      "epoch: 11 step: 257, loss is 0.05920327827334404\n",
      "epoch: 11 step: 258, loss is 0.4456098675727844\n",
      "epoch: 11 step: 259, loss is 0.22862470149993896\n",
      "epoch: 11 step: 260, loss is 0.2068977653980255\n",
      "epoch: 11 step: 261, loss is 0.18410591781139374\n",
      "epoch: 11 step: 262, loss is 0.20408128201961517\n",
      "epoch: 11 step: 263, loss is 0.13454797863960266\n",
      "epoch: 11 step: 264, loss is 0.21039721369743347\n",
      "epoch: 11 step: 265, loss is 0.2617833614349365\n",
      "epoch: 11 step: 266, loss is 0.20212526619434357\n",
      "epoch: 11 step: 267, loss is 0.20775079727172852\n",
      "epoch: 11 step: 268, loss is 0.20024071633815765\n",
      "epoch: 11 step: 269, loss is 0.22959081828594208\n",
      "epoch: 11 step: 270, loss is 0.38050010800361633\n",
      "epoch: 11 step: 271, loss is 0.15410451591014862\n",
      "epoch: 11 step: 272, loss is 0.3947499692440033\n",
      "epoch: 11 step: 273, loss is 0.20861461758613586\n",
      "epoch: 11 step: 274, loss is 0.21228648722171783\n",
      "epoch: 11 step: 275, loss is 0.1593369096517563\n",
      "epoch: 11 step: 276, loss is 0.18339836597442627\n",
      "epoch: 11 step: 277, loss is 0.28220683336257935\n",
      "epoch: 11 step: 278, loss is 0.17060242593288422\n",
      "epoch: 11 step: 279, loss is 0.20598037540912628\n",
      "epoch: 11 step: 280, loss is 0.21444055438041687\n",
      "epoch: 11 step: 281, loss is 0.23748403787612915\n",
      "epoch: 11 step: 282, loss is 0.2094644010066986\n",
      "epoch: 11 step: 283, loss is 0.1586831510066986\n",
      "epoch: 11 step: 284, loss is 0.23766423761844635\n",
      "epoch: 11 step: 285, loss is 0.12047912925481796\n",
      "epoch: 11 step: 286, loss is 0.4832301139831543\n",
      "epoch: 11 step: 287, loss is 0.21400125324726105\n",
      "epoch: 11 step: 288, loss is 0.25749272108078003\n",
      "epoch: 11 step: 289, loss is 0.2819865047931671\n",
      "epoch: 11 step: 290, loss is 0.08458690345287323\n",
      "epoch: 11 step: 291, loss is 0.2847677767276764\n",
      "epoch: 11 step: 292, loss is 0.18992650508880615\n",
      "epoch: 11 step: 293, loss is 0.25765419006347656\n",
      "epoch: 11 step: 294, loss is 0.3441169559955597\n",
      "epoch: 11 step: 295, loss is 0.15737734735012054\n",
      "epoch: 11 step: 296, loss is 0.37956133484840393\n",
      "epoch: 11 step: 297, loss is 0.14309343695640564\n",
      "epoch: 11 step: 298, loss is 0.21930767595767975\n",
      "epoch: 11 step: 299, loss is 0.14021214842796326\n",
      "epoch: 11 step: 300, loss is 0.18762876093387604\n",
      "epoch: 11 step: 301, loss is 0.24044786393642426\n",
      "epoch: 11 step: 302, loss is 0.23232480883598328\n",
      "epoch: 11 step: 303, loss is 0.2992008924484253\n",
      "epoch: 11 step: 304, loss is 0.23235009610652924\n",
      "epoch: 11 step: 305, loss is 0.1769300252199173\n",
      "epoch: 11 step: 306, loss is 0.36910194158554077\n",
      "epoch: 11 step: 307, loss is 0.20227696001529694\n",
      "epoch: 11 step: 308, loss is 0.18821649253368378\n",
      "epoch: 11 step: 309, loss is 0.27709272503852844\n",
      "epoch: 11 step: 310, loss is 0.1899380087852478\n",
      "epoch: 11 step: 311, loss is 0.123310387134552\n",
      "epoch: 11 step: 312, loss is 0.1471053510904312\n",
      "epoch: 11 step: 313, loss is 0.20457424223423004\n",
      "epoch: 11 step: 314, loss is 0.1405317336320877\n",
      "epoch: 11 step: 315, loss is 0.23458035290241241\n",
      "epoch: 11 step: 316, loss is 0.1968526691198349\n",
      "epoch: 11 step: 317, loss is 0.18516576290130615\n",
      "epoch: 11 step: 318, loss is 0.14693251252174377\n",
      "epoch: 11 step: 319, loss is 0.2659837007522583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 320, loss is 0.32378289103507996\n",
      "epoch: 11 step: 321, loss is 0.23056937754154205\n",
      "epoch: 11 step: 322, loss is 0.1795586347579956\n",
      "epoch: 11 step: 323, loss is 0.366504043340683\n",
      "epoch: 11 step: 324, loss is 0.20613965392112732\n",
      "epoch: 11 step: 325, loss is 0.42370671033859253\n",
      "epoch: 11 step: 326, loss is 0.2501322329044342\n",
      "epoch: 11 step: 327, loss is 0.24955138564109802\n",
      "epoch: 11 step: 328, loss is 0.19953666627407074\n",
      "epoch: 11 step: 329, loss is 0.25661033391952515\n",
      "epoch: 11 step: 330, loss is 0.21238066256046295\n",
      "epoch: 11 step: 331, loss is 0.12449460476636887\n",
      "epoch: 11 step: 332, loss is 0.23191319406032562\n",
      "epoch: 11 step: 333, loss is 0.19770939648151398\n",
      "epoch: 11 step: 334, loss is 0.2760572135448456\n",
      "epoch: 11 step: 335, loss is 0.29626286029815674\n",
      "epoch: 11 step: 336, loss is 0.2782624065876007\n",
      "epoch: 11 step: 337, loss is 0.2785361111164093\n",
      "epoch: 11 step: 338, loss is 0.2842846214771271\n",
      "epoch: 11 step: 339, loss is 0.3083437979221344\n",
      "epoch: 11 step: 340, loss is 0.29464077949523926\n",
      "epoch: 11 step: 341, loss is 0.3697755038738251\n",
      "epoch: 11 step: 342, loss is 0.15020506083965302\n",
      "epoch: 11 step: 343, loss is 0.14566583931446075\n",
      "epoch: 11 step: 344, loss is 0.28844353556632996\n",
      "epoch: 11 step: 345, loss is 0.0834779292345047\n",
      "epoch: 11 step: 346, loss is 0.2762681841850281\n",
      "epoch: 11 step: 347, loss is 0.33686670660972595\n",
      "epoch: 11 step: 348, loss is 0.11292307823896408\n",
      "epoch: 11 step: 349, loss is 0.343105286359787\n",
      "epoch: 11 step: 350, loss is 0.278703898191452\n",
      "epoch: 11 step: 351, loss is 0.1576239913702011\n",
      "epoch: 11 step: 352, loss is 0.4225202202796936\n",
      "epoch: 11 step: 353, loss is 0.26956722140312195\n",
      "epoch: 11 step: 354, loss is 0.15675102174282074\n",
      "epoch: 11 step: 355, loss is 0.25213101506233215\n",
      "epoch: 11 step: 356, loss is 0.29709312319755554\n",
      "epoch: 11 step: 357, loss is 0.19464042782783508\n",
      "epoch: 11 step: 358, loss is 0.2401951253414154\n",
      "epoch: 11 step: 359, loss is 0.27336201071739197\n",
      "epoch: 11 step: 360, loss is 0.23373018205165863\n",
      "epoch: 11 step: 361, loss is 0.16038087010383606\n",
      "epoch: 11 step: 362, loss is 0.32085031270980835\n",
      "epoch: 11 step: 363, loss is 0.3733987510204315\n",
      "epoch: 11 step: 364, loss is 0.12263905256986618\n",
      "epoch: 11 step: 365, loss is 0.2688411474227905\n",
      "epoch: 11 step: 366, loss is 0.33567753434181213\n",
      "epoch: 11 step: 367, loss is 0.31295138597488403\n",
      "epoch: 11 step: 368, loss is 0.2282739281654358\n",
      "epoch: 11 step: 369, loss is 0.4533413052558899\n",
      "epoch: 11 step: 370, loss is 0.311772882938385\n",
      "epoch: 11 step: 371, loss is 0.2724460959434509\n",
      "epoch: 11 step: 372, loss is 0.09383311122655869\n",
      "epoch: 11 step: 373, loss is 0.4187372326850891\n",
      "epoch: 11 step: 374, loss is 0.3229241371154785\n",
      "epoch: 11 step: 375, loss is 0.14431285858154297\n",
      "epoch: 11 step: 376, loss is 0.3008691072463989\n",
      "epoch: 11 step: 377, loss is 0.19631516933441162\n",
      "epoch: 11 step: 378, loss is 0.23713329434394836\n",
      "epoch: 11 step: 379, loss is 0.1435910165309906\n",
      "epoch: 11 step: 380, loss is 0.23841579258441925\n",
      "epoch: 11 step: 381, loss is 0.1755748838186264\n",
      "epoch: 11 step: 382, loss is 0.19355015456676483\n",
      "epoch: 11 step: 383, loss is 0.24481576681137085\n",
      "epoch: 11 step: 384, loss is 0.5086761713027954\n",
      "epoch: 11 step: 385, loss is 0.38286107778549194\n",
      "epoch: 11 step: 386, loss is 0.16087202727794647\n",
      "epoch: 11 step: 387, loss is 0.3137991428375244\n",
      "epoch: 11 step: 388, loss is 0.28201815485954285\n",
      "epoch: 11 step: 389, loss is 0.23484575748443604\n",
      "epoch: 11 step: 390, loss is 0.2816265821456909\n",
      "epoch: 11 step: 391, loss is 0.3324171006679535\n",
      "epoch: 11 step: 392, loss is 0.2591918408870697\n",
      "epoch: 11 step: 393, loss is 0.25772082805633545\n",
      "epoch: 11 step: 394, loss is 0.20366472005844116\n",
      "epoch: 11 step: 395, loss is 0.17812597751617432\n",
      "epoch: 11 step: 396, loss is 0.3837645351886749\n",
      "epoch: 11 step: 397, loss is 0.3231092393398285\n",
      "epoch: 11 step: 398, loss is 0.2961824834346771\n",
      "epoch: 11 step: 399, loss is 0.23029375076293945\n",
      "epoch: 11 step: 400, loss is 0.21218536794185638\n",
      "epoch: 11 step: 401, loss is 0.3793604373931885\n",
      "epoch: 11 step: 402, loss is 0.28692805767059326\n",
      "epoch: 11 step: 403, loss is 0.1288062185049057\n",
      "epoch: 11 step: 404, loss is 0.23243018984794617\n",
      "epoch: 11 step: 405, loss is 0.2106369286775589\n",
      "epoch: 11 step: 406, loss is 0.23652951419353485\n",
      "epoch: 11 step: 407, loss is 0.19052083790302277\n",
      "epoch: 11 step: 408, loss is 0.24189847707748413\n",
      "epoch: 11 step: 409, loss is 0.09194380789995193\n",
      "epoch: 11 step: 410, loss is 0.1478525847196579\n",
      "epoch: 11 step: 411, loss is 0.13189417123794556\n",
      "epoch: 11 step: 412, loss is 0.299420565366745\n",
      "epoch: 11 step: 413, loss is 0.11174684017896652\n",
      "epoch: 11 step: 414, loss is 0.10260069370269775\n",
      "epoch: 11 step: 415, loss is 0.1661929488182068\n",
      "epoch: 11 step: 416, loss is 0.35119181871414185\n",
      "epoch: 11 step: 417, loss is 0.23202110826969147\n",
      "epoch: 11 step: 418, loss is 0.4334835112094879\n",
      "epoch: 11 step: 419, loss is 0.2620699405670166\n",
      "epoch: 11 step: 420, loss is 0.3854309320449829\n",
      "epoch: 11 step: 421, loss is 0.2903323769569397\n",
      "epoch: 11 step: 422, loss is 0.30265700817108154\n",
      "epoch: 11 step: 423, loss is 0.14788401126861572\n",
      "epoch: 11 step: 424, loss is 0.2331957072019577\n",
      "epoch: 11 step: 425, loss is 0.2251831740140915\n",
      "epoch: 11 step: 426, loss is 0.19399477541446686\n",
      "epoch: 11 step: 427, loss is 0.2407565414905548\n",
      "epoch: 11 step: 428, loss is 0.1764720380306244\n",
      "epoch: 11 step: 429, loss is 0.2505858540534973\n",
      "epoch: 11 step: 430, loss is 0.1907462477684021\n",
      "epoch: 11 step: 431, loss is 0.2626754939556122\n",
      "epoch: 11 step: 432, loss is 0.20149001479148865\n",
      "epoch: 11 step: 433, loss is 0.18335601687431335\n",
      "epoch: 11 step: 434, loss is 0.25041916966438293\n",
      "epoch: 11 step: 435, loss is 0.3020322918891907\n",
      "epoch: 11 step: 436, loss is 0.2760842442512512\n",
      "epoch: 11 step: 437, loss is 0.304416298866272\n",
      "epoch: 11 step: 438, loss is 0.3367616534233093\n",
      "epoch: 11 step: 439, loss is 0.2506360113620758\n",
      "epoch: 11 step: 440, loss is 0.08686316013336182\n",
      "epoch: 11 step: 441, loss is 0.24791458249092102\n",
      "epoch: 11 step: 442, loss is 0.10558000206947327\n",
      "epoch: 11 step: 443, loss is 0.25942274928092957\n",
      "epoch: 11 step: 444, loss is 0.21764607727527618\n",
      "epoch: 11 step: 445, loss is 0.20830923318862915\n",
      "epoch: 11 step: 446, loss is 0.12423033267259598\n",
      "epoch: 11 step: 447, loss is 0.1864738166332245\n",
      "epoch: 11 step: 448, loss is 0.2263583540916443\n",
      "epoch: 11 step: 449, loss is 0.14911001920700073\n",
      "epoch: 11 step: 450, loss is 0.2716226279735565\n",
      "epoch: 11 step: 451, loss is 0.2799001634120941\n",
      "epoch: 11 step: 452, loss is 0.18910115957260132\n",
      "epoch: 11 step: 453, loss is 0.25022271275520325\n",
      "epoch: 11 step: 454, loss is 0.20578612387180328\n",
      "epoch: 11 step: 455, loss is 0.1758519411087036\n",
      "epoch: 11 step: 456, loss is 0.34886518120765686\n",
      "epoch: 11 step: 457, loss is 0.19453823566436768\n",
      "epoch: 11 step: 458, loss is 0.2627180516719818\n",
      "epoch: 11 step: 459, loss is 0.26780107617378235\n",
      "epoch: 11 step: 460, loss is 0.1401624232530594\n",
      "epoch: 11 step: 461, loss is 0.2172558456659317\n",
      "epoch: 11 step: 462, loss is 0.23684531450271606\n",
      "epoch: 11 step: 463, loss is 0.1740284264087677\n",
      "epoch: 11 step: 464, loss is 0.456895649433136\n",
      "epoch: 11 step: 465, loss is 0.10252432525157928\n",
      "epoch: 11 step: 466, loss is 0.3354286551475525\n",
      "epoch: 11 step: 467, loss is 0.1810137778520584\n",
      "epoch: 11 step: 468, loss is 0.15221066772937775\n",
      "epoch: 11 step: 469, loss is 0.2788401246070862\n",
      "epoch: 11 step: 470, loss is 0.23538482189178467\n",
      "epoch: 11 step: 471, loss is 0.33868613839149475\n",
      "epoch: 11 step: 472, loss is 0.31622397899627686\n",
      "epoch: 11 step: 473, loss is 0.15893828868865967\n",
      "epoch: 11 step: 474, loss is 0.2574174702167511\n",
      "epoch: 11 step: 475, loss is 0.22160139679908752\n",
      "epoch: 11 step: 476, loss is 0.15342071652412415\n",
      "epoch: 11 step: 477, loss is 0.28431057929992676\n",
      "epoch: 11 step: 478, loss is 0.3103838860988617\n",
      "epoch: 11 step: 479, loss is 0.3400650918483734\n",
      "epoch: 11 step: 480, loss is 0.173952117562294\n",
      "epoch: 11 step: 481, loss is 0.3380739092826843\n",
      "epoch: 11 step: 482, loss is 0.13839252293109894\n",
      "epoch: 11 step: 483, loss is 0.13784176111221313\n",
      "epoch: 11 step: 484, loss is 0.41342464089393616\n",
      "epoch: 11 step: 485, loss is 0.28164243698120117\n",
      "epoch: 11 step: 486, loss is 0.3048563599586487\n",
      "epoch: 11 step: 487, loss is 0.33836156129837036\n",
      "epoch: 11 step: 488, loss is 0.36410441994667053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 489, loss is 0.2623797059059143\n",
      "epoch: 11 step: 490, loss is 0.2185833901166916\n",
      "epoch: 11 step: 491, loss is 0.29822874069213867\n",
      "epoch: 11 step: 492, loss is 0.22811046242713928\n",
      "epoch: 11 step: 493, loss is 0.1964731365442276\n",
      "epoch: 11 step: 494, loss is 0.4024004638195038\n",
      "epoch: 11 step: 495, loss is 0.14471325278282166\n",
      "epoch: 11 step: 496, loss is 0.28988489508628845\n",
      "epoch: 11 step: 497, loss is 0.19361326098442078\n",
      "epoch: 11 step: 498, loss is 0.15470056235790253\n",
      "epoch: 11 step: 499, loss is 0.30158987641334534\n",
      "epoch: 11 step: 500, loss is 0.23236083984375\n",
      "epoch: 11 step: 501, loss is 0.3859204053878784\n",
      "epoch: 11 step: 502, loss is 0.2740105092525482\n",
      "epoch: 11 step: 503, loss is 0.1317906230688095\n",
      "epoch: 11 step: 504, loss is 0.13157963752746582\n",
      "epoch: 11 step: 505, loss is 0.3041607737541199\n",
      "epoch: 11 step: 506, loss is 0.1386745572090149\n",
      "epoch: 11 step: 507, loss is 0.20130673050880432\n",
      "epoch: 11 step: 508, loss is 0.3246914744377136\n",
      "epoch: 11 step: 509, loss is 0.3623609244823456\n",
      "epoch: 11 step: 510, loss is 0.34669479727745056\n",
      "epoch: 11 step: 511, loss is 0.1603488177061081\n",
      "epoch: 11 step: 512, loss is 0.23004986345767975\n",
      "epoch: 11 step: 513, loss is 0.1653888076543808\n",
      "epoch: 11 step: 514, loss is 0.2488521784543991\n",
      "epoch: 11 step: 515, loss is 0.38220250606536865\n",
      "epoch: 11 step: 516, loss is 0.07952877134084702\n",
      "epoch: 11 step: 517, loss is 0.30651217699050903\n",
      "epoch: 11 step: 518, loss is 0.1980467587709427\n",
      "epoch: 11 step: 519, loss is 0.19308319687843323\n",
      "epoch: 11 step: 520, loss is 0.2273716777563095\n",
      "epoch: 11 step: 521, loss is 0.19077005982398987\n",
      "epoch: 11 step: 522, loss is 0.19329842925071716\n",
      "epoch: 11 step: 523, loss is 0.24677355587482452\n",
      "epoch: 11 step: 524, loss is 0.17912155389785767\n",
      "epoch: 11 step: 525, loss is 0.14086629450321198\n",
      "epoch: 11 step: 526, loss is 0.2509506642818451\n",
      "epoch: 11 step: 527, loss is 0.17327697575092316\n",
      "epoch: 11 step: 528, loss is 0.4111280143260956\n",
      "epoch: 11 step: 529, loss is 0.25493237376213074\n",
      "epoch: 11 step: 530, loss is 0.4354378879070282\n",
      "epoch: 11 step: 531, loss is 0.14121253788471222\n",
      "epoch: 11 step: 532, loss is 0.2558048367500305\n",
      "epoch: 11 step: 533, loss is 0.14070551097393036\n",
      "epoch: 11 step: 534, loss is 0.208405539393425\n",
      "epoch: 11 step: 535, loss is 0.21285437047481537\n",
      "epoch: 11 step: 536, loss is 0.3280927240848541\n",
      "epoch: 11 step: 537, loss is 0.21879570186138153\n",
      "epoch: 11 step: 538, loss is 0.19074727594852448\n",
      "epoch: 11 step: 539, loss is 0.23569966852664948\n",
      "epoch: 11 step: 540, loss is 0.44399547576904297\n",
      "epoch: 11 step: 541, loss is 0.2624991834163666\n",
      "epoch: 11 step: 542, loss is 0.24626225233078003\n",
      "epoch: 11 step: 543, loss is 0.2510244846343994\n",
      "epoch: 11 step: 544, loss is 0.16935712099075317\n",
      "epoch: 11 step: 545, loss is 0.27478641271591187\n",
      "epoch: 11 step: 546, loss is 0.24501612782478333\n",
      "epoch: 11 step: 547, loss is 0.2210538685321808\n",
      "epoch: 11 step: 548, loss is 0.19593000411987305\n",
      "epoch: 11 step: 549, loss is 0.33757880330085754\n",
      "epoch: 11 step: 550, loss is 0.2703925669193268\n",
      "epoch: 11 step: 551, loss is 0.19492553174495697\n",
      "epoch: 11 step: 552, loss is 0.26941147446632385\n",
      "epoch: 11 step: 553, loss is 0.4508934020996094\n",
      "epoch: 11 step: 554, loss is 0.3028970956802368\n",
      "epoch: 11 step: 555, loss is 0.18009303510189056\n",
      "epoch: 11 step: 556, loss is 0.18580013513565063\n",
      "epoch: 11 step: 557, loss is 0.369116872549057\n",
      "epoch: 11 step: 558, loss is 0.2730809152126312\n",
      "epoch: 11 step: 559, loss is 0.12693741917610168\n",
      "epoch: 11 step: 560, loss is 0.2410905957221985\n",
      "epoch: 11 step: 561, loss is 0.2278139740228653\n",
      "epoch: 11 step: 562, loss is 0.1808656007051468\n",
      "epoch: 11 step: 563, loss is 0.19123579561710358\n",
      "epoch: 11 step: 564, loss is 0.23389601707458496\n",
      "epoch: 11 step: 565, loss is 0.11727697402238846\n",
      "epoch: 11 step: 566, loss is 0.27807050943374634\n",
      "epoch: 11 step: 567, loss is 0.19042877852916718\n",
      "epoch: 11 step: 568, loss is 0.3969365060329437\n",
      "epoch: 11 step: 569, loss is 0.25080856680870056\n",
      "epoch: 11 step: 570, loss is 0.22140228748321533\n",
      "epoch: 11 step: 571, loss is 0.11099614202976227\n",
      "epoch: 11 step: 572, loss is 0.2043331116437912\n",
      "epoch: 11 step: 573, loss is 0.19359028339385986\n",
      "epoch: 11 step: 574, loss is 0.21773594617843628\n",
      "epoch: 11 step: 575, loss is 0.24786444008350372\n",
      "epoch: 11 step: 576, loss is 0.21085794270038605\n",
      "epoch: 11 step: 577, loss is 0.26639366149902344\n",
      "epoch: 11 step: 578, loss is 0.21293194591999054\n",
      "epoch: 11 step: 579, loss is 0.20591755211353302\n",
      "epoch: 11 step: 580, loss is 0.21857339143753052\n",
      "epoch: 11 step: 581, loss is 0.28103315830230713\n",
      "epoch: 11 step: 582, loss is 0.2699509859085083\n",
      "epoch: 11 step: 583, loss is 0.24077746272087097\n",
      "epoch: 11 step: 584, loss is 0.3637680113315582\n",
      "epoch: 11 step: 585, loss is 0.3797190189361572\n",
      "epoch: 11 step: 586, loss is 0.24356500804424286\n",
      "epoch: 11 step: 587, loss is 0.10130079835653305\n",
      "epoch: 11 step: 588, loss is 0.2616274952888489\n",
      "epoch: 11 step: 589, loss is 0.25266382098197937\n",
      "epoch: 11 step: 590, loss is 0.07846888154745102\n",
      "epoch: 11 step: 591, loss is 0.32223445177078247\n",
      "epoch: 11 step: 592, loss is 0.2858140170574188\n",
      "epoch: 11 step: 593, loss is 0.2514502704143524\n",
      "epoch: 11 step: 594, loss is 0.30601027607917786\n",
      "epoch: 11 step: 595, loss is 0.38158977031707764\n",
      "epoch: 11 step: 596, loss is 0.22501736879348755\n",
      "epoch: 11 step: 597, loss is 0.17865322530269623\n",
      "epoch: 11 step: 598, loss is 0.22245143353939056\n",
      "epoch: 11 step: 599, loss is 0.2857794761657715\n",
      "epoch: 11 step: 600, loss is 0.07943253219127655\n",
      "epoch: 11 step: 601, loss is 0.3193977475166321\n",
      "epoch: 11 step: 602, loss is 0.1748408079147339\n",
      "epoch: 11 step: 603, loss is 0.2227577418088913\n",
      "epoch: 11 step: 604, loss is 0.1654573231935501\n",
      "epoch: 11 step: 605, loss is 0.3031006455421448\n",
      "epoch: 11 step: 606, loss is 0.2390012890100479\n",
      "epoch: 11 step: 607, loss is 0.25309571623802185\n",
      "epoch: 11 step: 608, loss is 0.2996108829975128\n",
      "epoch: 11 step: 609, loss is 0.34758463501930237\n",
      "epoch: 11 step: 610, loss is 0.1997247338294983\n",
      "epoch: 11 step: 611, loss is 0.17911739647388458\n",
      "epoch: 11 step: 612, loss is 0.24964454770088196\n",
      "epoch: 11 step: 613, loss is 0.30503934621810913\n",
      "epoch: 11 step: 614, loss is 0.14595435559749603\n",
      "epoch: 11 step: 615, loss is 0.1926078200340271\n",
      "epoch: 11 step: 616, loss is 0.28145742416381836\n",
      "epoch: 11 step: 617, loss is 0.2366446852684021\n",
      "epoch: 11 step: 618, loss is 0.25600042939186096\n",
      "epoch: 11 step: 619, loss is 0.3409442603588104\n",
      "epoch: 11 step: 620, loss is 0.24540624022483826\n",
      "epoch: 11 step: 621, loss is 0.25230589509010315\n",
      "epoch: 11 step: 622, loss is 0.3322509825229645\n",
      "epoch: 11 step: 623, loss is 0.28036072850227356\n",
      "epoch: 11 step: 624, loss is 0.2021770030260086\n",
      "epoch: 11 step: 625, loss is 0.30883529782295227\n",
      "epoch: 11 step: 626, loss is 0.3494599163532257\n",
      "epoch: 11 step: 627, loss is 0.38929876685142517\n",
      "epoch: 11 step: 628, loss is 0.21338015794754028\n",
      "epoch: 11 step: 629, loss is 0.16215234994888306\n",
      "epoch: 11 step: 630, loss is 0.29553350806236267\n",
      "epoch: 11 step: 631, loss is 0.12604454159736633\n",
      "epoch: 11 step: 632, loss is 0.25241318345069885\n",
      "epoch: 11 step: 633, loss is 0.2769031524658203\n",
      "epoch: 11 step: 634, loss is 0.26737257838249207\n",
      "epoch: 11 step: 635, loss is 0.2572324872016907\n",
      "epoch: 11 step: 636, loss is 0.5384015440940857\n",
      "epoch: 11 step: 637, loss is 0.16908232867717743\n",
      "epoch: 11 step: 638, loss is 0.22941358387470245\n",
      "epoch: 11 step: 639, loss is 0.14124798774719238\n",
      "epoch: 11 step: 640, loss is 0.2732465863227844\n",
      "epoch: 11 step: 641, loss is 0.2653234004974365\n",
      "epoch: 11 step: 642, loss is 0.12791171669960022\n",
      "epoch: 11 step: 643, loss is 0.13751040399074554\n",
      "epoch: 11 step: 644, loss is 0.2963968813419342\n",
      "epoch: 11 step: 645, loss is 0.23254531621932983\n",
      "epoch: 11 step: 646, loss is 0.32526376843452454\n",
      "epoch: 11 step: 647, loss is 0.3324268162250519\n",
      "epoch: 11 step: 648, loss is 0.15482904016971588\n",
      "epoch: 11 step: 649, loss is 0.1522333025932312\n",
      "epoch: 11 step: 650, loss is 0.20056374371051788\n",
      "epoch: 11 step: 651, loss is 0.23351256549358368\n",
      "epoch: 11 step: 652, loss is 0.23571641743183136\n",
      "epoch: 11 step: 653, loss is 0.29708579182624817\n",
      "epoch: 11 step: 654, loss is 0.3205082416534424\n",
      "epoch: 11 step: 655, loss is 0.30401092767715454\n",
      "epoch: 11 step: 656, loss is 0.1461605429649353\n",
      "epoch: 11 step: 657, loss is 0.30606067180633545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 658, loss is 0.21436333656311035\n",
      "epoch: 11 step: 659, loss is 0.2725476920604706\n",
      "epoch: 11 step: 660, loss is 0.29578202962875366\n",
      "epoch: 11 step: 661, loss is 0.32280147075653076\n",
      "epoch: 11 step: 662, loss is 0.14203879237174988\n",
      "epoch: 11 step: 663, loss is 0.32393330335617065\n",
      "epoch: 11 step: 664, loss is 0.18503111600875854\n",
      "epoch: 11 step: 665, loss is 0.26330143213272095\n",
      "epoch: 11 step: 666, loss is 0.3009363114833832\n",
      "epoch: 11 step: 667, loss is 0.19631598889827728\n",
      "epoch: 11 step: 668, loss is 0.1686640977859497\n",
      "epoch: 11 step: 669, loss is 0.3505370020866394\n",
      "epoch: 11 step: 670, loss is 0.36620065569877625\n",
      "epoch: 11 step: 671, loss is 0.18999333679676056\n",
      "epoch: 11 step: 672, loss is 0.2521118223667145\n",
      "epoch: 11 step: 673, loss is 0.14437033236026764\n",
      "epoch: 11 step: 674, loss is 0.1822851002216339\n",
      "epoch: 11 step: 675, loss is 0.2309296727180481\n",
      "epoch: 11 step: 676, loss is 0.22252970933914185\n",
      "epoch: 11 step: 677, loss is 0.1883167326450348\n",
      "epoch: 11 step: 678, loss is 0.2511846721172333\n",
      "epoch: 11 step: 679, loss is 0.12266385555267334\n",
      "epoch: 11 step: 680, loss is 0.10759761929512024\n",
      "epoch: 11 step: 681, loss is 0.316715806722641\n",
      "epoch: 11 step: 682, loss is 0.21896277368068695\n",
      "epoch: 11 step: 683, loss is 0.5613769888877869\n",
      "epoch: 11 step: 684, loss is 0.17560549080371857\n",
      "epoch: 11 step: 685, loss is 0.2909839153289795\n",
      "epoch: 11 step: 686, loss is 0.2571881413459778\n",
      "epoch: 11 step: 687, loss is 0.3509358763694763\n",
      "epoch: 11 step: 688, loss is 0.3181644678115845\n",
      "epoch: 11 step: 689, loss is 0.20164445042610168\n",
      "epoch: 11 step: 690, loss is 0.46525782346725464\n",
      "epoch: 11 step: 691, loss is 0.23407450318336487\n",
      "epoch: 11 step: 692, loss is 0.1405797004699707\n",
      "epoch: 11 step: 693, loss is 0.33968472480773926\n",
      "epoch: 11 step: 694, loss is 0.4559926688671112\n",
      "epoch: 11 step: 695, loss is 0.23694373667240143\n",
      "epoch: 11 step: 696, loss is 0.17411360144615173\n",
      "epoch: 11 step: 697, loss is 0.3128909170627594\n",
      "epoch: 11 step: 698, loss is 0.2042873352766037\n",
      "epoch: 11 step: 699, loss is 0.37040555477142334\n",
      "epoch: 11 step: 700, loss is 0.13907204568386078\n",
      "epoch: 11 step: 701, loss is 0.2674823999404907\n",
      "epoch: 11 step: 702, loss is 0.23183706402778625\n",
      "epoch: 11 step: 703, loss is 0.30401280522346497\n",
      "epoch: 11 step: 704, loss is 0.21383222937583923\n",
      "epoch: 11 step: 705, loss is 0.3715671896934509\n",
      "epoch: 11 step: 706, loss is 0.2625427544116974\n",
      "epoch: 11 step: 707, loss is 0.25503456592559814\n",
      "epoch: 11 step: 708, loss is 0.2820183038711548\n",
      "epoch: 11 step: 709, loss is 0.18795326352119446\n",
      "epoch: 11 step: 710, loss is 0.15949101746082306\n",
      "epoch: 11 step: 711, loss is 0.16390784084796906\n",
      "epoch: 11 step: 712, loss is 0.18999503552913666\n",
      "epoch: 11 step: 713, loss is 0.1163167655467987\n",
      "epoch: 11 step: 714, loss is 0.1962767243385315\n",
      "epoch: 11 step: 715, loss is 0.22815711796283722\n",
      "epoch: 11 step: 716, loss is 0.5025385618209839\n",
      "epoch: 11 step: 717, loss is 0.28471702337265015\n",
      "epoch: 11 step: 718, loss is 0.3476736843585968\n",
      "epoch: 11 step: 719, loss is 0.28600218892097473\n",
      "epoch: 11 step: 720, loss is 0.36483871936798096\n",
      "epoch: 11 step: 721, loss is 0.21585465967655182\n",
      "epoch: 11 step: 722, loss is 0.21147198975086212\n",
      "epoch: 11 step: 723, loss is 0.1973220407962799\n",
      "epoch: 11 step: 724, loss is 0.2536633312702179\n",
      "epoch: 11 step: 725, loss is 0.2880913317203522\n",
      "epoch: 11 step: 726, loss is 0.2503369450569153\n",
      "epoch: 11 step: 727, loss is 0.23463743925094604\n",
      "epoch: 11 step: 728, loss is 0.36689460277557373\n",
      "epoch: 11 step: 729, loss is 0.1523396223783493\n",
      "epoch: 11 step: 730, loss is 0.1979723572731018\n",
      "epoch: 11 step: 731, loss is 0.18269439041614532\n",
      "epoch: 11 step: 732, loss is 0.22501927614212036\n",
      "epoch: 11 step: 733, loss is 0.10563886910676956\n",
      "epoch: 11 step: 734, loss is 0.1148100271821022\n",
      "epoch: 11 step: 735, loss is 0.16755934059619904\n",
      "epoch: 11 step: 736, loss is 0.11111295968294144\n",
      "epoch: 11 step: 737, loss is 0.12966249883174896\n",
      "epoch: 11 step: 738, loss is 0.24002830684185028\n",
      "epoch: 11 step: 739, loss is 0.23985256254673004\n",
      "epoch: 11 step: 740, loss is 0.22727206349372864\n",
      "epoch: 11 step: 741, loss is 0.39082396030426025\n",
      "epoch: 11 step: 742, loss is 0.2538483142852783\n",
      "epoch: 11 step: 743, loss is 0.20977428555488586\n",
      "epoch: 11 step: 744, loss is 0.2280966341495514\n",
      "epoch: 11 step: 745, loss is 0.30211541056632996\n",
      "epoch: 11 step: 746, loss is 0.2654499411582947\n",
      "epoch: 11 step: 747, loss is 0.1690456122159958\n",
      "epoch: 11 step: 748, loss is 0.18940530717372894\n",
      "epoch: 11 step: 749, loss is 0.29237765073776245\n",
      "epoch: 11 step: 750, loss is 0.1405084878206253\n",
      "epoch: 11 step: 751, loss is 0.35451647639274597\n",
      "epoch: 11 step: 752, loss is 0.30546408891677856\n",
      "epoch: 11 step: 753, loss is 0.32145485281944275\n",
      "epoch: 11 step: 754, loss is 0.14242084324359894\n",
      "epoch: 11 step: 755, loss is 0.31242722272872925\n",
      "epoch: 11 step: 756, loss is 0.15424379706382751\n",
      "epoch: 11 step: 757, loss is 0.23679393529891968\n",
      "epoch: 11 step: 758, loss is 0.1754065454006195\n",
      "epoch: 11 step: 759, loss is 0.11537060886621475\n",
      "epoch: 11 step: 760, loss is 0.13162900507450104\n",
      "epoch: 11 step: 761, loss is 0.16741712391376495\n",
      "epoch: 11 step: 762, loss is 0.4676080048084259\n",
      "epoch: 11 step: 763, loss is 0.16060100495815277\n",
      "epoch: 11 step: 764, loss is 0.16204367578029633\n",
      "epoch: 11 step: 765, loss is 0.4031325876712799\n",
      "epoch: 11 step: 766, loss is 0.26616501808166504\n",
      "epoch: 11 step: 767, loss is 0.1788293868303299\n",
      "epoch: 11 step: 768, loss is 0.15288370847702026\n",
      "epoch: 11 step: 769, loss is 0.5029435753822327\n",
      "epoch: 11 step: 770, loss is 0.17992335557937622\n",
      "epoch: 11 step: 771, loss is 0.2000388354063034\n",
      "epoch: 11 step: 772, loss is 0.1388615369796753\n",
      "epoch: 11 step: 773, loss is 0.14535480737686157\n",
      "epoch: 11 step: 774, loss is 0.23958176374435425\n",
      "epoch: 11 step: 775, loss is 0.19094812870025635\n",
      "epoch: 11 step: 776, loss is 0.10180770605802536\n",
      "epoch: 11 step: 777, loss is 0.12465202808380127\n",
      "epoch: 11 step: 778, loss is 0.17029371857643127\n",
      "epoch: 11 step: 779, loss is 0.3015151619911194\n",
      "epoch: 11 step: 780, loss is 0.27086231112480164\n",
      "epoch: 11 step: 781, loss is 0.1499680131673813\n",
      "epoch: 11 step: 782, loss is 0.38793233036994934\n",
      "epoch: 11 step: 783, loss is 0.21457868814468384\n",
      "epoch: 11 step: 784, loss is 0.2947256863117218\n",
      "epoch: 11 step: 785, loss is 0.16010992228984833\n",
      "epoch: 11 step: 786, loss is 0.36193791031837463\n",
      "epoch: 11 step: 787, loss is 0.3655111491680145\n",
      "epoch: 11 step: 788, loss is 0.11465149372816086\n",
      "epoch: 11 step: 789, loss is 0.21336819231510162\n",
      "epoch: 11 step: 790, loss is 0.1048121303319931\n",
      "epoch: 11 step: 791, loss is 0.2640714645385742\n",
      "epoch: 11 step: 792, loss is 0.25117236375808716\n",
      "epoch: 11 step: 793, loss is 0.2116764932870865\n",
      "epoch: 11 step: 794, loss is 0.17752961814403534\n",
      "epoch: 11 step: 795, loss is 0.11646714061498642\n",
      "epoch: 11 step: 796, loss is 0.19188976287841797\n",
      "epoch: 11 step: 797, loss is 0.29231536388397217\n",
      "epoch: 11 step: 798, loss is 0.33953791856765747\n",
      "epoch: 11 step: 799, loss is 0.18633392453193665\n",
      "epoch: 11 step: 800, loss is 0.2623824179172516\n",
      "epoch: 11 step: 801, loss is 0.17028003931045532\n",
      "epoch: 11 step: 802, loss is 0.23863005638122559\n",
      "epoch: 11 step: 803, loss is 0.22134819626808167\n",
      "epoch: 11 step: 804, loss is 0.240475594997406\n",
      "epoch: 11 step: 805, loss is 0.29776012897491455\n",
      "epoch: 11 step: 806, loss is 0.3868105709552765\n",
      "epoch: 11 step: 807, loss is 0.12196322530508041\n",
      "epoch: 11 step: 808, loss is 0.26432400941848755\n",
      "epoch: 11 step: 809, loss is 0.2620179355144501\n",
      "epoch: 11 step: 810, loss is 0.28604406118392944\n",
      "epoch: 11 step: 811, loss is 0.24302229285240173\n",
      "epoch: 11 step: 812, loss is 0.25782424211502075\n",
      "epoch: 11 step: 813, loss is 0.12509943544864655\n",
      "epoch: 11 step: 814, loss is 0.2219042032957077\n",
      "epoch: 11 step: 815, loss is 0.1326807290315628\n",
      "epoch: 11 step: 816, loss is 0.42687463760375977\n",
      "epoch: 11 step: 817, loss is 0.1728137582540512\n",
      "epoch: 11 step: 818, loss is 0.18879085779190063\n",
      "epoch: 11 step: 819, loss is 0.20575693249702454\n",
      "epoch: 11 step: 820, loss is 0.09064182639122009\n",
      "epoch: 11 step: 821, loss is 0.2685946524143219\n",
      "epoch: 11 step: 822, loss is 0.19658955931663513\n",
      "epoch: 11 step: 823, loss is 0.15331660211086273\n",
      "epoch: 11 step: 824, loss is 0.21224144101142883\n",
      "epoch: 11 step: 825, loss is 0.1867070347070694\n",
      "epoch: 11 step: 826, loss is 0.3741580843925476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 827, loss is 0.17068015038967133\n",
      "epoch: 11 step: 828, loss is 0.20617887377738953\n",
      "epoch: 11 step: 829, loss is 0.2283099889755249\n",
      "epoch: 11 step: 830, loss is 0.24058790504932404\n",
      "epoch: 11 step: 831, loss is 0.3218994438648224\n",
      "epoch: 11 step: 832, loss is 0.24840623140335083\n",
      "epoch: 11 step: 833, loss is 0.2975784242153168\n",
      "epoch: 11 step: 834, loss is 0.23981821537017822\n",
      "epoch: 11 step: 835, loss is 0.22870850563049316\n",
      "epoch: 11 step: 836, loss is 0.25124937295913696\n",
      "epoch: 11 step: 837, loss is 0.10628604143857956\n",
      "epoch: 11 step: 838, loss is 0.24950115382671356\n",
      "epoch: 11 step: 839, loss is 0.4499764144420624\n",
      "epoch: 11 step: 840, loss is 0.16510669887065887\n",
      "epoch: 11 step: 841, loss is 0.22878065705299377\n",
      "epoch: 11 step: 842, loss is 0.1486949324607849\n",
      "epoch: 11 step: 843, loss is 0.2790050208568573\n",
      "epoch: 11 step: 844, loss is 0.3029625117778778\n",
      "epoch: 11 step: 845, loss is 0.15978506207466125\n",
      "epoch: 11 step: 846, loss is 0.28254780173301697\n",
      "epoch: 11 step: 847, loss is 0.3512580692768097\n",
      "epoch: 11 step: 848, loss is 0.27708083391189575\n",
      "epoch: 11 step: 849, loss is 0.23358355462551117\n",
      "epoch: 11 step: 850, loss is 0.25923827290534973\n",
      "epoch: 11 step: 851, loss is 0.2180255502462387\n",
      "epoch: 11 step: 852, loss is 0.18644626438617706\n",
      "epoch: 11 step: 853, loss is 0.2383468598127365\n",
      "epoch: 11 step: 854, loss is 0.24110102653503418\n",
      "epoch: 11 step: 855, loss is 0.24224437773227692\n",
      "epoch: 11 step: 856, loss is 0.13733553886413574\n",
      "epoch: 11 step: 857, loss is 0.2828184962272644\n",
      "epoch: 11 step: 858, loss is 0.28696414828300476\n",
      "epoch: 11 step: 859, loss is 0.4627489745616913\n",
      "epoch: 11 step: 860, loss is 0.26706063747406006\n",
      "epoch: 11 step: 861, loss is 0.40945616364479065\n",
      "epoch: 11 step: 862, loss is 0.25622791051864624\n",
      "epoch: 11 step: 863, loss is 0.17989428341388702\n",
      "epoch: 11 step: 864, loss is 0.36627939343452454\n",
      "epoch: 11 step: 865, loss is 0.1982421576976776\n",
      "epoch: 11 step: 866, loss is 0.14413531124591827\n",
      "epoch: 11 step: 867, loss is 0.07181626558303833\n",
      "epoch: 11 step: 868, loss is 0.3963286280632019\n",
      "epoch: 11 step: 869, loss is 0.2347649186849594\n",
      "epoch: 11 step: 870, loss is 0.3489210903644562\n",
      "epoch: 11 step: 871, loss is 0.36834481358528137\n",
      "epoch: 11 step: 872, loss is 0.22823777794837952\n",
      "epoch: 11 step: 873, loss is 0.1383335292339325\n",
      "epoch: 11 step: 874, loss is 0.23994213342666626\n",
      "epoch: 11 step: 875, loss is 0.285023033618927\n",
      "epoch: 11 step: 876, loss is 0.3854549825191498\n",
      "epoch: 11 step: 877, loss is 0.10104602575302124\n",
      "epoch: 11 step: 878, loss is 0.2557094693183899\n",
      "epoch: 11 step: 879, loss is 0.2011786550283432\n",
      "epoch: 11 step: 880, loss is 0.17896200716495514\n",
      "epoch: 11 step: 881, loss is 0.22672118246555328\n",
      "epoch: 11 step: 882, loss is 0.08045285940170288\n",
      "epoch: 11 step: 883, loss is 0.32467925548553467\n",
      "epoch: 11 step: 884, loss is 0.29449182748794556\n",
      "epoch: 11 step: 885, loss is 0.2171735018491745\n",
      "epoch: 11 step: 886, loss is 0.30105745792388916\n",
      "epoch: 11 step: 887, loss is 0.10304892063140869\n",
      "epoch: 11 step: 888, loss is 0.29777756333351135\n",
      "epoch: 11 step: 889, loss is 0.2428664267063141\n",
      "epoch: 11 step: 890, loss is 0.27525708079338074\n",
      "epoch: 11 step: 891, loss is 0.2291266918182373\n",
      "epoch: 11 step: 892, loss is 0.2903788089752197\n",
      "epoch: 11 step: 893, loss is 0.23175562918186188\n",
      "epoch: 11 step: 894, loss is 0.28758272528648376\n",
      "epoch: 11 step: 895, loss is 0.3223824203014374\n",
      "epoch: 11 step: 896, loss is 0.170012965798378\n",
      "epoch: 11 step: 897, loss is 0.4781527519226074\n",
      "epoch: 11 step: 898, loss is 0.10729105770587921\n",
      "epoch: 11 step: 899, loss is 0.19563131034374237\n",
      "epoch: 11 step: 900, loss is 0.16863608360290527\n",
      "epoch: 11 step: 901, loss is 0.11642074584960938\n",
      "epoch: 11 step: 902, loss is 0.34865403175354004\n",
      "epoch: 11 step: 903, loss is 0.21098650991916656\n",
      "epoch: 11 step: 904, loss is 0.34572577476501465\n",
      "epoch: 11 step: 905, loss is 0.16534724831581116\n",
      "epoch: 11 step: 906, loss is 0.2015075981616974\n",
      "epoch: 11 step: 907, loss is 0.15147128701210022\n",
      "epoch: 11 step: 908, loss is 0.26162880659103394\n",
      "epoch: 11 step: 909, loss is 0.3445294201374054\n",
      "epoch: 11 step: 910, loss is 0.26001623272895813\n",
      "epoch: 11 step: 911, loss is 0.22174249589443207\n",
      "epoch: 11 step: 912, loss is 0.37003475427627563\n",
      "epoch: 11 step: 913, loss is 0.28369593620300293\n",
      "epoch: 11 step: 914, loss is 0.11044052988290787\n",
      "epoch: 11 step: 915, loss is 0.29499149322509766\n",
      "epoch: 11 step: 916, loss is 0.19006015360355377\n",
      "epoch: 11 step: 917, loss is 0.2664477825164795\n",
      "epoch: 11 step: 918, loss is 0.25720077753067017\n",
      "epoch: 11 step: 919, loss is 0.231184184551239\n",
      "epoch: 11 step: 920, loss is 0.1397719383239746\n",
      "epoch: 11 step: 921, loss is 0.25764429569244385\n",
      "epoch: 11 step: 922, loss is 0.18922874331474304\n",
      "epoch: 11 step: 923, loss is 0.2902395725250244\n",
      "epoch: 11 step: 924, loss is 0.17238152027130127\n",
      "epoch: 11 step: 925, loss is 0.250751793384552\n",
      "epoch: 11 step: 926, loss is 0.26240208745002747\n",
      "epoch: 11 step: 927, loss is 0.30600473284721375\n",
      "epoch: 11 step: 928, loss is 0.2771245241165161\n",
      "epoch: 11 step: 929, loss is 0.3942345976829529\n",
      "epoch: 11 step: 930, loss is 0.2177565097808838\n",
      "epoch: 11 step: 931, loss is 0.24908365309238434\n",
      "epoch: 11 step: 932, loss is 0.2214611917734146\n",
      "epoch: 11 step: 933, loss is 0.304675430059433\n",
      "epoch: 11 step: 934, loss is 0.24774198234081268\n",
      "epoch: 11 step: 935, loss is 0.322905033826828\n",
      "epoch: 11 step: 936, loss is 0.2597479522228241\n",
      "epoch: 11 step: 937, loss is 0.2152603566646576\n",
      "epoch: 12 step: 1, loss is 0.20903252065181732\n",
      "epoch: 12 step: 2, loss is 0.1534535139799118\n",
      "epoch: 12 step: 3, loss is 0.15969163179397583\n",
      "epoch: 12 step: 4, loss is 0.3001081943511963\n",
      "epoch: 12 step: 5, loss is 0.1722400039434433\n",
      "epoch: 12 step: 6, loss is 0.13241642713546753\n",
      "epoch: 12 step: 7, loss is 0.29660239815711975\n",
      "epoch: 12 step: 8, loss is 0.14065653085708618\n",
      "epoch: 12 step: 9, loss is 0.24421869218349457\n",
      "epoch: 12 step: 10, loss is 0.20579978823661804\n",
      "epoch: 12 step: 11, loss is 0.26039305329322815\n",
      "epoch: 12 step: 12, loss is 0.34191927313804626\n",
      "epoch: 12 step: 13, loss is 0.08758727461099625\n",
      "epoch: 12 step: 14, loss is 0.1301538050174713\n",
      "epoch: 12 step: 15, loss is 0.19298705458641052\n",
      "epoch: 12 step: 16, loss is 0.20438003540039062\n",
      "epoch: 12 step: 17, loss is 0.11181841045618057\n",
      "epoch: 12 step: 18, loss is 0.3330777883529663\n",
      "epoch: 12 step: 19, loss is 0.2981383800506592\n",
      "epoch: 12 step: 20, loss is 0.17663389444351196\n",
      "epoch: 12 step: 21, loss is 0.27110204100608826\n",
      "epoch: 12 step: 22, loss is 0.29643335938453674\n",
      "epoch: 12 step: 23, loss is 0.2646067440509796\n",
      "epoch: 12 step: 24, loss is 0.14634187519550323\n",
      "epoch: 12 step: 25, loss is 0.3334640562534332\n",
      "epoch: 12 step: 26, loss is 0.26515138149261475\n",
      "epoch: 12 step: 27, loss is 0.4259580969810486\n",
      "epoch: 12 step: 28, loss is 0.20658141374588013\n",
      "epoch: 12 step: 29, loss is 0.3270418047904968\n",
      "epoch: 12 step: 30, loss is 0.34960225224494934\n",
      "epoch: 12 step: 31, loss is 0.26189175248146057\n",
      "epoch: 12 step: 32, loss is 0.15366873145103455\n",
      "epoch: 12 step: 33, loss is 0.21721112728118896\n",
      "epoch: 12 step: 34, loss is 0.1983707845211029\n",
      "epoch: 12 step: 35, loss is 0.1292867809534073\n",
      "epoch: 12 step: 36, loss is 0.2545455992221832\n",
      "epoch: 12 step: 37, loss is 0.23500089347362518\n",
      "epoch: 12 step: 38, loss is 0.1844753921031952\n",
      "epoch: 12 step: 39, loss is 0.1806456446647644\n",
      "epoch: 12 step: 40, loss is 0.14330072700977325\n",
      "epoch: 12 step: 41, loss is 0.23147191107273102\n",
      "epoch: 12 step: 42, loss is 0.21246962249279022\n",
      "epoch: 12 step: 43, loss is 0.19606956839561462\n",
      "epoch: 12 step: 44, loss is 0.23730650544166565\n",
      "epoch: 12 step: 45, loss is 0.20005843043327332\n",
      "epoch: 12 step: 46, loss is 0.16934359073638916\n",
      "epoch: 12 step: 47, loss is 0.16051419079303741\n",
      "epoch: 12 step: 48, loss is 0.15053847432136536\n",
      "epoch: 12 step: 49, loss is 0.1981937289237976\n",
      "epoch: 12 step: 50, loss is 0.23681025207042694\n",
      "epoch: 12 step: 51, loss is 0.23120173811912537\n",
      "epoch: 12 step: 52, loss is 0.2702522277832031\n",
      "epoch: 12 step: 53, loss is 0.2839137315750122\n",
      "epoch: 12 step: 54, loss is 0.2261832058429718\n",
      "epoch: 12 step: 55, loss is 0.1927415430545807\n",
      "epoch: 12 step: 56, loss is 0.19778276979923248\n",
      "epoch: 12 step: 57, loss is 0.4092728793621063\n",
      "epoch: 12 step: 58, loss is 0.18845002353191376\n",
      "epoch: 12 step: 59, loss is 0.11424431204795837\n",
      "epoch: 12 step: 60, loss is 0.14485640823841095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 61, loss is 0.3242381811141968\n",
      "epoch: 12 step: 62, loss is 0.2820858657360077\n",
      "epoch: 12 step: 63, loss is 0.35216960310935974\n",
      "epoch: 12 step: 64, loss is 0.20012694597244263\n",
      "epoch: 12 step: 65, loss is 0.24336101114749908\n",
      "epoch: 12 step: 66, loss is 0.1866612285375595\n",
      "epoch: 12 step: 67, loss is 0.23269346356391907\n",
      "epoch: 12 step: 68, loss is 0.14935408532619476\n",
      "epoch: 12 step: 69, loss is 0.14634111523628235\n",
      "epoch: 12 step: 70, loss is 0.17699196934700012\n",
      "epoch: 12 step: 71, loss is 0.15543155372142792\n",
      "epoch: 12 step: 72, loss is 0.19722943007946014\n",
      "epoch: 12 step: 73, loss is 0.32257065176963806\n",
      "epoch: 12 step: 74, loss is 0.3362855613231659\n",
      "epoch: 12 step: 75, loss is 0.2656077742576599\n",
      "epoch: 12 step: 76, loss is 0.13888537883758545\n",
      "epoch: 12 step: 77, loss is 0.17814891040325165\n",
      "epoch: 12 step: 78, loss is 0.28995537757873535\n",
      "epoch: 12 step: 79, loss is 0.08435241132974625\n",
      "epoch: 12 step: 80, loss is 0.24129758775234222\n",
      "epoch: 12 step: 81, loss is 0.256598562002182\n",
      "epoch: 12 step: 82, loss is 0.14311233162879944\n",
      "epoch: 12 step: 83, loss is 0.18034504354000092\n",
      "epoch: 12 step: 84, loss is 0.22523052990436554\n",
      "epoch: 12 step: 85, loss is 0.2289636731147766\n",
      "epoch: 12 step: 86, loss is 0.17786580324172974\n",
      "epoch: 12 step: 87, loss is 0.11113335937261581\n",
      "epoch: 12 step: 88, loss is 0.26036128401756287\n",
      "epoch: 12 step: 89, loss is 0.24313907325267792\n",
      "epoch: 12 step: 90, loss is 0.23952144384384155\n",
      "epoch: 12 step: 91, loss is 0.17464879155158997\n",
      "epoch: 12 step: 92, loss is 0.2520900368690491\n",
      "epoch: 12 step: 93, loss is 0.13670916855335236\n",
      "epoch: 12 step: 94, loss is 0.1849621683359146\n",
      "epoch: 12 step: 95, loss is 0.22405780851840973\n",
      "epoch: 12 step: 96, loss is 0.1842619925737381\n",
      "epoch: 12 step: 97, loss is 0.29770204424858093\n",
      "epoch: 12 step: 98, loss is 0.17521066963672638\n",
      "epoch: 12 step: 99, loss is 0.3600473701953888\n",
      "epoch: 12 step: 100, loss is 0.3784817159175873\n",
      "epoch: 12 step: 101, loss is 0.25364330410957336\n",
      "epoch: 12 step: 102, loss is 0.1154121533036232\n",
      "epoch: 12 step: 103, loss is 0.27237236499786377\n",
      "epoch: 12 step: 104, loss is 0.22443845868110657\n",
      "epoch: 12 step: 105, loss is 0.11385345458984375\n",
      "epoch: 12 step: 106, loss is 0.2099413424730301\n",
      "epoch: 12 step: 107, loss is 0.2287331372499466\n",
      "epoch: 12 step: 108, loss is 0.40412163734436035\n",
      "epoch: 12 step: 109, loss is 0.21531163156032562\n",
      "epoch: 12 step: 110, loss is 0.21522042155265808\n",
      "epoch: 12 step: 111, loss is 0.19666141271591187\n",
      "epoch: 12 step: 112, loss is 0.2415355145931244\n",
      "epoch: 12 step: 113, loss is 0.2645718455314636\n",
      "epoch: 12 step: 114, loss is 0.12048645317554474\n",
      "epoch: 12 step: 115, loss is 0.17531123757362366\n",
      "epoch: 12 step: 116, loss is 0.2557843327522278\n",
      "epoch: 12 step: 117, loss is 0.3294788897037506\n",
      "epoch: 12 step: 118, loss is 0.18604430556297302\n",
      "epoch: 12 step: 119, loss is 0.18087731301784515\n",
      "epoch: 12 step: 120, loss is 0.17798835039138794\n",
      "epoch: 12 step: 121, loss is 0.26427650451660156\n",
      "epoch: 12 step: 122, loss is 0.21268531680107117\n",
      "epoch: 12 step: 123, loss is 0.10265545547008514\n",
      "epoch: 12 step: 124, loss is 0.2704630494117737\n",
      "epoch: 12 step: 125, loss is 0.2549584209918976\n",
      "epoch: 12 step: 126, loss is 0.15121670067310333\n",
      "epoch: 12 step: 127, loss is 0.2559947967529297\n",
      "epoch: 12 step: 128, loss is 0.3135998249053955\n",
      "epoch: 12 step: 129, loss is 0.5355256795883179\n",
      "epoch: 12 step: 130, loss is 0.1788758486509323\n",
      "epoch: 12 step: 131, loss is 0.276649534702301\n",
      "epoch: 12 step: 132, loss is 0.2662980556488037\n",
      "epoch: 12 step: 133, loss is 0.36406344175338745\n",
      "epoch: 12 step: 134, loss is 0.2361641377210617\n",
      "epoch: 12 step: 135, loss is 0.3752971589565277\n",
      "epoch: 12 step: 136, loss is 0.13985030353069305\n",
      "epoch: 12 step: 137, loss is 0.18249773979187012\n",
      "epoch: 12 step: 138, loss is 0.2231866419315338\n",
      "epoch: 12 step: 139, loss is 0.18614569306373596\n",
      "epoch: 12 step: 140, loss is 0.17597873508930206\n",
      "epoch: 12 step: 141, loss is 0.2264152616262436\n",
      "epoch: 12 step: 142, loss is 0.16370531916618347\n",
      "epoch: 12 step: 143, loss is 0.22010301053524017\n",
      "epoch: 12 step: 144, loss is 0.4578871428966522\n",
      "epoch: 12 step: 145, loss is 0.13452863693237305\n",
      "epoch: 12 step: 146, loss is 0.4357839822769165\n",
      "epoch: 12 step: 147, loss is 0.3146596848964691\n",
      "epoch: 12 step: 148, loss is 0.3214156925678253\n",
      "epoch: 12 step: 149, loss is 0.17850814759731293\n",
      "epoch: 12 step: 150, loss is 0.17357775568962097\n",
      "epoch: 12 step: 151, loss is 0.1253640502691269\n",
      "epoch: 12 step: 152, loss is 0.32455968856811523\n",
      "epoch: 12 step: 153, loss is 0.2691505551338196\n",
      "epoch: 12 step: 154, loss is 0.2603376507759094\n",
      "epoch: 12 step: 155, loss is 0.13726769387722015\n",
      "epoch: 12 step: 156, loss is 0.32730910181999207\n",
      "epoch: 12 step: 157, loss is 0.37612444162368774\n",
      "epoch: 12 step: 158, loss is 0.17789942026138306\n",
      "epoch: 12 step: 159, loss is 0.19917671382427216\n",
      "epoch: 12 step: 160, loss is 0.2243015319108963\n",
      "epoch: 12 step: 161, loss is 0.16777479648590088\n",
      "epoch: 12 step: 162, loss is 0.1981983333826065\n",
      "epoch: 12 step: 163, loss is 0.25155073404312134\n",
      "epoch: 12 step: 164, loss is 0.36986494064331055\n",
      "epoch: 12 step: 165, loss is 0.11608770489692688\n",
      "epoch: 12 step: 166, loss is 0.17489054799079895\n",
      "epoch: 12 step: 167, loss is 0.21910245716571808\n",
      "epoch: 12 step: 168, loss is 0.22861754894256592\n",
      "epoch: 12 step: 169, loss is 0.10481651872396469\n",
      "epoch: 12 step: 170, loss is 0.19378924369812012\n",
      "epoch: 12 step: 171, loss is 0.1286555826663971\n",
      "epoch: 12 step: 172, loss is 0.18716515600681305\n",
      "epoch: 12 step: 173, loss is 0.24098627269268036\n",
      "epoch: 12 step: 174, loss is 0.18911173939704895\n",
      "epoch: 12 step: 175, loss is 0.19774767756462097\n",
      "epoch: 12 step: 176, loss is 0.2572319805622101\n",
      "epoch: 12 step: 177, loss is 0.14485161006450653\n",
      "epoch: 12 step: 178, loss is 0.14713063836097717\n",
      "epoch: 12 step: 179, loss is 0.20957258343696594\n",
      "epoch: 12 step: 180, loss is 0.4401518404483795\n",
      "epoch: 12 step: 181, loss is 0.27100804448127747\n",
      "epoch: 12 step: 182, loss is 0.25220683217048645\n",
      "epoch: 12 step: 183, loss is 0.1042405515909195\n",
      "epoch: 12 step: 184, loss is 0.21763354539871216\n",
      "epoch: 12 step: 185, loss is 0.4825914204120636\n",
      "epoch: 12 step: 186, loss is 0.24846163392066956\n",
      "epoch: 12 step: 187, loss is 0.17410185933113098\n",
      "epoch: 12 step: 188, loss is 0.052090030163526535\n",
      "epoch: 12 step: 189, loss is 0.18410904705524445\n",
      "epoch: 12 step: 190, loss is 0.29241353273391724\n",
      "epoch: 12 step: 191, loss is 0.36029958724975586\n",
      "epoch: 12 step: 192, loss is 0.12910740077495575\n",
      "epoch: 12 step: 193, loss is 0.10198930650949478\n",
      "epoch: 12 step: 194, loss is 0.18889494240283966\n",
      "epoch: 12 step: 195, loss is 0.39756256341934204\n",
      "epoch: 12 step: 196, loss is 0.20564717054367065\n",
      "epoch: 12 step: 197, loss is 0.2635813057422638\n",
      "epoch: 12 step: 198, loss is 0.19094739854335785\n",
      "epoch: 12 step: 199, loss is 0.15144439041614532\n",
      "epoch: 12 step: 200, loss is 0.23927268385887146\n",
      "epoch: 12 step: 201, loss is 0.18393070995807648\n",
      "epoch: 12 step: 202, loss is 0.24333558976650238\n",
      "epoch: 12 step: 203, loss is 0.19678261876106262\n",
      "epoch: 12 step: 204, loss is 0.3002842366695404\n",
      "epoch: 12 step: 205, loss is 0.3783487379550934\n",
      "epoch: 12 step: 206, loss is 0.41073641180992126\n",
      "epoch: 12 step: 207, loss is 0.3339378535747528\n",
      "epoch: 12 step: 208, loss is 0.2042192816734314\n",
      "epoch: 12 step: 209, loss is 0.19180914759635925\n",
      "epoch: 12 step: 210, loss is 0.26940324902534485\n",
      "epoch: 12 step: 211, loss is 0.3168146312236786\n",
      "epoch: 12 step: 212, loss is 0.23735344409942627\n",
      "epoch: 12 step: 213, loss is 0.2394135743379593\n",
      "epoch: 12 step: 214, loss is 0.21116025745868683\n",
      "epoch: 12 step: 215, loss is 0.21035148203372955\n",
      "epoch: 12 step: 216, loss is 0.11667191982269287\n",
      "epoch: 12 step: 217, loss is 0.5425527095794678\n",
      "epoch: 12 step: 218, loss is 0.18980945646762848\n",
      "epoch: 12 step: 219, loss is 0.1484348028898239\n",
      "epoch: 12 step: 220, loss is 0.2786647081375122\n",
      "epoch: 12 step: 221, loss is 0.22054901719093323\n",
      "epoch: 12 step: 222, loss is 0.2949758768081665\n",
      "epoch: 12 step: 223, loss is 0.41848623752593994\n",
      "epoch: 12 step: 224, loss is 0.37270388007164\n",
      "epoch: 12 step: 225, loss is 0.43684589862823486\n",
      "epoch: 12 step: 226, loss is 0.3859679102897644\n",
      "epoch: 12 step: 227, loss is 0.17691877484321594\n",
      "epoch: 12 step: 228, loss is 0.11140458285808563\n",
      "epoch: 12 step: 229, loss is 0.2231542319059372\n",
      "epoch: 12 step: 230, loss is 0.3392067551612854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 231, loss is 0.18498535454273224\n",
      "epoch: 12 step: 232, loss is 0.266301691532135\n",
      "epoch: 12 step: 233, loss is 0.13468381762504578\n",
      "epoch: 12 step: 234, loss is 0.28426623344421387\n",
      "epoch: 12 step: 235, loss is 0.1455117017030716\n",
      "epoch: 12 step: 236, loss is 0.23766572773456573\n",
      "epoch: 12 step: 237, loss is 0.23162595927715302\n",
      "epoch: 12 step: 238, loss is 0.1911800503730774\n",
      "epoch: 12 step: 239, loss is 0.2652413547039032\n",
      "epoch: 12 step: 240, loss is 0.2433975636959076\n",
      "epoch: 12 step: 241, loss is 0.18841181695461273\n",
      "epoch: 12 step: 242, loss is 0.16754403710365295\n",
      "epoch: 12 step: 243, loss is 0.4717305898666382\n",
      "epoch: 12 step: 244, loss is 0.3359363079071045\n",
      "epoch: 12 step: 245, loss is 0.12793734669685364\n",
      "epoch: 12 step: 246, loss is 0.22114433348178864\n",
      "epoch: 12 step: 247, loss is 0.1456836760044098\n",
      "epoch: 12 step: 248, loss is 0.2878769636154175\n",
      "epoch: 12 step: 249, loss is 0.41745951771736145\n",
      "epoch: 12 step: 250, loss is 0.20979437232017517\n",
      "epoch: 12 step: 251, loss is 0.16126345098018646\n",
      "epoch: 12 step: 252, loss is 0.14506952464580536\n",
      "epoch: 12 step: 253, loss is 0.26855361461639404\n",
      "epoch: 12 step: 254, loss is 0.18578335642814636\n",
      "epoch: 12 step: 255, loss is 0.1814037561416626\n",
      "epoch: 12 step: 256, loss is 0.2480682134628296\n",
      "epoch: 12 step: 257, loss is 0.2999614477157593\n",
      "epoch: 12 step: 258, loss is 0.1511455774307251\n",
      "epoch: 12 step: 259, loss is 0.18331308662891388\n",
      "epoch: 12 step: 260, loss is 0.2828296422958374\n",
      "epoch: 12 step: 261, loss is 0.09616844356060028\n",
      "epoch: 12 step: 262, loss is 0.30017516016960144\n",
      "epoch: 12 step: 263, loss is 0.10950636118650436\n",
      "epoch: 12 step: 264, loss is 0.15745078027248383\n",
      "epoch: 12 step: 265, loss is 0.29781100153923035\n",
      "epoch: 12 step: 266, loss is 0.12148084491491318\n",
      "epoch: 12 step: 267, loss is 0.13140782713890076\n",
      "epoch: 12 step: 268, loss is 0.26113688945770264\n",
      "epoch: 12 step: 269, loss is 0.35808292031288147\n",
      "epoch: 12 step: 270, loss is 0.26862868666648865\n",
      "epoch: 12 step: 271, loss is 0.060495633631944656\n",
      "epoch: 12 step: 272, loss is 0.269578754901886\n",
      "epoch: 12 step: 273, loss is 0.13727302849292755\n",
      "epoch: 12 step: 274, loss is 0.17026594281196594\n",
      "epoch: 12 step: 275, loss is 0.18849357962608337\n",
      "epoch: 12 step: 276, loss is 0.12772120535373688\n",
      "epoch: 12 step: 277, loss is 0.17453765869140625\n",
      "epoch: 12 step: 278, loss is 0.5032322406768799\n",
      "epoch: 12 step: 279, loss is 0.4877554178237915\n",
      "epoch: 12 step: 280, loss is 0.18027135729789734\n",
      "epoch: 12 step: 281, loss is 0.32054048776626587\n",
      "epoch: 12 step: 282, loss is 0.12316448986530304\n",
      "epoch: 12 step: 283, loss is 0.12673532962799072\n",
      "epoch: 12 step: 284, loss is 0.23501130938529968\n",
      "epoch: 12 step: 285, loss is 0.19274532794952393\n",
      "epoch: 12 step: 286, loss is 0.14110100269317627\n",
      "epoch: 12 step: 287, loss is 0.33481740951538086\n",
      "epoch: 12 step: 288, loss is 0.11550341546535492\n",
      "epoch: 12 step: 289, loss is 0.2709805965423584\n",
      "epoch: 12 step: 290, loss is 0.3115113079547882\n",
      "epoch: 12 step: 291, loss is 0.19288086891174316\n",
      "epoch: 12 step: 292, loss is 0.37288564443588257\n",
      "epoch: 12 step: 293, loss is 0.30452844500541687\n",
      "epoch: 12 step: 294, loss is 0.3293692171573639\n",
      "epoch: 12 step: 295, loss is 0.26260513067245483\n",
      "epoch: 12 step: 296, loss is 0.34443750977516174\n",
      "epoch: 12 step: 297, loss is 0.308089941740036\n",
      "epoch: 12 step: 298, loss is 0.2685982286930084\n",
      "epoch: 12 step: 299, loss is 0.1761557012796402\n",
      "epoch: 12 step: 300, loss is 0.13035236299037933\n",
      "epoch: 12 step: 301, loss is 0.14388784766197205\n",
      "epoch: 12 step: 302, loss is 0.1603909730911255\n",
      "epoch: 12 step: 303, loss is 0.3702351748943329\n",
      "epoch: 12 step: 304, loss is 0.12184145301580429\n",
      "epoch: 12 step: 305, loss is 0.21292875707149506\n",
      "epoch: 12 step: 306, loss is 0.13655996322631836\n",
      "epoch: 12 step: 307, loss is 0.37454721331596375\n",
      "epoch: 12 step: 308, loss is 0.19790464639663696\n",
      "epoch: 12 step: 309, loss is 0.20679253339767456\n",
      "epoch: 12 step: 310, loss is 0.3455032706260681\n",
      "epoch: 12 step: 311, loss is 0.27905765175819397\n",
      "epoch: 12 step: 312, loss is 0.14215406775474548\n",
      "epoch: 12 step: 313, loss is 0.21764345467090607\n",
      "epoch: 12 step: 314, loss is 0.20778772234916687\n",
      "epoch: 12 step: 315, loss is 0.1633545607328415\n",
      "epoch: 12 step: 316, loss is 0.15818916261196136\n",
      "epoch: 12 step: 317, loss is 0.22260123491287231\n",
      "epoch: 12 step: 318, loss is 0.1437290608882904\n",
      "epoch: 12 step: 319, loss is 0.3781996965408325\n",
      "epoch: 12 step: 320, loss is 0.23335875570774078\n",
      "epoch: 12 step: 321, loss is 0.19417504966259003\n",
      "epoch: 12 step: 322, loss is 0.31677189469337463\n",
      "epoch: 12 step: 323, loss is 0.22563967108726501\n",
      "epoch: 12 step: 324, loss is 0.21001748740673065\n",
      "epoch: 12 step: 325, loss is 0.25706911087036133\n",
      "epoch: 12 step: 326, loss is 0.16109007596969604\n",
      "epoch: 12 step: 327, loss is 0.23420116305351257\n",
      "epoch: 12 step: 328, loss is 0.21616336703300476\n",
      "epoch: 12 step: 329, loss is 0.2301928699016571\n",
      "epoch: 12 step: 330, loss is 0.11218823492527008\n",
      "epoch: 12 step: 331, loss is 0.295549213886261\n",
      "epoch: 12 step: 332, loss is 0.38136667013168335\n",
      "epoch: 12 step: 333, loss is 0.2838422656059265\n",
      "epoch: 12 step: 334, loss is 0.35071924328804016\n",
      "epoch: 12 step: 335, loss is 0.3160272538661957\n",
      "epoch: 12 step: 336, loss is 0.23015128076076508\n",
      "epoch: 12 step: 337, loss is 0.177463099360466\n",
      "epoch: 12 step: 338, loss is 0.17954276502132416\n",
      "epoch: 12 step: 339, loss is 0.21387964487075806\n",
      "epoch: 12 step: 340, loss is 0.2771209478378296\n",
      "epoch: 12 step: 341, loss is 0.1849375218153\n",
      "epoch: 12 step: 342, loss is 0.24778534471988678\n",
      "epoch: 12 step: 343, loss is 0.1867234706878662\n",
      "epoch: 12 step: 344, loss is 0.28811973333358765\n",
      "epoch: 12 step: 345, loss is 0.20174714922904968\n",
      "epoch: 12 step: 346, loss is 0.18708203732967377\n",
      "epoch: 12 step: 347, loss is 0.17812247574329376\n",
      "epoch: 12 step: 348, loss is 0.4195523262023926\n",
      "epoch: 12 step: 349, loss is 0.2292020171880722\n",
      "epoch: 12 step: 350, loss is 0.24814410507678986\n",
      "epoch: 12 step: 351, loss is 0.32552069425582886\n",
      "epoch: 12 step: 352, loss is 0.2095169574022293\n",
      "epoch: 12 step: 353, loss is 0.2084740847349167\n",
      "epoch: 12 step: 354, loss is 0.21456800401210785\n",
      "epoch: 12 step: 355, loss is 0.15690003335475922\n",
      "epoch: 12 step: 356, loss is 0.20296171307563782\n",
      "epoch: 12 step: 357, loss is 0.2517404556274414\n",
      "epoch: 12 step: 358, loss is 0.3782394230365753\n",
      "epoch: 12 step: 359, loss is 0.27713871002197266\n",
      "epoch: 12 step: 360, loss is 0.15306319296360016\n",
      "epoch: 12 step: 361, loss is 0.2956375181674957\n",
      "epoch: 12 step: 362, loss is 0.31799960136413574\n",
      "epoch: 12 step: 363, loss is 0.2044496387243271\n",
      "epoch: 12 step: 364, loss is 0.1778048723936081\n",
      "epoch: 12 step: 365, loss is 0.43429338932037354\n",
      "epoch: 12 step: 366, loss is 0.3157441318035126\n",
      "epoch: 12 step: 367, loss is 0.31843382120132446\n",
      "epoch: 12 step: 368, loss is 0.4559137225151062\n",
      "epoch: 12 step: 369, loss is 0.25032979249954224\n",
      "epoch: 12 step: 370, loss is 0.14876526594161987\n",
      "epoch: 12 step: 371, loss is 0.2741016447544098\n",
      "epoch: 12 step: 372, loss is 0.21770744025707245\n",
      "epoch: 12 step: 373, loss is 0.1949996054172516\n",
      "epoch: 12 step: 374, loss is 0.20696449279785156\n",
      "epoch: 12 step: 375, loss is 0.18882761895656586\n",
      "epoch: 12 step: 376, loss is 0.29474717378616333\n",
      "epoch: 12 step: 377, loss is 0.46389344334602356\n",
      "epoch: 12 step: 378, loss is 0.40571993589401245\n",
      "epoch: 12 step: 379, loss is 0.20022228360176086\n",
      "epoch: 12 step: 380, loss is 0.37279143929481506\n",
      "epoch: 12 step: 381, loss is 0.15558816492557526\n",
      "epoch: 12 step: 382, loss is 0.3839989900588989\n",
      "epoch: 12 step: 383, loss is 0.14903989434242249\n",
      "epoch: 12 step: 384, loss is 0.34640827775001526\n",
      "epoch: 12 step: 385, loss is 0.25317618250846863\n",
      "epoch: 12 step: 386, loss is 0.388801246881485\n",
      "epoch: 12 step: 387, loss is 0.23010462522506714\n",
      "epoch: 12 step: 388, loss is 0.21810990571975708\n",
      "epoch: 12 step: 389, loss is 0.3220583200454712\n",
      "epoch: 12 step: 390, loss is 0.15464110672473907\n",
      "epoch: 12 step: 391, loss is 0.16583071649074554\n",
      "epoch: 12 step: 392, loss is 0.17169946432113647\n",
      "epoch: 12 step: 393, loss is 0.31134241819381714\n",
      "epoch: 12 step: 394, loss is 0.29888099431991577\n",
      "epoch: 12 step: 395, loss is 0.2397128939628601\n",
      "epoch: 12 step: 396, loss is 0.3272998631000519\n",
      "epoch: 12 step: 397, loss is 0.14556942880153656\n",
      "epoch: 12 step: 398, loss is 0.19454050064086914\n",
      "epoch: 12 step: 399, loss is 0.2092260718345642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 400, loss is 0.21506176888942719\n",
      "epoch: 12 step: 401, loss is 0.25214865803718567\n",
      "epoch: 12 step: 402, loss is 0.16832014918327332\n",
      "epoch: 12 step: 403, loss is 0.29879769682884216\n",
      "epoch: 12 step: 404, loss is 0.18438056111335754\n",
      "epoch: 12 step: 405, loss is 0.1610919088125229\n",
      "epoch: 12 step: 406, loss is 0.1262674182653427\n",
      "epoch: 12 step: 407, loss is 0.2422967553138733\n",
      "epoch: 12 step: 408, loss is 0.21637466549873352\n",
      "epoch: 12 step: 409, loss is 0.20636789500713348\n",
      "epoch: 12 step: 410, loss is 0.23680123686790466\n",
      "epoch: 12 step: 411, loss is 0.2903233766555786\n",
      "epoch: 12 step: 412, loss is 0.27764827013015747\n",
      "epoch: 12 step: 413, loss is 0.1728779673576355\n",
      "epoch: 12 step: 414, loss is 0.13277457654476166\n",
      "epoch: 12 step: 415, loss is 0.18939641118049622\n",
      "epoch: 12 step: 416, loss is 0.22437337040901184\n",
      "epoch: 12 step: 417, loss is 0.31290313601493835\n",
      "epoch: 12 step: 418, loss is 0.3273870348930359\n",
      "epoch: 12 step: 419, loss is 0.4470139741897583\n",
      "epoch: 12 step: 420, loss is 0.23970334231853485\n",
      "epoch: 12 step: 421, loss is 0.34292224049568176\n",
      "epoch: 12 step: 422, loss is 0.23647649586200714\n",
      "epoch: 12 step: 423, loss is 0.2857375144958496\n",
      "epoch: 12 step: 424, loss is 0.1811194270849228\n",
      "epoch: 12 step: 425, loss is 0.22452892363071442\n",
      "epoch: 12 step: 426, loss is 0.18351420760154724\n",
      "epoch: 12 step: 427, loss is 0.19404982030391693\n",
      "epoch: 12 step: 428, loss is 0.15079635381698608\n",
      "epoch: 12 step: 429, loss is 0.1558733731508255\n",
      "epoch: 12 step: 430, loss is 0.25012245774269104\n",
      "epoch: 12 step: 431, loss is 0.20384757220745087\n",
      "epoch: 12 step: 432, loss is 0.12645027041435242\n",
      "epoch: 12 step: 433, loss is 0.12194975465536118\n",
      "epoch: 12 step: 434, loss is 0.1754227578639984\n",
      "epoch: 12 step: 435, loss is 0.3618505001068115\n",
      "epoch: 12 step: 436, loss is 0.3795353174209595\n",
      "epoch: 12 step: 437, loss is 0.16440731287002563\n",
      "epoch: 12 step: 438, loss is 0.24452556669712067\n",
      "epoch: 12 step: 439, loss is 0.28009963035583496\n",
      "epoch: 12 step: 440, loss is 0.13267678022384644\n",
      "epoch: 12 step: 441, loss is 0.24850866198539734\n",
      "epoch: 12 step: 442, loss is 0.19969980418682098\n",
      "epoch: 12 step: 443, loss is 0.31585240364074707\n",
      "epoch: 12 step: 444, loss is 0.23484593629837036\n",
      "epoch: 12 step: 445, loss is 0.1311134397983551\n",
      "epoch: 12 step: 446, loss is 0.1724667251110077\n",
      "epoch: 12 step: 447, loss is 0.3593808114528656\n",
      "epoch: 12 step: 448, loss is 0.2866226136684418\n",
      "epoch: 12 step: 449, loss is 0.251696914434433\n",
      "epoch: 12 step: 450, loss is 0.20509827136993408\n",
      "epoch: 12 step: 451, loss is 0.2269066721200943\n",
      "epoch: 12 step: 452, loss is 0.21174201369285583\n",
      "epoch: 12 step: 453, loss is 0.3794170618057251\n",
      "epoch: 12 step: 454, loss is 0.564948558807373\n",
      "epoch: 12 step: 455, loss is 0.15152466297149658\n",
      "epoch: 12 step: 456, loss is 0.27235016226768494\n",
      "epoch: 12 step: 457, loss is 0.16957487165927887\n",
      "epoch: 12 step: 458, loss is 0.16539087891578674\n",
      "epoch: 12 step: 459, loss is 0.17996276915073395\n",
      "epoch: 12 step: 460, loss is 0.15704751014709473\n",
      "epoch: 12 step: 461, loss is 0.14213012158870697\n",
      "epoch: 12 step: 462, loss is 0.1621907353401184\n",
      "epoch: 12 step: 463, loss is 0.24603934586048126\n",
      "epoch: 12 step: 464, loss is 0.13400256633758545\n",
      "epoch: 12 step: 465, loss is 0.24599690735340118\n",
      "epoch: 12 step: 466, loss is 0.1629660576581955\n",
      "epoch: 12 step: 467, loss is 0.12521393597126007\n",
      "epoch: 12 step: 468, loss is 0.31832364201545715\n",
      "epoch: 12 step: 469, loss is 0.15108931064605713\n",
      "epoch: 12 step: 470, loss is 0.2857849597930908\n",
      "epoch: 12 step: 471, loss is 0.3148353099822998\n",
      "epoch: 12 step: 472, loss is 0.1841711699962616\n",
      "epoch: 12 step: 473, loss is 0.268927663564682\n",
      "epoch: 12 step: 474, loss is 0.14392827451229095\n",
      "epoch: 12 step: 475, loss is 0.20489448308944702\n",
      "epoch: 12 step: 476, loss is 0.38213902711868286\n",
      "epoch: 12 step: 477, loss is 0.1263757199048996\n",
      "epoch: 12 step: 478, loss is 0.13926565647125244\n",
      "epoch: 12 step: 479, loss is 0.35861721634864807\n",
      "epoch: 12 step: 480, loss is 0.11458056420087814\n",
      "epoch: 12 step: 481, loss is 0.20854882895946503\n",
      "epoch: 12 step: 482, loss is 0.26053446531295776\n",
      "epoch: 12 step: 483, loss is 0.26820144057273865\n",
      "epoch: 12 step: 484, loss is 0.104841448366642\n",
      "epoch: 12 step: 485, loss is 0.17325688898563385\n",
      "epoch: 12 step: 486, loss is 0.39186859130859375\n",
      "epoch: 12 step: 487, loss is 0.2789570987224579\n",
      "epoch: 12 step: 488, loss is 0.10887220501899719\n",
      "epoch: 12 step: 489, loss is 0.1540636569261551\n",
      "epoch: 12 step: 490, loss is 0.24110539257526398\n",
      "epoch: 12 step: 491, loss is 0.21408602595329285\n",
      "epoch: 12 step: 492, loss is 0.11060385406017303\n",
      "epoch: 12 step: 493, loss is 0.2335200160741806\n",
      "epoch: 12 step: 494, loss is 0.3553581237792969\n",
      "epoch: 12 step: 495, loss is 0.24172256886959076\n",
      "epoch: 12 step: 496, loss is 0.19644735753536224\n",
      "epoch: 12 step: 497, loss is 0.26828864216804504\n",
      "epoch: 12 step: 498, loss is 0.34078195691108704\n",
      "epoch: 12 step: 499, loss is 0.1851302981376648\n",
      "epoch: 12 step: 500, loss is 0.32516899704933167\n",
      "epoch: 12 step: 501, loss is 0.33674776554107666\n",
      "epoch: 12 step: 502, loss is 0.15452638268470764\n",
      "epoch: 12 step: 503, loss is 0.36084645986557007\n",
      "epoch: 12 step: 504, loss is 0.1500340849161148\n",
      "epoch: 12 step: 505, loss is 0.16525481641292572\n",
      "epoch: 12 step: 506, loss is 0.08227016776800156\n",
      "epoch: 12 step: 507, loss is 0.27462470531463623\n",
      "epoch: 12 step: 508, loss is 0.3009059727191925\n",
      "epoch: 12 step: 509, loss is 0.20022545754909515\n",
      "epoch: 12 step: 510, loss is 0.18757149577140808\n",
      "epoch: 12 step: 511, loss is 0.2474818378686905\n",
      "epoch: 12 step: 512, loss is 0.28766003251075745\n",
      "epoch: 12 step: 513, loss is 0.1563868224620819\n",
      "epoch: 12 step: 514, loss is 0.3006684184074402\n",
      "epoch: 12 step: 515, loss is 0.3141500949859619\n",
      "epoch: 12 step: 516, loss is 0.39482349157333374\n",
      "epoch: 12 step: 517, loss is 0.21156737208366394\n",
      "epoch: 12 step: 518, loss is 0.5005249977111816\n",
      "epoch: 12 step: 519, loss is 0.2646608352661133\n",
      "epoch: 12 step: 520, loss is 0.26362428069114685\n",
      "epoch: 12 step: 521, loss is 0.252996563911438\n",
      "epoch: 12 step: 522, loss is 0.25492802262306213\n",
      "epoch: 12 step: 523, loss is 0.2765651047229767\n",
      "epoch: 12 step: 524, loss is 0.11712716519832611\n",
      "epoch: 12 step: 525, loss is 0.11599604785442352\n",
      "epoch: 12 step: 526, loss is 0.22064003348350525\n",
      "epoch: 12 step: 527, loss is 0.20318643748760223\n",
      "epoch: 12 step: 528, loss is 0.23448039591312408\n",
      "epoch: 12 step: 529, loss is 0.15566551685333252\n",
      "epoch: 12 step: 530, loss is 0.1917731910943985\n",
      "epoch: 12 step: 531, loss is 0.25544029474258423\n",
      "epoch: 12 step: 532, loss is 0.1825781911611557\n",
      "epoch: 12 step: 533, loss is 0.12887051701545715\n",
      "epoch: 12 step: 534, loss is 0.24663294851779938\n",
      "epoch: 12 step: 535, loss is 0.2994699776172638\n",
      "epoch: 12 step: 536, loss is 0.33562108874320984\n",
      "epoch: 12 step: 537, loss is 0.268105685710907\n",
      "epoch: 12 step: 538, loss is 0.11943069845438004\n",
      "epoch: 12 step: 539, loss is 0.1406523883342743\n",
      "epoch: 12 step: 540, loss is 0.16975539922714233\n",
      "epoch: 12 step: 541, loss is 0.17447422444820404\n",
      "epoch: 12 step: 542, loss is 0.22726957499980927\n",
      "epoch: 12 step: 543, loss is 0.40811726450920105\n",
      "epoch: 12 step: 544, loss is 0.30638670921325684\n",
      "epoch: 12 step: 545, loss is 0.31201380491256714\n",
      "epoch: 12 step: 546, loss is 0.2856372594833374\n",
      "epoch: 12 step: 547, loss is 0.24175913631916046\n",
      "epoch: 12 step: 548, loss is 0.33061107993125916\n",
      "epoch: 12 step: 549, loss is 0.07236705720424652\n",
      "epoch: 12 step: 550, loss is 0.25191041827201843\n",
      "epoch: 12 step: 551, loss is 0.16945035755634308\n",
      "epoch: 12 step: 552, loss is 0.23560506105422974\n",
      "epoch: 12 step: 553, loss is 0.21900467574596405\n",
      "epoch: 12 step: 554, loss is 0.17365828156471252\n",
      "epoch: 12 step: 555, loss is 0.5009672045707703\n",
      "epoch: 12 step: 556, loss is 0.1936100870370865\n",
      "epoch: 12 step: 557, loss is 0.21587564051151276\n",
      "epoch: 12 step: 558, loss is 0.27116212248802185\n",
      "epoch: 12 step: 559, loss is 0.20039182901382446\n",
      "epoch: 12 step: 560, loss is 0.26711034774780273\n",
      "epoch: 12 step: 561, loss is 0.24705058336257935\n",
      "epoch: 12 step: 562, loss is 0.07446455210447311\n",
      "epoch: 12 step: 563, loss is 0.30364954471588135\n",
      "epoch: 12 step: 564, loss is 0.35951119661331177\n",
      "epoch: 12 step: 565, loss is 0.30469807982444763\n",
      "epoch: 12 step: 566, loss is 0.132547989487648\n",
      "epoch: 12 step: 567, loss is 0.1919950395822525\n",
      "epoch: 12 step: 568, loss is 0.20140260457992554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 569, loss is 0.10008087754249573\n",
      "epoch: 12 step: 570, loss is 0.1678885966539383\n",
      "epoch: 12 step: 571, loss is 0.1568831354379654\n",
      "epoch: 12 step: 572, loss is 0.1403583288192749\n",
      "epoch: 12 step: 573, loss is 0.1829088181257248\n",
      "epoch: 12 step: 574, loss is 0.484506219625473\n",
      "epoch: 12 step: 575, loss is 0.2004510760307312\n",
      "epoch: 12 step: 576, loss is 0.16725841164588928\n",
      "epoch: 12 step: 577, loss is 0.197671577334404\n",
      "epoch: 12 step: 578, loss is 0.24291226267814636\n",
      "epoch: 12 step: 579, loss is 0.32579052448272705\n",
      "epoch: 12 step: 580, loss is 0.16704706847667694\n",
      "epoch: 12 step: 581, loss is 0.2530340254306793\n",
      "epoch: 12 step: 582, loss is 0.12840710580348969\n",
      "epoch: 12 step: 583, loss is 0.19350731372833252\n",
      "epoch: 12 step: 584, loss is 0.3005003333091736\n",
      "epoch: 12 step: 585, loss is 0.3031623661518097\n",
      "epoch: 12 step: 586, loss is 0.21303565800189972\n",
      "epoch: 12 step: 587, loss is 0.06620123982429504\n",
      "epoch: 12 step: 588, loss is 0.40833422541618347\n",
      "epoch: 12 step: 589, loss is 0.26265543699264526\n",
      "epoch: 12 step: 590, loss is 0.1731048822402954\n",
      "epoch: 12 step: 591, loss is 0.20716412365436554\n",
      "epoch: 12 step: 592, loss is 0.4059155285358429\n",
      "epoch: 12 step: 593, loss is 0.3561742901802063\n",
      "epoch: 12 step: 594, loss is 0.2705065906047821\n",
      "epoch: 12 step: 595, loss is 0.13269446790218353\n",
      "epoch: 12 step: 596, loss is 0.11009891331195831\n",
      "epoch: 12 step: 597, loss is 0.11124527454376221\n",
      "epoch: 12 step: 598, loss is 0.22098536789417267\n",
      "epoch: 12 step: 599, loss is 0.37204769253730774\n",
      "epoch: 12 step: 600, loss is 0.14787019789218903\n",
      "epoch: 12 step: 601, loss is 0.14821720123291016\n",
      "epoch: 12 step: 602, loss is 0.5126153826713562\n",
      "epoch: 12 step: 603, loss is 0.22322390973567963\n",
      "epoch: 12 step: 604, loss is 0.09829984605312347\n",
      "epoch: 12 step: 605, loss is 0.06446675956249237\n",
      "epoch: 12 step: 606, loss is 0.18836022913455963\n",
      "epoch: 12 step: 607, loss is 0.21783961355686188\n",
      "epoch: 12 step: 608, loss is 0.22898894548416138\n",
      "epoch: 12 step: 609, loss is 0.2471693754196167\n",
      "epoch: 12 step: 610, loss is 0.20172111690044403\n",
      "epoch: 12 step: 611, loss is 0.15776744484901428\n",
      "epoch: 12 step: 612, loss is 0.1293777972459793\n",
      "epoch: 12 step: 613, loss is 0.32402583956718445\n",
      "epoch: 12 step: 614, loss is 0.23632293939590454\n",
      "epoch: 12 step: 615, loss is 0.3118680417537689\n",
      "epoch: 12 step: 616, loss is 0.22057506442070007\n",
      "epoch: 12 step: 617, loss is 0.16127453744411469\n",
      "epoch: 12 step: 618, loss is 0.23027890920639038\n",
      "epoch: 12 step: 619, loss is 0.1421384960412979\n",
      "epoch: 12 step: 620, loss is 0.27441591024398804\n",
      "epoch: 12 step: 621, loss is 0.16026681661605835\n",
      "epoch: 12 step: 622, loss is 0.13794410228729248\n",
      "epoch: 12 step: 623, loss is 0.12061222642660141\n",
      "epoch: 12 step: 624, loss is 0.13571815192699432\n",
      "epoch: 12 step: 625, loss is 0.1355941891670227\n",
      "epoch: 12 step: 626, loss is 0.1652907431125641\n",
      "epoch: 12 step: 627, loss is 0.1506279855966568\n",
      "epoch: 12 step: 628, loss is 0.2103939801454544\n",
      "epoch: 12 step: 629, loss is 0.14294341206550598\n",
      "epoch: 12 step: 630, loss is 0.20762000977993011\n",
      "epoch: 12 step: 631, loss is 0.24306416511535645\n",
      "epoch: 12 step: 632, loss is 0.2762185335159302\n",
      "epoch: 12 step: 633, loss is 0.12835343182086945\n",
      "epoch: 12 step: 634, loss is 0.24902915954589844\n",
      "epoch: 12 step: 635, loss is 0.3037479817867279\n",
      "epoch: 12 step: 636, loss is 0.2405640035867691\n",
      "epoch: 12 step: 637, loss is 0.3431880474090576\n",
      "epoch: 12 step: 638, loss is 0.29423147439956665\n",
      "epoch: 12 step: 639, loss is 0.1832040250301361\n",
      "epoch: 12 step: 640, loss is 0.31391340494155884\n",
      "epoch: 12 step: 641, loss is 0.21726597845554352\n",
      "epoch: 12 step: 642, loss is 0.2972058355808258\n",
      "epoch: 12 step: 643, loss is 0.33995649218559265\n",
      "epoch: 12 step: 644, loss is 0.317119300365448\n",
      "epoch: 12 step: 645, loss is 0.23242901265621185\n",
      "epoch: 12 step: 646, loss is 0.14199253916740417\n",
      "epoch: 12 step: 647, loss is 0.139632448554039\n",
      "epoch: 12 step: 648, loss is 0.3185790777206421\n",
      "epoch: 12 step: 649, loss is 0.327144056558609\n",
      "epoch: 12 step: 650, loss is 0.21601735055446625\n",
      "epoch: 12 step: 651, loss is 0.2526243031024933\n",
      "epoch: 12 step: 652, loss is 0.15514463186264038\n",
      "epoch: 12 step: 653, loss is 0.25672146677970886\n",
      "epoch: 12 step: 654, loss is 0.1513684242963791\n",
      "epoch: 12 step: 655, loss is 0.3255916237831116\n",
      "epoch: 12 step: 656, loss is 0.10879364609718323\n",
      "epoch: 12 step: 657, loss is 0.33605867624282837\n",
      "epoch: 12 step: 658, loss is 0.250701904296875\n",
      "epoch: 12 step: 659, loss is 0.3054657280445099\n",
      "epoch: 12 step: 660, loss is 0.20614583790302277\n",
      "epoch: 12 step: 661, loss is 0.1934833973646164\n",
      "epoch: 12 step: 662, loss is 0.11584163457155228\n",
      "epoch: 12 step: 663, loss is 0.14629659056663513\n",
      "epoch: 12 step: 664, loss is 0.12352721393108368\n",
      "epoch: 12 step: 665, loss is 0.13848325610160828\n",
      "epoch: 12 step: 666, loss is 0.18263360857963562\n",
      "epoch: 12 step: 667, loss is 0.27224966883659363\n",
      "epoch: 12 step: 668, loss is 0.08699078112840652\n",
      "epoch: 12 step: 669, loss is 0.24630750715732574\n",
      "epoch: 12 step: 670, loss is 0.20397083461284637\n",
      "epoch: 12 step: 671, loss is 0.406910240650177\n",
      "epoch: 12 step: 672, loss is 0.43921589851379395\n",
      "epoch: 12 step: 673, loss is 0.2049160748720169\n",
      "epoch: 12 step: 674, loss is 0.22119317948818207\n",
      "epoch: 12 step: 675, loss is 0.191032275557518\n",
      "epoch: 12 step: 676, loss is 0.11907602846622467\n",
      "epoch: 12 step: 677, loss is 0.1345631629228592\n",
      "epoch: 12 step: 678, loss is 0.10321692377328873\n",
      "epoch: 12 step: 679, loss is 0.20029690861701965\n",
      "epoch: 12 step: 680, loss is 0.39561182260513306\n",
      "epoch: 12 step: 681, loss is 0.25906750559806824\n",
      "epoch: 12 step: 682, loss is 0.16734668612480164\n",
      "epoch: 12 step: 683, loss is 0.3120727837085724\n",
      "epoch: 12 step: 684, loss is 0.17611733078956604\n",
      "epoch: 12 step: 685, loss is 0.1751500815153122\n",
      "epoch: 12 step: 686, loss is 0.14015153050422668\n",
      "epoch: 12 step: 687, loss is 0.3655531108379364\n",
      "epoch: 12 step: 688, loss is 0.34703925251960754\n",
      "epoch: 12 step: 689, loss is 0.19850987195968628\n",
      "epoch: 12 step: 690, loss is 0.3158426284790039\n",
      "epoch: 12 step: 691, loss is 0.16519571840763092\n",
      "epoch: 12 step: 692, loss is 0.3962455093860626\n",
      "epoch: 12 step: 693, loss is 0.13730056583881378\n",
      "epoch: 12 step: 694, loss is 0.16795209050178528\n",
      "epoch: 12 step: 695, loss is 0.152898371219635\n",
      "epoch: 12 step: 696, loss is 0.24709075689315796\n",
      "epoch: 12 step: 697, loss is 0.26381510496139526\n",
      "epoch: 12 step: 698, loss is 0.34297794103622437\n",
      "epoch: 12 step: 699, loss is 0.21763846278190613\n",
      "epoch: 12 step: 700, loss is 0.5254039764404297\n",
      "epoch: 12 step: 701, loss is 0.3825802803039551\n",
      "epoch: 12 step: 702, loss is 0.10551557689905167\n",
      "epoch: 12 step: 703, loss is 0.27284127473831177\n",
      "epoch: 12 step: 704, loss is 0.19309201836585999\n",
      "epoch: 12 step: 705, loss is 0.22013796865940094\n",
      "epoch: 12 step: 706, loss is 0.28493940830230713\n",
      "epoch: 12 step: 707, loss is 0.3351684510707855\n",
      "epoch: 12 step: 708, loss is 0.13504676520824432\n",
      "epoch: 12 step: 709, loss is 0.17680199444293976\n",
      "epoch: 12 step: 710, loss is 0.19382058084011078\n",
      "epoch: 12 step: 711, loss is 0.2621205449104309\n",
      "epoch: 12 step: 712, loss is 0.20472654700279236\n",
      "epoch: 12 step: 713, loss is 0.11434260755777359\n",
      "epoch: 12 step: 714, loss is 0.26403605937957764\n",
      "epoch: 12 step: 715, loss is 0.18946310877799988\n",
      "epoch: 12 step: 716, loss is 0.1580764800310135\n",
      "epoch: 12 step: 717, loss is 0.08542845398187637\n",
      "epoch: 12 step: 718, loss is 0.19504079222679138\n",
      "epoch: 12 step: 719, loss is 0.2591377794742584\n",
      "epoch: 12 step: 720, loss is 0.22759416699409485\n",
      "epoch: 12 step: 721, loss is 0.28295257687568665\n",
      "epoch: 12 step: 722, loss is 0.1824798882007599\n",
      "epoch: 12 step: 723, loss is 0.18307049572467804\n",
      "epoch: 12 step: 724, loss is 0.19794681668281555\n",
      "epoch: 12 step: 725, loss is 0.16710424423217773\n",
      "epoch: 12 step: 726, loss is 0.24147950112819672\n",
      "epoch: 12 step: 727, loss is 0.2376716434955597\n",
      "epoch: 12 step: 728, loss is 0.11825037002563477\n",
      "epoch: 12 step: 729, loss is 0.4416598677635193\n",
      "epoch: 12 step: 730, loss is 0.2940332591533661\n",
      "epoch: 12 step: 731, loss is 0.257711797952652\n",
      "epoch: 12 step: 732, loss is 0.25547415018081665\n",
      "epoch: 12 step: 733, loss is 0.19505059719085693\n",
      "epoch: 12 step: 734, loss is 0.2821384370326996\n",
      "epoch: 12 step: 735, loss is 0.2582297623157501\n",
      "epoch: 12 step: 736, loss is 0.24256885051727295\n",
      "epoch: 12 step: 737, loss is 0.24524839222431183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 738, loss is 0.15601079165935516\n",
      "epoch: 12 step: 739, loss is 0.11478915065526962\n",
      "epoch: 12 step: 740, loss is 0.37410828471183777\n",
      "epoch: 12 step: 741, loss is 0.3001139760017395\n",
      "epoch: 12 step: 742, loss is 0.13105706870555878\n",
      "epoch: 12 step: 743, loss is 0.26668569445610046\n",
      "epoch: 12 step: 744, loss is 0.20471732318401337\n",
      "epoch: 12 step: 745, loss is 0.2739972770214081\n",
      "epoch: 12 step: 746, loss is 0.18741901218891144\n",
      "epoch: 12 step: 747, loss is 0.16536442935466766\n",
      "epoch: 12 step: 748, loss is 0.14288155734539032\n",
      "epoch: 12 step: 749, loss is 0.17083953320980072\n",
      "epoch: 12 step: 750, loss is 0.23837824165821075\n",
      "epoch: 12 step: 751, loss is 0.16601403057575226\n",
      "epoch: 12 step: 752, loss is 0.1411268264055252\n",
      "epoch: 12 step: 753, loss is 0.22363397479057312\n",
      "epoch: 12 step: 754, loss is 0.26713448762893677\n",
      "epoch: 12 step: 755, loss is 0.3287099301815033\n",
      "epoch: 12 step: 756, loss is 0.2521488666534424\n",
      "epoch: 12 step: 757, loss is 0.19115224480628967\n",
      "epoch: 12 step: 758, loss is 0.2649713158607483\n",
      "epoch: 12 step: 759, loss is 0.14108942449092865\n",
      "epoch: 12 step: 760, loss is 0.21854643523693085\n",
      "epoch: 12 step: 761, loss is 0.3013290762901306\n",
      "epoch: 12 step: 762, loss is 0.18368946015834808\n",
      "epoch: 12 step: 763, loss is 0.30126288533210754\n",
      "epoch: 12 step: 764, loss is 0.13545657694339752\n",
      "epoch: 12 step: 765, loss is 0.520272433757782\n",
      "epoch: 12 step: 766, loss is 0.22906258702278137\n",
      "epoch: 12 step: 767, loss is 0.20824943482875824\n",
      "epoch: 12 step: 768, loss is 0.3193736970424652\n",
      "epoch: 12 step: 769, loss is 0.33732402324676514\n",
      "epoch: 12 step: 770, loss is 0.25245603919029236\n",
      "epoch: 12 step: 771, loss is 0.25028297305107117\n",
      "epoch: 12 step: 772, loss is 0.15806518495082855\n",
      "epoch: 12 step: 773, loss is 0.2665538191795349\n",
      "epoch: 12 step: 774, loss is 0.2862914204597473\n",
      "epoch: 12 step: 775, loss is 0.08166240900754929\n",
      "epoch: 12 step: 776, loss is 0.42577314376831055\n",
      "epoch: 12 step: 777, loss is 0.16669540107250214\n",
      "epoch: 12 step: 778, loss is 0.2131035327911377\n",
      "epoch: 12 step: 779, loss is 0.21301637589931488\n",
      "epoch: 12 step: 780, loss is 0.18725557625293732\n",
      "epoch: 12 step: 781, loss is 0.2655907869338989\n",
      "epoch: 12 step: 782, loss is 0.1738990694284439\n",
      "epoch: 12 step: 783, loss is 0.18014073371887207\n",
      "epoch: 12 step: 784, loss is 0.32352641224861145\n",
      "epoch: 12 step: 785, loss is 0.20888492465019226\n",
      "epoch: 12 step: 786, loss is 0.3185153305530548\n",
      "epoch: 12 step: 787, loss is 0.23734134435653687\n",
      "epoch: 12 step: 788, loss is 0.1728757619857788\n",
      "epoch: 12 step: 789, loss is 0.19159050285816193\n",
      "epoch: 12 step: 790, loss is 0.13371887803077698\n",
      "epoch: 12 step: 791, loss is 0.1307159662246704\n",
      "epoch: 12 step: 792, loss is 0.13792970776557922\n",
      "epoch: 12 step: 793, loss is 0.23667354881763458\n",
      "epoch: 12 step: 794, loss is 0.22498778998851776\n",
      "epoch: 12 step: 795, loss is 0.1738717257976532\n",
      "epoch: 12 step: 796, loss is 0.2409999519586563\n",
      "epoch: 12 step: 797, loss is 0.1880815029144287\n",
      "epoch: 12 step: 798, loss is 0.1913139522075653\n",
      "epoch: 12 step: 799, loss is 0.21623317897319794\n",
      "epoch: 12 step: 800, loss is 0.2453087568283081\n",
      "epoch: 12 step: 801, loss is 0.24201294779777527\n",
      "epoch: 12 step: 802, loss is 0.19243620336055756\n",
      "epoch: 12 step: 803, loss is 0.3283875584602356\n",
      "epoch: 12 step: 804, loss is 0.13928337395191193\n",
      "epoch: 12 step: 805, loss is 0.1622585952281952\n",
      "epoch: 12 step: 806, loss is 0.21030473709106445\n",
      "epoch: 12 step: 807, loss is 0.27717354893684387\n",
      "epoch: 12 step: 808, loss is 0.1403060257434845\n",
      "epoch: 12 step: 809, loss is 0.21521015465259552\n",
      "epoch: 12 step: 810, loss is 0.1491851508617401\n",
      "epoch: 12 step: 811, loss is 0.16959846019744873\n",
      "epoch: 12 step: 812, loss is 0.10969389975070953\n",
      "epoch: 12 step: 813, loss is 0.2725067436695099\n",
      "epoch: 12 step: 814, loss is 0.21568244695663452\n",
      "epoch: 12 step: 815, loss is 0.2085718959569931\n",
      "epoch: 12 step: 816, loss is 0.11450468748807907\n",
      "epoch: 12 step: 817, loss is 0.3110910952091217\n",
      "epoch: 12 step: 818, loss is 0.6401113867759705\n",
      "epoch: 12 step: 819, loss is 0.12922799587249756\n",
      "epoch: 12 step: 820, loss is 0.16003860533237457\n",
      "epoch: 12 step: 821, loss is 0.1327751874923706\n",
      "epoch: 12 step: 822, loss is 0.21989043056964874\n",
      "epoch: 12 step: 823, loss is 0.20197324454784393\n",
      "epoch: 12 step: 824, loss is 0.22125130891799927\n",
      "epoch: 12 step: 825, loss is 0.16113461554050446\n",
      "epoch: 12 step: 826, loss is 0.22955182194709778\n",
      "epoch: 12 step: 827, loss is 0.20777009427547455\n",
      "epoch: 12 step: 828, loss is 0.12626883387565613\n",
      "epoch: 12 step: 829, loss is 0.22304534912109375\n",
      "epoch: 12 step: 830, loss is 0.2498503029346466\n",
      "epoch: 12 step: 831, loss is 0.14662760496139526\n",
      "epoch: 12 step: 832, loss is 0.27998989820480347\n",
      "epoch: 12 step: 833, loss is 0.30216512084007263\n",
      "epoch: 12 step: 834, loss is 0.12887324392795563\n",
      "epoch: 12 step: 835, loss is 0.14609012007713318\n",
      "epoch: 12 step: 836, loss is 0.19593402743339539\n",
      "epoch: 12 step: 837, loss is 0.2969644069671631\n",
      "epoch: 12 step: 838, loss is 0.18870843946933746\n",
      "epoch: 12 step: 839, loss is 0.27381882071495056\n",
      "epoch: 12 step: 840, loss is 0.14242640137672424\n",
      "epoch: 12 step: 841, loss is 0.21970491111278534\n",
      "epoch: 12 step: 842, loss is 0.1892828643321991\n",
      "epoch: 12 step: 843, loss is 0.3191239833831787\n",
      "epoch: 12 step: 844, loss is 0.191508948802948\n",
      "epoch: 12 step: 845, loss is 0.09008976817131042\n",
      "epoch: 12 step: 846, loss is 0.2321748286485672\n",
      "epoch: 12 step: 847, loss is 0.24473711848258972\n",
      "epoch: 12 step: 848, loss is 0.1654522716999054\n",
      "epoch: 12 step: 849, loss is 0.45014655590057373\n",
      "epoch: 12 step: 850, loss is 0.2033557891845703\n",
      "epoch: 12 step: 851, loss is 0.1709180474281311\n",
      "epoch: 12 step: 852, loss is 0.1499507576227188\n",
      "epoch: 12 step: 853, loss is 0.2575632631778717\n",
      "epoch: 12 step: 854, loss is 0.2799456715583801\n",
      "epoch: 12 step: 855, loss is 0.28949886560440063\n",
      "epoch: 12 step: 856, loss is 0.1986686736345291\n",
      "epoch: 12 step: 857, loss is 0.33843690156936646\n",
      "epoch: 12 step: 858, loss is 0.2226758748292923\n",
      "epoch: 12 step: 859, loss is 0.23040764033794403\n",
      "epoch: 12 step: 860, loss is 0.2540498673915863\n",
      "epoch: 12 step: 861, loss is 0.33924591541290283\n",
      "epoch: 12 step: 862, loss is 0.3016483783721924\n",
      "epoch: 12 step: 863, loss is 0.31285399198532104\n",
      "epoch: 12 step: 864, loss is 0.3370402455329895\n",
      "epoch: 12 step: 865, loss is 0.2965575158596039\n",
      "epoch: 12 step: 866, loss is 0.14297465980052948\n",
      "epoch: 12 step: 867, loss is 0.23849432170391083\n",
      "epoch: 12 step: 868, loss is 0.23933428525924683\n",
      "epoch: 12 step: 869, loss is 0.29639437794685364\n",
      "epoch: 12 step: 870, loss is 0.23582440614700317\n",
      "epoch: 12 step: 871, loss is 0.2872124910354614\n",
      "epoch: 12 step: 872, loss is 0.2268349677324295\n",
      "epoch: 12 step: 873, loss is 0.4356124699115753\n",
      "epoch: 12 step: 874, loss is 0.4301935136318207\n",
      "epoch: 12 step: 875, loss is 0.27245378494262695\n",
      "epoch: 12 step: 876, loss is 0.19444823265075684\n",
      "epoch: 12 step: 877, loss is 0.5582937002182007\n",
      "epoch: 12 step: 878, loss is 0.13346020877361298\n",
      "epoch: 12 step: 879, loss is 0.3345721364021301\n",
      "epoch: 12 step: 880, loss is 0.22059863805770874\n",
      "epoch: 12 step: 881, loss is 0.3541729152202606\n",
      "epoch: 12 step: 882, loss is 0.17633338272571564\n",
      "epoch: 12 step: 883, loss is 0.23157748579978943\n",
      "epoch: 12 step: 884, loss is 0.23009343445301056\n",
      "epoch: 12 step: 885, loss is 0.17275142669677734\n",
      "epoch: 12 step: 886, loss is 0.1662091314792633\n",
      "epoch: 12 step: 887, loss is 0.11879892647266388\n",
      "epoch: 12 step: 888, loss is 0.21156567335128784\n",
      "epoch: 12 step: 889, loss is 0.09653891623020172\n",
      "epoch: 12 step: 890, loss is 0.24158118665218353\n",
      "epoch: 12 step: 891, loss is 0.18643030524253845\n",
      "epoch: 12 step: 892, loss is 0.257856160402298\n",
      "epoch: 12 step: 893, loss is 0.1482534110546112\n",
      "epoch: 12 step: 894, loss is 0.18322652578353882\n",
      "epoch: 12 step: 895, loss is 0.2419288158416748\n",
      "epoch: 12 step: 896, loss is 0.2486056238412857\n",
      "epoch: 12 step: 897, loss is 0.09592010825872421\n",
      "epoch: 12 step: 898, loss is 0.17625565826892853\n",
      "epoch: 12 step: 899, loss is 0.24551533162593842\n",
      "epoch: 12 step: 900, loss is 0.24743743240833282\n",
      "epoch: 12 step: 901, loss is 0.35100844502449036\n",
      "epoch: 12 step: 902, loss is 0.2300005853176117\n",
      "epoch: 12 step: 903, loss is 0.16693663597106934\n",
      "epoch: 12 step: 904, loss is 0.15066015720367432\n",
      "epoch: 12 step: 905, loss is 0.2653408944606781\n",
      "epoch: 12 step: 906, loss is 0.2663527727127075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 907, loss is 0.25746098160743713\n",
      "epoch: 12 step: 908, loss is 0.2713579833507538\n",
      "epoch: 12 step: 909, loss is 0.217890202999115\n",
      "epoch: 12 step: 910, loss is 0.275422602891922\n",
      "epoch: 12 step: 911, loss is 0.13072557747364044\n",
      "epoch: 12 step: 912, loss is 0.28743085265159607\n",
      "epoch: 12 step: 913, loss is 0.2879578471183777\n",
      "epoch: 12 step: 914, loss is 0.1366807520389557\n",
      "epoch: 12 step: 915, loss is 0.2728795111179352\n",
      "epoch: 12 step: 916, loss is 0.12466078996658325\n",
      "epoch: 12 step: 917, loss is 0.30829307436943054\n",
      "epoch: 12 step: 918, loss is 0.21941907703876495\n",
      "epoch: 12 step: 919, loss is 0.1317654252052307\n",
      "epoch: 12 step: 920, loss is 0.4161103069782257\n",
      "epoch: 12 step: 921, loss is 0.17737337946891785\n",
      "epoch: 12 step: 922, loss is 0.32840514183044434\n",
      "epoch: 12 step: 923, loss is 0.26445597410202026\n",
      "epoch: 12 step: 924, loss is 0.14611013233661652\n",
      "epoch: 12 step: 925, loss is 0.1389838606119156\n",
      "epoch: 12 step: 926, loss is 0.2565055787563324\n",
      "epoch: 12 step: 927, loss is 0.18283504247665405\n",
      "epoch: 12 step: 928, loss is 0.2423904687166214\n",
      "epoch: 12 step: 929, loss is 0.13921621441841125\n",
      "epoch: 12 step: 930, loss is 0.21580299735069275\n",
      "epoch: 12 step: 931, loss is 0.20203901827335358\n",
      "epoch: 12 step: 932, loss is 0.21542826294898987\n",
      "epoch: 12 step: 933, loss is 0.16497579216957092\n",
      "epoch: 12 step: 934, loss is 0.15550707280635834\n",
      "epoch: 12 step: 935, loss is 0.08731874078512192\n",
      "epoch: 12 step: 936, loss is 0.4513363540172577\n",
      "epoch: 12 step: 937, loss is 0.245079904794693\n",
      "epoch: 13 step: 1, loss is 0.21629215776920319\n",
      "epoch: 13 step: 2, loss is 0.3182196021080017\n",
      "epoch: 13 step: 3, loss is 0.4162226617336273\n",
      "epoch: 13 step: 4, loss is 0.2820557653903961\n",
      "epoch: 13 step: 5, loss is 0.0735601931810379\n",
      "epoch: 13 step: 6, loss is 0.2589065730571747\n",
      "epoch: 13 step: 7, loss is 0.16746971011161804\n",
      "epoch: 13 step: 8, loss is 0.18390610814094543\n",
      "epoch: 13 step: 9, loss is 0.18061509728431702\n",
      "epoch: 13 step: 10, loss is 0.49730178713798523\n",
      "epoch: 13 step: 11, loss is 0.18241481482982635\n",
      "epoch: 13 step: 12, loss is 0.3572095036506653\n",
      "epoch: 13 step: 13, loss is 0.2729019224643707\n",
      "epoch: 13 step: 14, loss is 0.2663722634315491\n",
      "epoch: 13 step: 15, loss is 0.20146863162517548\n",
      "epoch: 13 step: 16, loss is 0.19299961626529694\n",
      "epoch: 13 step: 17, loss is 0.2148807793855667\n",
      "epoch: 13 step: 18, loss is 0.37347105145454407\n",
      "epoch: 13 step: 19, loss is 0.18344709277153015\n",
      "epoch: 13 step: 20, loss is 0.14262832701206207\n",
      "epoch: 13 step: 21, loss is 0.17598871886730194\n",
      "epoch: 13 step: 22, loss is 0.3334047794342041\n",
      "epoch: 13 step: 23, loss is 0.1694951206445694\n",
      "epoch: 13 step: 24, loss is 0.23119579255580902\n",
      "epoch: 13 step: 25, loss is 0.14468565583229065\n",
      "epoch: 13 step: 26, loss is 0.09984529763460159\n",
      "epoch: 13 step: 27, loss is 0.19965416193008423\n",
      "epoch: 13 step: 28, loss is 0.24065154790878296\n",
      "epoch: 13 step: 29, loss is 0.24655473232269287\n",
      "epoch: 13 step: 30, loss is 0.28326135873794556\n",
      "epoch: 13 step: 31, loss is 0.25462618470191956\n",
      "epoch: 13 step: 32, loss is 0.3486652970314026\n",
      "epoch: 13 step: 33, loss is 0.24657928943634033\n",
      "epoch: 13 step: 34, loss is 0.4141688048839569\n",
      "epoch: 13 step: 35, loss is 0.27475330233573914\n",
      "epoch: 13 step: 36, loss is 0.2338167279958725\n",
      "epoch: 13 step: 37, loss is 0.24494555592536926\n",
      "epoch: 13 step: 38, loss is 0.22583676874637604\n",
      "epoch: 13 step: 39, loss is 0.32632356882095337\n",
      "epoch: 13 step: 40, loss is 0.2500239312648773\n",
      "epoch: 13 step: 41, loss is 0.1189216896891594\n",
      "epoch: 13 step: 42, loss is 0.12773074209690094\n",
      "epoch: 13 step: 43, loss is 0.18011094629764557\n",
      "epoch: 13 step: 44, loss is 0.19233204424381256\n",
      "epoch: 13 step: 45, loss is 0.219353586435318\n",
      "epoch: 13 step: 46, loss is 0.24501974880695343\n",
      "epoch: 13 step: 47, loss is 0.33861464262008667\n",
      "epoch: 13 step: 48, loss is 0.23714931309223175\n",
      "epoch: 13 step: 49, loss is 0.12400050461292267\n",
      "epoch: 13 step: 50, loss is 0.14075179398059845\n",
      "epoch: 13 step: 51, loss is 0.290542334318161\n",
      "epoch: 13 step: 52, loss is 0.15058642625808716\n",
      "epoch: 13 step: 53, loss is 0.26546838879585266\n",
      "epoch: 13 step: 54, loss is 0.16639608144760132\n",
      "epoch: 13 step: 55, loss is 0.18443742394447327\n",
      "epoch: 13 step: 56, loss is 0.23191531002521515\n",
      "epoch: 13 step: 57, loss is 0.2527649998664856\n",
      "epoch: 13 step: 58, loss is 0.36366498470306396\n",
      "epoch: 13 step: 59, loss is 0.29134660959243774\n",
      "epoch: 13 step: 60, loss is 0.1590661108493805\n",
      "epoch: 13 step: 61, loss is 0.2642797529697418\n",
      "epoch: 13 step: 62, loss is 0.13370709121227264\n",
      "epoch: 13 step: 63, loss is 0.14666412770748138\n",
      "epoch: 13 step: 64, loss is 0.28788724541664124\n",
      "epoch: 13 step: 65, loss is 0.36172860860824585\n",
      "epoch: 13 step: 66, loss is 0.3623700737953186\n",
      "epoch: 13 step: 67, loss is 0.1257747858762741\n",
      "epoch: 13 step: 68, loss is 0.17327561974525452\n",
      "epoch: 13 step: 69, loss is 0.11941593885421753\n",
      "epoch: 13 step: 70, loss is 0.192603200674057\n",
      "epoch: 13 step: 71, loss is 0.1442657858133316\n",
      "epoch: 13 step: 72, loss is 0.26398965716362\n",
      "epoch: 13 step: 73, loss is 0.42853355407714844\n",
      "epoch: 13 step: 74, loss is 0.25404173135757446\n",
      "epoch: 13 step: 75, loss is 0.1189618855714798\n",
      "epoch: 13 step: 76, loss is 0.25887423753738403\n",
      "epoch: 13 step: 77, loss is 0.14437894523143768\n",
      "epoch: 13 step: 78, loss is 0.18364013731479645\n",
      "epoch: 13 step: 79, loss is 0.2616243362426758\n",
      "epoch: 13 step: 80, loss is 0.14331449568271637\n",
      "epoch: 13 step: 81, loss is 0.19887526333332062\n",
      "epoch: 13 step: 82, loss is 0.18516945838928223\n",
      "epoch: 13 step: 83, loss is 0.14778462052345276\n",
      "epoch: 13 step: 84, loss is 0.35069966316223145\n",
      "epoch: 13 step: 85, loss is 0.19373659789562225\n",
      "epoch: 13 step: 86, loss is 0.17565345764160156\n",
      "epoch: 13 step: 87, loss is 0.24217750132083893\n",
      "epoch: 13 step: 88, loss is 0.25903406739234924\n",
      "epoch: 13 step: 89, loss is 0.27116212248802185\n",
      "epoch: 13 step: 90, loss is 0.09539437294006348\n",
      "epoch: 13 step: 91, loss is 0.2781318724155426\n",
      "epoch: 13 step: 92, loss is 0.08959626406431198\n",
      "epoch: 13 step: 93, loss is 0.13508281111717224\n",
      "epoch: 13 step: 94, loss is 0.24920791387557983\n",
      "epoch: 13 step: 95, loss is 0.16040675342082977\n",
      "epoch: 13 step: 96, loss is 0.22707287967205048\n",
      "epoch: 13 step: 97, loss is 0.20669025182724\n",
      "epoch: 13 step: 98, loss is 0.09164221584796906\n",
      "epoch: 13 step: 99, loss is 0.2768792510032654\n",
      "epoch: 13 step: 100, loss is 0.15057861804962158\n",
      "epoch: 13 step: 101, loss is 0.20184864103794098\n",
      "epoch: 13 step: 102, loss is 0.2827811539173126\n",
      "epoch: 13 step: 103, loss is 0.19935539364814758\n",
      "epoch: 13 step: 104, loss is 0.17206647992134094\n",
      "epoch: 13 step: 105, loss is 0.1587682068347931\n",
      "epoch: 13 step: 106, loss is 0.22733815014362335\n",
      "epoch: 13 step: 107, loss is 0.16321471333503723\n",
      "epoch: 13 step: 108, loss is 0.18627533316612244\n",
      "epoch: 13 step: 109, loss is 0.22670143842697144\n",
      "epoch: 13 step: 110, loss is 0.3843125104904175\n",
      "epoch: 13 step: 111, loss is 0.30566149950027466\n",
      "epoch: 13 step: 112, loss is 0.1869315356016159\n",
      "epoch: 13 step: 113, loss is 0.31801116466522217\n",
      "epoch: 13 step: 114, loss is 0.24995334446430206\n",
      "epoch: 13 step: 115, loss is 0.37612247467041016\n",
      "epoch: 13 step: 116, loss is 0.3276602327823639\n",
      "epoch: 13 step: 117, loss is 0.21805395185947418\n",
      "epoch: 13 step: 118, loss is 0.17512033879756927\n",
      "epoch: 13 step: 119, loss is 0.185154527425766\n",
      "epoch: 13 step: 120, loss is 0.28667616844177246\n",
      "epoch: 13 step: 121, loss is 0.299757719039917\n",
      "epoch: 13 step: 122, loss is 0.16514426469802856\n",
      "epoch: 13 step: 123, loss is 0.23092012107372284\n",
      "epoch: 13 step: 124, loss is 0.10511433333158493\n",
      "epoch: 13 step: 125, loss is 0.17985308170318604\n",
      "epoch: 13 step: 126, loss is 0.34996727108955383\n",
      "epoch: 13 step: 127, loss is 0.1995873898267746\n",
      "epoch: 13 step: 128, loss is 0.1983615905046463\n",
      "epoch: 13 step: 129, loss is 0.17321841418743134\n",
      "epoch: 13 step: 130, loss is 0.24144373834133148\n",
      "epoch: 13 step: 131, loss is 0.2711988687515259\n",
      "epoch: 13 step: 132, loss is 0.1721659004688263\n",
      "epoch: 13 step: 133, loss is 0.09837143123149872\n",
      "epoch: 13 step: 134, loss is 0.2516481578350067\n",
      "epoch: 13 step: 135, loss is 0.2358192652463913\n",
      "epoch: 13 step: 136, loss is 0.1833314597606659\n",
      "epoch: 13 step: 137, loss is 0.14212588965892792\n",
      "epoch: 13 step: 138, loss is 0.2998577654361725\n",
      "epoch: 13 step: 139, loss is 0.2625279128551483\n",
      "epoch: 13 step: 140, loss is 0.14148446917533875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 141, loss is 0.17719972133636475\n",
      "epoch: 13 step: 142, loss is 0.18343091011047363\n",
      "epoch: 13 step: 143, loss is 0.26198357343673706\n",
      "epoch: 13 step: 144, loss is 0.22756464779376984\n",
      "epoch: 13 step: 145, loss is 0.21271221339702606\n",
      "epoch: 13 step: 146, loss is 0.19597271084785461\n",
      "epoch: 13 step: 147, loss is 0.3003205358982086\n",
      "epoch: 13 step: 148, loss is 0.07531522959470749\n",
      "epoch: 13 step: 149, loss is 0.16967099905014038\n",
      "epoch: 13 step: 150, loss is 0.20849764347076416\n",
      "epoch: 13 step: 151, loss is 0.3879322409629822\n",
      "epoch: 13 step: 152, loss is 0.14414064586162567\n",
      "epoch: 13 step: 153, loss is 0.25120046734809875\n",
      "epoch: 13 step: 154, loss is 0.2190326303243637\n",
      "epoch: 13 step: 155, loss is 0.15626868605613708\n",
      "epoch: 13 step: 156, loss is 0.18923354148864746\n",
      "epoch: 13 step: 157, loss is 0.22138847410678864\n",
      "epoch: 13 step: 158, loss is 0.23030231893062592\n",
      "epoch: 13 step: 159, loss is 0.16931793093681335\n",
      "epoch: 13 step: 160, loss is 0.26421329379081726\n",
      "epoch: 13 step: 161, loss is 0.15984703600406647\n",
      "epoch: 13 step: 162, loss is 0.04549479857087135\n",
      "epoch: 13 step: 163, loss is 0.2866258919239044\n",
      "epoch: 13 step: 164, loss is 0.26892706751823425\n",
      "epoch: 13 step: 165, loss is 0.2236373871564865\n",
      "epoch: 13 step: 166, loss is 0.24030032753944397\n",
      "epoch: 13 step: 167, loss is 0.17792920768260956\n",
      "epoch: 13 step: 168, loss is 0.22545991837978363\n",
      "epoch: 13 step: 169, loss is 0.2518260180950165\n",
      "epoch: 13 step: 170, loss is 0.16175122559070587\n",
      "epoch: 13 step: 171, loss is 0.10214472562074661\n",
      "epoch: 13 step: 172, loss is 0.14352014660835266\n",
      "epoch: 13 step: 173, loss is 0.10745379328727722\n",
      "epoch: 13 step: 174, loss is 0.17236176133155823\n",
      "epoch: 13 step: 175, loss is 0.25999364256858826\n",
      "epoch: 13 step: 176, loss is 0.28212955594062805\n",
      "epoch: 13 step: 177, loss is 0.41254252195358276\n",
      "epoch: 13 step: 178, loss is 0.09901143610477448\n",
      "epoch: 13 step: 179, loss is 0.3001253306865692\n",
      "epoch: 13 step: 180, loss is 0.18255269527435303\n",
      "epoch: 13 step: 181, loss is 0.34498924016952515\n",
      "epoch: 13 step: 182, loss is 0.3491944968700409\n",
      "epoch: 13 step: 183, loss is 0.1482214629650116\n",
      "epoch: 13 step: 184, loss is 0.11628979444503784\n",
      "epoch: 13 step: 185, loss is 0.1677602082490921\n",
      "epoch: 13 step: 186, loss is 0.4254452884197235\n",
      "epoch: 13 step: 187, loss is 0.2827828526496887\n",
      "epoch: 13 step: 188, loss is 0.24856023490428925\n",
      "epoch: 13 step: 189, loss is 0.20832879841327667\n",
      "epoch: 13 step: 190, loss is 0.19782871007919312\n",
      "epoch: 13 step: 191, loss is 0.2088450789451599\n",
      "epoch: 13 step: 192, loss is 0.31132301688194275\n",
      "epoch: 13 step: 193, loss is 0.20422755181789398\n",
      "epoch: 13 step: 194, loss is 0.30624496936798096\n",
      "epoch: 13 step: 195, loss is 0.2719240188598633\n",
      "epoch: 13 step: 196, loss is 0.22544211149215698\n",
      "epoch: 13 step: 197, loss is 0.20702898502349854\n",
      "epoch: 13 step: 198, loss is 0.30280575156211853\n",
      "epoch: 13 step: 199, loss is 0.24998095631599426\n",
      "epoch: 13 step: 200, loss is 0.3150251507759094\n",
      "epoch: 13 step: 201, loss is 0.22804127633571625\n",
      "epoch: 13 step: 202, loss is 0.30784711241722107\n",
      "epoch: 13 step: 203, loss is 0.199235737323761\n",
      "epoch: 13 step: 204, loss is 0.30759233236312866\n",
      "epoch: 13 step: 205, loss is 0.3575517237186432\n",
      "epoch: 13 step: 206, loss is 0.3039160966873169\n",
      "epoch: 13 step: 207, loss is 0.22089794278144836\n",
      "epoch: 13 step: 208, loss is 0.17227528989315033\n",
      "epoch: 13 step: 209, loss is 0.20412921905517578\n",
      "epoch: 13 step: 210, loss is 0.2873772978782654\n",
      "epoch: 13 step: 211, loss is 0.30812421441078186\n",
      "epoch: 13 step: 212, loss is 0.36187732219696045\n",
      "epoch: 13 step: 213, loss is 0.2607630789279938\n",
      "epoch: 13 step: 214, loss is 0.0802823156118393\n",
      "epoch: 13 step: 215, loss is 0.33427169919013977\n",
      "epoch: 13 step: 216, loss is 0.13453331589698792\n",
      "epoch: 13 step: 217, loss is 0.2457982450723648\n",
      "epoch: 13 step: 218, loss is 0.2652501165866852\n",
      "epoch: 13 step: 219, loss is 0.36182111501693726\n",
      "epoch: 13 step: 220, loss is 0.15331816673278809\n",
      "epoch: 13 step: 221, loss is 0.243752121925354\n",
      "epoch: 13 step: 222, loss is 0.11148343235254288\n",
      "epoch: 13 step: 223, loss is 0.2172744870185852\n",
      "epoch: 13 step: 224, loss is 0.2941920757293701\n",
      "epoch: 13 step: 225, loss is 0.308481901884079\n",
      "epoch: 13 step: 226, loss is 0.22197522222995758\n",
      "epoch: 13 step: 227, loss is 0.18967381119728088\n",
      "epoch: 13 step: 228, loss is 0.10430224239826202\n",
      "epoch: 13 step: 229, loss is 0.11159786581993103\n",
      "epoch: 13 step: 230, loss is 0.12877130508422852\n",
      "epoch: 13 step: 231, loss is 0.21338161826133728\n",
      "epoch: 13 step: 232, loss is 0.21358036994934082\n",
      "epoch: 13 step: 233, loss is 0.19248202443122864\n",
      "epoch: 13 step: 234, loss is 0.1355924904346466\n",
      "epoch: 13 step: 235, loss is 0.24345572292804718\n",
      "epoch: 13 step: 236, loss is 0.22840552031993866\n",
      "epoch: 13 step: 237, loss is 0.164934903383255\n",
      "epoch: 13 step: 238, loss is 0.23698629438877106\n",
      "epoch: 13 step: 239, loss is 0.1462671458721161\n",
      "epoch: 13 step: 240, loss is 0.33593809604644775\n",
      "epoch: 13 step: 241, loss is 0.18061912059783936\n",
      "epoch: 13 step: 242, loss is 0.2835681438446045\n",
      "epoch: 13 step: 243, loss is 0.40561643242836\n",
      "epoch: 13 step: 244, loss is 0.16313689947128296\n",
      "epoch: 13 step: 245, loss is 0.23411792516708374\n",
      "epoch: 13 step: 246, loss is 0.15777155756950378\n",
      "epoch: 13 step: 247, loss is 0.23096394538879395\n",
      "epoch: 13 step: 248, loss is 0.2591048777103424\n",
      "epoch: 13 step: 249, loss is 0.18295487761497498\n",
      "epoch: 13 step: 250, loss is 0.21774275600910187\n",
      "epoch: 13 step: 251, loss is 0.08238838613033295\n",
      "epoch: 13 step: 252, loss is 0.138080894947052\n",
      "epoch: 13 step: 253, loss is 0.07410459965467453\n",
      "epoch: 13 step: 254, loss is 0.2223142832517624\n",
      "epoch: 13 step: 255, loss is 0.41029202938079834\n",
      "epoch: 13 step: 256, loss is 0.2523149847984314\n",
      "epoch: 13 step: 257, loss is 0.08657136559486389\n",
      "epoch: 13 step: 258, loss is 0.17255491018295288\n",
      "epoch: 13 step: 259, loss is 0.25242510437965393\n",
      "epoch: 13 step: 260, loss is 0.13241194188594818\n",
      "epoch: 13 step: 261, loss is 0.25689274072647095\n",
      "epoch: 13 step: 262, loss is 0.4126248061656952\n",
      "epoch: 13 step: 263, loss is 0.21425287425518036\n",
      "epoch: 13 step: 264, loss is 0.23640042543411255\n",
      "epoch: 13 step: 265, loss is 0.20552483201026917\n",
      "epoch: 13 step: 266, loss is 0.28311607241630554\n",
      "epoch: 13 step: 267, loss is 0.4839363992214203\n",
      "epoch: 13 step: 268, loss is 0.18476495146751404\n",
      "epoch: 13 step: 269, loss is 0.26351043581962585\n",
      "epoch: 13 step: 270, loss is 0.3007314205169678\n",
      "epoch: 13 step: 271, loss is 0.26123467087745667\n",
      "epoch: 13 step: 272, loss is 0.2358667552471161\n",
      "epoch: 13 step: 273, loss is 0.24917255342006683\n",
      "epoch: 13 step: 274, loss is 0.21087729930877686\n",
      "epoch: 13 step: 275, loss is 0.3726399540901184\n",
      "epoch: 13 step: 276, loss is 0.21634835004806519\n",
      "epoch: 13 step: 277, loss is 0.21487312018871307\n",
      "epoch: 13 step: 278, loss is 0.14384756982326508\n",
      "epoch: 13 step: 279, loss is 0.23268386721611023\n",
      "epoch: 13 step: 280, loss is 0.21303506195545197\n",
      "epoch: 13 step: 281, loss is 0.31390947103500366\n",
      "epoch: 13 step: 282, loss is 0.2755674719810486\n",
      "epoch: 13 step: 283, loss is 0.21608389914035797\n",
      "epoch: 13 step: 284, loss is 0.17756009101867676\n",
      "epoch: 13 step: 285, loss is 0.23098236322402954\n",
      "epoch: 13 step: 286, loss is 0.18573880195617676\n",
      "epoch: 13 step: 287, loss is 0.19990617036819458\n",
      "epoch: 13 step: 288, loss is 0.35319891571998596\n",
      "epoch: 13 step: 289, loss is 0.30681484937667847\n",
      "epoch: 13 step: 290, loss is 0.25131329894065857\n",
      "epoch: 13 step: 291, loss is 0.26583048701286316\n",
      "epoch: 13 step: 292, loss is 0.3285248875617981\n",
      "epoch: 13 step: 293, loss is 0.26727721095085144\n",
      "epoch: 13 step: 294, loss is 0.34538644552230835\n",
      "epoch: 13 step: 295, loss is 0.1078311949968338\n",
      "epoch: 13 step: 296, loss is 0.26490047574043274\n",
      "epoch: 13 step: 297, loss is 0.1826624572277069\n",
      "epoch: 13 step: 298, loss is 0.3059038817882538\n",
      "epoch: 13 step: 299, loss is 0.17032159864902496\n",
      "epoch: 13 step: 300, loss is 0.4001464545726776\n",
      "epoch: 13 step: 301, loss is 0.17314426600933075\n",
      "epoch: 13 step: 302, loss is 0.18388447165489197\n",
      "epoch: 13 step: 303, loss is 0.3203299045562744\n",
      "epoch: 13 step: 304, loss is 0.19918015599250793\n",
      "epoch: 13 step: 305, loss is 0.18612094223499298\n",
      "epoch: 13 step: 306, loss is 0.22486612200737\n",
      "epoch: 13 step: 307, loss is 0.26968133449554443\n",
      "epoch: 13 step: 308, loss is 0.10579269379377365\n",
      "epoch: 13 step: 309, loss is 0.3625093400478363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 310, loss is 0.3464913070201874\n",
      "epoch: 13 step: 311, loss is 0.19474701583385468\n",
      "epoch: 13 step: 312, loss is 0.3756848871707916\n",
      "epoch: 13 step: 313, loss is 0.1758752018213272\n",
      "epoch: 13 step: 314, loss is 0.17144162952899933\n",
      "epoch: 13 step: 315, loss is 0.27066290378570557\n",
      "epoch: 13 step: 316, loss is 0.23873527348041534\n",
      "epoch: 13 step: 317, loss is 0.19353002309799194\n",
      "epoch: 13 step: 318, loss is 0.19624176621437073\n",
      "epoch: 13 step: 319, loss is 0.20625662803649902\n",
      "epoch: 13 step: 320, loss is 0.1638152301311493\n",
      "epoch: 13 step: 321, loss is 0.3910757899284363\n",
      "epoch: 13 step: 322, loss is 0.17479877173900604\n",
      "epoch: 13 step: 323, loss is 0.22350873053073883\n",
      "epoch: 13 step: 324, loss is 0.29041728377342224\n",
      "epoch: 13 step: 325, loss is 0.2278541773557663\n",
      "epoch: 13 step: 326, loss is 0.21959125995635986\n",
      "epoch: 13 step: 327, loss is 0.1923951506614685\n",
      "epoch: 13 step: 328, loss is 0.3189029097557068\n",
      "epoch: 13 step: 329, loss is 0.168975830078125\n",
      "epoch: 13 step: 330, loss is 0.36963000893592834\n",
      "epoch: 13 step: 331, loss is 0.1544371247291565\n",
      "epoch: 13 step: 332, loss is 0.3348730206489563\n",
      "epoch: 13 step: 333, loss is 0.13585735857486725\n",
      "epoch: 13 step: 334, loss is 0.22280055284500122\n",
      "epoch: 13 step: 335, loss is 0.09624989330768585\n",
      "epoch: 13 step: 336, loss is 0.3524233400821686\n",
      "epoch: 13 step: 337, loss is 0.23121842741966248\n",
      "epoch: 13 step: 338, loss is 0.11582616716623306\n",
      "epoch: 13 step: 339, loss is 0.3263514041900635\n",
      "epoch: 13 step: 340, loss is 0.2226288616657257\n",
      "epoch: 13 step: 341, loss is 0.3378939628601074\n",
      "epoch: 13 step: 342, loss is 0.2490161508321762\n",
      "epoch: 13 step: 343, loss is 0.16305124759674072\n",
      "epoch: 13 step: 344, loss is 0.22293102741241455\n",
      "epoch: 13 step: 345, loss is 0.16605284810066223\n",
      "epoch: 13 step: 346, loss is 0.16901282966136932\n",
      "epoch: 13 step: 347, loss is 0.09627506881952286\n",
      "epoch: 13 step: 348, loss is 0.2927054762840271\n",
      "epoch: 13 step: 349, loss is 0.37522509694099426\n",
      "epoch: 13 step: 350, loss is 0.20801369845867157\n",
      "epoch: 13 step: 351, loss is 0.34545615315437317\n",
      "epoch: 13 step: 352, loss is 0.2217492312192917\n",
      "epoch: 13 step: 353, loss is 0.35374388098716736\n",
      "epoch: 13 step: 354, loss is 0.20546269416809082\n",
      "epoch: 13 step: 355, loss is 0.1901257038116455\n",
      "epoch: 13 step: 356, loss is 0.23424014449119568\n",
      "epoch: 13 step: 357, loss is 0.2156403362751007\n",
      "epoch: 13 step: 358, loss is 0.14477474987506866\n",
      "epoch: 13 step: 359, loss is 0.33119675517082214\n",
      "epoch: 13 step: 360, loss is 0.22043488919734955\n",
      "epoch: 13 step: 361, loss is 0.26285138726234436\n",
      "epoch: 13 step: 362, loss is 0.23650521039962769\n",
      "epoch: 13 step: 363, loss is 0.21346884965896606\n",
      "epoch: 13 step: 364, loss is 0.2997419536113739\n",
      "epoch: 13 step: 365, loss is 0.23658540844917297\n",
      "epoch: 13 step: 366, loss is 0.13747045397758484\n",
      "epoch: 13 step: 367, loss is 0.2572660446166992\n",
      "epoch: 13 step: 368, loss is 0.31350067257881165\n",
      "epoch: 13 step: 369, loss is 0.2845194637775421\n",
      "epoch: 13 step: 370, loss is 0.43092596530914307\n",
      "epoch: 13 step: 371, loss is 0.22541400790214539\n",
      "epoch: 13 step: 372, loss is 0.2408573031425476\n",
      "epoch: 13 step: 373, loss is 0.24898278713226318\n",
      "epoch: 13 step: 374, loss is 0.3256201446056366\n",
      "epoch: 13 step: 375, loss is 0.13842707872390747\n",
      "epoch: 13 step: 376, loss is 0.12948612868785858\n",
      "epoch: 13 step: 377, loss is 0.2733495831489563\n",
      "epoch: 13 step: 378, loss is 0.1264714151620865\n",
      "epoch: 13 step: 379, loss is 0.12411036342382431\n",
      "epoch: 13 step: 380, loss is 0.21688038110733032\n",
      "epoch: 13 step: 381, loss is 0.2430800348520279\n",
      "epoch: 13 step: 382, loss is 0.3365400433540344\n",
      "epoch: 13 step: 383, loss is 0.5082561373710632\n",
      "epoch: 13 step: 384, loss is 0.20075945556163788\n",
      "epoch: 13 step: 385, loss is 0.23951828479766846\n",
      "epoch: 13 step: 386, loss is 0.1910565197467804\n",
      "epoch: 13 step: 387, loss is 0.35275495052337646\n",
      "epoch: 13 step: 388, loss is 0.14952047169208527\n",
      "epoch: 13 step: 389, loss is 0.21374373137950897\n",
      "epoch: 13 step: 390, loss is 0.21787738800048828\n",
      "epoch: 13 step: 391, loss is 0.11714579164981842\n",
      "epoch: 13 step: 392, loss is 0.19732017815113068\n",
      "epoch: 13 step: 393, loss is 0.30455565452575684\n",
      "epoch: 13 step: 394, loss is 0.24900242686271667\n",
      "epoch: 13 step: 395, loss is 0.13403794169425964\n",
      "epoch: 13 step: 396, loss is 0.28981655836105347\n",
      "epoch: 13 step: 397, loss is 0.26889166235923767\n",
      "epoch: 13 step: 398, loss is 0.22848451137542725\n",
      "epoch: 13 step: 399, loss is 0.23842450976371765\n",
      "epoch: 13 step: 400, loss is 0.17408086359500885\n",
      "epoch: 13 step: 401, loss is 0.14542879164218903\n",
      "epoch: 13 step: 402, loss is 0.14335349202156067\n",
      "epoch: 13 step: 403, loss is 0.15249356627464294\n",
      "epoch: 13 step: 404, loss is 0.19778035581111908\n",
      "epoch: 13 step: 405, loss is 0.25323349237442017\n",
      "epoch: 13 step: 406, loss is 0.13210390508174896\n",
      "epoch: 13 step: 407, loss is 0.136037215590477\n",
      "epoch: 13 step: 408, loss is 0.20563258230686188\n",
      "epoch: 13 step: 409, loss is 0.19203218817710876\n",
      "epoch: 13 step: 410, loss is 0.5254181027412415\n",
      "epoch: 13 step: 411, loss is 0.15386071801185608\n",
      "epoch: 13 step: 412, loss is 0.23202833533287048\n",
      "epoch: 13 step: 413, loss is 0.0951034426689148\n",
      "epoch: 13 step: 414, loss is 0.2532491981983185\n",
      "epoch: 13 step: 415, loss is 0.2014116793870926\n",
      "epoch: 13 step: 416, loss is 0.18261782824993134\n",
      "epoch: 13 step: 417, loss is 0.3485170602798462\n",
      "epoch: 13 step: 418, loss is 0.20843157172203064\n",
      "epoch: 13 step: 419, loss is 0.2563394010066986\n",
      "epoch: 13 step: 420, loss is 0.21152982115745544\n",
      "epoch: 13 step: 421, loss is 0.15470479428768158\n",
      "epoch: 13 step: 422, loss is 0.09685979038476944\n",
      "epoch: 13 step: 423, loss is 0.33497947454452515\n",
      "epoch: 13 step: 424, loss is 0.3170648217201233\n",
      "epoch: 13 step: 425, loss is 0.28407183289527893\n",
      "epoch: 13 step: 426, loss is 0.12132333964109421\n",
      "epoch: 13 step: 427, loss is 0.2666547894477844\n",
      "epoch: 13 step: 428, loss is 0.2046450525522232\n",
      "epoch: 13 step: 429, loss is 0.1372711956501007\n",
      "epoch: 13 step: 430, loss is 0.241027370095253\n",
      "epoch: 13 step: 431, loss is 0.2976953387260437\n",
      "epoch: 13 step: 432, loss is 0.404397189617157\n",
      "epoch: 13 step: 433, loss is 0.3691125512123108\n",
      "epoch: 13 step: 434, loss is 0.2835738956928253\n",
      "epoch: 13 step: 435, loss is 0.1877136379480362\n",
      "epoch: 13 step: 436, loss is 0.29276198148727417\n",
      "epoch: 13 step: 437, loss is 0.20096799731254578\n",
      "epoch: 13 step: 438, loss is 0.17680484056472778\n",
      "epoch: 13 step: 439, loss is 0.27766284346580505\n",
      "epoch: 13 step: 440, loss is 0.14270807802677155\n",
      "epoch: 13 step: 441, loss is 0.09296689182519913\n",
      "epoch: 13 step: 442, loss is 0.3953293263912201\n",
      "epoch: 13 step: 443, loss is 0.1497424691915512\n",
      "epoch: 13 step: 444, loss is 0.22841551899909973\n",
      "epoch: 13 step: 445, loss is 0.2483474612236023\n",
      "epoch: 13 step: 446, loss is 0.27210918068885803\n",
      "epoch: 13 step: 447, loss is 0.26544979214668274\n",
      "epoch: 13 step: 448, loss is 0.22895896434783936\n",
      "epoch: 13 step: 449, loss is 0.19522081315517426\n",
      "epoch: 13 step: 450, loss is 0.3701437711715698\n",
      "epoch: 13 step: 451, loss is 0.19447937607765198\n",
      "epoch: 13 step: 452, loss is 0.23153012990951538\n",
      "epoch: 13 step: 453, loss is 0.14913251996040344\n",
      "epoch: 13 step: 454, loss is 0.0955180674791336\n",
      "epoch: 13 step: 455, loss is 0.20870128273963928\n",
      "epoch: 13 step: 456, loss is 0.11321993172168732\n",
      "epoch: 13 step: 457, loss is 0.17306742072105408\n",
      "epoch: 13 step: 458, loss is 0.14184042811393738\n",
      "epoch: 13 step: 459, loss is 0.1480926126241684\n",
      "epoch: 13 step: 460, loss is 0.2888595461845398\n",
      "epoch: 13 step: 461, loss is 0.3273587226867676\n",
      "epoch: 13 step: 462, loss is 0.28315505385398865\n",
      "epoch: 13 step: 463, loss is 0.504955530166626\n",
      "epoch: 13 step: 464, loss is 0.2960829734802246\n",
      "epoch: 13 step: 465, loss is 0.24493402242660522\n",
      "epoch: 13 step: 466, loss is 0.10501427203416824\n",
      "epoch: 13 step: 467, loss is 0.19990316033363342\n",
      "epoch: 13 step: 468, loss is 0.20841288566589355\n",
      "epoch: 13 step: 469, loss is 0.13225996494293213\n",
      "epoch: 13 step: 470, loss is 0.2808508276939392\n",
      "epoch: 13 step: 471, loss is 0.13343074917793274\n",
      "epoch: 13 step: 472, loss is 0.28038477897644043\n",
      "epoch: 13 step: 473, loss is 0.14214949309825897\n",
      "epoch: 13 step: 474, loss is 0.31308141350746155\n",
      "epoch: 13 step: 475, loss is 0.14256782829761505\n",
      "epoch: 13 step: 476, loss is 0.18375936150550842\n",
      "epoch: 13 step: 477, loss is 0.3096768260002136\n",
      "epoch: 13 step: 478, loss is 0.27218368649482727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 479, loss is 0.19854727387428284\n",
      "epoch: 13 step: 480, loss is 0.30108389258384705\n",
      "epoch: 13 step: 481, loss is 0.10574430227279663\n",
      "epoch: 13 step: 482, loss is 0.15614166855812073\n",
      "epoch: 13 step: 483, loss is 0.15295834839344025\n",
      "epoch: 13 step: 484, loss is 0.21322406828403473\n",
      "epoch: 13 step: 485, loss is 0.2025202363729477\n",
      "epoch: 13 step: 486, loss is 0.23725134134292603\n",
      "epoch: 13 step: 487, loss is 0.10042551159858704\n",
      "epoch: 13 step: 488, loss is 0.19215127825737\n",
      "epoch: 13 step: 489, loss is 0.22429399192333221\n",
      "epoch: 13 step: 490, loss is 0.2514701783657074\n",
      "epoch: 13 step: 491, loss is 0.2738265097141266\n",
      "epoch: 13 step: 492, loss is 0.24117408692836761\n",
      "epoch: 13 step: 493, loss is 0.20583604276180267\n",
      "epoch: 13 step: 494, loss is 0.15445668995380402\n",
      "epoch: 13 step: 495, loss is 0.10830762982368469\n",
      "epoch: 13 step: 496, loss is 0.16306822001934052\n",
      "epoch: 13 step: 497, loss is 0.2071739286184311\n",
      "epoch: 13 step: 498, loss is 0.14826104044914246\n",
      "epoch: 13 step: 499, loss is 0.2651803195476532\n",
      "epoch: 13 step: 500, loss is 0.14070971310138702\n",
      "epoch: 13 step: 501, loss is 0.20679603517055511\n",
      "epoch: 13 step: 502, loss is 0.14843790233135223\n",
      "epoch: 13 step: 503, loss is 0.18098585307598114\n",
      "epoch: 13 step: 504, loss is 0.20725113153457642\n",
      "epoch: 13 step: 505, loss is 0.3010740876197815\n",
      "epoch: 13 step: 506, loss is 0.14353181421756744\n",
      "epoch: 13 step: 507, loss is 0.3155836760997772\n",
      "epoch: 13 step: 508, loss is 0.1826275736093521\n",
      "epoch: 13 step: 509, loss is 0.3466169834136963\n",
      "epoch: 13 step: 510, loss is 0.2101462185382843\n",
      "epoch: 13 step: 511, loss is 0.25033846497535706\n",
      "epoch: 13 step: 512, loss is 0.03164975717663765\n",
      "epoch: 13 step: 513, loss is 0.33175113797187805\n",
      "epoch: 13 step: 514, loss is 0.2899233102798462\n",
      "epoch: 13 step: 515, loss is 0.2681523561477661\n",
      "epoch: 13 step: 516, loss is 0.3158382773399353\n",
      "epoch: 13 step: 517, loss is 0.1623443067073822\n",
      "epoch: 13 step: 518, loss is 0.18548576533794403\n",
      "epoch: 13 step: 519, loss is 0.2689572870731354\n",
      "epoch: 13 step: 520, loss is 0.21759118139743805\n",
      "epoch: 13 step: 521, loss is 0.25144076347351074\n",
      "epoch: 13 step: 522, loss is 0.2310888022184372\n",
      "epoch: 13 step: 523, loss is 0.20936833322048187\n",
      "epoch: 13 step: 524, loss is 0.2754807472229004\n",
      "epoch: 13 step: 525, loss is 0.16794723272323608\n",
      "epoch: 13 step: 526, loss is 0.20150570571422577\n",
      "epoch: 13 step: 527, loss is 0.28088730573654175\n",
      "epoch: 13 step: 528, loss is 0.09040971100330353\n",
      "epoch: 13 step: 529, loss is 0.3785563111305237\n",
      "epoch: 13 step: 530, loss is 0.29798129200935364\n",
      "epoch: 13 step: 531, loss is 0.17912305891513824\n",
      "epoch: 13 step: 532, loss is 0.29988715052604675\n",
      "epoch: 13 step: 533, loss is 0.1840713620185852\n",
      "epoch: 13 step: 534, loss is 0.4711320698261261\n",
      "epoch: 13 step: 535, loss is 0.21258261799812317\n",
      "epoch: 13 step: 536, loss is 0.09001102298498154\n",
      "epoch: 13 step: 537, loss is 0.2442827671766281\n",
      "epoch: 13 step: 538, loss is 0.21119962632656097\n",
      "epoch: 13 step: 539, loss is 0.22226949036121368\n",
      "epoch: 13 step: 540, loss is 0.3018033504486084\n",
      "epoch: 13 step: 541, loss is 0.3724645674228668\n",
      "epoch: 13 step: 542, loss is 0.19472648203372955\n",
      "epoch: 13 step: 543, loss is 0.10745640844106674\n",
      "epoch: 13 step: 544, loss is 0.26002925634384155\n",
      "epoch: 13 step: 545, loss is 0.12129174917936325\n",
      "epoch: 13 step: 546, loss is 0.22688470780849457\n",
      "epoch: 13 step: 547, loss is 0.26968708634376526\n",
      "epoch: 13 step: 548, loss is 0.2164197862148285\n",
      "epoch: 13 step: 549, loss is 0.211676225066185\n",
      "epoch: 13 step: 550, loss is 0.13359612226486206\n",
      "epoch: 13 step: 551, loss is 0.2192923128604889\n",
      "epoch: 13 step: 552, loss is 0.2967650890350342\n",
      "epoch: 13 step: 553, loss is 0.21862159669399261\n",
      "epoch: 13 step: 554, loss is 0.3602898120880127\n",
      "epoch: 13 step: 555, loss is 0.19923631846904755\n",
      "epoch: 13 step: 556, loss is 0.1188470721244812\n",
      "epoch: 13 step: 557, loss is 0.2099379450082779\n",
      "epoch: 13 step: 558, loss is 0.22655737400054932\n",
      "epoch: 13 step: 559, loss is 0.17210674285888672\n",
      "epoch: 13 step: 560, loss is 0.32078999280929565\n",
      "epoch: 13 step: 561, loss is 0.21511432528495789\n",
      "epoch: 13 step: 562, loss is 0.18463188409805298\n",
      "epoch: 13 step: 563, loss is 0.2289513200521469\n",
      "epoch: 13 step: 564, loss is 0.3351006507873535\n",
      "epoch: 13 step: 565, loss is 0.1978767216205597\n",
      "epoch: 13 step: 566, loss is 0.22581981122493744\n",
      "epoch: 13 step: 567, loss is 0.25170576572418213\n",
      "epoch: 13 step: 568, loss is 0.1930270791053772\n",
      "epoch: 13 step: 569, loss is 0.1191496029496193\n",
      "epoch: 13 step: 570, loss is 0.12114804238080978\n",
      "epoch: 13 step: 571, loss is 0.14058993756771088\n",
      "epoch: 13 step: 572, loss is 0.27768924832344055\n",
      "epoch: 13 step: 573, loss is 0.24186192452907562\n",
      "epoch: 13 step: 574, loss is 0.22213436663150787\n",
      "epoch: 13 step: 575, loss is 0.09041234105825424\n",
      "epoch: 13 step: 576, loss is 0.20962487161159515\n",
      "epoch: 13 step: 577, loss is 0.3011368215084076\n",
      "epoch: 13 step: 578, loss is 0.2459607720375061\n",
      "epoch: 13 step: 579, loss is 0.22950606048107147\n",
      "epoch: 13 step: 580, loss is 0.2592506408691406\n",
      "epoch: 13 step: 581, loss is 0.22349613904953003\n",
      "epoch: 13 step: 582, loss is 0.2748943269252777\n",
      "epoch: 13 step: 583, loss is 0.365959495306015\n",
      "epoch: 13 step: 584, loss is 0.1134672462940216\n",
      "epoch: 13 step: 585, loss is 0.4458421766757965\n",
      "epoch: 13 step: 586, loss is 0.13408033549785614\n",
      "epoch: 13 step: 587, loss is 0.2793143391609192\n",
      "epoch: 13 step: 588, loss is 0.10212752968072891\n",
      "epoch: 13 step: 589, loss is 0.13901305198669434\n",
      "epoch: 13 step: 590, loss is 0.27329549193382263\n",
      "epoch: 13 step: 591, loss is 0.2911798655986786\n",
      "epoch: 13 step: 592, loss is 0.23452143371105194\n",
      "epoch: 13 step: 593, loss is 0.30267176032066345\n",
      "epoch: 13 step: 594, loss is 0.09778371453285217\n",
      "epoch: 13 step: 595, loss is 0.15249435603618622\n",
      "epoch: 13 step: 596, loss is 0.22419473528862\n",
      "epoch: 13 step: 597, loss is 0.18185211718082428\n",
      "epoch: 13 step: 598, loss is 0.2416272759437561\n",
      "epoch: 13 step: 599, loss is 0.2953886389732361\n",
      "epoch: 13 step: 600, loss is 0.1600290834903717\n",
      "epoch: 13 step: 601, loss is 0.3245379626750946\n",
      "epoch: 13 step: 602, loss is 0.14660950005054474\n",
      "epoch: 13 step: 603, loss is 0.3087378144264221\n",
      "epoch: 13 step: 604, loss is 0.5007203221321106\n",
      "epoch: 13 step: 605, loss is 0.24006113409996033\n",
      "epoch: 13 step: 606, loss is 0.19651605188846588\n",
      "epoch: 13 step: 607, loss is 0.34737953543663025\n",
      "epoch: 13 step: 608, loss is 0.15908849239349365\n",
      "epoch: 13 step: 609, loss is 0.18824084103107452\n",
      "epoch: 13 step: 610, loss is 0.21652132272720337\n",
      "epoch: 13 step: 611, loss is 0.22317756712436676\n",
      "epoch: 13 step: 612, loss is 0.2889114320278168\n",
      "epoch: 13 step: 613, loss is 0.18529106676578522\n",
      "epoch: 13 step: 614, loss is 0.21124465763568878\n",
      "epoch: 13 step: 615, loss is 0.20515894889831543\n",
      "epoch: 13 step: 616, loss is 0.19492141902446747\n",
      "epoch: 13 step: 617, loss is 0.23673605918884277\n",
      "epoch: 13 step: 618, loss is 0.2682599425315857\n",
      "epoch: 13 step: 619, loss is 0.18486164510250092\n",
      "epoch: 13 step: 620, loss is 0.18335796892642975\n",
      "epoch: 13 step: 621, loss is 0.41003525257110596\n",
      "epoch: 13 step: 622, loss is 0.34106481075286865\n",
      "epoch: 13 step: 623, loss is 0.3860945999622345\n",
      "epoch: 13 step: 624, loss is 0.22041217982769012\n",
      "epoch: 13 step: 625, loss is 0.33617570996284485\n",
      "epoch: 13 step: 626, loss is 0.15785928070545197\n",
      "epoch: 13 step: 627, loss is 0.26700228452682495\n",
      "epoch: 13 step: 628, loss is 0.21355125308036804\n",
      "epoch: 13 step: 629, loss is 0.11812224984169006\n",
      "epoch: 13 step: 630, loss is 0.25910499691963196\n",
      "epoch: 13 step: 631, loss is 0.15057633817195892\n",
      "epoch: 13 step: 632, loss is 0.21872951090335846\n",
      "epoch: 13 step: 633, loss is 0.2221256047487259\n",
      "epoch: 13 step: 634, loss is 0.2457839846611023\n",
      "epoch: 13 step: 635, loss is 0.16421331465244293\n",
      "epoch: 13 step: 636, loss is 0.2951737344264984\n",
      "epoch: 13 step: 637, loss is 0.16007071733474731\n",
      "epoch: 13 step: 638, loss is 0.20977811515331268\n",
      "epoch: 13 step: 639, loss is 0.28558626770973206\n",
      "epoch: 13 step: 640, loss is 0.16171911358833313\n",
      "epoch: 13 step: 641, loss is 0.25329869985580444\n",
      "epoch: 13 step: 642, loss is 0.27607840299606323\n",
      "epoch: 13 step: 643, loss is 0.1947946697473526\n",
      "epoch: 13 step: 644, loss is 0.14557580649852753\n",
      "epoch: 13 step: 645, loss is 0.2784031629562378\n",
      "epoch: 13 step: 646, loss is 0.18349754810333252\n",
      "epoch: 13 step: 647, loss is 0.32295873761177063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 648, loss is 0.44002678990364075\n",
      "epoch: 13 step: 649, loss is 0.2998642325401306\n",
      "epoch: 13 step: 650, loss is 0.1917559951543808\n",
      "epoch: 13 step: 651, loss is 0.28180041909217834\n",
      "epoch: 13 step: 652, loss is 0.19624783098697662\n",
      "epoch: 13 step: 653, loss is 0.6053094267845154\n",
      "epoch: 13 step: 654, loss is 0.332500159740448\n",
      "epoch: 13 step: 655, loss is 0.14901413023471832\n",
      "epoch: 13 step: 656, loss is 0.22680094838142395\n",
      "epoch: 13 step: 657, loss is 0.18638597428798676\n",
      "epoch: 13 step: 658, loss is 0.31877562403678894\n",
      "epoch: 13 step: 659, loss is 0.17425400018692017\n",
      "epoch: 13 step: 660, loss is 0.40630146861076355\n",
      "epoch: 13 step: 661, loss is 0.2521759867668152\n",
      "epoch: 13 step: 662, loss is 0.08703697472810745\n",
      "epoch: 13 step: 663, loss is 0.36040011048316956\n",
      "epoch: 13 step: 664, loss is 0.13805317878723145\n",
      "epoch: 13 step: 665, loss is 0.2099713236093521\n",
      "epoch: 13 step: 666, loss is 0.1852409392595291\n",
      "epoch: 13 step: 667, loss is 0.32700738310813904\n",
      "epoch: 13 step: 668, loss is 0.24023602902889252\n",
      "epoch: 13 step: 669, loss is 0.31894174218177795\n",
      "epoch: 13 step: 670, loss is 0.12438298016786575\n",
      "epoch: 13 step: 671, loss is 0.25576266646385193\n",
      "epoch: 13 step: 672, loss is 0.2615067958831787\n",
      "epoch: 13 step: 673, loss is 0.24722334742546082\n",
      "epoch: 13 step: 674, loss is 0.213228240609169\n",
      "epoch: 13 step: 675, loss is 0.19374053180217743\n",
      "epoch: 13 step: 676, loss is 0.30743613839149475\n",
      "epoch: 13 step: 677, loss is 0.3091166317462921\n",
      "epoch: 13 step: 678, loss is 0.24801549315452576\n",
      "epoch: 13 step: 679, loss is 0.14490152895450592\n",
      "epoch: 13 step: 680, loss is 0.30088305473327637\n",
      "epoch: 13 step: 681, loss is 0.1559886783361435\n",
      "epoch: 13 step: 682, loss is 0.23210863769054413\n",
      "epoch: 13 step: 683, loss is 0.21401028335094452\n",
      "epoch: 13 step: 684, loss is 0.18392014503479004\n",
      "epoch: 13 step: 685, loss is 0.2705875635147095\n",
      "epoch: 13 step: 686, loss is 0.2739640474319458\n",
      "epoch: 13 step: 687, loss is 0.2538456618785858\n",
      "epoch: 13 step: 688, loss is 0.24810607731342316\n",
      "epoch: 13 step: 689, loss is 0.1479407399892807\n",
      "epoch: 13 step: 690, loss is 0.2900131940841675\n",
      "epoch: 13 step: 691, loss is 0.2675527334213257\n",
      "epoch: 13 step: 692, loss is 0.1456397920846939\n",
      "epoch: 13 step: 693, loss is 0.2564087510108948\n",
      "epoch: 13 step: 694, loss is 0.15685071051120758\n",
      "epoch: 13 step: 695, loss is 0.24795693159103394\n",
      "epoch: 13 step: 696, loss is 0.07686081528663635\n",
      "epoch: 13 step: 697, loss is 0.21743032336235046\n",
      "epoch: 13 step: 698, loss is 0.18305093050003052\n",
      "epoch: 13 step: 699, loss is 0.18218888342380524\n",
      "epoch: 13 step: 700, loss is 0.3364259600639343\n",
      "epoch: 13 step: 701, loss is 0.2503005564212799\n",
      "epoch: 13 step: 702, loss is 0.15736570954322815\n",
      "epoch: 13 step: 703, loss is 0.195694237947464\n",
      "epoch: 13 step: 704, loss is 0.12221037596464157\n",
      "epoch: 13 step: 705, loss is 0.2285590022802353\n",
      "epoch: 13 step: 706, loss is 0.383395254611969\n",
      "epoch: 13 step: 707, loss is 0.25955265760421753\n",
      "epoch: 13 step: 708, loss is 0.18853920698165894\n",
      "epoch: 13 step: 709, loss is 0.16225747764110565\n",
      "epoch: 13 step: 710, loss is 0.29403162002563477\n",
      "epoch: 13 step: 711, loss is 0.18667243421077728\n",
      "epoch: 13 step: 712, loss is 0.22963613271713257\n",
      "epoch: 13 step: 713, loss is 0.21692995727062225\n",
      "epoch: 13 step: 714, loss is 0.3004674017429352\n",
      "epoch: 13 step: 715, loss is 0.26281294226646423\n",
      "epoch: 13 step: 716, loss is 0.11672726273536682\n",
      "epoch: 13 step: 717, loss is 0.20019878447055817\n",
      "epoch: 13 step: 718, loss is 0.14912548661231995\n",
      "epoch: 13 step: 719, loss is 0.18744386732578278\n",
      "epoch: 13 step: 720, loss is 0.24049383401870728\n",
      "epoch: 13 step: 721, loss is 0.22750239074230194\n",
      "epoch: 13 step: 722, loss is 0.24917270243167877\n",
      "epoch: 13 step: 723, loss is 0.3085532784461975\n",
      "epoch: 13 step: 724, loss is 0.2158314734697342\n",
      "epoch: 13 step: 725, loss is 0.20261381566524506\n",
      "epoch: 13 step: 726, loss is 0.14481806755065918\n",
      "epoch: 13 step: 727, loss is 0.3617700934410095\n",
      "epoch: 13 step: 728, loss is 0.19699575006961823\n",
      "epoch: 13 step: 729, loss is 0.09708182513713837\n",
      "epoch: 13 step: 730, loss is 0.2922205328941345\n",
      "epoch: 13 step: 731, loss is 0.17205408215522766\n",
      "epoch: 13 step: 732, loss is 0.1571917086839676\n",
      "epoch: 13 step: 733, loss is 0.2517815828323364\n",
      "epoch: 13 step: 734, loss is 0.16029272973537445\n",
      "epoch: 13 step: 735, loss is 0.2527110278606415\n",
      "epoch: 13 step: 736, loss is 0.26757925748825073\n",
      "epoch: 13 step: 737, loss is 0.10749566555023193\n",
      "epoch: 13 step: 738, loss is 0.22043484449386597\n",
      "epoch: 13 step: 739, loss is 0.4108668863773346\n",
      "epoch: 13 step: 740, loss is 0.2743789255619049\n",
      "epoch: 13 step: 741, loss is 0.44844287633895874\n",
      "epoch: 13 step: 742, loss is 0.10625092685222626\n",
      "epoch: 13 step: 743, loss is 0.15501715242862701\n",
      "epoch: 13 step: 744, loss is 0.16371674835681915\n",
      "epoch: 13 step: 745, loss is 0.2423875331878662\n",
      "epoch: 13 step: 746, loss is 0.13033248484134674\n",
      "epoch: 13 step: 747, loss is 0.22192461788654327\n",
      "epoch: 13 step: 748, loss is 0.26068612933158875\n",
      "epoch: 13 step: 749, loss is 0.10746104270219803\n",
      "epoch: 13 step: 750, loss is 0.35548996925354004\n",
      "epoch: 13 step: 751, loss is 0.2996603548526764\n",
      "epoch: 13 step: 752, loss is 0.21133483946323395\n",
      "epoch: 13 step: 753, loss is 0.19151970744132996\n",
      "epoch: 13 step: 754, loss is 0.22698403894901276\n",
      "epoch: 13 step: 755, loss is 0.22529835999011993\n",
      "epoch: 13 step: 756, loss is 0.24734701216220856\n",
      "epoch: 13 step: 757, loss is 0.18950682878494263\n",
      "epoch: 13 step: 758, loss is 0.28273504972457886\n",
      "epoch: 13 step: 759, loss is 0.2234523743391037\n",
      "epoch: 13 step: 760, loss is 0.1478600949048996\n",
      "epoch: 13 step: 761, loss is 0.4361342489719391\n",
      "epoch: 13 step: 762, loss is 0.26069870591163635\n",
      "epoch: 13 step: 763, loss is 0.2521754503250122\n",
      "epoch: 13 step: 764, loss is 0.16674047708511353\n",
      "epoch: 13 step: 765, loss is 0.2255392074584961\n",
      "epoch: 13 step: 766, loss is 0.21925513446331024\n",
      "epoch: 13 step: 767, loss is 0.2146022617816925\n",
      "epoch: 13 step: 768, loss is 0.11443103104829788\n",
      "epoch: 13 step: 769, loss is 0.1533161997795105\n",
      "epoch: 13 step: 770, loss is 0.2502719759941101\n",
      "epoch: 13 step: 771, loss is 0.20600080490112305\n",
      "epoch: 13 step: 772, loss is 0.26265189051628113\n",
      "epoch: 13 step: 773, loss is 0.266357421875\n",
      "epoch: 13 step: 774, loss is 0.2039979100227356\n",
      "epoch: 13 step: 775, loss is 0.1598380208015442\n",
      "epoch: 13 step: 776, loss is 0.2700892984867096\n",
      "epoch: 13 step: 777, loss is 0.2501077950000763\n",
      "epoch: 13 step: 778, loss is 0.23419243097305298\n",
      "epoch: 13 step: 779, loss is 0.1178361028432846\n",
      "epoch: 13 step: 780, loss is 0.41069111227989197\n",
      "epoch: 13 step: 781, loss is 0.3275105059146881\n",
      "epoch: 13 step: 782, loss is 0.1382312774658203\n",
      "epoch: 13 step: 783, loss is 0.09703406691551208\n",
      "epoch: 13 step: 784, loss is 0.1586572378873825\n",
      "epoch: 13 step: 785, loss is 0.47268545627593994\n",
      "epoch: 13 step: 786, loss is 0.1093742623925209\n",
      "epoch: 13 step: 787, loss is 0.09109599888324738\n",
      "epoch: 13 step: 788, loss is 0.1942996382713318\n",
      "epoch: 13 step: 789, loss is 0.23764905333518982\n",
      "epoch: 13 step: 790, loss is 0.2657831907272339\n",
      "epoch: 13 step: 791, loss is 0.15066756308078766\n",
      "epoch: 13 step: 792, loss is 0.23324823379516602\n",
      "epoch: 13 step: 793, loss is 0.5597965717315674\n",
      "epoch: 13 step: 794, loss is 0.16995975375175476\n",
      "epoch: 13 step: 795, loss is 0.1349271982908249\n",
      "epoch: 13 step: 796, loss is 0.3432508707046509\n",
      "epoch: 13 step: 797, loss is 0.21496231853961945\n",
      "epoch: 13 step: 798, loss is 0.18931075930595398\n",
      "epoch: 13 step: 799, loss is 0.2327384501695633\n",
      "epoch: 13 step: 800, loss is 0.2226560264825821\n",
      "epoch: 13 step: 801, loss is 0.19825147092342377\n",
      "epoch: 13 step: 802, loss is 0.20836780965328217\n",
      "epoch: 13 step: 803, loss is 0.2814084589481354\n",
      "epoch: 13 step: 804, loss is 0.17201903462409973\n",
      "epoch: 13 step: 805, loss is 0.2541564702987671\n",
      "epoch: 13 step: 806, loss is 0.25831887125968933\n",
      "epoch: 13 step: 807, loss is 0.14048263430595398\n",
      "epoch: 13 step: 808, loss is 0.24604679644107819\n",
      "epoch: 13 step: 809, loss is 0.32480278611183167\n",
      "epoch: 13 step: 810, loss is 0.23324792087078094\n",
      "epoch: 13 step: 811, loss is 0.2176307737827301\n",
      "epoch: 13 step: 812, loss is 0.19869142770767212\n",
      "epoch: 13 step: 813, loss is 0.3161889910697937\n",
      "epoch: 13 step: 814, loss is 0.18906861543655396\n",
      "epoch: 13 step: 815, loss is 0.1978716254234314\n",
      "epoch: 13 step: 816, loss is 0.16449296474456787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 817, loss is 0.22444747388362885\n",
      "epoch: 13 step: 818, loss is 0.23468750715255737\n",
      "epoch: 13 step: 819, loss is 0.13781704008579254\n",
      "epoch: 13 step: 820, loss is 0.23746944963932037\n",
      "epoch: 13 step: 821, loss is 0.23501484096050262\n",
      "epoch: 13 step: 822, loss is 0.44194790720939636\n",
      "epoch: 13 step: 823, loss is 0.19975392520427704\n",
      "epoch: 13 step: 824, loss is 0.10142811387777328\n",
      "epoch: 13 step: 825, loss is 0.19348564743995667\n",
      "epoch: 13 step: 826, loss is 0.3116493225097656\n",
      "epoch: 13 step: 827, loss is 0.2165980488061905\n",
      "epoch: 13 step: 828, loss is 0.21153871715068817\n",
      "epoch: 13 step: 829, loss is 0.14393991231918335\n",
      "epoch: 13 step: 830, loss is 0.09396843612194061\n",
      "epoch: 13 step: 831, loss is 0.29745131731033325\n",
      "epoch: 13 step: 832, loss is 0.23907703161239624\n",
      "epoch: 13 step: 833, loss is 0.19106055796146393\n",
      "epoch: 13 step: 834, loss is 0.209344744682312\n",
      "epoch: 13 step: 835, loss is 0.14192692935466766\n",
      "epoch: 13 step: 836, loss is 0.36787304282188416\n",
      "epoch: 13 step: 837, loss is 0.34047824144363403\n",
      "epoch: 13 step: 838, loss is 0.339998722076416\n",
      "epoch: 13 step: 839, loss is 0.21464382112026215\n",
      "epoch: 13 step: 840, loss is 0.25189200043678284\n",
      "epoch: 13 step: 841, loss is 0.23397022485733032\n",
      "epoch: 13 step: 842, loss is 0.16192354261875153\n",
      "epoch: 13 step: 843, loss is 0.12224084883928299\n",
      "epoch: 13 step: 844, loss is 0.1411394476890564\n",
      "epoch: 13 step: 845, loss is 0.2718835175037384\n",
      "epoch: 13 step: 846, loss is 0.2709576487541199\n",
      "epoch: 13 step: 847, loss is 0.27539363503456116\n",
      "epoch: 13 step: 848, loss is 0.24577373266220093\n",
      "epoch: 13 step: 849, loss is 0.22043350338935852\n",
      "epoch: 13 step: 850, loss is 0.2744472622871399\n",
      "epoch: 13 step: 851, loss is 0.3644150495529175\n",
      "epoch: 13 step: 852, loss is 0.16470776498317719\n",
      "epoch: 13 step: 853, loss is 0.25444507598876953\n",
      "epoch: 13 step: 854, loss is 0.28308379650115967\n",
      "epoch: 13 step: 855, loss is 0.17587044835090637\n",
      "epoch: 13 step: 856, loss is 0.14546307921409607\n",
      "epoch: 13 step: 857, loss is 0.2790799140930176\n",
      "epoch: 13 step: 858, loss is 0.241678386926651\n",
      "epoch: 13 step: 859, loss is 0.12748010456562042\n",
      "epoch: 13 step: 860, loss is 0.15708482265472412\n",
      "epoch: 13 step: 861, loss is 0.22101345658302307\n",
      "epoch: 13 step: 862, loss is 0.24524588882923126\n",
      "epoch: 13 step: 863, loss is 0.18668971955776215\n",
      "epoch: 13 step: 864, loss is 0.18854008615016937\n",
      "epoch: 13 step: 865, loss is 0.3520842492580414\n",
      "epoch: 13 step: 866, loss is 0.19610685110092163\n",
      "epoch: 13 step: 867, loss is 0.1777162253856659\n",
      "epoch: 13 step: 868, loss is 0.16293780505657196\n",
      "epoch: 13 step: 869, loss is 0.2791126072406769\n",
      "epoch: 13 step: 870, loss is 0.18968473374843597\n",
      "epoch: 13 step: 871, loss is 0.2372657209634781\n",
      "epoch: 13 step: 872, loss is 0.20760324597358704\n",
      "epoch: 13 step: 873, loss is 0.24276800453662872\n",
      "epoch: 13 step: 874, loss is 0.1117527112364769\n",
      "epoch: 13 step: 875, loss is 0.3306497037410736\n",
      "epoch: 13 step: 876, loss is 0.22489264607429504\n",
      "epoch: 13 step: 877, loss is 0.1568058282136917\n",
      "epoch: 13 step: 878, loss is 0.25913944840431213\n",
      "epoch: 13 step: 879, loss is 0.3116876482963562\n",
      "epoch: 13 step: 880, loss is 0.35920366644859314\n",
      "epoch: 13 step: 881, loss is 0.21349185705184937\n",
      "epoch: 13 step: 882, loss is 0.19914601743221283\n",
      "epoch: 13 step: 883, loss is 0.26430556178092957\n",
      "epoch: 13 step: 884, loss is 0.5234801173210144\n",
      "epoch: 13 step: 885, loss is 0.2939777672290802\n",
      "epoch: 13 step: 886, loss is 0.11913974583148956\n",
      "epoch: 13 step: 887, loss is 0.29432231187820435\n",
      "epoch: 13 step: 888, loss is 0.1454501450061798\n",
      "epoch: 13 step: 889, loss is 0.21340708434581757\n",
      "epoch: 13 step: 890, loss is 0.2874507009983063\n",
      "epoch: 13 step: 891, loss is 0.24457530677318573\n",
      "epoch: 13 step: 892, loss is 0.26603537797927856\n",
      "epoch: 13 step: 893, loss is 0.3378952443599701\n",
      "epoch: 13 step: 894, loss is 0.26132410764694214\n",
      "epoch: 13 step: 895, loss is 0.22785381972789764\n",
      "epoch: 13 step: 896, loss is 0.19974137842655182\n",
      "epoch: 13 step: 897, loss is 0.14663086831569672\n",
      "epoch: 13 step: 898, loss is 0.14710499346256256\n",
      "epoch: 13 step: 899, loss is 0.08753087371587753\n",
      "epoch: 13 step: 900, loss is 0.12993210554122925\n",
      "epoch: 13 step: 901, loss is 0.2690296471118927\n",
      "epoch: 13 step: 902, loss is 0.29227927327156067\n",
      "epoch: 13 step: 903, loss is 0.17104068398475647\n",
      "epoch: 13 step: 904, loss is 0.17305848002433777\n",
      "epoch: 13 step: 905, loss is 0.14348071813583374\n",
      "epoch: 13 step: 906, loss is 0.27153995633125305\n",
      "epoch: 13 step: 907, loss is 0.2567398250102997\n",
      "epoch: 13 step: 908, loss is 0.29257094860076904\n",
      "epoch: 13 step: 909, loss is 0.2522874176502228\n",
      "epoch: 13 step: 910, loss is 0.5089641213417053\n",
      "epoch: 13 step: 911, loss is 0.21681800484657288\n",
      "epoch: 13 step: 912, loss is 0.23898543417453766\n",
      "epoch: 13 step: 913, loss is 0.22185325622558594\n",
      "epoch: 13 step: 914, loss is 0.2516222894191742\n",
      "epoch: 13 step: 915, loss is 0.17330054938793182\n",
      "epoch: 13 step: 916, loss is 0.18312203884124756\n",
      "epoch: 13 step: 917, loss is 0.2258511483669281\n",
      "epoch: 13 step: 918, loss is 0.1438695788383484\n",
      "epoch: 13 step: 919, loss is 0.1695544719696045\n",
      "epoch: 13 step: 920, loss is 0.34527549147605896\n",
      "epoch: 13 step: 921, loss is 0.2375674843788147\n",
      "epoch: 13 step: 922, loss is 0.1861201822757721\n",
      "epoch: 13 step: 923, loss is 0.1535353809595108\n",
      "epoch: 13 step: 924, loss is 0.12669417262077332\n",
      "epoch: 13 step: 925, loss is 0.1863899827003479\n",
      "epoch: 13 step: 926, loss is 0.11238318681716919\n",
      "epoch: 13 step: 927, loss is 0.1271328181028366\n",
      "epoch: 13 step: 928, loss is 0.1582677960395813\n",
      "epoch: 13 step: 929, loss is 0.2770962119102478\n",
      "epoch: 13 step: 930, loss is 0.23232153058052063\n",
      "epoch: 13 step: 931, loss is 0.19956102967262268\n",
      "epoch: 13 step: 932, loss is 0.10408393293619156\n",
      "epoch: 13 step: 933, loss is 0.23909488320350647\n",
      "epoch: 13 step: 934, loss is 0.23301665484905243\n",
      "epoch: 13 step: 935, loss is 0.307828813791275\n",
      "epoch: 13 step: 936, loss is 0.15767255425453186\n",
      "epoch: 13 step: 937, loss is 0.09890014678239822\n",
      "epoch: 14 step: 1, loss is 0.36704882979393005\n",
      "epoch: 14 step: 2, loss is 0.33495083451271057\n",
      "epoch: 14 step: 3, loss is 0.21729707717895508\n",
      "epoch: 14 step: 4, loss is 0.17383769154548645\n",
      "epoch: 14 step: 5, loss is 0.1853806972503662\n",
      "epoch: 14 step: 6, loss is 0.17910175025463104\n",
      "epoch: 14 step: 7, loss is 0.5191013216972351\n",
      "epoch: 14 step: 8, loss is 0.3889758586883545\n",
      "epoch: 14 step: 9, loss is 0.1749424934387207\n",
      "epoch: 14 step: 10, loss is 0.23809252679347992\n",
      "epoch: 14 step: 11, loss is 0.28830552101135254\n",
      "epoch: 14 step: 12, loss is 0.23479755222797394\n",
      "epoch: 14 step: 13, loss is 0.2355383187532425\n",
      "epoch: 14 step: 14, loss is 0.19194446504116058\n",
      "epoch: 14 step: 15, loss is 0.18522027134895325\n",
      "epoch: 14 step: 16, loss is 0.17666886746883392\n",
      "epoch: 14 step: 17, loss is 0.32693368196487427\n",
      "epoch: 14 step: 18, loss is 0.15286433696746826\n",
      "epoch: 14 step: 19, loss is 0.2266569435596466\n",
      "epoch: 14 step: 20, loss is 0.37507808208465576\n",
      "epoch: 14 step: 21, loss is 0.08993608504533768\n",
      "epoch: 14 step: 22, loss is 0.21441251039505005\n",
      "epoch: 14 step: 23, loss is 0.16658301651477814\n",
      "epoch: 14 step: 24, loss is 0.3080352544784546\n",
      "epoch: 14 step: 25, loss is 0.23180626332759857\n",
      "epoch: 14 step: 26, loss is 0.23461173474788666\n",
      "epoch: 14 step: 27, loss is 0.18444986641407013\n",
      "epoch: 14 step: 28, loss is 0.25232335925102234\n",
      "epoch: 14 step: 29, loss is 0.20441105961799622\n",
      "epoch: 14 step: 30, loss is 0.1066519245505333\n",
      "epoch: 14 step: 31, loss is 0.17414182424545288\n",
      "epoch: 14 step: 32, loss is 0.11951488256454468\n",
      "epoch: 14 step: 33, loss is 0.1531137228012085\n",
      "epoch: 14 step: 34, loss is 0.24114637076854706\n",
      "epoch: 14 step: 35, loss is 0.24766582250595093\n",
      "epoch: 14 step: 36, loss is 0.13277019560337067\n",
      "epoch: 14 step: 37, loss is 0.2405667006969452\n",
      "epoch: 14 step: 38, loss is 0.2251172512769699\n",
      "epoch: 14 step: 39, loss is 0.2489539533853531\n",
      "epoch: 14 step: 40, loss is 0.2428225874900818\n",
      "epoch: 14 step: 41, loss is 0.21042439341545105\n",
      "epoch: 14 step: 42, loss is 0.10111912339925766\n",
      "epoch: 14 step: 43, loss is 0.15812957286834717\n",
      "epoch: 14 step: 44, loss is 0.14893919229507446\n",
      "epoch: 14 step: 45, loss is 0.13271841406822205\n",
      "epoch: 14 step: 46, loss is 0.10136084258556366\n",
      "epoch: 14 step: 47, loss is 0.2176290899515152\n",
      "epoch: 14 step: 48, loss is 0.36151233315467834\n",
      "epoch: 14 step: 49, loss is 0.14781354367733002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 50, loss is 0.10620865225791931\n",
      "epoch: 14 step: 51, loss is 0.17957139015197754\n",
      "epoch: 14 step: 52, loss is 0.1468513011932373\n",
      "epoch: 14 step: 53, loss is 0.20930978655815125\n",
      "epoch: 14 step: 54, loss is 0.1880977600812912\n",
      "epoch: 14 step: 55, loss is 0.3132176995277405\n",
      "epoch: 14 step: 56, loss is 0.21056179702281952\n",
      "epoch: 14 step: 57, loss is 0.28286296129226685\n",
      "epoch: 14 step: 58, loss is 0.3639794588088989\n",
      "epoch: 14 step: 59, loss is 0.32295772433280945\n",
      "epoch: 14 step: 60, loss is 0.20850136876106262\n",
      "epoch: 14 step: 61, loss is 0.18464970588684082\n",
      "epoch: 14 step: 62, loss is 0.1356142908334732\n",
      "epoch: 14 step: 63, loss is 0.2523699104785919\n",
      "epoch: 14 step: 64, loss is 0.260760635137558\n",
      "epoch: 14 step: 65, loss is 0.21126064658164978\n",
      "epoch: 14 step: 66, loss is 0.24752211570739746\n",
      "epoch: 14 step: 67, loss is 0.1340515911579132\n",
      "epoch: 14 step: 68, loss is 0.3643249273300171\n",
      "epoch: 14 step: 69, loss is 0.22579555213451385\n",
      "epoch: 14 step: 70, loss is 0.13245898485183716\n",
      "epoch: 14 step: 71, loss is 0.2707907557487488\n",
      "epoch: 14 step: 72, loss is 0.2454117089509964\n",
      "epoch: 14 step: 73, loss is 0.23107515275478363\n",
      "epoch: 14 step: 74, loss is 0.26597627997398376\n",
      "epoch: 14 step: 75, loss is 0.18655820190906525\n",
      "epoch: 14 step: 76, loss is 0.16093182563781738\n",
      "epoch: 14 step: 77, loss is 0.2168673723936081\n",
      "epoch: 14 step: 78, loss is 0.14700213074684143\n",
      "epoch: 14 step: 79, loss is 0.37276649475097656\n",
      "epoch: 14 step: 80, loss is 0.32733985781669617\n",
      "epoch: 14 step: 81, loss is 0.12215633690357208\n",
      "epoch: 14 step: 82, loss is 0.30345115065574646\n",
      "epoch: 14 step: 83, loss is 0.18343311548233032\n",
      "epoch: 14 step: 84, loss is 0.12184522300958633\n",
      "epoch: 14 step: 85, loss is 0.11862806230783463\n",
      "epoch: 14 step: 86, loss is 0.29950815439224243\n",
      "epoch: 14 step: 87, loss is 0.21645353734493256\n",
      "epoch: 14 step: 88, loss is 0.25216346979141235\n",
      "epoch: 14 step: 89, loss is 0.2558450698852539\n",
      "epoch: 14 step: 90, loss is 0.23706263303756714\n",
      "epoch: 14 step: 91, loss is 0.21660128235816956\n",
      "epoch: 14 step: 92, loss is 0.21606828272342682\n",
      "epoch: 14 step: 93, loss is 0.1744939088821411\n",
      "epoch: 14 step: 94, loss is 0.3357873558998108\n",
      "epoch: 14 step: 95, loss is 0.18861567974090576\n",
      "epoch: 14 step: 96, loss is 0.15341754257678986\n",
      "epoch: 14 step: 97, loss is 0.1782490313053131\n",
      "epoch: 14 step: 98, loss is 0.1926051378250122\n",
      "epoch: 14 step: 99, loss is 0.160382479429245\n",
      "epoch: 14 step: 100, loss is 0.34066352248191833\n",
      "epoch: 14 step: 101, loss is 0.16271445155143738\n",
      "epoch: 14 step: 102, loss is 0.25872424244880676\n",
      "epoch: 14 step: 103, loss is 0.16356104612350464\n",
      "epoch: 14 step: 104, loss is 0.3357807695865631\n",
      "epoch: 14 step: 105, loss is 0.1545819342136383\n",
      "epoch: 14 step: 106, loss is 0.35337793827056885\n",
      "epoch: 14 step: 107, loss is 0.1904759705066681\n",
      "epoch: 14 step: 108, loss is 0.11873458325862885\n",
      "epoch: 14 step: 109, loss is 0.13496410846710205\n",
      "epoch: 14 step: 110, loss is 0.26628637313842773\n",
      "epoch: 14 step: 111, loss is 0.2265748828649521\n",
      "epoch: 14 step: 112, loss is 0.09881475567817688\n",
      "epoch: 14 step: 113, loss is 0.3281845152378082\n",
      "epoch: 14 step: 114, loss is 0.23667143285274506\n",
      "epoch: 14 step: 115, loss is 0.15263108909130096\n",
      "epoch: 14 step: 116, loss is 0.2979602813720703\n",
      "epoch: 14 step: 117, loss is 0.08932697772979736\n",
      "epoch: 14 step: 118, loss is 0.1248607337474823\n",
      "epoch: 14 step: 119, loss is 0.1945497691631317\n",
      "epoch: 14 step: 120, loss is 0.1778274029493332\n",
      "epoch: 14 step: 121, loss is 0.2516525983810425\n",
      "epoch: 14 step: 122, loss is 0.29188281297683716\n",
      "epoch: 14 step: 123, loss is 0.19622838497161865\n",
      "epoch: 14 step: 124, loss is 0.21132291853427887\n",
      "epoch: 14 step: 125, loss is 0.2684633135795593\n",
      "epoch: 14 step: 126, loss is 0.2715904414653778\n",
      "epoch: 14 step: 127, loss is 0.09984268993139267\n",
      "epoch: 14 step: 128, loss is 0.18434244394302368\n",
      "epoch: 14 step: 129, loss is 0.2684457302093506\n",
      "epoch: 14 step: 130, loss is 0.19301386177539825\n",
      "epoch: 14 step: 131, loss is 0.11779282987117767\n",
      "epoch: 14 step: 132, loss is 0.1275985836982727\n",
      "epoch: 14 step: 133, loss is 0.17759129405021667\n",
      "epoch: 14 step: 134, loss is 0.35741540789604187\n",
      "epoch: 14 step: 135, loss is 0.24342218041419983\n",
      "epoch: 14 step: 136, loss is 0.09972873330116272\n",
      "epoch: 14 step: 137, loss is 0.23822689056396484\n",
      "epoch: 14 step: 138, loss is 0.11993800103664398\n",
      "epoch: 14 step: 139, loss is 0.3884080946445465\n",
      "epoch: 14 step: 140, loss is 0.2783866226673126\n",
      "epoch: 14 step: 141, loss is 0.29867318272590637\n",
      "epoch: 14 step: 142, loss is 0.20161554217338562\n",
      "epoch: 14 step: 143, loss is 0.2555939853191376\n",
      "epoch: 14 step: 144, loss is 0.2474224716424942\n",
      "epoch: 14 step: 145, loss is 0.1148204579949379\n",
      "epoch: 14 step: 146, loss is 0.202941432595253\n",
      "epoch: 14 step: 147, loss is 0.11388809978961945\n",
      "epoch: 14 step: 148, loss is 0.30479222536087036\n",
      "epoch: 14 step: 149, loss is 0.29559364914894104\n",
      "epoch: 14 step: 150, loss is 0.26864388585090637\n",
      "epoch: 14 step: 151, loss is 0.20028910040855408\n",
      "epoch: 14 step: 152, loss is 0.26815974712371826\n",
      "epoch: 14 step: 153, loss is 0.1978408396244049\n",
      "epoch: 14 step: 154, loss is 0.15282797813415527\n",
      "epoch: 14 step: 155, loss is 0.22871826589107513\n",
      "epoch: 14 step: 156, loss is 0.29956597089767456\n",
      "epoch: 14 step: 157, loss is 0.1640540212392807\n",
      "epoch: 14 step: 158, loss is 0.12582248449325562\n",
      "epoch: 14 step: 159, loss is 0.15825290977954865\n",
      "epoch: 14 step: 160, loss is 0.1192793920636177\n",
      "epoch: 14 step: 161, loss is 0.1558832973241806\n",
      "epoch: 14 step: 162, loss is 0.18004845082759857\n",
      "epoch: 14 step: 163, loss is 0.2831646502017975\n",
      "epoch: 14 step: 164, loss is 0.1721201241016388\n",
      "epoch: 14 step: 165, loss is 0.15740244090557098\n",
      "epoch: 14 step: 166, loss is 0.12867796421051025\n",
      "epoch: 14 step: 167, loss is 0.29560568928718567\n",
      "epoch: 14 step: 168, loss is 0.2928301990032196\n",
      "epoch: 14 step: 169, loss is 0.23837287724018097\n",
      "epoch: 14 step: 170, loss is 0.11680712550878525\n",
      "epoch: 14 step: 171, loss is 0.15193472802639008\n",
      "epoch: 14 step: 172, loss is 0.2961198389530182\n",
      "epoch: 14 step: 173, loss is 0.051308415830135345\n",
      "epoch: 14 step: 174, loss is 0.374370276927948\n",
      "epoch: 14 step: 175, loss is 0.10799647122621536\n",
      "epoch: 14 step: 176, loss is 0.2616007328033447\n",
      "epoch: 14 step: 177, loss is 0.23863284289836884\n",
      "epoch: 14 step: 178, loss is 0.10790956765413284\n",
      "epoch: 14 step: 179, loss is 0.16140127182006836\n",
      "epoch: 14 step: 180, loss is 0.08536338061094284\n",
      "epoch: 14 step: 181, loss is 0.186426043510437\n",
      "epoch: 14 step: 182, loss is 0.3160731792449951\n",
      "epoch: 14 step: 183, loss is 0.14934861660003662\n",
      "epoch: 14 step: 184, loss is 0.13742688298225403\n",
      "epoch: 14 step: 185, loss is 0.10616232454776764\n",
      "epoch: 14 step: 186, loss is 0.28888073563575745\n",
      "epoch: 14 step: 187, loss is 0.1506575495004654\n",
      "epoch: 14 step: 188, loss is 0.2452864795923233\n",
      "epoch: 14 step: 189, loss is 0.11072419583797455\n",
      "epoch: 14 step: 190, loss is 0.14863891899585724\n",
      "epoch: 14 step: 191, loss is 0.2615794837474823\n",
      "epoch: 14 step: 192, loss is 0.16879162192344666\n",
      "epoch: 14 step: 193, loss is 0.12934429943561554\n",
      "epoch: 14 step: 194, loss is 0.353171706199646\n",
      "epoch: 14 step: 195, loss is 0.20248325169086456\n",
      "epoch: 14 step: 196, loss is 0.310654878616333\n",
      "epoch: 14 step: 197, loss is 0.25717470049858093\n",
      "epoch: 14 step: 198, loss is 0.37777090072631836\n",
      "epoch: 14 step: 199, loss is 0.2326335608959198\n",
      "epoch: 14 step: 200, loss is 0.17999736964702606\n",
      "epoch: 14 step: 201, loss is 0.13782183825969696\n",
      "epoch: 14 step: 202, loss is 0.276111900806427\n",
      "epoch: 14 step: 203, loss is 0.13828809559345245\n",
      "epoch: 14 step: 204, loss is 0.25833651423454285\n",
      "epoch: 14 step: 205, loss is 0.10989756882190704\n",
      "epoch: 14 step: 206, loss is 0.1457226425409317\n",
      "epoch: 14 step: 207, loss is 0.21635498106479645\n",
      "epoch: 14 step: 208, loss is 0.14165009558200836\n",
      "epoch: 14 step: 209, loss is 0.21432051062583923\n",
      "epoch: 14 step: 210, loss is 0.19232577085494995\n",
      "epoch: 14 step: 211, loss is 0.1352931261062622\n",
      "epoch: 14 step: 212, loss is 0.3643399178981781\n",
      "epoch: 14 step: 213, loss is 0.2272520214319229\n",
      "epoch: 14 step: 214, loss is 0.3025476932525635\n",
      "epoch: 14 step: 215, loss is 0.37237465381622314\n",
      "epoch: 14 step: 216, loss is 0.23336631059646606\n",
      "epoch: 14 step: 217, loss is 0.11148061603307724\n",
      "epoch: 14 step: 218, loss is 0.12557964026927948\n",
      "epoch: 14 step: 219, loss is 0.1182999387383461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 220, loss is 0.1908576786518097\n",
      "epoch: 14 step: 221, loss is 0.39632999897003174\n",
      "epoch: 14 step: 222, loss is 0.3442418575286865\n",
      "epoch: 14 step: 223, loss is 0.24441388249397278\n",
      "epoch: 14 step: 224, loss is 0.19575704634189606\n",
      "epoch: 14 step: 225, loss is 0.21958030760288239\n",
      "epoch: 14 step: 226, loss is 0.19086435437202454\n",
      "epoch: 14 step: 227, loss is 0.33164289593696594\n",
      "epoch: 14 step: 228, loss is 0.17093834280967712\n",
      "epoch: 14 step: 229, loss is 0.18178921937942505\n",
      "epoch: 14 step: 230, loss is 0.20880183577537537\n",
      "epoch: 14 step: 231, loss is 0.18880808353424072\n",
      "epoch: 14 step: 232, loss is 0.1652604639530182\n",
      "epoch: 14 step: 233, loss is 0.19272401928901672\n",
      "epoch: 14 step: 234, loss is 0.2744244337081909\n",
      "epoch: 14 step: 235, loss is 0.17457084357738495\n",
      "epoch: 14 step: 236, loss is 0.23462441563606262\n",
      "epoch: 14 step: 237, loss is 0.20954810082912445\n",
      "epoch: 14 step: 238, loss is 0.2547343075275421\n",
      "epoch: 14 step: 239, loss is 0.22816449403762817\n",
      "epoch: 14 step: 240, loss is 0.16534416377544403\n",
      "epoch: 14 step: 241, loss is 0.24536892771720886\n",
      "epoch: 14 step: 242, loss is 0.3586728572845459\n",
      "epoch: 14 step: 243, loss is 0.18496444821357727\n",
      "epoch: 14 step: 244, loss is 0.2198990285396576\n",
      "epoch: 14 step: 245, loss is 0.30893808603286743\n",
      "epoch: 14 step: 246, loss is 0.35977858304977417\n",
      "epoch: 14 step: 247, loss is 0.2934633493423462\n",
      "epoch: 14 step: 248, loss is 0.3469725549221039\n",
      "epoch: 14 step: 249, loss is 0.20602935552597046\n",
      "epoch: 14 step: 250, loss is 0.3411237597465515\n",
      "epoch: 14 step: 251, loss is 0.22016701102256775\n",
      "epoch: 14 step: 252, loss is 0.20998865365982056\n",
      "epoch: 14 step: 253, loss is 0.37945088744163513\n",
      "epoch: 14 step: 254, loss is 0.3493238687515259\n",
      "epoch: 14 step: 255, loss is 0.2988070845603943\n",
      "epoch: 14 step: 256, loss is 0.22516506910324097\n",
      "epoch: 14 step: 257, loss is 0.21044187247753143\n",
      "epoch: 14 step: 258, loss is 0.1384323239326477\n",
      "epoch: 14 step: 259, loss is 0.21075165271759033\n",
      "epoch: 14 step: 260, loss is 0.10590570420026779\n",
      "epoch: 14 step: 261, loss is 0.1599096655845642\n",
      "epoch: 14 step: 262, loss is 0.12562496960163116\n",
      "epoch: 14 step: 263, loss is 0.16388840973377228\n",
      "epoch: 14 step: 264, loss is 0.3202880024909973\n",
      "epoch: 14 step: 265, loss is 0.24879828095436096\n",
      "epoch: 14 step: 266, loss is 0.2211044728755951\n",
      "epoch: 14 step: 267, loss is 0.3560774326324463\n",
      "epoch: 14 step: 268, loss is 0.35764971375465393\n",
      "epoch: 14 step: 269, loss is 0.1338253766298294\n",
      "epoch: 14 step: 270, loss is 0.2770357131958008\n",
      "epoch: 14 step: 271, loss is 0.1495312750339508\n",
      "epoch: 14 step: 272, loss is 0.3732491731643677\n",
      "epoch: 14 step: 273, loss is 0.22898173332214355\n",
      "epoch: 14 step: 274, loss is 0.15926988422870636\n",
      "epoch: 14 step: 275, loss is 0.2938673496246338\n",
      "epoch: 14 step: 276, loss is 0.34684228897094727\n",
      "epoch: 14 step: 277, loss is 0.09149488806724548\n",
      "epoch: 14 step: 278, loss is 0.21188805997371674\n",
      "epoch: 14 step: 279, loss is 0.2742825448513031\n",
      "epoch: 14 step: 280, loss is 0.2541586458683014\n",
      "epoch: 14 step: 281, loss is 0.15830877423286438\n",
      "epoch: 14 step: 282, loss is 0.6378651857376099\n",
      "epoch: 14 step: 283, loss is 0.25186294317245483\n",
      "epoch: 14 step: 284, loss is 0.21589767932891846\n",
      "epoch: 14 step: 285, loss is 0.13913166522979736\n",
      "epoch: 14 step: 286, loss is 0.18115279078483582\n",
      "epoch: 14 step: 287, loss is 0.2612145245075226\n",
      "epoch: 14 step: 288, loss is 0.07953876256942749\n",
      "epoch: 14 step: 289, loss is 0.26134613156318665\n",
      "epoch: 14 step: 290, loss is 0.17636476457118988\n",
      "epoch: 14 step: 291, loss is 0.2546635568141937\n",
      "epoch: 14 step: 292, loss is 0.241948664188385\n",
      "epoch: 14 step: 293, loss is 0.18953776359558105\n",
      "epoch: 14 step: 294, loss is 0.28972408175468445\n",
      "epoch: 14 step: 295, loss is 0.2888704240322113\n",
      "epoch: 14 step: 296, loss is 0.23154215514659882\n",
      "epoch: 14 step: 297, loss is 0.16924692690372467\n",
      "epoch: 14 step: 298, loss is 0.27274858951568604\n",
      "epoch: 14 step: 299, loss is 0.2437366098165512\n",
      "epoch: 14 step: 300, loss is 0.3560331463813782\n",
      "epoch: 14 step: 301, loss is 0.22258126735687256\n",
      "epoch: 14 step: 302, loss is 0.12054183334112167\n",
      "epoch: 14 step: 303, loss is 0.17448318004608154\n",
      "epoch: 14 step: 304, loss is 0.3250129222869873\n",
      "epoch: 14 step: 305, loss is 0.24251267313957214\n",
      "epoch: 14 step: 306, loss is 0.3227153420448303\n",
      "epoch: 14 step: 307, loss is 0.21625827252864838\n",
      "epoch: 14 step: 308, loss is 0.1938183307647705\n",
      "epoch: 14 step: 309, loss is 0.2277361899614334\n",
      "epoch: 14 step: 310, loss is 0.1771516501903534\n",
      "epoch: 14 step: 311, loss is 0.2945479154586792\n",
      "epoch: 14 step: 312, loss is 0.12183121591806412\n",
      "epoch: 14 step: 313, loss is 0.39569780230522156\n",
      "epoch: 14 step: 314, loss is 0.2085818201303482\n",
      "epoch: 14 step: 315, loss is 0.20752756297588348\n",
      "epoch: 14 step: 316, loss is 0.30643177032470703\n",
      "epoch: 14 step: 317, loss is 0.20522470772266388\n",
      "epoch: 14 step: 318, loss is 0.1710261106491089\n",
      "epoch: 14 step: 319, loss is 0.20022045075893402\n",
      "epoch: 14 step: 320, loss is 0.39262139797210693\n",
      "epoch: 14 step: 321, loss is 0.12325701117515564\n",
      "epoch: 14 step: 322, loss is 0.19963635504245758\n",
      "epoch: 14 step: 323, loss is 0.37575867772102356\n",
      "epoch: 14 step: 324, loss is 0.24449428915977478\n",
      "epoch: 14 step: 325, loss is 0.22786587476730347\n",
      "epoch: 14 step: 326, loss is 0.18823687732219696\n",
      "epoch: 14 step: 327, loss is 0.23350390791893005\n",
      "epoch: 14 step: 328, loss is 0.19326266646385193\n",
      "epoch: 14 step: 329, loss is 0.17898181080818176\n",
      "epoch: 14 step: 330, loss is 0.20710651576519012\n",
      "epoch: 14 step: 331, loss is 0.16753214597702026\n",
      "epoch: 14 step: 332, loss is 0.18541322648525238\n",
      "epoch: 14 step: 333, loss is 0.19026200473308563\n",
      "epoch: 14 step: 334, loss is 0.12271060049533844\n",
      "epoch: 14 step: 335, loss is 0.244820699095726\n",
      "epoch: 14 step: 336, loss is 0.24969613552093506\n",
      "epoch: 14 step: 337, loss is 0.14073583483695984\n",
      "epoch: 14 step: 338, loss is 0.1252792328596115\n",
      "epoch: 14 step: 339, loss is 0.3698527216911316\n",
      "epoch: 14 step: 340, loss is 0.24434277415275574\n",
      "epoch: 14 step: 341, loss is 0.29500892758369446\n",
      "epoch: 14 step: 342, loss is 0.14208674430847168\n",
      "epoch: 14 step: 343, loss is 0.4173681139945984\n",
      "epoch: 14 step: 344, loss is 0.19465430080890656\n",
      "epoch: 14 step: 345, loss is 0.22296272218227386\n",
      "epoch: 14 step: 346, loss is 0.20420968532562256\n",
      "epoch: 14 step: 347, loss is 0.17132502794265747\n",
      "epoch: 14 step: 348, loss is 0.19508251547813416\n",
      "epoch: 14 step: 349, loss is 0.2122391164302826\n",
      "epoch: 14 step: 350, loss is 0.15746843814849854\n",
      "epoch: 14 step: 351, loss is 0.06968356668949127\n",
      "epoch: 14 step: 352, loss is 0.297536700963974\n",
      "epoch: 14 step: 353, loss is 0.15630502998828888\n",
      "epoch: 14 step: 354, loss is 0.2513831555843353\n",
      "epoch: 14 step: 355, loss is 0.16267475485801697\n",
      "epoch: 14 step: 356, loss is 0.32025307416915894\n",
      "epoch: 14 step: 357, loss is 0.15171407163143158\n",
      "epoch: 14 step: 358, loss is 0.22963477671146393\n",
      "epoch: 14 step: 359, loss is 0.3073192238807678\n",
      "epoch: 14 step: 360, loss is 0.25082334876060486\n",
      "epoch: 14 step: 361, loss is 0.24479810893535614\n",
      "epoch: 14 step: 362, loss is 0.14938564598560333\n",
      "epoch: 14 step: 363, loss is 0.22116617858409882\n",
      "epoch: 14 step: 364, loss is 0.24919599294662476\n",
      "epoch: 14 step: 365, loss is 0.19209542870521545\n",
      "epoch: 14 step: 366, loss is 0.12588995695114136\n",
      "epoch: 14 step: 367, loss is 0.26576200127601624\n",
      "epoch: 14 step: 368, loss is 0.16288302838802338\n",
      "epoch: 14 step: 369, loss is 0.14319980144500732\n",
      "epoch: 14 step: 370, loss is 0.11030080914497375\n",
      "epoch: 14 step: 371, loss is 0.1134508028626442\n",
      "epoch: 14 step: 372, loss is 0.22163578867912292\n",
      "epoch: 14 step: 373, loss is 0.2813797891139984\n",
      "epoch: 14 step: 374, loss is 0.2966097593307495\n",
      "epoch: 14 step: 375, loss is 0.13526669144630432\n",
      "epoch: 14 step: 376, loss is 0.21653641760349274\n",
      "epoch: 14 step: 377, loss is 0.2557767629623413\n",
      "epoch: 14 step: 378, loss is 0.24593104422092438\n",
      "epoch: 14 step: 379, loss is 0.3819062113761902\n",
      "epoch: 14 step: 380, loss is 0.1775510460138321\n",
      "epoch: 14 step: 381, loss is 0.14599350094795227\n",
      "epoch: 14 step: 382, loss is 0.14907142519950867\n",
      "epoch: 14 step: 383, loss is 0.33012646436691284\n",
      "epoch: 14 step: 384, loss is 0.24003754556179047\n",
      "epoch: 14 step: 385, loss is 0.09107928723096848\n",
      "epoch: 14 step: 386, loss is 0.2549344599246979\n",
      "epoch: 14 step: 387, loss is 0.09712255746126175\n",
      "epoch: 14 step: 388, loss is 0.2669345736503601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 389, loss is 0.18673980236053467\n",
      "epoch: 14 step: 390, loss is 0.4497230052947998\n",
      "epoch: 14 step: 391, loss is 0.1760566532611847\n",
      "epoch: 14 step: 392, loss is 0.18515169620513916\n",
      "epoch: 14 step: 393, loss is 0.11360133439302444\n",
      "epoch: 14 step: 394, loss is 0.280783474445343\n",
      "epoch: 14 step: 395, loss is 0.22986768186092377\n",
      "epoch: 14 step: 396, loss is 0.17378337681293488\n",
      "epoch: 14 step: 397, loss is 0.17600786685943604\n",
      "epoch: 14 step: 398, loss is 0.19170242547988892\n",
      "epoch: 14 step: 399, loss is 0.12385710328817368\n",
      "epoch: 14 step: 400, loss is 0.11885803192853928\n",
      "epoch: 14 step: 401, loss is 0.36739158630371094\n",
      "epoch: 14 step: 402, loss is 0.23250819742679596\n",
      "epoch: 14 step: 403, loss is 0.18519604206085205\n",
      "epoch: 14 step: 404, loss is 0.2962712347507477\n",
      "epoch: 14 step: 405, loss is 0.19494576752185822\n",
      "epoch: 14 step: 406, loss is 0.12482578307390213\n",
      "epoch: 14 step: 407, loss is 0.16890864074230194\n",
      "epoch: 14 step: 408, loss is 0.2861481010913849\n",
      "epoch: 14 step: 409, loss is 0.28067052364349365\n",
      "epoch: 14 step: 410, loss is 0.34878009557724\n",
      "epoch: 14 step: 411, loss is 0.41180795431137085\n",
      "epoch: 14 step: 412, loss is 0.2735162377357483\n",
      "epoch: 14 step: 413, loss is 0.2703488767147064\n",
      "epoch: 14 step: 414, loss is 0.2146461308002472\n",
      "epoch: 14 step: 415, loss is 0.19571320712566376\n",
      "epoch: 14 step: 416, loss is 0.2027484029531479\n",
      "epoch: 14 step: 417, loss is 0.2093782275915146\n",
      "epoch: 14 step: 418, loss is 0.09711991995573044\n",
      "epoch: 14 step: 419, loss is 0.17504680156707764\n",
      "epoch: 14 step: 420, loss is 0.38085615634918213\n",
      "epoch: 14 step: 421, loss is 0.2668989300727844\n",
      "epoch: 14 step: 422, loss is 0.20560841262340546\n",
      "epoch: 14 step: 423, loss is 0.1686684638261795\n",
      "epoch: 14 step: 424, loss is 0.19393876194953918\n",
      "epoch: 14 step: 425, loss is 0.3007403612136841\n",
      "epoch: 14 step: 426, loss is 0.19235362112522125\n",
      "epoch: 14 step: 427, loss is 0.34740015864372253\n",
      "epoch: 14 step: 428, loss is 0.24020153284072876\n",
      "epoch: 14 step: 429, loss is 0.21070247888565063\n",
      "epoch: 14 step: 430, loss is 0.30607521533966064\n",
      "epoch: 14 step: 431, loss is 0.24394814670085907\n",
      "epoch: 14 step: 432, loss is 0.2005159854888916\n",
      "epoch: 14 step: 433, loss is 0.25845691561698914\n",
      "epoch: 14 step: 434, loss is 0.21286796033382416\n",
      "epoch: 14 step: 435, loss is 0.19917139410972595\n",
      "epoch: 14 step: 436, loss is 0.1170090064406395\n",
      "epoch: 14 step: 437, loss is 0.28474608063697815\n",
      "epoch: 14 step: 438, loss is 0.18969261646270752\n",
      "epoch: 14 step: 439, loss is 0.20703263580799103\n",
      "epoch: 14 step: 440, loss is 0.19236724078655243\n",
      "epoch: 14 step: 441, loss is 0.11253660172224045\n",
      "epoch: 14 step: 442, loss is 0.20296768844127655\n",
      "epoch: 14 step: 443, loss is 0.19651834666728973\n",
      "epoch: 14 step: 444, loss is 0.07388559728860855\n",
      "epoch: 14 step: 445, loss is 0.3612072169780731\n",
      "epoch: 14 step: 446, loss is 0.25867608189582825\n",
      "epoch: 14 step: 447, loss is 0.560581624507904\n",
      "epoch: 14 step: 448, loss is 0.27787747979164124\n",
      "epoch: 14 step: 449, loss is 0.25052744150161743\n",
      "epoch: 14 step: 450, loss is 0.15876969695091248\n",
      "epoch: 14 step: 451, loss is 0.21465370059013367\n",
      "epoch: 14 step: 452, loss is 0.2824825048446655\n",
      "epoch: 14 step: 453, loss is 0.19051726162433624\n",
      "epoch: 14 step: 454, loss is 0.16559183597564697\n",
      "epoch: 14 step: 455, loss is 0.1943921148777008\n",
      "epoch: 14 step: 456, loss is 0.14957140386104584\n",
      "epoch: 14 step: 457, loss is 0.21405375003814697\n",
      "epoch: 14 step: 458, loss is 0.1671755313873291\n",
      "epoch: 14 step: 459, loss is 0.19046658277511597\n",
      "epoch: 14 step: 460, loss is 0.20174269378185272\n",
      "epoch: 14 step: 461, loss is 0.3502279818058014\n",
      "epoch: 14 step: 462, loss is 0.7512825131416321\n",
      "epoch: 14 step: 463, loss is 0.3488866686820984\n",
      "epoch: 14 step: 464, loss is 0.20638133585453033\n",
      "epoch: 14 step: 465, loss is 0.17349635064601898\n",
      "epoch: 14 step: 466, loss is 0.21318763494491577\n",
      "epoch: 14 step: 467, loss is 0.3644752502441406\n",
      "epoch: 14 step: 468, loss is 0.32916682958602905\n",
      "epoch: 14 step: 469, loss is 0.22425836324691772\n",
      "epoch: 14 step: 470, loss is 0.34578198194503784\n",
      "epoch: 14 step: 471, loss is 0.3524983823299408\n",
      "epoch: 14 step: 472, loss is 0.3683773875236511\n",
      "epoch: 14 step: 473, loss is 0.1534847468137741\n",
      "epoch: 14 step: 474, loss is 0.2478308230638504\n",
      "epoch: 14 step: 475, loss is 0.12623494863510132\n",
      "epoch: 14 step: 476, loss is 0.2192833572626114\n",
      "epoch: 14 step: 477, loss is 0.23544643819332123\n",
      "epoch: 14 step: 478, loss is 0.12165874987840652\n",
      "epoch: 14 step: 479, loss is 0.23343737423419952\n",
      "epoch: 14 step: 480, loss is 0.22743484377861023\n",
      "epoch: 14 step: 481, loss is 0.15974316000938416\n",
      "epoch: 14 step: 482, loss is 0.23204989731311798\n",
      "epoch: 14 step: 483, loss is 0.4028911888599396\n",
      "epoch: 14 step: 484, loss is 0.15624819695949554\n",
      "epoch: 14 step: 485, loss is 0.30389270186424255\n",
      "epoch: 14 step: 486, loss is 0.3732450008392334\n",
      "epoch: 14 step: 487, loss is 0.13269564509391785\n",
      "epoch: 14 step: 488, loss is 0.25308409333229065\n",
      "epoch: 14 step: 489, loss is 0.24424763023853302\n",
      "epoch: 14 step: 490, loss is 0.22285418212413788\n",
      "epoch: 14 step: 491, loss is 0.18067477643489838\n",
      "epoch: 14 step: 492, loss is 0.30451273918151855\n",
      "epoch: 14 step: 493, loss is 0.1683768928050995\n",
      "epoch: 14 step: 494, loss is 0.2669753134250641\n",
      "epoch: 14 step: 495, loss is 0.2378038614988327\n",
      "epoch: 14 step: 496, loss is 0.19443613290786743\n",
      "epoch: 14 step: 497, loss is 0.20230020582675934\n",
      "epoch: 14 step: 498, loss is 0.11015705019235611\n",
      "epoch: 14 step: 499, loss is 0.2043609321117401\n",
      "epoch: 14 step: 500, loss is 0.2986285984516144\n",
      "epoch: 14 step: 501, loss is 0.3048878014087677\n",
      "epoch: 14 step: 502, loss is 0.24179686605930328\n",
      "epoch: 14 step: 503, loss is 0.17242471873760223\n",
      "epoch: 14 step: 504, loss is 0.2291915863752365\n",
      "epoch: 14 step: 505, loss is 0.1527891308069229\n",
      "epoch: 14 step: 506, loss is 0.09545475244522095\n",
      "epoch: 14 step: 507, loss is 0.2406083196401596\n",
      "epoch: 14 step: 508, loss is 0.27437737584114075\n",
      "epoch: 14 step: 509, loss is 0.17954915761947632\n",
      "epoch: 14 step: 510, loss is 0.12277257442474365\n",
      "epoch: 14 step: 511, loss is 0.12605366110801697\n",
      "epoch: 14 step: 512, loss is 0.06958912312984467\n",
      "epoch: 14 step: 513, loss is 0.2144298553466797\n",
      "epoch: 14 step: 514, loss is 0.21323998272418976\n",
      "epoch: 14 step: 515, loss is 0.11912664771080017\n",
      "epoch: 14 step: 516, loss is 0.21204626560211182\n",
      "epoch: 14 step: 517, loss is 0.2801184356212616\n",
      "epoch: 14 step: 518, loss is 0.19177813827991486\n",
      "epoch: 14 step: 519, loss is 0.2918172776699066\n",
      "epoch: 14 step: 520, loss is 0.1486710011959076\n",
      "epoch: 14 step: 521, loss is 0.17000798881053925\n",
      "epoch: 14 step: 522, loss is 0.07991137355566025\n",
      "epoch: 14 step: 523, loss is 0.2931711673736572\n",
      "epoch: 14 step: 524, loss is 0.1428496390581131\n",
      "epoch: 14 step: 525, loss is 0.15160706639289856\n",
      "epoch: 14 step: 526, loss is 0.132851704955101\n",
      "epoch: 14 step: 527, loss is 0.1627284288406372\n",
      "epoch: 14 step: 528, loss is 0.16964150965213776\n",
      "epoch: 14 step: 529, loss is 0.14140141010284424\n",
      "epoch: 14 step: 530, loss is 0.24527333676815033\n",
      "epoch: 14 step: 531, loss is 0.15468722581863403\n",
      "epoch: 14 step: 532, loss is 0.3469938337802887\n",
      "epoch: 14 step: 533, loss is 0.10371115058660507\n",
      "epoch: 14 step: 534, loss is 0.14465782046318054\n",
      "epoch: 14 step: 535, loss is 0.25811436772346497\n",
      "epoch: 14 step: 536, loss is 0.2603408396244049\n",
      "epoch: 14 step: 537, loss is 0.20119823515415192\n",
      "epoch: 14 step: 538, loss is 0.14078544080257416\n",
      "epoch: 14 step: 539, loss is 0.18470792472362518\n",
      "epoch: 14 step: 540, loss is 0.32511594891548157\n",
      "epoch: 14 step: 541, loss is 0.2704032063484192\n",
      "epoch: 14 step: 542, loss is 0.3737834393978119\n",
      "epoch: 14 step: 543, loss is 0.1677049994468689\n",
      "epoch: 14 step: 544, loss is 0.21204213798046112\n",
      "epoch: 14 step: 545, loss is 0.2799109220504761\n",
      "epoch: 14 step: 546, loss is 0.28486838936805725\n",
      "epoch: 14 step: 547, loss is 0.3421373963356018\n",
      "epoch: 14 step: 548, loss is 0.3327156603336334\n",
      "epoch: 14 step: 549, loss is 0.2624366581439972\n",
      "epoch: 14 step: 550, loss is 0.1146201342344284\n",
      "epoch: 14 step: 551, loss is 0.2518098056316376\n",
      "epoch: 14 step: 552, loss is 0.2539759874343872\n",
      "epoch: 14 step: 553, loss is 0.2962464690208435\n",
      "epoch: 14 step: 554, loss is 0.18821655213832855\n",
      "epoch: 14 step: 555, loss is 0.24434469640254974\n",
      "epoch: 14 step: 556, loss is 0.30357542634010315\n",
      "epoch: 14 step: 557, loss is 0.12769947946071625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 558, loss is 0.22086302936077118\n",
      "epoch: 14 step: 559, loss is 0.28525757789611816\n",
      "epoch: 14 step: 560, loss is 0.3333936333656311\n",
      "epoch: 14 step: 561, loss is 0.21179670095443726\n",
      "epoch: 14 step: 562, loss is 0.2716073989868164\n",
      "epoch: 14 step: 563, loss is 0.22531628608703613\n",
      "epoch: 14 step: 564, loss is 0.2063847780227661\n",
      "epoch: 14 step: 565, loss is 0.18593379855155945\n",
      "epoch: 14 step: 566, loss is 0.20524747669696808\n",
      "epoch: 14 step: 567, loss is 0.15854887664318085\n",
      "epoch: 14 step: 568, loss is 0.27991271018981934\n",
      "epoch: 14 step: 569, loss is 0.20422452688217163\n",
      "epoch: 14 step: 570, loss is 0.23205658793449402\n",
      "epoch: 14 step: 571, loss is 0.07466153055429459\n",
      "epoch: 14 step: 572, loss is 0.2861090302467346\n",
      "epoch: 14 step: 573, loss is 0.1903700977563858\n",
      "epoch: 14 step: 574, loss is 0.2963976562023163\n",
      "epoch: 14 step: 575, loss is 0.16954275965690613\n",
      "epoch: 14 step: 576, loss is 0.21818500757217407\n",
      "epoch: 14 step: 577, loss is 0.28739139437675476\n",
      "epoch: 14 step: 578, loss is 0.19746412336826324\n",
      "epoch: 14 step: 579, loss is 0.2837703824043274\n",
      "epoch: 14 step: 580, loss is 0.3465704321861267\n",
      "epoch: 14 step: 581, loss is 0.1164184957742691\n",
      "epoch: 14 step: 582, loss is 0.14386771619319916\n",
      "epoch: 14 step: 583, loss is 0.19965405762195587\n",
      "epoch: 14 step: 584, loss is 0.27269789576530457\n",
      "epoch: 14 step: 585, loss is 0.25312817096710205\n",
      "epoch: 14 step: 586, loss is 0.4515440762042999\n",
      "epoch: 14 step: 587, loss is 0.1688450574874878\n",
      "epoch: 14 step: 588, loss is 0.2239733636379242\n",
      "epoch: 14 step: 589, loss is 0.34550175070762634\n",
      "epoch: 14 step: 590, loss is 0.3054999113082886\n",
      "epoch: 14 step: 591, loss is 0.23602870106697083\n",
      "epoch: 14 step: 592, loss is 0.29590123891830444\n",
      "epoch: 14 step: 593, loss is 0.319994181394577\n",
      "epoch: 14 step: 594, loss is 0.25519293546676636\n",
      "epoch: 14 step: 595, loss is 0.18562966585159302\n",
      "epoch: 14 step: 596, loss is 0.10889411717653275\n",
      "epoch: 14 step: 597, loss is 0.21410050988197327\n",
      "epoch: 14 step: 598, loss is 0.18974806368350983\n",
      "epoch: 14 step: 599, loss is 0.3780924677848816\n",
      "epoch: 14 step: 600, loss is 0.2791428864002228\n",
      "epoch: 14 step: 601, loss is 0.14658530056476593\n",
      "epoch: 14 step: 602, loss is 0.2765403091907501\n",
      "epoch: 14 step: 603, loss is 0.3087712824344635\n",
      "epoch: 14 step: 604, loss is 0.15828655660152435\n",
      "epoch: 14 step: 605, loss is 0.13891448080539703\n",
      "epoch: 14 step: 606, loss is 0.14318342506885529\n",
      "epoch: 14 step: 607, loss is 0.24973604083061218\n",
      "epoch: 14 step: 608, loss is 0.2284848541021347\n",
      "epoch: 14 step: 609, loss is 0.19648191332817078\n",
      "epoch: 14 step: 610, loss is 0.280315101146698\n",
      "epoch: 14 step: 611, loss is 0.27016136050224304\n",
      "epoch: 14 step: 612, loss is 0.26981881260871887\n",
      "epoch: 14 step: 613, loss is 0.1363482028245926\n",
      "epoch: 14 step: 614, loss is 0.24022610485553741\n",
      "epoch: 14 step: 615, loss is 0.33416396379470825\n",
      "epoch: 14 step: 616, loss is 0.18072737753391266\n",
      "epoch: 14 step: 617, loss is 0.34254494309425354\n",
      "epoch: 14 step: 618, loss is 0.10585957020521164\n",
      "epoch: 14 step: 619, loss is 0.28712281584739685\n",
      "epoch: 14 step: 620, loss is 0.2272847592830658\n",
      "epoch: 14 step: 621, loss is 0.13433001935482025\n",
      "epoch: 14 step: 622, loss is 0.17111480236053467\n",
      "epoch: 14 step: 623, loss is 0.26332107186317444\n",
      "epoch: 14 step: 624, loss is 0.14030912518501282\n",
      "epoch: 14 step: 625, loss is 0.2798689007759094\n",
      "epoch: 14 step: 626, loss is 0.26399239897727966\n",
      "epoch: 14 step: 627, loss is 0.2838130593299866\n",
      "epoch: 14 step: 628, loss is 0.2287602722644806\n",
      "epoch: 14 step: 629, loss is 0.3142556846141815\n",
      "epoch: 14 step: 630, loss is 0.4028172194957733\n",
      "epoch: 14 step: 631, loss is 0.1485990583896637\n",
      "epoch: 14 step: 632, loss is 0.30719664692878723\n",
      "epoch: 14 step: 633, loss is 0.4118535816669464\n",
      "epoch: 14 step: 634, loss is 0.25549083948135376\n",
      "epoch: 14 step: 635, loss is 0.13294976949691772\n",
      "epoch: 14 step: 636, loss is 0.30955970287323\n",
      "epoch: 14 step: 637, loss is 0.21035997569561005\n",
      "epoch: 14 step: 638, loss is 0.11195050925016403\n",
      "epoch: 14 step: 639, loss is 0.23526069521903992\n",
      "epoch: 14 step: 640, loss is 0.20871949195861816\n",
      "epoch: 14 step: 641, loss is 0.1880352944135666\n",
      "epoch: 14 step: 642, loss is 0.12396842986345291\n",
      "epoch: 14 step: 643, loss is 0.20144522190093994\n",
      "epoch: 14 step: 644, loss is 0.19826848804950714\n",
      "epoch: 14 step: 645, loss is 0.15003705024719238\n",
      "epoch: 14 step: 646, loss is 0.3495837152004242\n",
      "epoch: 14 step: 647, loss is 0.3192090094089508\n",
      "epoch: 14 step: 648, loss is 0.3523183763027191\n",
      "epoch: 14 step: 649, loss is 0.3172335624694824\n",
      "epoch: 14 step: 650, loss is 0.3225835859775543\n",
      "epoch: 14 step: 651, loss is 0.18641282618045807\n",
      "epoch: 14 step: 652, loss is 0.22718140482902527\n",
      "epoch: 14 step: 653, loss is 0.19799363613128662\n",
      "epoch: 14 step: 654, loss is 0.2691517174243927\n",
      "epoch: 14 step: 655, loss is 0.31757551431655884\n",
      "epoch: 14 step: 656, loss is 0.3952721357345581\n",
      "epoch: 14 step: 657, loss is 0.2403624802827835\n",
      "epoch: 14 step: 658, loss is 0.26099100708961487\n",
      "epoch: 14 step: 659, loss is 0.2869858145713806\n",
      "epoch: 14 step: 660, loss is 0.1897263079881668\n",
      "epoch: 14 step: 661, loss is 0.0677582323551178\n",
      "epoch: 14 step: 662, loss is 0.18421243131160736\n",
      "epoch: 14 step: 663, loss is 0.1535756140947342\n",
      "epoch: 14 step: 664, loss is 0.33004331588745117\n",
      "epoch: 14 step: 665, loss is 0.25217312574386597\n",
      "epoch: 14 step: 666, loss is 0.20918671786785126\n",
      "epoch: 14 step: 667, loss is 0.2085820436477661\n",
      "epoch: 14 step: 668, loss is 0.3275861144065857\n",
      "epoch: 14 step: 669, loss is 0.15956534445285797\n",
      "epoch: 14 step: 670, loss is 0.23178374767303467\n",
      "epoch: 14 step: 671, loss is 0.14742839336395264\n",
      "epoch: 14 step: 672, loss is 0.28758537769317627\n",
      "epoch: 14 step: 673, loss is 0.2570813298225403\n",
      "epoch: 14 step: 674, loss is 0.371919184923172\n",
      "epoch: 14 step: 675, loss is 0.22787202894687653\n",
      "epoch: 14 step: 676, loss is 0.15946434438228607\n",
      "epoch: 14 step: 677, loss is 0.25643619894981384\n",
      "epoch: 14 step: 678, loss is 0.14265558123588562\n",
      "epoch: 14 step: 679, loss is 0.33741524815559387\n",
      "epoch: 14 step: 680, loss is 0.23728300631046295\n",
      "epoch: 14 step: 681, loss is 0.20908381044864655\n",
      "epoch: 14 step: 682, loss is 0.1421308070421219\n",
      "epoch: 14 step: 683, loss is 0.22855022549629211\n",
      "epoch: 14 step: 684, loss is 0.21019862592220306\n",
      "epoch: 14 step: 685, loss is 0.1967487633228302\n",
      "epoch: 14 step: 686, loss is 0.12364456802606583\n",
      "epoch: 14 step: 687, loss is 0.3890434205532074\n",
      "epoch: 14 step: 688, loss is 0.11369333416223526\n",
      "epoch: 14 step: 689, loss is 0.2869030833244324\n",
      "epoch: 14 step: 690, loss is 0.13822047412395477\n",
      "epoch: 14 step: 691, loss is 0.26916396617889404\n",
      "epoch: 14 step: 692, loss is 0.4260270595550537\n",
      "epoch: 14 step: 693, loss is 0.1567031741142273\n",
      "epoch: 14 step: 694, loss is 0.3200604021549225\n",
      "epoch: 14 step: 695, loss is 0.3417746126651764\n",
      "epoch: 14 step: 696, loss is 0.22701027989387512\n",
      "epoch: 14 step: 697, loss is 0.2653014659881592\n",
      "epoch: 14 step: 698, loss is 0.3801819682121277\n",
      "epoch: 14 step: 699, loss is 0.16204647719860077\n",
      "epoch: 14 step: 700, loss is 0.23514007031917572\n",
      "epoch: 14 step: 701, loss is 0.16302554309368134\n",
      "epoch: 14 step: 702, loss is 0.20603935420513153\n",
      "epoch: 14 step: 703, loss is 0.3834124505519867\n",
      "epoch: 14 step: 704, loss is 0.17671164870262146\n",
      "epoch: 14 step: 705, loss is 0.21547509729862213\n",
      "epoch: 14 step: 706, loss is 0.21123106777668\n",
      "epoch: 14 step: 707, loss is 0.276634156703949\n",
      "epoch: 14 step: 708, loss is 0.14420606195926666\n",
      "epoch: 14 step: 709, loss is 0.2751157283782959\n",
      "epoch: 14 step: 710, loss is 0.17394904792308807\n",
      "epoch: 14 step: 711, loss is 0.3316758871078491\n",
      "epoch: 14 step: 712, loss is 0.1738337278366089\n",
      "epoch: 14 step: 713, loss is 0.18774792551994324\n",
      "epoch: 14 step: 714, loss is 0.20784926414489746\n",
      "epoch: 14 step: 715, loss is 0.16120536625385284\n",
      "epoch: 14 step: 716, loss is 0.30486124753952026\n",
      "epoch: 14 step: 717, loss is 0.19202572107315063\n",
      "epoch: 14 step: 718, loss is 0.2035645842552185\n",
      "epoch: 14 step: 719, loss is 0.17506685853004456\n",
      "epoch: 14 step: 720, loss is 0.29082608222961426\n",
      "epoch: 14 step: 721, loss is 0.17227676510810852\n",
      "epoch: 14 step: 722, loss is 0.36838874220848083\n",
      "epoch: 14 step: 723, loss is 0.16297125816345215\n",
      "epoch: 14 step: 724, loss is 0.18157434463500977\n",
      "epoch: 14 step: 725, loss is 0.1641935557126999\n",
      "epoch: 14 step: 726, loss is 0.13694992661476135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 727, loss is 0.18114958703517914\n",
      "epoch: 14 step: 728, loss is 0.16524438560009003\n",
      "epoch: 14 step: 729, loss is 0.25983425974845886\n",
      "epoch: 14 step: 730, loss is 0.3406074643135071\n",
      "epoch: 14 step: 731, loss is 0.2961357831954956\n",
      "epoch: 14 step: 732, loss is 0.1374041885137558\n",
      "epoch: 14 step: 733, loss is 0.07997876405715942\n",
      "epoch: 14 step: 734, loss is 0.17250747978687286\n",
      "epoch: 14 step: 735, loss is 0.20784924924373627\n",
      "epoch: 14 step: 736, loss is 0.5244014859199524\n",
      "epoch: 14 step: 737, loss is 0.14266079664230347\n",
      "epoch: 14 step: 738, loss is 0.347515344619751\n",
      "epoch: 14 step: 739, loss is 0.2846734821796417\n",
      "epoch: 14 step: 740, loss is 0.29585787653923035\n",
      "epoch: 14 step: 741, loss is 0.1360335499048233\n",
      "epoch: 14 step: 742, loss is 0.23607297241687775\n",
      "epoch: 14 step: 743, loss is 0.164976567029953\n",
      "epoch: 14 step: 744, loss is 0.3901163935661316\n",
      "epoch: 14 step: 745, loss is 0.2099069207906723\n",
      "epoch: 14 step: 746, loss is 0.2273082286119461\n",
      "epoch: 14 step: 747, loss is 0.10081762075424194\n",
      "epoch: 14 step: 748, loss is 0.24892759323120117\n",
      "epoch: 14 step: 749, loss is 0.13667859137058258\n",
      "epoch: 14 step: 750, loss is 0.3634204864501953\n",
      "epoch: 14 step: 751, loss is 0.3658667206764221\n",
      "epoch: 14 step: 752, loss is 0.19638873636722565\n",
      "epoch: 14 step: 753, loss is 0.3424615263938904\n",
      "epoch: 14 step: 754, loss is 0.17567558586597443\n",
      "epoch: 14 step: 755, loss is 0.1697520762681961\n",
      "epoch: 14 step: 756, loss is 0.18261578679084778\n",
      "epoch: 14 step: 757, loss is 0.09340329468250275\n",
      "epoch: 14 step: 758, loss is 0.276946485042572\n",
      "epoch: 14 step: 759, loss is 0.2602027356624603\n",
      "epoch: 14 step: 760, loss is 0.22979260981082916\n",
      "epoch: 14 step: 761, loss is 0.16840407252311707\n",
      "epoch: 14 step: 762, loss is 0.2518962025642395\n",
      "epoch: 14 step: 763, loss is 0.22536130249500275\n",
      "epoch: 14 step: 764, loss is 0.2649323642253876\n",
      "epoch: 14 step: 765, loss is 0.22511076927185059\n",
      "epoch: 14 step: 766, loss is 0.2056126892566681\n",
      "epoch: 14 step: 767, loss is 0.318899005651474\n",
      "epoch: 14 step: 768, loss is 0.3492150604724884\n",
      "epoch: 14 step: 769, loss is 0.2397640496492386\n",
      "epoch: 14 step: 770, loss is 0.2611828148365021\n",
      "epoch: 14 step: 771, loss is 0.1810559630393982\n",
      "epoch: 14 step: 772, loss is 0.09106236696243286\n",
      "epoch: 14 step: 773, loss is 0.09251169860363007\n",
      "epoch: 14 step: 774, loss is 0.15656384825706482\n",
      "epoch: 14 step: 775, loss is 0.3891953229904175\n",
      "epoch: 14 step: 776, loss is 0.32339417934417725\n",
      "epoch: 14 step: 777, loss is 0.20266994833946228\n",
      "epoch: 14 step: 778, loss is 0.13627663254737854\n",
      "epoch: 14 step: 779, loss is 0.2862844467163086\n",
      "epoch: 14 step: 780, loss is 0.19137082993984222\n",
      "epoch: 14 step: 781, loss is 0.15633946657180786\n",
      "epoch: 14 step: 782, loss is 0.2826487720012665\n",
      "epoch: 14 step: 783, loss is 0.20923592150211334\n",
      "epoch: 14 step: 784, loss is 0.23292843997478485\n",
      "epoch: 14 step: 785, loss is 0.10870418697595596\n",
      "epoch: 14 step: 786, loss is 0.32248732447624207\n",
      "epoch: 14 step: 787, loss is 0.1913171261548996\n",
      "epoch: 14 step: 788, loss is 0.27919667959213257\n",
      "epoch: 14 step: 789, loss is 0.20114357769489288\n",
      "epoch: 14 step: 790, loss is 0.2696027159690857\n",
      "epoch: 14 step: 791, loss is 0.22404208779335022\n",
      "epoch: 14 step: 792, loss is 0.09403276443481445\n",
      "epoch: 14 step: 793, loss is 0.11289457976818085\n",
      "epoch: 14 step: 794, loss is 0.1783289611339569\n",
      "epoch: 14 step: 795, loss is 0.2172365039587021\n",
      "epoch: 14 step: 796, loss is 0.0991801843047142\n",
      "epoch: 14 step: 797, loss is 0.38170796632766724\n",
      "epoch: 14 step: 798, loss is 0.27071696519851685\n",
      "epoch: 14 step: 799, loss is 0.2082495391368866\n",
      "epoch: 14 step: 800, loss is 0.2661774158477783\n",
      "epoch: 14 step: 801, loss is 0.25901368260383606\n",
      "epoch: 14 step: 802, loss is 0.19010671973228455\n",
      "epoch: 14 step: 803, loss is 0.1526300460100174\n",
      "epoch: 14 step: 804, loss is 0.2447510063648224\n",
      "epoch: 14 step: 805, loss is 0.1690426468849182\n",
      "epoch: 14 step: 806, loss is 0.20691683888435364\n",
      "epoch: 14 step: 807, loss is 0.17888720333576202\n",
      "epoch: 14 step: 808, loss is 0.1779039353132248\n",
      "epoch: 14 step: 809, loss is 0.20517782866954803\n",
      "epoch: 14 step: 810, loss is 0.2111157327890396\n",
      "epoch: 14 step: 811, loss is 0.31013041734695435\n",
      "epoch: 14 step: 812, loss is 0.1742689311504364\n",
      "epoch: 14 step: 813, loss is 0.22181472182273865\n",
      "epoch: 14 step: 814, loss is 0.11636725068092346\n",
      "epoch: 14 step: 815, loss is 0.2642020583152771\n",
      "epoch: 14 step: 816, loss is 0.36439451575279236\n",
      "epoch: 14 step: 817, loss is 0.08024552464485168\n",
      "epoch: 14 step: 818, loss is 0.32140183448791504\n",
      "epoch: 14 step: 819, loss is 0.21898742020130157\n",
      "epoch: 14 step: 820, loss is 0.17645177245140076\n",
      "epoch: 14 step: 821, loss is 0.1438637226819992\n",
      "epoch: 14 step: 822, loss is 0.32549116015434265\n",
      "epoch: 14 step: 823, loss is 0.1147523745894432\n",
      "epoch: 14 step: 824, loss is 0.27723827958106995\n",
      "epoch: 14 step: 825, loss is 0.09309902042150497\n",
      "epoch: 14 step: 826, loss is 0.37164783477783203\n",
      "epoch: 14 step: 827, loss is 0.22098201513290405\n",
      "epoch: 14 step: 828, loss is 0.2660924792289734\n",
      "epoch: 14 step: 829, loss is 0.179142028093338\n",
      "epoch: 14 step: 830, loss is 0.16841652989387512\n",
      "epoch: 14 step: 831, loss is 0.22752933204174042\n",
      "epoch: 14 step: 832, loss is 0.20816047489643097\n",
      "epoch: 14 step: 833, loss is 0.2533538043498993\n",
      "epoch: 14 step: 834, loss is 0.18428723514080048\n",
      "epoch: 14 step: 835, loss is 0.25678732991218567\n",
      "epoch: 14 step: 836, loss is 0.22692711651325226\n",
      "epoch: 14 step: 837, loss is 0.14617615938186646\n",
      "epoch: 14 step: 838, loss is 0.07391563057899475\n",
      "epoch: 14 step: 839, loss is 0.2811981737613678\n",
      "epoch: 14 step: 840, loss is 0.40714550018310547\n",
      "epoch: 14 step: 841, loss is 0.21156945824623108\n",
      "epoch: 14 step: 842, loss is 0.07139362394809723\n",
      "epoch: 14 step: 843, loss is 0.21650204062461853\n",
      "epoch: 14 step: 844, loss is 0.3055621385574341\n",
      "epoch: 14 step: 845, loss is 0.17476287484169006\n",
      "epoch: 14 step: 846, loss is 0.170905202627182\n",
      "epoch: 14 step: 847, loss is 0.13388606905937195\n",
      "epoch: 14 step: 848, loss is 0.24692478775978088\n",
      "epoch: 14 step: 849, loss is 0.21803726255893707\n",
      "epoch: 14 step: 850, loss is 0.31902211904525757\n",
      "epoch: 14 step: 851, loss is 0.29017528891563416\n",
      "epoch: 14 step: 852, loss is 0.21939623355865479\n",
      "epoch: 14 step: 853, loss is 0.11985793709754944\n",
      "epoch: 14 step: 854, loss is 0.21738919615745544\n",
      "epoch: 14 step: 855, loss is 0.29467612504959106\n",
      "epoch: 14 step: 856, loss is 0.13630886375904083\n",
      "epoch: 14 step: 857, loss is 0.29631099104881287\n",
      "epoch: 14 step: 858, loss is 0.3015333116054535\n",
      "epoch: 14 step: 859, loss is 0.11326496303081512\n",
      "epoch: 14 step: 860, loss is 0.2072981894016266\n",
      "epoch: 14 step: 861, loss is 0.21626262366771698\n",
      "epoch: 14 step: 862, loss is 0.24538178741931915\n",
      "epoch: 14 step: 863, loss is 0.14826826751232147\n",
      "epoch: 14 step: 864, loss is 0.2218071073293686\n",
      "epoch: 14 step: 865, loss is 0.17136792838573456\n",
      "epoch: 14 step: 866, loss is 0.09990197420120239\n",
      "epoch: 14 step: 867, loss is 0.22412937879562378\n",
      "epoch: 14 step: 868, loss is 0.2610539197921753\n",
      "epoch: 14 step: 869, loss is 0.18309441208839417\n",
      "epoch: 14 step: 870, loss is 0.37812894582748413\n",
      "epoch: 14 step: 871, loss is 0.11932965368032455\n",
      "epoch: 14 step: 872, loss is 0.30055567622184753\n",
      "epoch: 14 step: 873, loss is 0.16745886206626892\n",
      "epoch: 14 step: 874, loss is 0.3455163240432739\n",
      "epoch: 14 step: 875, loss is 0.36939314007759094\n",
      "epoch: 14 step: 876, loss is 0.19866083562374115\n",
      "epoch: 14 step: 877, loss is 0.30927467346191406\n",
      "epoch: 14 step: 878, loss is 0.13778337836265564\n",
      "epoch: 14 step: 879, loss is 0.12424314767122269\n",
      "epoch: 14 step: 880, loss is 0.2217615842819214\n",
      "epoch: 14 step: 881, loss is 0.16910500824451447\n",
      "epoch: 14 step: 882, loss is 0.10210118442773819\n",
      "epoch: 14 step: 883, loss is 0.24732176959514618\n",
      "epoch: 14 step: 884, loss is 0.34149646759033203\n",
      "epoch: 14 step: 885, loss is 0.1661759614944458\n",
      "epoch: 14 step: 886, loss is 0.1806352734565735\n",
      "epoch: 14 step: 887, loss is 0.1959315687417984\n",
      "epoch: 14 step: 888, loss is 0.20835062861442566\n",
      "epoch: 14 step: 889, loss is 0.15602126717567444\n",
      "epoch: 14 step: 890, loss is 0.3378428518772125\n",
      "epoch: 14 step: 891, loss is 0.17787283658981323\n",
      "epoch: 14 step: 892, loss is 0.24519489705562592\n",
      "epoch: 14 step: 893, loss is 0.17496541142463684\n",
      "epoch: 14 step: 894, loss is 0.2517435848712921\n",
      "epoch: 14 step: 895, loss is 0.11981482058763504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 896, loss is 0.14802737534046173\n",
      "epoch: 14 step: 897, loss is 0.2826751172542572\n",
      "epoch: 14 step: 898, loss is 0.09339600056409836\n",
      "epoch: 14 step: 899, loss is 0.20711952447891235\n",
      "epoch: 14 step: 900, loss is 0.17419318854808807\n",
      "epoch: 14 step: 901, loss is 0.2586999833583832\n",
      "epoch: 14 step: 902, loss is 0.12758904695510864\n",
      "epoch: 14 step: 903, loss is 0.3663446009159088\n",
      "epoch: 14 step: 904, loss is 0.26952654123306274\n",
      "epoch: 14 step: 905, loss is 0.2270115315914154\n",
      "epoch: 14 step: 906, loss is 0.2822210490703583\n",
      "epoch: 14 step: 907, loss is 0.3202124536037445\n",
      "epoch: 14 step: 908, loss is 0.22938120365142822\n",
      "epoch: 14 step: 909, loss is 0.2609001100063324\n",
      "epoch: 14 step: 910, loss is 0.15242034196853638\n",
      "epoch: 14 step: 911, loss is 0.2974207103252411\n",
      "epoch: 14 step: 912, loss is 0.1547081172466278\n",
      "epoch: 14 step: 913, loss is 0.27119582891464233\n",
      "epoch: 14 step: 914, loss is 0.14978179335594177\n",
      "epoch: 14 step: 915, loss is 0.29114291071891785\n",
      "epoch: 14 step: 916, loss is 0.20922505855560303\n",
      "epoch: 14 step: 917, loss is 0.252271443605423\n",
      "epoch: 14 step: 918, loss is 0.23163947463035583\n",
      "epoch: 14 step: 919, loss is 0.2757594883441925\n",
      "epoch: 14 step: 920, loss is 0.15326057374477386\n",
      "epoch: 14 step: 921, loss is 0.17477010190486908\n",
      "epoch: 14 step: 922, loss is 0.25306805968284607\n",
      "epoch: 14 step: 923, loss is 0.21888810396194458\n",
      "epoch: 14 step: 924, loss is 0.24335627257823944\n",
      "epoch: 14 step: 925, loss is 0.23289981484413147\n",
      "epoch: 14 step: 926, loss is 0.3996421992778778\n",
      "epoch: 14 step: 927, loss is 0.21538397669792175\n",
      "epoch: 14 step: 928, loss is 0.1679123044013977\n",
      "epoch: 14 step: 929, loss is 0.17297948896884918\n",
      "epoch: 14 step: 930, loss is 0.23257683217525482\n",
      "epoch: 14 step: 931, loss is 0.4047384262084961\n",
      "epoch: 14 step: 932, loss is 0.18498966097831726\n",
      "epoch: 14 step: 933, loss is 0.17602317035198212\n",
      "epoch: 14 step: 934, loss is 0.17298144102096558\n",
      "epoch: 14 step: 935, loss is 0.2268810272216797\n",
      "epoch: 14 step: 936, loss is 0.19990026950836182\n",
      "epoch: 14 step: 937, loss is 0.2724493145942688\n",
      "epoch: 15 step: 1, loss is 0.32527482509613037\n",
      "epoch: 15 step: 2, loss is 0.14609657227993011\n",
      "epoch: 15 step: 3, loss is 0.1810346096754074\n",
      "epoch: 15 step: 4, loss is 0.16416007280349731\n",
      "epoch: 15 step: 5, loss is 0.32475796341896057\n",
      "epoch: 15 step: 6, loss is 0.14384780824184418\n",
      "epoch: 15 step: 7, loss is 0.1790703386068344\n",
      "epoch: 15 step: 8, loss is 0.19630923867225647\n",
      "epoch: 15 step: 9, loss is 0.1701706051826477\n",
      "epoch: 15 step: 10, loss is 0.1494147628545761\n",
      "epoch: 15 step: 11, loss is 0.17916972935199738\n",
      "epoch: 15 step: 12, loss is 0.1187572255730629\n",
      "epoch: 15 step: 13, loss is 0.31405580043792725\n",
      "epoch: 15 step: 14, loss is 0.2282833307981491\n",
      "epoch: 15 step: 15, loss is 0.2211633324623108\n",
      "epoch: 15 step: 16, loss is 0.25852417945861816\n",
      "epoch: 15 step: 17, loss is 0.2470623403787613\n",
      "epoch: 15 step: 18, loss is 0.1534428745508194\n",
      "epoch: 15 step: 19, loss is 0.25299766659736633\n",
      "epoch: 15 step: 20, loss is 0.12786568701267242\n",
      "epoch: 15 step: 21, loss is 0.2004832625389099\n",
      "epoch: 15 step: 22, loss is 0.17806361615657806\n",
      "epoch: 15 step: 23, loss is 0.1973998099565506\n",
      "epoch: 15 step: 24, loss is 0.21009813249111176\n",
      "epoch: 15 step: 25, loss is 0.25313320755958557\n",
      "epoch: 15 step: 26, loss is 0.17486245930194855\n",
      "epoch: 15 step: 27, loss is 0.1333140730857849\n",
      "epoch: 15 step: 28, loss is 0.2981957495212555\n",
      "epoch: 15 step: 29, loss is 0.1922747939825058\n",
      "epoch: 15 step: 30, loss is 0.1555185168981552\n",
      "epoch: 15 step: 31, loss is 0.2876933813095093\n",
      "epoch: 15 step: 32, loss is 0.27182435989379883\n",
      "epoch: 15 step: 33, loss is 0.2976077198982239\n",
      "epoch: 15 step: 34, loss is 0.12935775518417358\n",
      "epoch: 15 step: 35, loss is 0.1091579720377922\n",
      "epoch: 15 step: 36, loss is 0.21576286852359772\n",
      "epoch: 15 step: 37, loss is 0.15328380465507507\n",
      "epoch: 15 step: 38, loss is 0.16160701215267181\n",
      "epoch: 15 step: 39, loss is 0.17864379286766052\n",
      "epoch: 15 step: 40, loss is 0.20827189087867737\n",
      "epoch: 15 step: 41, loss is 0.18088780343532562\n",
      "epoch: 15 step: 42, loss is 0.2054421752691269\n",
      "epoch: 15 step: 43, loss is 0.0783313512802124\n",
      "epoch: 15 step: 44, loss is 0.14748404920101166\n",
      "epoch: 15 step: 45, loss is 0.14603927731513977\n",
      "epoch: 15 step: 46, loss is 0.34079989790916443\n",
      "epoch: 15 step: 47, loss is 0.3128851354122162\n",
      "epoch: 15 step: 48, loss is 0.3657630383968353\n",
      "epoch: 15 step: 49, loss is 0.29697901010513306\n",
      "epoch: 15 step: 50, loss is 0.24171318113803864\n",
      "epoch: 15 step: 51, loss is 0.2563489079475403\n",
      "epoch: 15 step: 52, loss is 0.2556029260158539\n",
      "epoch: 15 step: 53, loss is 0.22455182671546936\n",
      "epoch: 15 step: 54, loss is 0.2630457282066345\n",
      "epoch: 15 step: 55, loss is 0.1564117670059204\n",
      "epoch: 15 step: 56, loss is 0.1522798389196396\n",
      "epoch: 15 step: 57, loss is 0.24062861502170563\n",
      "epoch: 15 step: 58, loss is 0.22295288741588593\n",
      "epoch: 15 step: 59, loss is 0.3252715766429901\n",
      "epoch: 15 step: 60, loss is 0.21923618018627167\n",
      "epoch: 15 step: 61, loss is 0.1465272307395935\n",
      "epoch: 15 step: 62, loss is 0.19124838709831238\n",
      "epoch: 15 step: 63, loss is 0.1869116723537445\n",
      "epoch: 15 step: 64, loss is 0.11688774824142456\n",
      "epoch: 15 step: 65, loss is 0.40205061435699463\n",
      "epoch: 15 step: 66, loss is 0.21181397140026093\n",
      "epoch: 15 step: 67, loss is 0.15591579675674438\n",
      "epoch: 15 step: 68, loss is 0.12053830176591873\n",
      "epoch: 15 step: 69, loss is 0.2129030078649521\n",
      "epoch: 15 step: 70, loss is 0.22211681306362152\n",
      "epoch: 15 step: 71, loss is 0.15756988525390625\n",
      "epoch: 15 step: 72, loss is 0.5103054642677307\n",
      "epoch: 15 step: 73, loss is 0.21804280579090118\n",
      "epoch: 15 step: 74, loss is 0.1728648841381073\n",
      "epoch: 15 step: 75, loss is 0.34896305203437805\n",
      "epoch: 15 step: 76, loss is 0.21372133493423462\n",
      "epoch: 15 step: 77, loss is 0.3190092146396637\n",
      "epoch: 15 step: 78, loss is 0.37530866265296936\n",
      "epoch: 15 step: 79, loss is 0.3064987063407898\n",
      "epoch: 15 step: 80, loss is 0.17766164243221283\n",
      "epoch: 15 step: 81, loss is 0.21026748418807983\n",
      "epoch: 15 step: 82, loss is 0.2776045799255371\n",
      "epoch: 15 step: 83, loss is 0.2568507194519043\n",
      "epoch: 15 step: 84, loss is 0.10085898637771606\n",
      "epoch: 15 step: 85, loss is 0.179618701338768\n",
      "epoch: 15 step: 86, loss is 0.32971471548080444\n",
      "epoch: 15 step: 87, loss is 0.12760312855243683\n",
      "epoch: 15 step: 88, loss is 0.11520986258983612\n",
      "epoch: 15 step: 89, loss is 0.3060520589351654\n",
      "epoch: 15 step: 90, loss is 0.246086984872818\n",
      "epoch: 15 step: 91, loss is 0.19054007530212402\n",
      "epoch: 15 step: 92, loss is 0.07488217949867249\n",
      "epoch: 15 step: 93, loss is 0.2786279320716858\n",
      "epoch: 15 step: 94, loss is 0.3375564515590668\n",
      "epoch: 15 step: 95, loss is 0.23350916802883148\n",
      "epoch: 15 step: 96, loss is 0.16677245497703552\n",
      "epoch: 15 step: 97, loss is 0.19223831593990326\n",
      "epoch: 15 step: 98, loss is 0.1022518053650856\n",
      "epoch: 15 step: 99, loss is 0.2357977330684662\n",
      "epoch: 15 step: 100, loss is 0.19328676164150238\n",
      "epoch: 15 step: 101, loss is 0.32929402589797974\n",
      "epoch: 15 step: 102, loss is 0.17550383508205414\n",
      "epoch: 15 step: 103, loss is 0.1869545429944992\n",
      "epoch: 15 step: 104, loss is 0.2197740375995636\n",
      "epoch: 15 step: 105, loss is 0.17666876316070557\n",
      "epoch: 15 step: 106, loss is 0.13336534798145294\n",
      "epoch: 15 step: 107, loss is 0.20801566541194916\n",
      "epoch: 15 step: 108, loss is 0.1924021989107132\n",
      "epoch: 15 step: 109, loss is 0.14198638498783112\n",
      "epoch: 15 step: 110, loss is 0.3049730360507965\n",
      "epoch: 15 step: 111, loss is 0.3170381188392639\n",
      "epoch: 15 step: 112, loss is 0.22433587908744812\n",
      "epoch: 15 step: 113, loss is 0.19913892447948456\n",
      "epoch: 15 step: 114, loss is 0.19296225905418396\n",
      "epoch: 15 step: 115, loss is 0.18400034308433533\n",
      "epoch: 15 step: 116, loss is 0.12374088168144226\n",
      "epoch: 15 step: 117, loss is 0.21278756856918335\n",
      "epoch: 15 step: 118, loss is 0.15901558101177216\n",
      "epoch: 15 step: 119, loss is 0.17876163125038147\n",
      "epoch: 15 step: 120, loss is 0.18023400008678436\n",
      "epoch: 15 step: 121, loss is 0.1621389389038086\n",
      "epoch: 15 step: 122, loss is 0.3474876284599304\n",
      "epoch: 15 step: 123, loss is 0.14093098044395447\n",
      "epoch: 15 step: 124, loss is 0.22309964895248413\n",
      "epoch: 15 step: 125, loss is 0.2640513777732849\n",
      "epoch: 15 step: 126, loss is 0.1847347915172577\n",
      "epoch: 15 step: 127, loss is 0.12643566727638245\n",
      "epoch: 15 step: 128, loss is 0.09123439341783524\n",
      "epoch: 15 step: 129, loss is 0.22641275823116302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 130, loss is 0.14167389273643494\n",
      "epoch: 15 step: 131, loss is 0.2609287202358246\n",
      "epoch: 15 step: 132, loss is 0.1013503447175026\n",
      "epoch: 15 step: 133, loss is 0.1803518384695053\n",
      "epoch: 15 step: 134, loss is 0.13675746321678162\n",
      "epoch: 15 step: 135, loss is 0.21718406677246094\n",
      "epoch: 15 step: 136, loss is 0.18920926749706268\n",
      "epoch: 15 step: 137, loss is 0.20821912586688995\n",
      "epoch: 15 step: 138, loss is 0.23180551826953888\n",
      "epoch: 15 step: 139, loss is 0.12690280377864838\n",
      "epoch: 15 step: 140, loss is 0.25395336747169495\n",
      "epoch: 15 step: 141, loss is 0.09941399842500687\n",
      "epoch: 15 step: 142, loss is 0.18614459037780762\n",
      "epoch: 15 step: 143, loss is 0.21978874504566193\n",
      "epoch: 15 step: 144, loss is 0.23965641856193542\n",
      "epoch: 15 step: 145, loss is 0.14137420058250427\n",
      "epoch: 15 step: 146, loss is 0.24393221735954285\n",
      "epoch: 15 step: 147, loss is 0.264970064163208\n",
      "epoch: 15 step: 148, loss is 0.24502646923065186\n",
      "epoch: 15 step: 149, loss is 0.12589730322360992\n",
      "epoch: 15 step: 150, loss is 0.19640015065670013\n",
      "epoch: 15 step: 151, loss is 0.18835483491420746\n",
      "epoch: 15 step: 152, loss is 0.30079150199890137\n",
      "epoch: 15 step: 153, loss is 0.3224461078643799\n",
      "epoch: 15 step: 154, loss is 0.2797051966190338\n",
      "epoch: 15 step: 155, loss is 0.1919875293970108\n",
      "epoch: 15 step: 156, loss is 0.25022345781326294\n",
      "epoch: 15 step: 157, loss is 0.1577625572681427\n",
      "epoch: 15 step: 158, loss is 0.3403690755367279\n",
      "epoch: 15 step: 159, loss is 0.2751387059688568\n",
      "epoch: 15 step: 160, loss is 0.22508840262889862\n",
      "epoch: 15 step: 161, loss is 0.1756325364112854\n",
      "epoch: 15 step: 162, loss is 0.176533505320549\n",
      "epoch: 15 step: 163, loss is 0.13450515270233154\n",
      "epoch: 15 step: 164, loss is 0.19744211435317993\n",
      "epoch: 15 step: 165, loss is 0.24805182218551636\n",
      "epoch: 15 step: 166, loss is 0.12850967049598694\n",
      "epoch: 15 step: 167, loss is 0.1194256842136383\n",
      "epoch: 15 step: 168, loss is 0.1429145485162735\n",
      "epoch: 15 step: 169, loss is 0.310145765542984\n",
      "epoch: 15 step: 170, loss is 0.1305670291185379\n",
      "epoch: 15 step: 171, loss is 0.21908918023109436\n",
      "epoch: 15 step: 172, loss is 0.16049465537071228\n",
      "epoch: 15 step: 173, loss is 0.2899085581302643\n",
      "epoch: 15 step: 174, loss is 0.3194614350795746\n",
      "epoch: 15 step: 175, loss is 0.07231516391038895\n",
      "epoch: 15 step: 176, loss is 0.3257889747619629\n",
      "epoch: 15 step: 177, loss is 0.16468629240989685\n",
      "epoch: 15 step: 178, loss is 0.19837947189807892\n",
      "epoch: 15 step: 179, loss is 0.23598416149616241\n",
      "epoch: 15 step: 180, loss is 0.13887235522270203\n",
      "epoch: 15 step: 181, loss is 0.20474068820476532\n",
      "epoch: 15 step: 182, loss is 0.25155705213546753\n",
      "epoch: 15 step: 183, loss is 0.2787945866584778\n",
      "epoch: 15 step: 184, loss is 0.14510735869407654\n",
      "epoch: 15 step: 185, loss is 0.12871992588043213\n",
      "epoch: 15 step: 186, loss is 0.18078523874282837\n",
      "epoch: 15 step: 187, loss is 0.31042876839637756\n",
      "epoch: 15 step: 188, loss is 0.15096603333950043\n",
      "epoch: 15 step: 189, loss is 0.1285693496465683\n",
      "epoch: 15 step: 190, loss is 0.13610036671161652\n",
      "epoch: 15 step: 191, loss is 0.19502486288547516\n",
      "epoch: 15 step: 192, loss is 0.2741946876049042\n",
      "epoch: 15 step: 193, loss is 0.21154215931892395\n",
      "epoch: 15 step: 194, loss is 0.25789982080459595\n",
      "epoch: 15 step: 195, loss is 0.1567317247390747\n",
      "epoch: 15 step: 196, loss is 0.1790793240070343\n",
      "epoch: 15 step: 197, loss is 0.25184908509254456\n",
      "epoch: 15 step: 198, loss is 0.19621863961219788\n",
      "epoch: 15 step: 199, loss is 0.22254614531993866\n",
      "epoch: 15 step: 200, loss is 0.3528851270675659\n",
      "epoch: 15 step: 201, loss is 0.2377706915140152\n",
      "epoch: 15 step: 202, loss is 0.25802507996559143\n",
      "epoch: 15 step: 203, loss is 0.08797085285186768\n",
      "epoch: 15 step: 204, loss is 0.20126177370548248\n",
      "epoch: 15 step: 205, loss is 0.18009577691555023\n",
      "epoch: 15 step: 206, loss is 0.18557868897914886\n",
      "epoch: 15 step: 207, loss is 0.3066132068634033\n",
      "epoch: 15 step: 208, loss is 0.151860311627388\n",
      "epoch: 15 step: 209, loss is 0.324891060590744\n",
      "epoch: 15 step: 210, loss is 0.28270336985588074\n",
      "epoch: 15 step: 211, loss is 0.2315799593925476\n",
      "epoch: 15 step: 212, loss is 0.29760852456092834\n",
      "epoch: 15 step: 213, loss is 0.2695677578449249\n",
      "epoch: 15 step: 214, loss is 0.2883678674697876\n",
      "epoch: 15 step: 215, loss is 0.13520532846450806\n",
      "epoch: 15 step: 216, loss is 0.20776930451393127\n",
      "epoch: 15 step: 217, loss is 0.4414392411708832\n",
      "epoch: 15 step: 218, loss is 0.20667892694473267\n",
      "epoch: 15 step: 219, loss is 0.1508755385875702\n",
      "epoch: 15 step: 220, loss is 0.3034413754940033\n",
      "epoch: 15 step: 221, loss is 0.19360604882240295\n",
      "epoch: 15 step: 222, loss is 0.1996670365333557\n",
      "epoch: 15 step: 223, loss is 0.28413647413253784\n",
      "epoch: 15 step: 224, loss is 0.21520546078681946\n",
      "epoch: 15 step: 225, loss is 0.03404939919710159\n",
      "epoch: 15 step: 226, loss is 0.09919927269220352\n",
      "epoch: 15 step: 227, loss is 0.18523456156253815\n",
      "epoch: 15 step: 228, loss is 0.2651787996292114\n",
      "epoch: 15 step: 229, loss is 0.19987726211547852\n",
      "epoch: 15 step: 230, loss is 0.3928656280040741\n",
      "epoch: 15 step: 231, loss is 0.15551869571208954\n",
      "epoch: 15 step: 232, loss is 0.12238450348377228\n",
      "epoch: 15 step: 233, loss is 0.21162648499011993\n",
      "epoch: 15 step: 234, loss is 0.16653567552566528\n",
      "epoch: 15 step: 235, loss is 0.2765725553035736\n",
      "epoch: 15 step: 236, loss is 0.3583641052246094\n",
      "epoch: 15 step: 237, loss is 0.2331089824438095\n",
      "epoch: 15 step: 238, loss is 0.16077017784118652\n",
      "epoch: 15 step: 239, loss is 0.10939715802669525\n",
      "epoch: 15 step: 240, loss is 0.25009381771087646\n",
      "epoch: 15 step: 241, loss is 0.24633356928825378\n",
      "epoch: 15 step: 242, loss is 0.14315257966518402\n",
      "epoch: 15 step: 243, loss is 0.1624155342578888\n",
      "epoch: 15 step: 244, loss is 0.2151944786310196\n",
      "epoch: 15 step: 245, loss is 0.29350438714027405\n",
      "epoch: 15 step: 246, loss is 0.11915507167577744\n",
      "epoch: 15 step: 247, loss is 0.13638457655906677\n",
      "epoch: 15 step: 248, loss is 0.23004795610904694\n",
      "epoch: 15 step: 249, loss is 0.22773687541484833\n",
      "epoch: 15 step: 250, loss is 0.1879398226737976\n",
      "epoch: 15 step: 251, loss is 0.10275967419147491\n",
      "epoch: 15 step: 252, loss is 0.11265093088150024\n",
      "epoch: 15 step: 253, loss is 0.2382632941007614\n",
      "epoch: 15 step: 254, loss is 0.1905928999185562\n",
      "epoch: 15 step: 255, loss is 0.1634959876537323\n",
      "epoch: 15 step: 256, loss is 0.3119180500507355\n",
      "epoch: 15 step: 257, loss is 0.18152931332588196\n",
      "epoch: 15 step: 258, loss is 0.2102852314710617\n",
      "epoch: 15 step: 259, loss is 0.12473299354314804\n",
      "epoch: 15 step: 260, loss is 0.12640441954135895\n",
      "epoch: 15 step: 261, loss is 0.24276401102542877\n",
      "epoch: 15 step: 262, loss is 0.3233552575111389\n",
      "epoch: 15 step: 263, loss is 0.41775408387184143\n",
      "epoch: 15 step: 264, loss is 0.19764694571495056\n",
      "epoch: 15 step: 265, loss is 0.22588779032230377\n",
      "epoch: 15 step: 266, loss is 0.11350326985120773\n",
      "epoch: 15 step: 267, loss is 0.24905699491500854\n",
      "epoch: 15 step: 268, loss is 0.129295215010643\n",
      "epoch: 15 step: 269, loss is 0.3291172981262207\n",
      "epoch: 15 step: 270, loss is 0.1638995260000229\n",
      "epoch: 15 step: 271, loss is 0.36722859740257263\n",
      "epoch: 15 step: 272, loss is 0.1308297961950302\n",
      "epoch: 15 step: 273, loss is 0.046908263117074966\n",
      "epoch: 15 step: 274, loss is 0.1935977339744568\n",
      "epoch: 15 step: 275, loss is 0.20232340693473816\n",
      "epoch: 15 step: 276, loss is 0.3001553416252136\n",
      "epoch: 15 step: 277, loss is 0.4117513597011566\n",
      "epoch: 15 step: 278, loss is 0.17999267578125\n",
      "epoch: 15 step: 279, loss is 0.28646335005760193\n",
      "epoch: 15 step: 280, loss is 0.23377732932567596\n",
      "epoch: 15 step: 281, loss is 0.17373310029506683\n",
      "epoch: 15 step: 282, loss is 0.2783515751361847\n",
      "epoch: 15 step: 283, loss is 0.23100192844867706\n",
      "epoch: 15 step: 284, loss is 0.1510138213634491\n",
      "epoch: 15 step: 285, loss is 0.20415270328521729\n",
      "epoch: 15 step: 286, loss is 0.06155075505375862\n",
      "epoch: 15 step: 287, loss is 0.2445041835308075\n",
      "epoch: 15 step: 288, loss is 0.30827397108078003\n",
      "epoch: 15 step: 289, loss is 0.2870965003967285\n",
      "epoch: 15 step: 290, loss is 0.23344305157661438\n",
      "epoch: 15 step: 291, loss is 0.3013131022453308\n",
      "epoch: 15 step: 292, loss is 0.18024586141109467\n",
      "epoch: 15 step: 293, loss is 0.40168917179107666\n",
      "epoch: 15 step: 294, loss is 0.24335552752017975\n",
      "epoch: 15 step: 295, loss is 0.11636604368686676\n",
      "epoch: 15 step: 296, loss is 0.11744778603315353\n",
      "epoch: 15 step: 297, loss is 0.2227638065814972\n",
      "epoch: 15 step: 298, loss is 0.2960303723812103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 299, loss is 0.2113656997680664\n",
      "epoch: 15 step: 300, loss is 0.24668091535568237\n",
      "epoch: 15 step: 301, loss is 0.31473276019096375\n",
      "epoch: 15 step: 302, loss is 0.24604877829551697\n",
      "epoch: 15 step: 303, loss is 0.33151963353157043\n",
      "epoch: 15 step: 304, loss is 0.30777066946029663\n",
      "epoch: 15 step: 305, loss is 0.30953261256217957\n",
      "epoch: 15 step: 306, loss is 0.2438775897026062\n",
      "epoch: 15 step: 307, loss is 0.13794244825839996\n",
      "epoch: 15 step: 308, loss is 0.3424838185310364\n",
      "epoch: 15 step: 309, loss is 0.25651195645332336\n",
      "epoch: 15 step: 310, loss is 0.11479609459638596\n",
      "epoch: 15 step: 311, loss is 0.14628295600414276\n",
      "epoch: 15 step: 312, loss is 0.25626516342163086\n",
      "epoch: 15 step: 313, loss is 0.44483649730682373\n",
      "epoch: 15 step: 314, loss is 0.16894792020320892\n",
      "epoch: 15 step: 315, loss is 0.1498190462589264\n",
      "epoch: 15 step: 316, loss is 0.3543480634689331\n",
      "epoch: 15 step: 317, loss is 0.12023791670799255\n",
      "epoch: 15 step: 318, loss is 0.18896764516830444\n",
      "epoch: 15 step: 319, loss is 0.1843772530555725\n",
      "epoch: 15 step: 320, loss is 0.2176697701215744\n",
      "epoch: 15 step: 321, loss is 0.3976823091506958\n",
      "epoch: 15 step: 322, loss is 0.17969807982444763\n",
      "epoch: 15 step: 323, loss is 0.11206518858671188\n",
      "epoch: 15 step: 324, loss is 0.2317032516002655\n",
      "epoch: 15 step: 325, loss is 0.24913068115711212\n",
      "epoch: 15 step: 326, loss is 0.17802350223064423\n",
      "epoch: 15 step: 327, loss is 0.14794892072677612\n",
      "epoch: 15 step: 328, loss is 0.2725251615047455\n",
      "epoch: 15 step: 329, loss is 0.10526759922504425\n",
      "epoch: 15 step: 330, loss is 0.2506001591682434\n",
      "epoch: 15 step: 331, loss is 0.21858203411102295\n",
      "epoch: 15 step: 332, loss is 0.2666527032852173\n",
      "epoch: 15 step: 333, loss is 0.3133071959018707\n",
      "epoch: 15 step: 334, loss is 0.1353578120470047\n",
      "epoch: 15 step: 335, loss is 0.2593775689601898\n",
      "epoch: 15 step: 336, loss is 0.1724328100681305\n",
      "epoch: 15 step: 337, loss is 0.2549198567867279\n",
      "epoch: 15 step: 338, loss is 0.09907209128141403\n",
      "epoch: 15 step: 339, loss is 0.0682358518242836\n",
      "epoch: 15 step: 340, loss is 0.29590994119644165\n",
      "epoch: 15 step: 341, loss is 0.1722632199525833\n",
      "epoch: 15 step: 342, loss is 0.20168393850326538\n",
      "epoch: 15 step: 343, loss is 0.2654659152030945\n",
      "epoch: 15 step: 344, loss is 0.18005132675170898\n",
      "epoch: 15 step: 345, loss is 0.2263505905866623\n",
      "epoch: 15 step: 346, loss is 0.1248273178935051\n",
      "epoch: 15 step: 347, loss is 0.11496081948280334\n",
      "epoch: 15 step: 348, loss is 0.17965446412563324\n",
      "epoch: 15 step: 349, loss is 0.3402685821056366\n",
      "epoch: 15 step: 350, loss is 0.22438973188400269\n",
      "epoch: 15 step: 351, loss is 0.1273524910211563\n",
      "epoch: 15 step: 352, loss is 0.09734740853309631\n",
      "epoch: 15 step: 353, loss is 0.14282186329364777\n",
      "epoch: 15 step: 354, loss is 0.18068861961364746\n",
      "epoch: 15 step: 355, loss is 0.2419542521238327\n",
      "epoch: 15 step: 356, loss is 0.229398712515831\n",
      "epoch: 15 step: 357, loss is 0.30720287561416626\n",
      "epoch: 15 step: 358, loss is 0.14542615413665771\n",
      "epoch: 15 step: 359, loss is 0.1309339851140976\n",
      "epoch: 15 step: 360, loss is 0.15901227295398712\n",
      "epoch: 15 step: 361, loss is 0.19627338647842407\n",
      "epoch: 15 step: 362, loss is 0.2163436859846115\n",
      "epoch: 15 step: 363, loss is 0.1569405198097229\n",
      "epoch: 15 step: 364, loss is 0.21044974029064178\n",
      "epoch: 15 step: 365, loss is 0.12665635347366333\n",
      "epoch: 15 step: 366, loss is 0.06992032378911972\n",
      "epoch: 15 step: 367, loss is 0.05370244383811951\n",
      "epoch: 15 step: 368, loss is 0.38018548488616943\n",
      "epoch: 15 step: 369, loss is 0.24927861988544464\n",
      "epoch: 15 step: 370, loss is 0.1955365687608719\n",
      "epoch: 15 step: 371, loss is 0.15279273688793182\n",
      "epoch: 15 step: 372, loss is 0.39324378967285156\n",
      "epoch: 15 step: 373, loss is 0.23131833970546722\n",
      "epoch: 15 step: 374, loss is 0.3971393406391144\n",
      "epoch: 15 step: 375, loss is 0.2707900106906891\n",
      "epoch: 15 step: 376, loss is 0.27058547735214233\n",
      "epoch: 15 step: 377, loss is 0.3815372586250305\n",
      "epoch: 15 step: 378, loss is 0.25154581665992737\n",
      "epoch: 15 step: 379, loss is 0.14121177792549133\n",
      "epoch: 15 step: 380, loss is 0.19829128682613373\n",
      "epoch: 15 step: 381, loss is 0.2751980423927307\n",
      "epoch: 15 step: 382, loss is 0.352561891078949\n",
      "epoch: 15 step: 383, loss is 0.1956489235162735\n",
      "epoch: 15 step: 384, loss is 0.27031728625297546\n",
      "epoch: 15 step: 385, loss is 0.22908686101436615\n",
      "epoch: 15 step: 386, loss is 0.2853772044181824\n",
      "epoch: 15 step: 387, loss is 0.14289891719818115\n",
      "epoch: 15 step: 388, loss is 0.20626714825630188\n",
      "epoch: 15 step: 389, loss is 0.1450498402118683\n",
      "epoch: 15 step: 390, loss is 0.36525511741638184\n",
      "epoch: 15 step: 391, loss is 0.17662413418293\n",
      "epoch: 15 step: 392, loss is 0.2709737718105316\n",
      "epoch: 15 step: 393, loss is 0.19304259121418\n",
      "epoch: 15 step: 394, loss is 0.25746414065361023\n",
      "epoch: 15 step: 395, loss is 0.30244410037994385\n",
      "epoch: 15 step: 396, loss is 0.1632259339094162\n",
      "epoch: 15 step: 397, loss is 0.210767462849617\n",
      "epoch: 15 step: 398, loss is 0.2774478793144226\n",
      "epoch: 15 step: 399, loss is 0.269972562789917\n",
      "epoch: 15 step: 400, loss is 0.20365339517593384\n",
      "epoch: 15 step: 401, loss is 0.1578206568956375\n",
      "epoch: 15 step: 402, loss is 0.17082159221172333\n",
      "epoch: 15 step: 403, loss is 0.30759570002555847\n",
      "epoch: 15 step: 404, loss is 0.2940162718296051\n",
      "epoch: 15 step: 405, loss is 0.19004863500595093\n",
      "epoch: 15 step: 406, loss is 0.3408863842487335\n",
      "epoch: 15 step: 407, loss is 0.4275256097316742\n",
      "epoch: 15 step: 408, loss is 0.14179959893226624\n",
      "epoch: 15 step: 409, loss is 0.3034781515598297\n",
      "epoch: 15 step: 410, loss is 0.15226194262504578\n",
      "epoch: 15 step: 411, loss is 0.18362171947956085\n",
      "epoch: 15 step: 412, loss is 0.24495576322078705\n",
      "epoch: 15 step: 413, loss is 0.2900345027446747\n",
      "epoch: 15 step: 414, loss is 0.2960456609725952\n",
      "epoch: 15 step: 415, loss is 0.29519593715667725\n",
      "epoch: 15 step: 416, loss is 0.5400662422180176\n",
      "epoch: 15 step: 417, loss is 0.1670040488243103\n",
      "epoch: 15 step: 418, loss is 0.24794289469718933\n",
      "epoch: 15 step: 419, loss is 0.16128908097743988\n",
      "epoch: 15 step: 420, loss is 0.26350605487823486\n",
      "epoch: 15 step: 421, loss is 0.1442440152168274\n",
      "epoch: 15 step: 422, loss is 0.2821853458881378\n",
      "epoch: 15 step: 423, loss is 0.2364141196012497\n",
      "epoch: 15 step: 424, loss is 0.25007182359695435\n",
      "epoch: 15 step: 425, loss is 0.2649189531803131\n",
      "epoch: 15 step: 426, loss is 0.2207220196723938\n",
      "epoch: 15 step: 427, loss is 0.21953822672367096\n",
      "epoch: 15 step: 428, loss is 0.18042373657226562\n",
      "epoch: 15 step: 429, loss is 0.2179354429244995\n",
      "epoch: 15 step: 430, loss is 0.20002888143062592\n",
      "epoch: 15 step: 431, loss is 0.23361122608184814\n",
      "epoch: 15 step: 432, loss is 0.2028704434633255\n",
      "epoch: 15 step: 433, loss is 0.2775513827800751\n",
      "epoch: 15 step: 434, loss is 0.12140220403671265\n",
      "epoch: 15 step: 435, loss is 0.31465044617652893\n",
      "epoch: 15 step: 436, loss is 0.25005510449409485\n",
      "epoch: 15 step: 437, loss is 0.1408078968524933\n",
      "epoch: 15 step: 438, loss is 0.19745071232318878\n",
      "epoch: 15 step: 439, loss is 0.23717276751995087\n",
      "epoch: 15 step: 440, loss is 0.19254305958747864\n",
      "epoch: 15 step: 441, loss is 0.14994901418685913\n",
      "epoch: 15 step: 442, loss is 0.18622395396232605\n",
      "epoch: 15 step: 443, loss is 0.2610694169998169\n",
      "epoch: 15 step: 444, loss is 0.2513744831085205\n",
      "epoch: 15 step: 445, loss is 0.1782355159521103\n",
      "epoch: 15 step: 446, loss is 0.13744476437568665\n",
      "epoch: 15 step: 447, loss is 0.10002483427524567\n",
      "epoch: 15 step: 448, loss is 0.3304094672203064\n",
      "epoch: 15 step: 449, loss is 0.2071692794561386\n",
      "epoch: 15 step: 450, loss is 0.2933950126171112\n",
      "epoch: 15 step: 451, loss is 0.20447272062301636\n",
      "epoch: 15 step: 452, loss is 0.1795165240764618\n",
      "epoch: 15 step: 453, loss is 0.1449730396270752\n",
      "epoch: 15 step: 454, loss is 0.3034859001636505\n",
      "epoch: 15 step: 455, loss is 0.16875119507312775\n",
      "epoch: 15 step: 456, loss is 0.1362561583518982\n",
      "epoch: 15 step: 457, loss is 0.2509338855743408\n",
      "epoch: 15 step: 458, loss is 0.3429688811302185\n",
      "epoch: 15 step: 459, loss is 0.2649610936641693\n",
      "epoch: 15 step: 460, loss is 0.3975313901901245\n",
      "epoch: 15 step: 461, loss is 0.1889212727546692\n",
      "epoch: 15 step: 462, loss is 0.20988579094409943\n",
      "epoch: 15 step: 463, loss is 0.21740835905075073\n",
      "epoch: 15 step: 464, loss is 0.18705716729164124\n",
      "epoch: 15 step: 465, loss is 0.19115984439849854\n",
      "epoch: 15 step: 466, loss is 0.2801257371902466\n",
      "epoch: 15 step: 467, loss is 0.21603547036647797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 468, loss is 0.1451328992843628\n",
      "epoch: 15 step: 469, loss is 0.31029894948005676\n",
      "epoch: 15 step: 470, loss is 0.24381598830223083\n",
      "epoch: 15 step: 471, loss is 0.16185753047466278\n",
      "epoch: 15 step: 472, loss is 0.17463964223861694\n",
      "epoch: 15 step: 473, loss is 0.1907048225402832\n",
      "epoch: 15 step: 474, loss is 0.09162726253271103\n",
      "epoch: 15 step: 475, loss is 0.30527693033218384\n",
      "epoch: 15 step: 476, loss is 0.3931741714477539\n",
      "epoch: 15 step: 477, loss is 0.2809813320636749\n",
      "epoch: 15 step: 478, loss is 0.2074619084596634\n",
      "epoch: 15 step: 479, loss is 0.10036677122116089\n",
      "epoch: 15 step: 480, loss is 0.28518548607826233\n",
      "epoch: 15 step: 481, loss is 0.21797066926956177\n",
      "epoch: 15 step: 482, loss is 0.1403023898601532\n",
      "epoch: 15 step: 483, loss is 0.15349620580673218\n",
      "epoch: 15 step: 484, loss is 0.25139063596725464\n",
      "epoch: 15 step: 485, loss is 0.17180214822292328\n",
      "epoch: 15 step: 486, loss is 0.28235316276550293\n",
      "epoch: 15 step: 487, loss is 0.1563994288444519\n",
      "epoch: 15 step: 488, loss is 0.19359000027179718\n",
      "epoch: 15 step: 489, loss is 0.23733749985694885\n",
      "epoch: 15 step: 490, loss is 0.20140376687049866\n",
      "epoch: 15 step: 491, loss is 0.17624711990356445\n",
      "epoch: 15 step: 492, loss is 0.27061954140663147\n",
      "epoch: 15 step: 493, loss is 0.16540703177452087\n",
      "epoch: 15 step: 494, loss is 0.23301191627979279\n",
      "epoch: 15 step: 495, loss is 0.2019909769296646\n",
      "epoch: 15 step: 496, loss is 0.17783775925636292\n",
      "epoch: 15 step: 497, loss is 0.3664325475692749\n",
      "epoch: 15 step: 498, loss is 0.13195134699344635\n",
      "epoch: 15 step: 499, loss is 0.3202868402004242\n",
      "epoch: 15 step: 500, loss is 0.28204065561294556\n",
      "epoch: 15 step: 501, loss is 0.2591807544231415\n",
      "epoch: 15 step: 502, loss is 0.09824385493993759\n",
      "epoch: 15 step: 503, loss is 0.258924275636673\n",
      "epoch: 15 step: 504, loss is 0.3060489296913147\n",
      "epoch: 15 step: 505, loss is 0.15594486892223358\n",
      "epoch: 15 step: 506, loss is 0.08744558691978455\n",
      "epoch: 15 step: 507, loss is 0.29200422763824463\n",
      "epoch: 15 step: 508, loss is 0.22918009757995605\n",
      "epoch: 15 step: 509, loss is 0.14838136732578278\n",
      "epoch: 15 step: 510, loss is 0.17885133624076843\n",
      "epoch: 15 step: 511, loss is 0.16580136120319366\n",
      "epoch: 15 step: 512, loss is 0.2677209973335266\n",
      "epoch: 15 step: 513, loss is 0.2784733474254608\n",
      "epoch: 15 step: 514, loss is 0.24271167814731598\n",
      "epoch: 15 step: 515, loss is 0.17842620611190796\n",
      "epoch: 15 step: 516, loss is 0.21167460083961487\n",
      "epoch: 15 step: 517, loss is 0.17272554337978363\n",
      "epoch: 15 step: 518, loss is 0.30887219309806824\n",
      "epoch: 15 step: 519, loss is 0.2341841459274292\n",
      "epoch: 15 step: 520, loss is 0.17353589832782745\n",
      "epoch: 15 step: 521, loss is 0.19294427335262299\n",
      "epoch: 15 step: 522, loss is 0.18019253015518188\n",
      "epoch: 15 step: 523, loss is 0.15788909792900085\n",
      "epoch: 15 step: 524, loss is 0.10448939353227615\n",
      "epoch: 15 step: 525, loss is 0.20177556574344635\n",
      "epoch: 15 step: 526, loss is 0.2470162808895111\n",
      "epoch: 15 step: 527, loss is 0.15471266210079193\n",
      "epoch: 15 step: 528, loss is 0.32266780734062195\n",
      "epoch: 15 step: 529, loss is 0.3910677134990692\n",
      "epoch: 15 step: 530, loss is 0.27631810307502747\n",
      "epoch: 15 step: 531, loss is 0.23812897503376007\n",
      "epoch: 15 step: 532, loss is 0.11996664851903915\n",
      "epoch: 15 step: 533, loss is 0.26145339012145996\n",
      "epoch: 15 step: 534, loss is 0.294864296913147\n",
      "epoch: 15 step: 535, loss is 0.15560711920261383\n",
      "epoch: 15 step: 536, loss is 0.2173410952091217\n",
      "epoch: 15 step: 537, loss is 0.0851043090224266\n",
      "epoch: 15 step: 538, loss is 0.3318977653980255\n",
      "epoch: 15 step: 539, loss is 0.17684803903102875\n",
      "epoch: 15 step: 540, loss is 0.175578311085701\n",
      "epoch: 15 step: 541, loss is 0.08296097069978714\n",
      "epoch: 15 step: 542, loss is 0.09444563090801239\n",
      "epoch: 15 step: 543, loss is 0.2233905792236328\n",
      "epoch: 15 step: 544, loss is 0.3313353359699249\n",
      "epoch: 15 step: 545, loss is 0.16993077099323273\n",
      "epoch: 15 step: 546, loss is 0.25498607754707336\n",
      "epoch: 15 step: 547, loss is 0.10167723894119263\n",
      "epoch: 15 step: 548, loss is 0.14885467290878296\n",
      "epoch: 15 step: 549, loss is 0.2008104920387268\n",
      "epoch: 15 step: 550, loss is 0.14018498361110687\n",
      "epoch: 15 step: 551, loss is 0.29794633388519287\n",
      "epoch: 15 step: 552, loss is 0.17842186987400055\n",
      "epoch: 15 step: 553, loss is 0.2519979476928711\n",
      "epoch: 15 step: 554, loss is 0.12329024821519852\n",
      "epoch: 15 step: 555, loss is 0.19038067758083344\n",
      "epoch: 15 step: 556, loss is 0.32553449273109436\n",
      "epoch: 15 step: 557, loss is 0.20092441141605377\n",
      "epoch: 15 step: 558, loss is 0.13976916670799255\n",
      "epoch: 15 step: 559, loss is 0.21016213297843933\n",
      "epoch: 15 step: 560, loss is 0.17309169471263885\n",
      "epoch: 15 step: 561, loss is 0.12051527947187424\n",
      "epoch: 15 step: 562, loss is 0.1590154767036438\n",
      "epoch: 15 step: 563, loss is 0.23234792053699493\n",
      "epoch: 15 step: 564, loss is 0.24290171265602112\n",
      "epoch: 15 step: 565, loss is 0.3090864419937134\n",
      "epoch: 15 step: 566, loss is 0.15538986027240753\n",
      "epoch: 15 step: 567, loss is 0.27811530232429504\n",
      "epoch: 15 step: 568, loss is 0.1848779320716858\n",
      "epoch: 15 step: 569, loss is 0.20392297208309174\n",
      "epoch: 15 step: 570, loss is 0.2697809338569641\n",
      "epoch: 15 step: 571, loss is 0.21342042088508606\n",
      "epoch: 15 step: 572, loss is 0.28664132952690125\n",
      "epoch: 15 step: 573, loss is 0.2014559507369995\n",
      "epoch: 15 step: 574, loss is 0.25533026456832886\n",
      "epoch: 15 step: 575, loss is 0.2164611965417862\n",
      "epoch: 15 step: 576, loss is 0.30843231081962585\n",
      "epoch: 15 step: 577, loss is 0.2500936985015869\n",
      "epoch: 15 step: 578, loss is 0.21783453226089478\n",
      "epoch: 15 step: 579, loss is 0.2154635339975357\n",
      "epoch: 15 step: 580, loss is 0.22000333666801453\n",
      "epoch: 15 step: 581, loss is 0.12172586470842361\n",
      "epoch: 15 step: 582, loss is 0.23927035927772522\n",
      "epoch: 15 step: 583, loss is 0.17092105746269226\n",
      "epoch: 15 step: 584, loss is 0.1796858012676239\n",
      "epoch: 15 step: 585, loss is 0.380202054977417\n",
      "epoch: 15 step: 586, loss is 0.17097805440425873\n",
      "epoch: 15 step: 587, loss is 0.19962704181671143\n",
      "epoch: 15 step: 588, loss is 0.1279192566871643\n",
      "epoch: 15 step: 589, loss is 0.19274643063545227\n",
      "epoch: 15 step: 590, loss is 0.19968530535697937\n",
      "epoch: 15 step: 591, loss is 0.16770601272583008\n",
      "epoch: 15 step: 592, loss is 0.10253812372684479\n",
      "epoch: 15 step: 593, loss is 0.1914106160402298\n",
      "epoch: 15 step: 594, loss is 0.27105849981307983\n",
      "epoch: 15 step: 595, loss is 0.14323975145816803\n",
      "epoch: 15 step: 596, loss is 0.18135710060596466\n",
      "epoch: 15 step: 597, loss is 0.3161928355693817\n",
      "epoch: 15 step: 598, loss is 0.16659124195575714\n",
      "epoch: 15 step: 599, loss is 0.19092108309268951\n",
      "epoch: 15 step: 600, loss is 0.24593722820281982\n",
      "epoch: 15 step: 601, loss is 0.12334877252578735\n",
      "epoch: 15 step: 602, loss is 0.12066008895635605\n",
      "epoch: 15 step: 603, loss is 0.17134258151054382\n",
      "epoch: 15 step: 604, loss is 0.22338367998600006\n",
      "epoch: 15 step: 605, loss is 0.31699153780937195\n",
      "epoch: 15 step: 606, loss is 0.2310548573732376\n",
      "epoch: 15 step: 607, loss is 0.21077707409858704\n",
      "epoch: 15 step: 608, loss is 0.28004297614097595\n",
      "epoch: 15 step: 609, loss is 0.28971782326698303\n",
      "epoch: 15 step: 610, loss is 0.147593155503273\n",
      "epoch: 15 step: 611, loss is 0.17565257847309113\n",
      "epoch: 15 step: 612, loss is 0.1855984330177307\n",
      "epoch: 15 step: 613, loss is 0.37983494997024536\n",
      "epoch: 15 step: 614, loss is 0.2913711369037628\n",
      "epoch: 15 step: 615, loss is 0.06666701287031174\n",
      "epoch: 15 step: 616, loss is 0.2734855115413666\n",
      "epoch: 15 step: 617, loss is 0.2861670255661011\n",
      "epoch: 15 step: 618, loss is 0.2035546600818634\n",
      "epoch: 15 step: 619, loss is 0.23150305449962616\n",
      "epoch: 15 step: 620, loss is 0.34109559655189514\n",
      "epoch: 15 step: 621, loss is 0.25194501876831055\n",
      "epoch: 15 step: 622, loss is 0.25623124837875366\n",
      "epoch: 15 step: 623, loss is 0.1126624345779419\n",
      "epoch: 15 step: 624, loss is 0.1792476773262024\n",
      "epoch: 15 step: 625, loss is 0.16963106393814087\n",
      "epoch: 15 step: 626, loss is 0.22234225273132324\n",
      "epoch: 15 step: 627, loss is 0.20515558123588562\n",
      "epoch: 15 step: 628, loss is 0.32895344495773315\n",
      "epoch: 15 step: 629, loss is 0.214835986495018\n",
      "epoch: 15 step: 630, loss is 0.368537575006485\n",
      "epoch: 15 step: 631, loss is 0.10208433121442795\n",
      "epoch: 15 step: 632, loss is 0.21297526359558105\n",
      "epoch: 15 step: 633, loss is 0.3420177102088928\n",
      "epoch: 15 step: 634, loss is 0.14687994122505188\n",
      "epoch: 15 step: 635, loss is 0.29700466990470886\n",
      "epoch: 15 step: 636, loss is 0.15225891768932343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 637, loss is 0.14499354362487793\n",
      "epoch: 15 step: 638, loss is 0.1866208016872406\n",
      "epoch: 15 step: 639, loss is 0.21112409234046936\n",
      "epoch: 15 step: 640, loss is 0.1788666546344757\n",
      "epoch: 15 step: 641, loss is 0.30194756388664246\n",
      "epoch: 15 step: 642, loss is 0.19434557855129242\n",
      "epoch: 15 step: 643, loss is 0.2967132031917572\n",
      "epoch: 15 step: 644, loss is 0.1656007468700409\n",
      "epoch: 15 step: 645, loss is 0.33238887786865234\n",
      "epoch: 15 step: 646, loss is 0.3402945399284363\n",
      "epoch: 15 step: 647, loss is 0.2889018952846527\n",
      "epoch: 15 step: 648, loss is 0.2064853012561798\n",
      "epoch: 15 step: 649, loss is 0.16782836616039276\n",
      "epoch: 15 step: 650, loss is 0.22216114401817322\n",
      "epoch: 15 step: 651, loss is 0.2521030306816101\n",
      "epoch: 15 step: 652, loss is 0.13988998532295227\n",
      "epoch: 15 step: 653, loss is 0.19654697179794312\n",
      "epoch: 15 step: 654, loss is 0.13382548093795776\n",
      "epoch: 15 step: 655, loss is 0.1989755928516388\n",
      "epoch: 15 step: 656, loss is 0.20438314974308014\n",
      "epoch: 15 step: 657, loss is 0.22257794439792633\n",
      "epoch: 15 step: 658, loss is 0.13169264793395996\n",
      "epoch: 15 step: 659, loss is 0.19040708243846893\n",
      "epoch: 15 step: 660, loss is 0.3958589434623718\n",
      "epoch: 15 step: 661, loss is 0.3579275906085968\n",
      "epoch: 15 step: 662, loss is 0.196843221783638\n",
      "epoch: 15 step: 663, loss is 0.20518343150615692\n",
      "epoch: 15 step: 664, loss is 0.16684488952159882\n",
      "epoch: 15 step: 665, loss is 0.2968754768371582\n",
      "epoch: 15 step: 666, loss is 0.24642138183116913\n",
      "epoch: 15 step: 667, loss is 0.3023548126220703\n",
      "epoch: 15 step: 668, loss is 0.20601888000965118\n",
      "epoch: 15 step: 669, loss is 0.2849557399749756\n",
      "epoch: 15 step: 670, loss is 0.14763221144676208\n",
      "epoch: 15 step: 671, loss is 0.15396718680858612\n",
      "epoch: 15 step: 672, loss is 0.1877305805683136\n",
      "epoch: 15 step: 673, loss is 0.33872759342193604\n",
      "epoch: 15 step: 674, loss is 0.32756665349006653\n",
      "epoch: 15 step: 675, loss is 0.26509323716163635\n",
      "epoch: 15 step: 676, loss is 0.38807958364486694\n",
      "epoch: 15 step: 677, loss is 0.31909820437431335\n",
      "epoch: 15 step: 678, loss is 0.4141077697277069\n",
      "epoch: 15 step: 679, loss is 0.21930763125419617\n",
      "epoch: 15 step: 680, loss is 0.1873309314250946\n",
      "epoch: 15 step: 681, loss is 0.2561662495136261\n",
      "epoch: 15 step: 682, loss is 0.19691509008407593\n",
      "epoch: 15 step: 683, loss is 0.2798740565776825\n",
      "epoch: 15 step: 684, loss is 0.32018378376960754\n",
      "epoch: 15 step: 685, loss is 0.22891373932361603\n",
      "epoch: 15 step: 686, loss is 0.17583660781383514\n",
      "epoch: 15 step: 687, loss is 0.17699956893920898\n",
      "epoch: 15 step: 688, loss is 0.17184273898601532\n",
      "epoch: 15 step: 689, loss is 0.18829207122325897\n",
      "epoch: 15 step: 690, loss is 0.07574863731861115\n",
      "epoch: 15 step: 691, loss is 0.3750028610229492\n",
      "epoch: 15 step: 692, loss is 0.24492710828781128\n",
      "epoch: 15 step: 693, loss is 0.27002033591270447\n",
      "epoch: 15 step: 694, loss is 0.15336443483829498\n",
      "epoch: 15 step: 695, loss is 0.3973722457885742\n",
      "epoch: 15 step: 696, loss is 0.20381319522857666\n",
      "epoch: 15 step: 697, loss is 0.1409441977739334\n",
      "epoch: 15 step: 698, loss is 0.1955757886171341\n",
      "epoch: 15 step: 699, loss is 0.23265571892261505\n",
      "epoch: 15 step: 700, loss is 0.2627487778663635\n",
      "epoch: 15 step: 701, loss is 0.2003009021282196\n",
      "epoch: 15 step: 702, loss is 0.26132407784461975\n",
      "epoch: 15 step: 703, loss is 0.09589911997318268\n",
      "epoch: 15 step: 704, loss is 0.07370013743638992\n",
      "epoch: 15 step: 705, loss is 0.1930651068687439\n",
      "epoch: 15 step: 706, loss is 0.12770752608776093\n",
      "epoch: 15 step: 707, loss is 0.24589620530605316\n",
      "epoch: 15 step: 708, loss is 0.16845735907554626\n",
      "epoch: 15 step: 709, loss is 0.1932041198015213\n",
      "epoch: 15 step: 710, loss is 0.17846406996250153\n",
      "epoch: 15 step: 711, loss is 0.2940049469470978\n",
      "epoch: 15 step: 712, loss is 0.20318661630153656\n",
      "epoch: 15 step: 713, loss is 0.11135989427566528\n",
      "epoch: 15 step: 714, loss is 0.21044380962848663\n",
      "epoch: 15 step: 715, loss is 0.14252538979053497\n",
      "epoch: 15 step: 716, loss is 0.07762935757637024\n",
      "epoch: 15 step: 717, loss is 0.2411249428987503\n",
      "epoch: 15 step: 718, loss is 0.1945449262857437\n",
      "epoch: 15 step: 719, loss is 0.10420028865337372\n",
      "epoch: 15 step: 720, loss is 0.14975979924201965\n",
      "epoch: 15 step: 721, loss is 0.17763105034828186\n",
      "epoch: 15 step: 722, loss is 0.2720085680484772\n",
      "epoch: 15 step: 723, loss is 0.08181893825531006\n",
      "epoch: 15 step: 724, loss is 0.35622093081474304\n",
      "epoch: 15 step: 725, loss is 0.2225954383611679\n",
      "epoch: 15 step: 726, loss is 0.22859947383403778\n",
      "epoch: 15 step: 727, loss is 0.18711522221565247\n",
      "epoch: 15 step: 728, loss is 0.2013428807258606\n",
      "epoch: 15 step: 729, loss is 0.1949816197156906\n",
      "epoch: 15 step: 730, loss is 0.26600104570388794\n",
      "epoch: 15 step: 731, loss is 0.24404191970825195\n",
      "epoch: 15 step: 732, loss is 0.31696683168411255\n",
      "epoch: 15 step: 733, loss is 0.25902724266052246\n",
      "epoch: 15 step: 734, loss is 0.2805750370025635\n",
      "epoch: 15 step: 735, loss is 0.19093190133571625\n",
      "epoch: 15 step: 736, loss is 0.16150566935539246\n",
      "epoch: 15 step: 737, loss is 0.18796071410179138\n",
      "epoch: 15 step: 738, loss is 0.09955064952373505\n",
      "epoch: 15 step: 739, loss is 0.20415015518665314\n",
      "epoch: 15 step: 740, loss is 0.13581551611423492\n",
      "epoch: 15 step: 741, loss is 0.3459573984146118\n",
      "epoch: 15 step: 742, loss is 0.31601211428642273\n",
      "epoch: 15 step: 743, loss is 0.31230002641677856\n",
      "epoch: 15 step: 744, loss is 0.21785490214824677\n",
      "epoch: 15 step: 745, loss is 0.13151796162128448\n",
      "epoch: 15 step: 746, loss is 0.3079914152622223\n",
      "epoch: 15 step: 747, loss is 0.20177780091762543\n",
      "epoch: 15 step: 748, loss is 0.1281396448612213\n",
      "epoch: 15 step: 749, loss is 0.4022238552570343\n",
      "epoch: 15 step: 750, loss is 0.195347398519516\n",
      "epoch: 15 step: 751, loss is 0.34143057465553284\n",
      "epoch: 15 step: 752, loss is 0.2689572870731354\n",
      "epoch: 15 step: 753, loss is 0.25617313385009766\n",
      "epoch: 15 step: 754, loss is 0.3406927287578583\n",
      "epoch: 15 step: 755, loss is 0.22497162222862244\n",
      "epoch: 15 step: 756, loss is 0.27039089798927307\n",
      "epoch: 15 step: 757, loss is 0.19181853532791138\n",
      "epoch: 15 step: 758, loss is 0.32147443294525146\n",
      "epoch: 15 step: 759, loss is 0.24967314302921295\n",
      "epoch: 15 step: 760, loss is 0.18352144956588745\n",
      "epoch: 15 step: 761, loss is 0.1772601455450058\n",
      "epoch: 15 step: 762, loss is 0.24192923307418823\n",
      "epoch: 15 step: 763, loss is 0.24460557103157043\n",
      "epoch: 15 step: 764, loss is 0.27343904972076416\n",
      "epoch: 15 step: 765, loss is 0.15865153074264526\n",
      "epoch: 15 step: 766, loss is 0.21302200853824615\n",
      "epoch: 15 step: 767, loss is 0.21899019181728363\n",
      "epoch: 15 step: 768, loss is 0.297044038772583\n",
      "epoch: 15 step: 769, loss is 0.23589074611663818\n",
      "epoch: 15 step: 770, loss is 0.08523394167423248\n",
      "epoch: 15 step: 771, loss is 0.21443019807338715\n",
      "epoch: 15 step: 772, loss is 0.25271350145339966\n",
      "epoch: 15 step: 773, loss is 0.26151594519615173\n",
      "epoch: 15 step: 774, loss is 0.21309560537338257\n",
      "epoch: 15 step: 775, loss is 0.27591195702552795\n",
      "epoch: 15 step: 776, loss is 0.21662621200084686\n",
      "epoch: 15 step: 777, loss is 0.23851662874221802\n",
      "epoch: 15 step: 778, loss is 0.30685463547706604\n",
      "epoch: 15 step: 779, loss is 0.14002053439617157\n",
      "epoch: 15 step: 780, loss is 0.23349736630916595\n",
      "epoch: 15 step: 781, loss is 0.17919699847698212\n",
      "epoch: 15 step: 782, loss is 0.2636561691761017\n",
      "epoch: 15 step: 783, loss is 0.24681782722473145\n",
      "epoch: 15 step: 784, loss is 0.16904808580875397\n",
      "epoch: 15 step: 785, loss is 0.24726268649101257\n",
      "epoch: 15 step: 786, loss is 0.3442179262638092\n",
      "epoch: 15 step: 787, loss is 0.14517861604690552\n",
      "epoch: 15 step: 788, loss is 0.14943088591098785\n",
      "epoch: 15 step: 789, loss is 0.400654673576355\n",
      "epoch: 15 step: 790, loss is 0.2503827214241028\n",
      "epoch: 15 step: 791, loss is 0.2174334079027176\n",
      "epoch: 15 step: 792, loss is 0.23983079195022583\n",
      "epoch: 15 step: 793, loss is 0.32320263981819153\n",
      "epoch: 15 step: 794, loss is 0.37201258540153503\n",
      "epoch: 15 step: 795, loss is 0.26908040046691895\n",
      "epoch: 15 step: 796, loss is 0.16328935325145721\n",
      "epoch: 15 step: 797, loss is 0.20067113637924194\n",
      "epoch: 15 step: 798, loss is 0.2519089877605438\n",
      "epoch: 15 step: 799, loss is 0.30607059597969055\n",
      "epoch: 15 step: 800, loss is 0.16962562501430511\n",
      "epoch: 15 step: 801, loss is 0.17241263389587402\n",
      "epoch: 15 step: 802, loss is 0.22724378108978271\n",
      "epoch: 15 step: 803, loss is 0.14235474169254303\n",
      "epoch: 15 step: 804, loss is 0.25437653064727783\n",
      "epoch: 15 step: 805, loss is 0.14019893109798431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 806, loss is 0.2842065691947937\n",
      "epoch: 15 step: 807, loss is 0.21831658482551575\n",
      "epoch: 15 step: 808, loss is 0.26635459065437317\n",
      "epoch: 15 step: 809, loss is 0.126353457570076\n",
      "epoch: 15 step: 810, loss is 0.37559348344802856\n",
      "epoch: 15 step: 811, loss is 0.34258875250816345\n",
      "epoch: 15 step: 812, loss is 0.21916253864765167\n",
      "epoch: 15 step: 813, loss is 0.1886589527130127\n",
      "epoch: 15 step: 814, loss is 0.19685113430023193\n",
      "epoch: 15 step: 815, loss is 0.22686170041561127\n",
      "epoch: 15 step: 816, loss is 0.15067945420742035\n",
      "epoch: 15 step: 817, loss is 0.1328217089176178\n",
      "epoch: 15 step: 818, loss is 0.29181531071662903\n",
      "epoch: 15 step: 819, loss is 0.42299845814704895\n",
      "epoch: 15 step: 820, loss is 0.2560749351978302\n",
      "epoch: 15 step: 821, loss is 0.18261229991912842\n",
      "epoch: 15 step: 822, loss is 0.10341072827577591\n",
      "epoch: 15 step: 823, loss is 0.3346097767353058\n",
      "epoch: 15 step: 824, loss is 0.19196613132953644\n",
      "epoch: 15 step: 825, loss is 0.21407543122768402\n",
      "epoch: 15 step: 826, loss is 0.1194034293293953\n",
      "epoch: 15 step: 827, loss is 0.26860326528549194\n",
      "epoch: 15 step: 828, loss is 0.18330830335617065\n",
      "epoch: 15 step: 829, loss is 0.31452810764312744\n",
      "epoch: 15 step: 830, loss is 0.08823754638433456\n",
      "epoch: 15 step: 831, loss is 0.1483529806137085\n",
      "epoch: 15 step: 832, loss is 0.25357311964035034\n",
      "epoch: 15 step: 833, loss is 0.11151453852653503\n",
      "epoch: 15 step: 834, loss is 0.30540910363197327\n",
      "epoch: 15 step: 835, loss is 0.13188278675079346\n",
      "epoch: 15 step: 836, loss is 0.3469220697879791\n",
      "epoch: 15 step: 837, loss is 0.11317883431911469\n",
      "epoch: 15 step: 838, loss is 0.1172664687037468\n",
      "epoch: 15 step: 839, loss is 0.14947916567325592\n",
      "epoch: 15 step: 840, loss is 0.2971022427082062\n",
      "epoch: 15 step: 841, loss is 0.283761203289032\n",
      "epoch: 15 step: 842, loss is 0.47685590386390686\n",
      "epoch: 15 step: 843, loss is 0.29704946279525757\n",
      "epoch: 15 step: 844, loss is 0.18760427832603455\n",
      "epoch: 15 step: 845, loss is 0.2327987104654312\n",
      "epoch: 15 step: 846, loss is 0.20511625707149506\n",
      "epoch: 15 step: 847, loss is 0.49089911580085754\n",
      "epoch: 15 step: 848, loss is 0.21964988112449646\n",
      "epoch: 15 step: 849, loss is 0.1557987928390503\n",
      "epoch: 15 step: 850, loss is 0.26358696818351746\n",
      "epoch: 15 step: 851, loss is 0.23091289401054382\n",
      "epoch: 15 step: 852, loss is 0.36300602555274963\n",
      "epoch: 15 step: 853, loss is 0.23694519698619843\n",
      "epoch: 15 step: 854, loss is 0.23850944638252258\n",
      "epoch: 15 step: 855, loss is 0.18006779253482819\n",
      "epoch: 15 step: 856, loss is 0.14285314083099365\n",
      "epoch: 15 step: 857, loss is 0.1974307745695114\n",
      "epoch: 15 step: 858, loss is 0.1911509782075882\n",
      "epoch: 15 step: 859, loss is 0.3526689410209656\n",
      "epoch: 15 step: 860, loss is 0.16306687891483307\n",
      "epoch: 15 step: 861, loss is 0.13848376274108887\n",
      "epoch: 15 step: 862, loss is 0.2456514984369278\n",
      "epoch: 15 step: 863, loss is 0.30736520886421204\n",
      "epoch: 15 step: 864, loss is 0.0933712050318718\n",
      "epoch: 15 step: 865, loss is 0.17819924652576447\n",
      "epoch: 15 step: 866, loss is 0.09078505635261536\n",
      "epoch: 15 step: 867, loss is 0.24382472038269043\n",
      "epoch: 15 step: 868, loss is 0.1684793084859848\n",
      "epoch: 15 step: 869, loss is 0.17252539098262787\n",
      "epoch: 15 step: 870, loss is 0.16996797919273376\n",
      "epoch: 15 step: 871, loss is 0.2206469625234604\n",
      "epoch: 15 step: 872, loss is 0.20483435690402985\n",
      "epoch: 15 step: 873, loss is 0.10905065387487411\n",
      "epoch: 15 step: 874, loss is 0.15846826136112213\n",
      "epoch: 15 step: 875, loss is 0.10548599809408188\n",
      "epoch: 15 step: 876, loss is 0.25391021370887756\n",
      "epoch: 15 step: 877, loss is 0.17643815279006958\n",
      "epoch: 15 step: 878, loss is 0.239798903465271\n",
      "epoch: 15 step: 879, loss is 0.18608412146568298\n",
      "epoch: 15 step: 880, loss is 0.2308439463376999\n",
      "epoch: 15 step: 881, loss is 0.16496172547340393\n",
      "epoch: 15 step: 882, loss is 0.35142087936401367\n",
      "epoch: 15 step: 883, loss is 0.35807400941848755\n",
      "epoch: 15 step: 884, loss is 0.4349488615989685\n",
      "epoch: 15 step: 885, loss is 0.21896536648273468\n",
      "epoch: 15 step: 886, loss is 0.14891642332077026\n",
      "epoch: 15 step: 887, loss is 0.17968341708183289\n",
      "epoch: 15 step: 888, loss is 0.08978348970413208\n",
      "epoch: 15 step: 889, loss is 0.1838528960943222\n",
      "epoch: 15 step: 890, loss is 0.36423808336257935\n",
      "epoch: 15 step: 891, loss is 0.25357434153556824\n",
      "epoch: 15 step: 892, loss is 0.3345314562320709\n",
      "epoch: 15 step: 893, loss is 0.1386450231075287\n",
      "epoch: 15 step: 894, loss is 0.2110450565814972\n",
      "epoch: 15 step: 895, loss is 0.3147489130496979\n",
      "epoch: 15 step: 896, loss is 0.25597506761550903\n",
      "epoch: 15 step: 897, loss is 0.358121395111084\n",
      "epoch: 15 step: 898, loss is 0.39525237679481506\n",
      "epoch: 15 step: 899, loss is 0.24834467470645905\n",
      "epoch: 15 step: 900, loss is 0.2920718193054199\n",
      "epoch: 15 step: 901, loss is 0.38984525203704834\n",
      "epoch: 15 step: 902, loss is 0.2552315890789032\n",
      "epoch: 15 step: 903, loss is 0.34261950850486755\n",
      "epoch: 15 step: 904, loss is 0.14870524406433105\n",
      "epoch: 15 step: 905, loss is 0.26550281047821045\n",
      "epoch: 15 step: 906, loss is 0.19252964854240417\n",
      "epoch: 15 step: 907, loss is 0.19811242818832397\n",
      "epoch: 15 step: 908, loss is 0.20181892812252045\n",
      "epoch: 15 step: 909, loss is 0.2160036563873291\n",
      "epoch: 15 step: 910, loss is 0.16889220476150513\n",
      "epoch: 15 step: 911, loss is 0.16576145589351654\n",
      "epoch: 15 step: 912, loss is 0.2730397880077362\n",
      "epoch: 15 step: 913, loss is 0.26066407561302185\n",
      "epoch: 15 step: 914, loss is 0.32323604822158813\n",
      "epoch: 15 step: 915, loss is 0.1011359840631485\n",
      "epoch: 15 step: 916, loss is 0.11726496368646622\n",
      "epoch: 15 step: 917, loss is 0.26048120856285095\n",
      "epoch: 15 step: 918, loss is 0.19682499766349792\n",
      "epoch: 15 step: 919, loss is 0.18999835848808289\n",
      "epoch: 15 step: 920, loss is 0.15784001350402832\n",
      "epoch: 15 step: 921, loss is 0.21990172564983368\n",
      "epoch: 15 step: 922, loss is 0.3494362533092499\n",
      "epoch: 15 step: 923, loss is 0.22470919787883759\n",
      "epoch: 15 step: 924, loss is 0.24222061038017273\n",
      "epoch: 15 step: 925, loss is 0.19986942410469055\n",
      "epoch: 15 step: 926, loss is 0.1858023703098297\n",
      "epoch: 15 step: 927, loss is 0.34949991106987\n",
      "epoch: 15 step: 928, loss is 0.27142852544784546\n",
      "epoch: 15 step: 929, loss is 0.12164252251386642\n",
      "epoch: 15 step: 930, loss is 0.19409024715423584\n",
      "epoch: 15 step: 931, loss is 0.2486012727022171\n",
      "epoch: 15 step: 932, loss is 0.3245585560798645\n",
      "epoch: 15 step: 933, loss is 0.25793135166168213\n",
      "epoch: 15 step: 934, loss is 0.26179182529449463\n",
      "epoch: 15 step: 935, loss is 0.15032300353050232\n",
      "epoch: 15 step: 936, loss is 0.16020390391349792\n",
      "epoch: 15 step: 937, loss is 0.227495014667511\n",
      "epoch: 16 step: 1, loss is 0.25867074728012085\n",
      "epoch: 16 step: 2, loss is 0.1751343309879303\n",
      "epoch: 16 step: 3, loss is 0.23938456177711487\n",
      "epoch: 16 step: 4, loss is 0.1299768090248108\n",
      "epoch: 16 step: 5, loss is 0.19496475160121918\n",
      "epoch: 16 step: 6, loss is 0.2349083423614502\n",
      "epoch: 16 step: 7, loss is 0.22712893784046173\n",
      "epoch: 16 step: 8, loss is 0.2525825500488281\n",
      "epoch: 16 step: 9, loss is 0.16059529781341553\n",
      "epoch: 16 step: 10, loss is 0.14368586242198944\n",
      "epoch: 16 step: 11, loss is 0.23968297243118286\n",
      "epoch: 16 step: 12, loss is 0.19081300497055054\n",
      "epoch: 16 step: 13, loss is 0.2420627474784851\n",
      "epoch: 16 step: 14, loss is 0.16603557765483856\n",
      "epoch: 16 step: 15, loss is 0.1993064433336258\n",
      "epoch: 16 step: 16, loss is 0.27853724360466003\n",
      "epoch: 16 step: 17, loss is 0.20468008518218994\n",
      "epoch: 16 step: 18, loss is 0.15979701280593872\n",
      "epoch: 16 step: 19, loss is 0.21624813973903656\n",
      "epoch: 16 step: 20, loss is 0.16534453630447388\n",
      "epoch: 16 step: 21, loss is 0.32761815190315247\n",
      "epoch: 16 step: 22, loss is 0.1345405876636505\n",
      "epoch: 16 step: 23, loss is 0.3071254789829254\n",
      "epoch: 16 step: 24, loss is 0.22345000505447388\n",
      "epoch: 16 step: 25, loss is 0.12544745206832886\n",
      "epoch: 16 step: 26, loss is 0.20945122838020325\n",
      "epoch: 16 step: 27, loss is 0.24071510136127472\n",
      "epoch: 16 step: 28, loss is 0.1809777319431305\n",
      "epoch: 16 step: 29, loss is 0.26162806153297424\n",
      "epoch: 16 step: 30, loss is 0.3266143202781677\n",
      "epoch: 16 step: 31, loss is 0.13550499081611633\n",
      "epoch: 16 step: 32, loss is 0.18628710508346558\n",
      "epoch: 16 step: 33, loss is 0.12793248891830444\n",
      "epoch: 16 step: 34, loss is 0.16825991868972778\n",
      "epoch: 16 step: 35, loss is 0.2931853234767914\n",
      "epoch: 16 step: 36, loss is 0.1966673731803894\n",
      "epoch: 16 step: 37, loss is 0.16103890538215637\n",
      "epoch: 16 step: 38, loss is 0.19877228140830994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 39, loss is 0.20337225496768951\n",
      "epoch: 16 step: 40, loss is 0.2496507316827774\n",
      "epoch: 16 step: 41, loss is 0.2567736804485321\n",
      "epoch: 16 step: 42, loss is 0.16625720262527466\n",
      "epoch: 16 step: 43, loss is 0.1789812594652176\n",
      "epoch: 16 step: 44, loss is 0.24857455492019653\n",
      "epoch: 16 step: 45, loss is 0.2553771138191223\n",
      "epoch: 16 step: 46, loss is 0.11220365762710571\n",
      "epoch: 16 step: 47, loss is 0.1946474015712738\n",
      "epoch: 16 step: 48, loss is 0.10622334480285645\n",
      "epoch: 16 step: 49, loss is 0.30661460757255554\n",
      "epoch: 16 step: 50, loss is 0.2625378370285034\n",
      "epoch: 16 step: 51, loss is 0.3140002489089966\n",
      "epoch: 16 step: 52, loss is 0.21368202567100525\n",
      "epoch: 16 step: 53, loss is 0.15401093661785126\n",
      "epoch: 16 step: 54, loss is 0.3789121210575104\n",
      "epoch: 16 step: 55, loss is 0.14291155338287354\n",
      "epoch: 16 step: 56, loss is 0.3501603305339813\n",
      "epoch: 16 step: 57, loss is 0.18910805881023407\n",
      "epoch: 16 step: 58, loss is 0.19391925632953644\n",
      "epoch: 16 step: 59, loss is 0.3318769633769989\n",
      "epoch: 16 step: 60, loss is 0.13717016577720642\n",
      "epoch: 16 step: 61, loss is 0.24937209486961365\n",
      "epoch: 16 step: 62, loss is 0.30290329456329346\n",
      "epoch: 16 step: 63, loss is 0.21761177480220795\n",
      "epoch: 16 step: 64, loss is 0.2399759739637375\n",
      "epoch: 16 step: 65, loss is 0.2917649447917938\n",
      "epoch: 16 step: 66, loss is 0.1898043006658554\n",
      "epoch: 16 step: 67, loss is 0.17753766477108002\n",
      "epoch: 16 step: 68, loss is 0.1450929492712021\n",
      "epoch: 16 step: 69, loss is 0.2665869891643524\n",
      "epoch: 16 step: 70, loss is 0.269313246011734\n",
      "epoch: 16 step: 71, loss is 0.2678575813770294\n",
      "epoch: 16 step: 72, loss is 0.16672371327877045\n",
      "epoch: 16 step: 73, loss is 0.0692155510187149\n",
      "epoch: 16 step: 74, loss is 0.29687413573265076\n",
      "epoch: 16 step: 75, loss is 0.10022963583469391\n",
      "epoch: 16 step: 76, loss is 0.2838458716869354\n",
      "epoch: 16 step: 77, loss is 0.20539148151874542\n",
      "epoch: 16 step: 78, loss is 0.19947756826877594\n",
      "epoch: 16 step: 79, loss is 0.2399989813566208\n",
      "epoch: 16 step: 80, loss is 0.18196561932563782\n",
      "epoch: 16 step: 81, loss is 0.07461175322532654\n",
      "epoch: 16 step: 82, loss is 0.22637803852558136\n",
      "epoch: 16 step: 83, loss is 0.09775020182132721\n",
      "epoch: 16 step: 84, loss is 0.22914619743824005\n",
      "epoch: 16 step: 85, loss is 0.14895793795585632\n",
      "epoch: 16 step: 86, loss is 0.29744958877563477\n",
      "epoch: 16 step: 87, loss is 0.16966944932937622\n",
      "epoch: 16 step: 88, loss is 0.23910152912139893\n",
      "epoch: 16 step: 89, loss is 0.2961633801460266\n",
      "epoch: 16 step: 90, loss is 0.4612203538417816\n",
      "epoch: 16 step: 91, loss is 0.1504182517528534\n",
      "epoch: 16 step: 92, loss is 0.15974442660808563\n",
      "epoch: 16 step: 93, loss is 0.27295419573783875\n",
      "epoch: 16 step: 94, loss is 0.4197627007961273\n",
      "epoch: 16 step: 95, loss is 0.22371533513069153\n",
      "epoch: 16 step: 96, loss is 0.3300412595272064\n",
      "epoch: 16 step: 97, loss is 0.10720982402563095\n",
      "epoch: 16 step: 98, loss is 0.135685533285141\n",
      "epoch: 16 step: 99, loss is 0.23507258296012878\n",
      "epoch: 16 step: 100, loss is 0.1821759194135666\n",
      "epoch: 16 step: 101, loss is 0.3456876873970032\n",
      "epoch: 16 step: 102, loss is 0.2421267181634903\n",
      "epoch: 16 step: 103, loss is 0.1236434057354927\n",
      "epoch: 16 step: 104, loss is 0.36039233207702637\n",
      "epoch: 16 step: 105, loss is 0.26319366693496704\n",
      "epoch: 16 step: 106, loss is 0.17310617864131927\n",
      "epoch: 16 step: 107, loss is 0.2822670042514801\n",
      "epoch: 16 step: 108, loss is 0.18881960213184357\n",
      "epoch: 16 step: 109, loss is 0.2632061839103699\n",
      "epoch: 16 step: 110, loss is 0.15928837656974792\n",
      "epoch: 16 step: 111, loss is 0.15084095299243927\n",
      "epoch: 16 step: 112, loss is 0.14565181732177734\n",
      "epoch: 16 step: 113, loss is 0.22323819994926453\n",
      "epoch: 16 step: 114, loss is 0.14288169145584106\n",
      "epoch: 16 step: 115, loss is 0.19362559914588928\n",
      "epoch: 16 step: 116, loss is 0.23612986505031586\n",
      "epoch: 16 step: 117, loss is 0.1694396585226059\n",
      "epoch: 16 step: 118, loss is 0.213232159614563\n",
      "epoch: 16 step: 119, loss is 0.09179722517728806\n",
      "epoch: 16 step: 120, loss is 0.1535360962152481\n",
      "epoch: 16 step: 121, loss is 0.3396148979663849\n",
      "epoch: 16 step: 122, loss is 0.16335681080818176\n",
      "epoch: 16 step: 123, loss is 0.1821833699941635\n",
      "epoch: 16 step: 124, loss is 0.219106987118721\n",
      "epoch: 16 step: 125, loss is 0.19510404765605927\n",
      "epoch: 16 step: 126, loss is 0.12871341407299042\n",
      "epoch: 16 step: 127, loss is 0.21485503017902374\n",
      "epoch: 16 step: 128, loss is 0.11713085323572159\n",
      "epoch: 16 step: 129, loss is 0.1293737143278122\n",
      "epoch: 16 step: 130, loss is 0.11088703572750092\n",
      "epoch: 16 step: 131, loss is 0.24092811346054077\n",
      "epoch: 16 step: 132, loss is 0.18494324386119843\n",
      "epoch: 16 step: 133, loss is 0.14201167225837708\n",
      "epoch: 16 step: 134, loss is 0.1144193634390831\n",
      "epoch: 16 step: 135, loss is 0.222353994846344\n",
      "epoch: 16 step: 136, loss is 0.3350108861923218\n",
      "epoch: 16 step: 137, loss is 0.17913973331451416\n",
      "epoch: 16 step: 138, loss is 0.22030292451381683\n",
      "epoch: 16 step: 139, loss is 0.24320466816425323\n",
      "epoch: 16 step: 140, loss is 0.14667744934558868\n",
      "epoch: 16 step: 141, loss is 0.18569085001945496\n",
      "epoch: 16 step: 142, loss is 0.33399492502212524\n",
      "epoch: 16 step: 143, loss is 0.19253215193748474\n",
      "epoch: 16 step: 144, loss is 0.11833272874355316\n",
      "epoch: 16 step: 145, loss is 0.10111773014068604\n",
      "epoch: 16 step: 146, loss is 0.3917194604873657\n",
      "epoch: 16 step: 147, loss is 0.10152257233858109\n",
      "epoch: 16 step: 148, loss is 0.29846715927124023\n",
      "epoch: 16 step: 149, loss is 0.3240624964237213\n",
      "epoch: 16 step: 150, loss is 0.15790076553821564\n",
      "epoch: 16 step: 151, loss is 0.3238838016986847\n",
      "epoch: 16 step: 152, loss is 0.19833913445472717\n",
      "epoch: 16 step: 153, loss is 0.16978555917739868\n",
      "epoch: 16 step: 154, loss is 0.2835434377193451\n",
      "epoch: 16 step: 155, loss is 0.23321452736854553\n",
      "epoch: 16 step: 156, loss is 0.16383396089076996\n",
      "epoch: 16 step: 157, loss is 0.17347480356693268\n",
      "epoch: 16 step: 158, loss is 0.4166647791862488\n",
      "epoch: 16 step: 159, loss is 0.22135816514492035\n",
      "epoch: 16 step: 160, loss is 0.2256270796060562\n",
      "epoch: 16 step: 161, loss is 0.14937613904476166\n",
      "epoch: 16 step: 162, loss is 0.21878623962402344\n",
      "epoch: 16 step: 163, loss is 0.2900903820991516\n",
      "epoch: 16 step: 164, loss is 0.16550178825855255\n",
      "epoch: 16 step: 165, loss is 0.22375929355621338\n",
      "epoch: 16 step: 166, loss is 0.11610016226768494\n",
      "epoch: 16 step: 167, loss is 0.12605901062488556\n",
      "epoch: 16 step: 168, loss is 0.27045753598213196\n",
      "epoch: 16 step: 169, loss is 0.1630478948354721\n",
      "epoch: 16 step: 170, loss is 0.1773044914007187\n",
      "epoch: 16 step: 171, loss is 0.37130311131477356\n",
      "epoch: 16 step: 172, loss is 0.20711199939250946\n",
      "epoch: 16 step: 173, loss is 0.27544116973876953\n",
      "epoch: 16 step: 174, loss is 0.3939244747161865\n",
      "epoch: 16 step: 175, loss is 0.12996049225330353\n",
      "epoch: 16 step: 176, loss is 0.4061910808086395\n",
      "epoch: 16 step: 177, loss is 0.1667056679725647\n",
      "epoch: 16 step: 178, loss is 0.27997371554374695\n",
      "epoch: 16 step: 179, loss is 0.18420752882957458\n",
      "epoch: 16 step: 180, loss is 0.13674238324165344\n",
      "epoch: 16 step: 181, loss is 0.10201390832662582\n",
      "epoch: 16 step: 182, loss is 0.22320270538330078\n",
      "epoch: 16 step: 183, loss is 0.1228703036904335\n",
      "epoch: 16 step: 184, loss is 0.07494622468948364\n",
      "epoch: 16 step: 185, loss is 0.16355273127555847\n",
      "epoch: 16 step: 186, loss is 0.1936085969209671\n",
      "epoch: 16 step: 187, loss is 0.3105420172214508\n",
      "epoch: 16 step: 188, loss is 0.2589215934276581\n",
      "epoch: 16 step: 189, loss is 0.2142394781112671\n",
      "epoch: 16 step: 190, loss is 0.29922232031822205\n",
      "epoch: 16 step: 191, loss is 0.10160074383020401\n",
      "epoch: 16 step: 192, loss is 0.17127099633216858\n",
      "epoch: 16 step: 193, loss is 0.21973125636577606\n",
      "epoch: 16 step: 194, loss is 0.26394739747047424\n",
      "epoch: 16 step: 195, loss is 0.30768346786499023\n",
      "epoch: 16 step: 196, loss is 0.14586757123470306\n",
      "epoch: 16 step: 197, loss is 0.22448596358299255\n",
      "epoch: 16 step: 198, loss is 0.1820751428604126\n",
      "epoch: 16 step: 199, loss is 0.18787828087806702\n",
      "epoch: 16 step: 200, loss is 0.23269158601760864\n",
      "epoch: 16 step: 201, loss is 0.2979793846607208\n",
      "epoch: 16 step: 202, loss is 0.09672805666923523\n",
      "epoch: 16 step: 203, loss is 0.13678474724292755\n",
      "epoch: 16 step: 204, loss is 0.13390854001045227\n",
      "epoch: 16 step: 205, loss is 0.1339591145515442\n",
      "epoch: 16 step: 206, loss is 0.21069049835205078\n",
      "epoch: 16 step: 207, loss is 0.2663765549659729\n",
      "epoch: 16 step: 208, loss is 0.15501919388771057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 209, loss is 0.22329500317573547\n",
      "epoch: 16 step: 210, loss is 0.15848778188228607\n",
      "epoch: 16 step: 211, loss is 0.09242098033428192\n",
      "epoch: 16 step: 212, loss is 0.15593291819095612\n",
      "epoch: 16 step: 213, loss is 0.27677029371261597\n",
      "epoch: 16 step: 214, loss is 0.17715218663215637\n",
      "epoch: 16 step: 215, loss is 0.1169077605009079\n",
      "epoch: 16 step: 216, loss is 0.15421746671199799\n",
      "epoch: 16 step: 217, loss is 0.19684261083602905\n",
      "epoch: 16 step: 218, loss is 0.3335508704185486\n",
      "epoch: 16 step: 219, loss is 0.3580986261367798\n",
      "epoch: 16 step: 220, loss is 0.12496428191661835\n",
      "epoch: 16 step: 221, loss is 0.25795090198516846\n",
      "epoch: 16 step: 222, loss is 0.22556190192699432\n",
      "epoch: 16 step: 223, loss is 0.19955740869045258\n",
      "epoch: 16 step: 224, loss is 0.14793537557125092\n",
      "epoch: 16 step: 225, loss is 0.17852653563022614\n",
      "epoch: 16 step: 226, loss is 0.08012112230062485\n",
      "epoch: 16 step: 227, loss is 0.0888228639960289\n",
      "epoch: 16 step: 228, loss is 0.18001356720924377\n",
      "epoch: 16 step: 229, loss is 0.20067697763442993\n",
      "epoch: 16 step: 230, loss is 0.33843088150024414\n",
      "epoch: 16 step: 231, loss is 0.1527376025915146\n",
      "epoch: 16 step: 232, loss is 0.2741870880126953\n",
      "epoch: 16 step: 233, loss is 0.19701392948627472\n",
      "epoch: 16 step: 234, loss is 0.18902963399887085\n",
      "epoch: 16 step: 235, loss is 0.13517867028713226\n",
      "epoch: 16 step: 236, loss is 0.27364689111709595\n",
      "epoch: 16 step: 237, loss is 0.3244144022464752\n",
      "epoch: 16 step: 238, loss is 0.15975147485733032\n",
      "epoch: 16 step: 239, loss is 0.2648638188838959\n",
      "epoch: 16 step: 240, loss is 0.10432836413383484\n",
      "epoch: 16 step: 241, loss is 0.1730337142944336\n",
      "epoch: 16 step: 242, loss is 0.22033673524856567\n",
      "epoch: 16 step: 243, loss is 0.17825791239738464\n",
      "epoch: 16 step: 244, loss is 0.19709889590740204\n",
      "epoch: 16 step: 245, loss is 0.16695661842823029\n",
      "epoch: 16 step: 246, loss is 0.15552400052547455\n",
      "epoch: 16 step: 247, loss is 0.33187952637672424\n",
      "epoch: 16 step: 248, loss is 0.23816540837287903\n",
      "epoch: 16 step: 249, loss is 0.12468277662992477\n",
      "epoch: 16 step: 250, loss is 0.16862964630126953\n",
      "epoch: 16 step: 251, loss is 0.3453896641731262\n",
      "epoch: 16 step: 252, loss is 0.1754388064146042\n",
      "epoch: 16 step: 253, loss is 0.1694616973400116\n",
      "epoch: 16 step: 254, loss is 0.13667520880699158\n",
      "epoch: 16 step: 255, loss is 0.19596728682518005\n",
      "epoch: 16 step: 256, loss is 0.217726930975914\n",
      "epoch: 16 step: 257, loss is 0.21488241851329803\n",
      "epoch: 16 step: 258, loss is 0.260763019323349\n",
      "epoch: 16 step: 259, loss is 0.20634490251541138\n",
      "epoch: 16 step: 260, loss is 0.2100047916173935\n",
      "epoch: 16 step: 261, loss is 0.11846300959587097\n",
      "epoch: 16 step: 262, loss is 0.4502124786376953\n",
      "epoch: 16 step: 263, loss is 0.14655177295207977\n",
      "epoch: 16 step: 264, loss is 0.22015689313411713\n",
      "epoch: 16 step: 265, loss is 0.20970846712589264\n",
      "epoch: 16 step: 266, loss is 0.15372535586357117\n",
      "epoch: 16 step: 267, loss is 0.21487443149089813\n",
      "epoch: 16 step: 268, loss is 0.24547921121120453\n",
      "epoch: 16 step: 269, loss is 0.11595477908849716\n",
      "epoch: 16 step: 270, loss is 0.21761171519756317\n",
      "epoch: 16 step: 271, loss is 0.23306693136692047\n",
      "epoch: 16 step: 272, loss is 0.12335964292287827\n",
      "epoch: 16 step: 273, loss is 0.1869010031223297\n",
      "epoch: 16 step: 274, loss is 0.19104793667793274\n",
      "epoch: 16 step: 275, loss is 0.33305642008781433\n",
      "epoch: 16 step: 276, loss is 0.11715004593133926\n",
      "epoch: 16 step: 277, loss is 0.11685876548290253\n",
      "epoch: 16 step: 278, loss is 0.37104445695877075\n",
      "epoch: 16 step: 279, loss is 0.24207958579063416\n",
      "epoch: 16 step: 280, loss is 0.19666226208209991\n",
      "epoch: 16 step: 281, loss is 0.37404578924179077\n",
      "epoch: 16 step: 282, loss is 0.17410781979560852\n",
      "epoch: 16 step: 283, loss is 0.20179817080497742\n",
      "epoch: 16 step: 284, loss is 0.17907822132110596\n",
      "epoch: 16 step: 285, loss is 0.21318736672401428\n",
      "epoch: 16 step: 286, loss is 0.28854960203170776\n",
      "epoch: 16 step: 287, loss is 0.22271983325481415\n",
      "epoch: 16 step: 288, loss is 0.1886082887649536\n",
      "epoch: 16 step: 289, loss is 0.342028945684433\n",
      "epoch: 16 step: 290, loss is 0.16473153233528137\n",
      "epoch: 16 step: 291, loss is 0.2278997302055359\n",
      "epoch: 16 step: 292, loss is 0.09870212525129318\n",
      "epoch: 16 step: 293, loss is 0.21820205450057983\n",
      "epoch: 16 step: 294, loss is 0.20121803879737854\n",
      "epoch: 16 step: 295, loss is 0.14186105132102966\n",
      "epoch: 16 step: 296, loss is 0.2389823943376541\n",
      "epoch: 16 step: 297, loss is 0.17958150804042816\n",
      "epoch: 16 step: 298, loss is 0.14078867435455322\n",
      "epoch: 16 step: 299, loss is 0.18530182540416718\n",
      "epoch: 16 step: 300, loss is 0.09691402316093445\n",
      "epoch: 16 step: 301, loss is 0.20172567665576935\n",
      "epoch: 16 step: 302, loss is 0.2661082446575165\n",
      "epoch: 16 step: 303, loss is 0.21284615993499756\n",
      "epoch: 16 step: 304, loss is 0.19009070098400116\n",
      "epoch: 16 step: 305, loss is 0.2223488986492157\n",
      "epoch: 16 step: 306, loss is 0.3893222510814667\n",
      "epoch: 16 step: 307, loss is 0.21075251698493958\n",
      "epoch: 16 step: 308, loss is 0.2571772336959839\n",
      "epoch: 16 step: 309, loss is 0.2042585164308548\n",
      "epoch: 16 step: 310, loss is 0.10841028392314911\n",
      "epoch: 16 step: 311, loss is 0.17121587693691254\n",
      "epoch: 16 step: 312, loss is 0.17461444437503815\n",
      "epoch: 16 step: 313, loss is 0.13351668417453766\n",
      "epoch: 16 step: 314, loss is 0.22930468618869781\n",
      "epoch: 16 step: 315, loss is 0.22005833685398102\n",
      "epoch: 16 step: 316, loss is 0.22618520259857178\n",
      "epoch: 16 step: 317, loss is 0.265513151884079\n",
      "epoch: 16 step: 318, loss is 0.28218528628349304\n",
      "epoch: 16 step: 319, loss is 0.20693737268447876\n",
      "epoch: 16 step: 320, loss is 0.24008378386497498\n",
      "epoch: 16 step: 321, loss is 0.2954990267753601\n",
      "epoch: 16 step: 322, loss is 0.18229764699935913\n",
      "epoch: 16 step: 323, loss is 0.12886367738246918\n",
      "epoch: 16 step: 324, loss is 0.21672563254833221\n",
      "epoch: 16 step: 325, loss is 0.11652014404535294\n",
      "epoch: 16 step: 326, loss is 0.18715837597846985\n",
      "epoch: 16 step: 327, loss is 0.3626449704170227\n",
      "epoch: 16 step: 328, loss is 0.23808997869491577\n",
      "epoch: 16 step: 329, loss is 0.1679890751838684\n",
      "epoch: 16 step: 330, loss is 0.20478640496730804\n",
      "epoch: 16 step: 331, loss is 0.19041283428668976\n",
      "epoch: 16 step: 332, loss is 0.12406692653894424\n",
      "epoch: 16 step: 333, loss is 0.16087348759174347\n",
      "epoch: 16 step: 334, loss is 0.28684180974960327\n",
      "epoch: 16 step: 335, loss is 0.20951762795448303\n",
      "epoch: 16 step: 336, loss is 0.1657678484916687\n",
      "epoch: 16 step: 337, loss is 0.27159446477890015\n",
      "epoch: 16 step: 338, loss is 0.14746013283729553\n",
      "epoch: 16 step: 339, loss is 0.20216859877109528\n",
      "epoch: 16 step: 340, loss is 0.2026420682668686\n",
      "epoch: 16 step: 341, loss is 0.19683414697647095\n",
      "epoch: 16 step: 342, loss is 0.3459989130496979\n",
      "epoch: 16 step: 343, loss is 0.22993813455104828\n",
      "epoch: 16 step: 344, loss is 0.21519547700881958\n",
      "epoch: 16 step: 345, loss is 0.14822883903980255\n",
      "epoch: 16 step: 346, loss is 0.28698068857192993\n",
      "epoch: 16 step: 347, loss is 0.42859259247779846\n",
      "epoch: 16 step: 348, loss is 0.2927287220954895\n",
      "epoch: 16 step: 349, loss is 0.1290837824344635\n",
      "epoch: 16 step: 350, loss is 0.3796132802963257\n",
      "epoch: 16 step: 351, loss is 0.16866853833198547\n",
      "epoch: 16 step: 352, loss is 0.22970230877399445\n",
      "epoch: 16 step: 353, loss is 0.2062312215566635\n",
      "epoch: 16 step: 354, loss is 0.20359215140342712\n",
      "epoch: 16 step: 355, loss is 0.08438987284898758\n",
      "epoch: 16 step: 356, loss is 0.21020513772964478\n",
      "epoch: 16 step: 357, loss is 0.2465229034423828\n",
      "epoch: 16 step: 358, loss is 0.20584455132484436\n",
      "epoch: 16 step: 359, loss is 0.13726140558719635\n",
      "epoch: 16 step: 360, loss is 0.1522195041179657\n",
      "epoch: 16 step: 361, loss is 0.17020700871944427\n",
      "epoch: 16 step: 362, loss is 0.1344173699617386\n",
      "epoch: 16 step: 363, loss is 0.16308315098285675\n",
      "epoch: 16 step: 364, loss is 0.13123013079166412\n",
      "epoch: 16 step: 365, loss is 0.14832645654678345\n",
      "epoch: 16 step: 366, loss is 0.16050609946250916\n",
      "epoch: 16 step: 367, loss is 0.2453489750623703\n",
      "epoch: 16 step: 368, loss is 0.12716145813465118\n",
      "epoch: 16 step: 369, loss is 0.16190923750400543\n",
      "epoch: 16 step: 370, loss is 0.32284072041511536\n",
      "epoch: 16 step: 371, loss is 0.29860633611679077\n",
      "epoch: 16 step: 372, loss is 0.2846326529979706\n",
      "epoch: 16 step: 373, loss is 0.17832647264003754\n",
      "epoch: 16 step: 374, loss is 0.2519027292728424\n",
      "epoch: 16 step: 375, loss is 0.28057634830474854\n",
      "epoch: 16 step: 376, loss is 0.29218631982803345\n",
      "epoch: 16 step: 377, loss is 0.27561768889427185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 378, loss is 0.21994522213935852\n",
      "epoch: 16 step: 379, loss is 0.2854273021221161\n",
      "epoch: 16 step: 380, loss is 0.0775209292769432\n",
      "epoch: 16 step: 381, loss is 0.26719212532043457\n",
      "epoch: 16 step: 382, loss is 0.2766355574131012\n",
      "epoch: 16 step: 383, loss is 0.2068883180618286\n",
      "epoch: 16 step: 384, loss is 0.12808160483837128\n",
      "epoch: 16 step: 385, loss is 0.16132858395576477\n",
      "epoch: 16 step: 386, loss is 0.250654935836792\n",
      "epoch: 16 step: 387, loss is 0.2576035261154175\n",
      "epoch: 16 step: 388, loss is 0.21859915554523468\n",
      "epoch: 16 step: 389, loss is 0.1355179101228714\n",
      "epoch: 16 step: 390, loss is 0.2990058660507202\n",
      "epoch: 16 step: 391, loss is 0.2866419851779938\n",
      "epoch: 16 step: 392, loss is 0.15641628205776215\n",
      "epoch: 16 step: 393, loss is 0.24321012198925018\n",
      "epoch: 16 step: 394, loss is 0.13036295771598816\n",
      "epoch: 16 step: 395, loss is 0.1288638859987259\n",
      "epoch: 16 step: 396, loss is 0.18017040193080902\n",
      "epoch: 16 step: 397, loss is 0.08816798031330109\n",
      "epoch: 16 step: 398, loss is 0.10069143027067184\n",
      "epoch: 16 step: 399, loss is 0.18609832227230072\n",
      "epoch: 16 step: 400, loss is 0.125822976231575\n",
      "epoch: 16 step: 401, loss is 0.1753462702035904\n",
      "epoch: 16 step: 402, loss is 0.19176237285137177\n",
      "epoch: 16 step: 403, loss is 0.34747081995010376\n",
      "epoch: 16 step: 404, loss is 0.15395204722881317\n",
      "epoch: 16 step: 405, loss is 0.10158434510231018\n",
      "epoch: 16 step: 406, loss is 0.1990474909543991\n",
      "epoch: 16 step: 407, loss is 0.11936823278665543\n",
      "epoch: 16 step: 408, loss is 0.13662804663181305\n",
      "epoch: 16 step: 409, loss is 0.30447298288345337\n",
      "epoch: 16 step: 410, loss is 0.11205661296844482\n",
      "epoch: 16 step: 411, loss is 0.16291995346546173\n",
      "epoch: 16 step: 412, loss is 0.30418136715888977\n",
      "epoch: 16 step: 413, loss is 0.21232090890407562\n",
      "epoch: 16 step: 414, loss is 0.18395091593265533\n",
      "epoch: 16 step: 415, loss is 0.2067091464996338\n",
      "epoch: 16 step: 416, loss is 0.2195974737405777\n",
      "epoch: 16 step: 417, loss is 0.18880556523799896\n",
      "epoch: 16 step: 418, loss is 0.11548800021409988\n",
      "epoch: 16 step: 419, loss is 0.1775287538766861\n",
      "epoch: 16 step: 420, loss is 0.12336236238479614\n",
      "epoch: 16 step: 421, loss is 0.2337193489074707\n",
      "epoch: 16 step: 422, loss is 0.28543272614479065\n",
      "epoch: 16 step: 423, loss is 0.31360378861427307\n",
      "epoch: 16 step: 424, loss is 0.2551863491535187\n",
      "epoch: 16 step: 425, loss is 0.13609136641025543\n",
      "epoch: 16 step: 426, loss is 0.2794589698314667\n",
      "epoch: 16 step: 427, loss is 0.2708103656768799\n",
      "epoch: 16 step: 428, loss is 0.27194449305534363\n",
      "epoch: 16 step: 429, loss is 0.4628153145313263\n",
      "epoch: 16 step: 430, loss is 0.213254913687706\n",
      "epoch: 16 step: 431, loss is 0.1899953931570053\n",
      "epoch: 16 step: 432, loss is 0.20718927681446075\n",
      "epoch: 16 step: 433, loss is 0.17833521962165833\n",
      "epoch: 16 step: 434, loss is 0.1639503836631775\n",
      "epoch: 16 step: 435, loss is 0.12657178938388824\n",
      "epoch: 16 step: 436, loss is 0.2599155902862549\n",
      "epoch: 16 step: 437, loss is 0.09517862647771835\n",
      "epoch: 16 step: 438, loss is 0.24645093083381653\n",
      "epoch: 16 step: 439, loss is 0.27705901861190796\n",
      "epoch: 16 step: 440, loss is 0.11191567778587341\n",
      "epoch: 16 step: 441, loss is 0.1544472575187683\n",
      "epoch: 16 step: 442, loss is 0.23390130698680878\n",
      "epoch: 16 step: 443, loss is 0.15517833828926086\n",
      "epoch: 16 step: 444, loss is 0.3037586212158203\n",
      "epoch: 16 step: 445, loss is 0.15940099954605103\n",
      "epoch: 16 step: 446, loss is 0.1991809904575348\n",
      "epoch: 16 step: 447, loss is 0.24387924373149872\n",
      "epoch: 16 step: 448, loss is 0.1630006581544876\n",
      "epoch: 16 step: 449, loss is 0.1400613784790039\n",
      "epoch: 16 step: 450, loss is 0.12870904803276062\n",
      "epoch: 16 step: 451, loss is 0.1523902267217636\n",
      "epoch: 16 step: 452, loss is 0.2533521056175232\n",
      "epoch: 16 step: 453, loss is 0.30702266097068787\n",
      "epoch: 16 step: 454, loss is 0.2263696938753128\n",
      "epoch: 16 step: 455, loss is 0.3925710618495941\n",
      "epoch: 16 step: 456, loss is 0.16809174418449402\n",
      "epoch: 16 step: 457, loss is 0.17256949841976166\n",
      "epoch: 16 step: 458, loss is 0.26598167419433594\n",
      "epoch: 16 step: 459, loss is 0.16434834897518158\n",
      "epoch: 16 step: 460, loss is 0.14219172298908234\n",
      "epoch: 16 step: 461, loss is 0.2681402564048767\n",
      "epoch: 16 step: 462, loss is 0.24157263338565826\n",
      "epoch: 16 step: 463, loss is 0.15157447755336761\n",
      "epoch: 16 step: 464, loss is 0.25875425338745117\n",
      "epoch: 16 step: 465, loss is 0.17709603905677795\n",
      "epoch: 16 step: 466, loss is 0.26003479957580566\n",
      "epoch: 16 step: 467, loss is 0.33141446113586426\n",
      "epoch: 16 step: 468, loss is 0.24679377675056458\n",
      "epoch: 16 step: 469, loss is 0.3075508773326874\n",
      "epoch: 16 step: 470, loss is 0.13329775631427765\n",
      "epoch: 16 step: 471, loss is 0.14724208414554596\n",
      "epoch: 16 step: 472, loss is 0.20865854620933533\n",
      "epoch: 16 step: 473, loss is 0.2506396770477295\n",
      "epoch: 16 step: 474, loss is 0.26606759428977966\n",
      "epoch: 16 step: 475, loss is 0.25573933124542236\n",
      "epoch: 16 step: 476, loss is 0.2769532799720764\n",
      "epoch: 16 step: 477, loss is 0.19701382517814636\n",
      "epoch: 16 step: 478, loss is 0.2178294062614441\n",
      "epoch: 16 step: 479, loss is 0.38494181632995605\n",
      "epoch: 16 step: 480, loss is 0.19626392424106598\n",
      "epoch: 16 step: 481, loss is 0.17987599968910217\n",
      "epoch: 16 step: 482, loss is 0.2651080787181854\n",
      "epoch: 16 step: 483, loss is 0.18982915580272675\n",
      "epoch: 16 step: 484, loss is 0.19520927965641022\n",
      "epoch: 16 step: 485, loss is 0.17496468126773834\n",
      "epoch: 16 step: 486, loss is 0.2037603259086609\n",
      "epoch: 16 step: 487, loss is 0.32574349641799927\n",
      "epoch: 16 step: 488, loss is 0.37671923637390137\n",
      "epoch: 16 step: 489, loss is 0.16264012455940247\n",
      "epoch: 16 step: 490, loss is 0.18352572619915009\n",
      "epoch: 16 step: 491, loss is 0.09148790687322617\n",
      "epoch: 16 step: 492, loss is 0.15670552849769592\n",
      "epoch: 16 step: 493, loss is 0.37575921416282654\n",
      "epoch: 16 step: 494, loss is 0.3177849352359772\n",
      "epoch: 16 step: 495, loss is 0.3147534430027008\n",
      "epoch: 16 step: 496, loss is 0.22864700853824615\n",
      "epoch: 16 step: 497, loss is 0.34232383966445923\n",
      "epoch: 16 step: 498, loss is 0.19223615527153015\n",
      "epoch: 16 step: 499, loss is 0.10546328872442245\n",
      "epoch: 16 step: 500, loss is 0.07380218803882599\n",
      "epoch: 16 step: 501, loss is 0.26112955808639526\n",
      "epoch: 16 step: 502, loss is 0.2801413834095001\n",
      "epoch: 16 step: 503, loss is 0.21569247543811798\n",
      "epoch: 16 step: 504, loss is 0.20140200853347778\n",
      "epoch: 16 step: 505, loss is 0.23443178832530975\n",
      "epoch: 16 step: 506, loss is 0.20128294825553894\n",
      "epoch: 16 step: 507, loss is 0.20533481240272522\n",
      "epoch: 16 step: 508, loss is 0.2637522220611572\n",
      "epoch: 16 step: 509, loss is 0.19225451350212097\n",
      "epoch: 16 step: 510, loss is 0.21306197345256805\n",
      "epoch: 16 step: 511, loss is 0.2179252952337265\n",
      "epoch: 16 step: 512, loss is 0.26649364829063416\n",
      "epoch: 16 step: 513, loss is 0.2300557792186737\n",
      "epoch: 16 step: 514, loss is 0.2345435470342636\n",
      "epoch: 16 step: 515, loss is 0.2533690929412842\n",
      "epoch: 16 step: 516, loss is 0.1323670893907547\n",
      "epoch: 16 step: 517, loss is 0.14591649174690247\n",
      "epoch: 16 step: 518, loss is 0.30126866698265076\n",
      "epoch: 16 step: 519, loss is 0.3050931990146637\n",
      "epoch: 16 step: 520, loss is 0.1318550556898117\n",
      "epoch: 16 step: 521, loss is 0.18138109147548676\n",
      "epoch: 16 step: 522, loss is 0.16061216592788696\n",
      "epoch: 16 step: 523, loss is 0.1962687373161316\n",
      "epoch: 16 step: 524, loss is 0.3628080189228058\n",
      "epoch: 16 step: 525, loss is 0.16043415665626526\n",
      "epoch: 16 step: 526, loss is 0.08272892981767654\n",
      "epoch: 16 step: 527, loss is 0.20610451698303223\n",
      "epoch: 16 step: 528, loss is 0.18951398134231567\n",
      "epoch: 16 step: 529, loss is 0.28924715518951416\n",
      "epoch: 16 step: 530, loss is 0.3583031892776489\n",
      "epoch: 16 step: 531, loss is 0.5520073771476746\n",
      "epoch: 16 step: 532, loss is 0.19737392663955688\n",
      "epoch: 16 step: 533, loss is 0.17597393691539764\n",
      "epoch: 16 step: 534, loss is 0.19206982851028442\n",
      "epoch: 16 step: 535, loss is 0.10390979796648026\n",
      "epoch: 16 step: 536, loss is 0.32367488741874695\n",
      "epoch: 16 step: 537, loss is 0.2242310345172882\n",
      "epoch: 16 step: 538, loss is 0.2709716558456421\n",
      "epoch: 16 step: 539, loss is 0.13854658603668213\n",
      "epoch: 16 step: 540, loss is 0.1299046277999878\n",
      "epoch: 16 step: 541, loss is 0.16536115109920502\n",
      "epoch: 16 step: 542, loss is 0.09720440953969955\n",
      "epoch: 16 step: 543, loss is 0.1456967145204544\n",
      "epoch: 16 step: 544, loss is 0.18846896290779114\n",
      "epoch: 16 step: 545, loss is 0.2928713262081146\n",
      "epoch: 16 step: 546, loss is 0.3460060954093933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 547, loss is 0.25509676337242126\n",
      "epoch: 16 step: 548, loss is 0.2132309526205063\n",
      "epoch: 16 step: 549, loss is 0.3127261996269226\n",
      "epoch: 16 step: 550, loss is 0.12990771234035492\n",
      "epoch: 16 step: 551, loss is 0.13374559581279755\n",
      "epoch: 16 step: 552, loss is 0.12299515306949615\n",
      "epoch: 16 step: 553, loss is 0.34317323565483093\n",
      "epoch: 16 step: 554, loss is 0.3507857024669647\n",
      "epoch: 16 step: 555, loss is 0.16395840048789978\n",
      "epoch: 16 step: 556, loss is 0.15594153106212616\n",
      "epoch: 16 step: 557, loss is 0.19102849066257477\n",
      "epoch: 16 step: 558, loss is 0.20276926457881927\n",
      "epoch: 16 step: 559, loss is 0.3162689805030823\n",
      "epoch: 16 step: 560, loss is 0.16700375080108643\n",
      "epoch: 16 step: 561, loss is 0.19935910403728485\n",
      "epoch: 16 step: 562, loss is 0.17371240258216858\n",
      "epoch: 16 step: 563, loss is 0.24268996715545654\n",
      "epoch: 16 step: 564, loss is 0.14881044626235962\n",
      "epoch: 16 step: 565, loss is 0.10824602097272873\n",
      "epoch: 16 step: 566, loss is 0.24992837011814117\n",
      "epoch: 16 step: 567, loss is 0.22483767569065094\n",
      "epoch: 16 step: 568, loss is 0.22250956296920776\n",
      "epoch: 16 step: 569, loss is 0.15633656084537506\n",
      "epoch: 16 step: 570, loss is 0.2700565457344055\n",
      "epoch: 16 step: 571, loss is 0.2523878216743469\n",
      "epoch: 16 step: 572, loss is 0.20069290697574615\n",
      "epoch: 16 step: 573, loss is 0.07604347169399261\n",
      "epoch: 16 step: 574, loss is 0.15314429998397827\n",
      "epoch: 16 step: 575, loss is 0.13578589260578156\n",
      "epoch: 16 step: 576, loss is 0.2106872946023941\n",
      "epoch: 16 step: 577, loss is 0.14331872761249542\n",
      "epoch: 16 step: 578, loss is 0.24238774180412292\n",
      "epoch: 16 step: 579, loss is 0.12661370635032654\n",
      "epoch: 16 step: 580, loss is 0.25006434321403503\n",
      "epoch: 16 step: 581, loss is 0.29068565368652344\n",
      "epoch: 16 step: 582, loss is 0.3775803744792938\n",
      "epoch: 16 step: 583, loss is 0.18138808012008667\n",
      "epoch: 16 step: 584, loss is 0.1780540496110916\n",
      "epoch: 16 step: 585, loss is 0.24679242074489594\n",
      "epoch: 16 step: 586, loss is 0.2587742507457733\n",
      "epoch: 16 step: 587, loss is 0.09708831459283829\n",
      "epoch: 16 step: 588, loss is 0.2719569802284241\n",
      "epoch: 16 step: 589, loss is 0.24456699192523956\n",
      "epoch: 16 step: 590, loss is 0.19860880076885223\n",
      "epoch: 16 step: 591, loss is 0.19000974297523499\n",
      "epoch: 16 step: 592, loss is 0.1528516262769699\n",
      "epoch: 16 step: 593, loss is 0.2139289379119873\n",
      "epoch: 16 step: 594, loss is 0.29142284393310547\n",
      "epoch: 16 step: 595, loss is 0.27565038204193115\n",
      "epoch: 16 step: 596, loss is 0.19983530044555664\n",
      "epoch: 16 step: 597, loss is 0.15515880286693573\n",
      "epoch: 16 step: 598, loss is 0.15445579588413239\n",
      "epoch: 16 step: 599, loss is 0.18825843930244446\n",
      "epoch: 16 step: 600, loss is 0.1377715766429901\n",
      "epoch: 16 step: 601, loss is 0.08850531280040741\n",
      "epoch: 16 step: 602, loss is 0.2960086762905121\n",
      "epoch: 16 step: 603, loss is 0.2008720189332962\n",
      "epoch: 16 step: 604, loss is 0.14252805709838867\n",
      "epoch: 16 step: 605, loss is 0.338255375623703\n",
      "epoch: 16 step: 606, loss is 0.21848246455192566\n",
      "epoch: 16 step: 607, loss is 0.22027871012687683\n",
      "epoch: 16 step: 608, loss is 0.241646870970726\n",
      "epoch: 16 step: 609, loss is 0.09205085039138794\n",
      "epoch: 16 step: 610, loss is 0.2347605973482132\n",
      "epoch: 16 step: 611, loss is 0.3144272565841675\n",
      "epoch: 16 step: 612, loss is 0.34840890765190125\n",
      "epoch: 16 step: 613, loss is 0.29363813996315\n",
      "epoch: 16 step: 614, loss is 0.17914411425590515\n",
      "epoch: 16 step: 615, loss is 0.21596775949001312\n",
      "epoch: 16 step: 616, loss is 0.30064254999160767\n",
      "epoch: 16 step: 617, loss is 0.211446613073349\n",
      "epoch: 16 step: 618, loss is 0.20939181745052338\n",
      "epoch: 16 step: 619, loss is 0.24652688205242157\n",
      "epoch: 16 step: 620, loss is 0.3520966172218323\n",
      "epoch: 16 step: 621, loss is 0.2610938251018524\n",
      "epoch: 16 step: 622, loss is 0.3091695010662079\n",
      "epoch: 16 step: 623, loss is 0.3709469735622406\n",
      "epoch: 16 step: 624, loss is 0.12129800021648407\n",
      "epoch: 16 step: 625, loss is 0.33618077635765076\n",
      "epoch: 16 step: 626, loss is 0.27837589383125305\n",
      "epoch: 16 step: 627, loss is 0.26702508330345154\n",
      "epoch: 16 step: 628, loss is 0.1604933887720108\n",
      "epoch: 16 step: 629, loss is 0.2957417964935303\n",
      "epoch: 16 step: 630, loss is 0.2881462574005127\n",
      "epoch: 16 step: 631, loss is 0.2037322223186493\n",
      "epoch: 16 step: 632, loss is 0.26079061627388\n",
      "epoch: 16 step: 633, loss is 0.11613867431879044\n",
      "epoch: 16 step: 634, loss is 0.22475460171699524\n",
      "epoch: 16 step: 635, loss is 0.3589949905872345\n",
      "epoch: 16 step: 636, loss is 0.16413386166095734\n",
      "epoch: 16 step: 637, loss is 0.21165116131305695\n",
      "epoch: 16 step: 638, loss is 0.14581914246082306\n",
      "epoch: 16 step: 639, loss is 0.22761279344558716\n",
      "epoch: 16 step: 640, loss is 0.1274084895849228\n",
      "epoch: 16 step: 641, loss is 0.22719387710094452\n",
      "epoch: 16 step: 642, loss is 0.11992759257555008\n",
      "epoch: 16 step: 643, loss is 0.3222595751285553\n",
      "epoch: 16 step: 644, loss is 0.0877596065402031\n",
      "epoch: 16 step: 645, loss is 0.1986306756734848\n",
      "epoch: 16 step: 646, loss is 0.2866203486919403\n",
      "epoch: 16 step: 647, loss is 0.18816013634204865\n",
      "epoch: 16 step: 648, loss is 0.22054848074913025\n",
      "epoch: 16 step: 649, loss is 0.21841782331466675\n",
      "epoch: 16 step: 650, loss is 0.41622135043144226\n",
      "epoch: 16 step: 651, loss is 0.21198907494544983\n",
      "epoch: 16 step: 652, loss is 0.4397624731063843\n",
      "epoch: 16 step: 653, loss is 0.2589535713195801\n",
      "epoch: 16 step: 654, loss is 0.17387856543064117\n",
      "epoch: 16 step: 655, loss is 0.13885252177715302\n",
      "epoch: 16 step: 656, loss is 0.2524401843547821\n",
      "epoch: 16 step: 657, loss is 0.28511956334114075\n",
      "epoch: 16 step: 658, loss is 0.1351524442434311\n",
      "epoch: 16 step: 659, loss is 0.22420750558376312\n",
      "epoch: 16 step: 660, loss is 0.2415604442358017\n",
      "epoch: 16 step: 661, loss is 0.20507632195949554\n",
      "epoch: 16 step: 662, loss is 0.13442082703113556\n",
      "epoch: 16 step: 663, loss is 0.17565838992595673\n",
      "epoch: 16 step: 664, loss is 0.42406538128852844\n",
      "epoch: 16 step: 665, loss is 0.1160721704363823\n",
      "epoch: 16 step: 666, loss is 0.2745947241783142\n",
      "epoch: 16 step: 667, loss is 0.16498400270938873\n",
      "epoch: 16 step: 668, loss is 0.1216367781162262\n",
      "epoch: 16 step: 669, loss is 0.16428019106388092\n",
      "epoch: 16 step: 670, loss is 0.2401757836341858\n",
      "epoch: 16 step: 671, loss is 0.16152524948120117\n",
      "epoch: 16 step: 672, loss is 0.20698967576026917\n",
      "epoch: 16 step: 673, loss is 0.13057690858840942\n",
      "epoch: 16 step: 674, loss is 0.15281973779201508\n",
      "epoch: 16 step: 675, loss is 0.1361122876405716\n",
      "epoch: 16 step: 676, loss is 0.26501989364624023\n",
      "epoch: 16 step: 677, loss is 0.2839740812778473\n",
      "epoch: 16 step: 678, loss is 0.26486077904701233\n",
      "epoch: 16 step: 679, loss is 0.34887945652008057\n",
      "epoch: 16 step: 680, loss is 0.27156683802604675\n",
      "epoch: 16 step: 681, loss is 0.1851416379213333\n",
      "epoch: 16 step: 682, loss is 0.15292906761169434\n",
      "epoch: 16 step: 683, loss is 0.30767562985420227\n",
      "epoch: 16 step: 684, loss is 0.2450626790523529\n",
      "epoch: 16 step: 685, loss is 0.4181235730648041\n",
      "epoch: 16 step: 686, loss is 0.1743398904800415\n",
      "epoch: 16 step: 687, loss is 0.20089897513389587\n",
      "epoch: 16 step: 688, loss is 0.2676166594028473\n",
      "epoch: 16 step: 689, loss is 0.17756612598896027\n",
      "epoch: 16 step: 690, loss is 0.15210789442062378\n",
      "epoch: 16 step: 691, loss is 0.2872563898563385\n",
      "epoch: 16 step: 692, loss is 0.3190154731273651\n",
      "epoch: 16 step: 693, loss is 0.13922575116157532\n",
      "epoch: 16 step: 694, loss is 0.14004230499267578\n",
      "epoch: 16 step: 695, loss is 0.11099208891391754\n",
      "epoch: 16 step: 696, loss is 0.19954147934913635\n",
      "epoch: 16 step: 697, loss is 0.1954488903284073\n",
      "epoch: 16 step: 698, loss is 0.186985045671463\n",
      "epoch: 16 step: 699, loss is 0.07591572403907776\n",
      "epoch: 16 step: 700, loss is 0.26288625597953796\n",
      "epoch: 16 step: 701, loss is 0.2663722038269043\n",
      "epoch: 16 step: 702, loss is 0.2004248946905136\n",
      "epoch: 16 step: 703, loss is 0.25499624013900757\n",
      "epoch: 16 step: 704, loss is 0.16079778969287872\n",
      "epoch: 16 step: 705, loss is 0.2044645994901657\n",
      "epoch: 16 step: 706, loss is 0.1265621781349182\n",
      "epoch: 16 step: 707, loss is 0.08789629489183426\n",
      "epoch: 16 step: 708, loss is 0.1094188466668129\n",
      "epoch: 16 step: 709, loss is 0.19994038343429565\n",
      "epoch: 16 step: 710, loss is 0.4648723900318146\n",
      "epoch: 16 step: 711, loss is 0.2491769790649414\n",
      "epoch: 16 step: 712, loss is 0.0728183463215828\n",
      "epoch: 16 step: 713, loss is 0.2576315104961395\n",
      "epoch: 16 step: 714, loss is 0.3251151740550995\n",
      "epoch: 16 step: 715, loss is 0.3083973526954651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 716, loss is 0.20348580181598663\n",
      "epoch: 16 step: 717, loss is 0.2077246606349945\n",
      "epoch: 16 step: 718, loss is 0.17466749250888824\n",
      "epoch: 16 step: 719, loss is 0.17490074038505554\n",
      "epoch: 16 step: 720, loss is 0.08931650966405869\n",
      "epoch: 16 step: 721, loss is 0.19737111032009125\n",
      "epoch: 16 step: 722, loss is 0.18134371936321259\n",
      "epoch: 16 step: 723, loss is 0.12565477192401886\n",
      "epoch: 16 step: 724, loss is 0.23894521594047546\n",
      "epoch: 16 step: 725, loss is 0.12926866114139557\n",
      "epoch: 16 step: 726, loss is 0.15619024634361267\n",
      "epoch: 16 step: 727, loss is 0.19593310356140137\n",
      "epoch: 16 step: 728, loss is 0.24213892221450806\n",
      "epoch: 16 step: 729, loss is 0.3170269727706909\n",
      "epoch: 16 step: 730, loss is 0.18300014734268188\n",
      "epoch: 16 step: 731, loss is 0.27502110600471497\n",
      "epoch: 16 step: 732, loss is 0.33062100410461426\n",
      "epoch: 16 step: 733, loss is 0.20045185089111328\n",
      "epoch: 16 step: 734, loss is 0.24808922410011292\n",
      "epoch: 16 step: 735, loss is 0.14944520592689514\n",
      "epoch: 16 step: 736, loss is 0.1783820390701294\n",
      "epoch: 16 step: 737, loss is 0.17338022589683533\n",
      "epoch: 16 step: 738, loss is 0.22956229746341705\n",
      "epoch: 16 step: 739, loss is 0.27171117067337036\n",
      "epoch: 16 step: 740, loss is 0.2146889567375183\n",
      "epoch: 16 step: 741, loss is 0.19957374036312103\n",
      "epoch: 16 step: 742, loss is 0.18255411088466644\n",
      "epoch: 16 step: 743, loss is 0.2041391283273697\n",
      "epoch: 16 step: 744, loss is 0.3198012411594391\n",
      "epoch: 16 step: 745, loss is 0.25716692209243774\n",
      "epoch: 16 step: 746, loss is 0.20345674455165863\n",
      "epoch: 16 step: 747, loss is 0.1840074211359024\n",
      "epoch: 16 step: 748, loss is 0.2091689109802246\n",
      "epoch: 16 step: 749, loss is 0.12345370650291443\n",
      "epoch: 16 step: 750, loss is 0.32187584042549133\n",
      "epoch: 16 step: 751, loss is 0.38913601636886597\n",
      "epoch: 16 step: 752, loss is 0.4359439015388489\n",
      "epoch: 16 step: 753, loss is 0.324974924325943\n",
      "epoch: 16 step: 754, loss is 0.2759357690811157\n",
      "epoch: 16 step: 755, loss is 0.16915160417556763\n",
      "epoch: 16 step: 756, loss is 0.13231463730335236\n",
      "epoch: 16 step: 757, loss is 0.21947063505649567\n",
      "epoch: 16 step: 758, loss is 0.1400132030248642\n",
      "epoch: 16 step: 759, loss is 0.32249608635902405\n",
      "epoch: 16 step: 760, loss is 0.19035768508911133\n",
      "epoch: 16 step: 761, loss is 0.09298539161682129\n",
      "epoch: 16 step: 762, loss is 0.21580545604228973\n",
      "epoch: 16 step: 763, loss is 0.3151918649673462\n",
      "epoch: 16 step: 764, loss is 0.3136911392211914\n",
      "epoch: 16 step: 765, loss is 0.272982656955719\n",
      "epoch: 16 step: 766, loss is 0.38640281558036804\n",
      "epoch: 16 step: 767, loss is 0.2146201878786087\n",
      "epoch: 16 step: 768, loss is 0.0940241888165474\n",
      "epoch: 16 step: 769, loss is 0.32165029644966125\n",
      "epoch: 16 step: 770, loss is 0.2665356397628784\n",
      "epoch: 16 step: 771, loss is 0.29926449060440063\n",
      "epoch: 16 step: 772, loss is 0.2039472460746765\n",
      "epoch: 16 step: 773, loss is 0.2501133680343628\n",
      "epoch: 16 step: 774, loss is 0.15529140830039978\n",
      "epoch: 16 step: 775, loss is 0.23566694557666779\n",
      "epoch: 16 step: 776, loss is 0.15295352041721344\n",
      "epoch: 16 step: 777, loss is 0.1794891357421875\n",
      "epoch: 16 step: 778, loss is 0.29316291213035583\n",
      "epoch: 16 step: 779, loss is 0.08003726601600647\n",
      "epoch: 16 step: 780, loss is 0.06505488604307175\n",
      "epoch: 16 step: 781, loss is 0.18691059947013855\n",
      "epoch: 16 step: 782, loss is 0.3325500190258026\n",
      "epoch: 16 step: 783, loss is 0.4286596179008484\n",
      "epoch: 16 step: 784, loss is 0.3507406711578369\n",
      "epoch: 16 step: 785, loss is 0.32440385222435\n",
      "epoch: 16 step: 786, loss is 0.11768759787082672\n",
      "epoch: 16 step: 787, loss is 0.13329383730888367\n",
      "epoch: 16 step: 788, loss is 0.311318039894104\n",
      "epoch: 16 step: 789, loss is 0.13386476039886475\n",
      "epoch: 16 step: 790, loss is 0.20629362761974335\n",
      "epoch: 16 step: 791, loss is 0.2126808762550354\n",
      "epoch: 16 step: 792, loss is 0.2371968924999237\n",
      "epoch: 16 step: 793, loss is 0.13299860060214996\n",
      "epoch: 16 step: 794, loss is 0.1882992535829544\n",
      "epoch: 16 step: 795, loss is 0.12552712857723236\n",
      "epoch: 16 step: 796, loss is 0.12541325390338898\n",
      "epoch: 16 step: 797, loss is 0.21825917065143585\n",
      "epoch: 16 step: 798, loss is 0.15868161618709564\n",
      "epoch: 16 step: 799, loss is 0.1331969052553177\n",
      "epoch: 16 step: 800, loss is 0.2603277564048767\n",
      "epoch: 16 step: 801, loss is 0.29717159271240234\n",
      "epoch: 16 step: 802, loss is 0.1890840381383896\n",
      "epoch: 16 step: 803, loss is 0.26025423407554626\n",
      "epoch: 16 step: 804, loss is 0.2831355929374695\n",
      "epoch: 16 step: 805, loss is 0.23523926734924316\n",
      "epoch: 16 step: 806, loss is 0.2577940821647644\n",
      "epoch: 16 step: 807, loss is 0.2170657366514206\n",
      "epoch: 16 step: 808, loss is 0.2154570072889328\n",
      "epoch: 16 step: 809, loss is 0.3039986193180084\n",
      "epoch: 16 step: 810, loss is 0.4640244245529175\n",
      "epoch: 16 step: 811, loss is 0.16420887410640717\n",
      "epoch: 16 step: 812, loss is 0.18254919350147247\n",
      "epoch: 16 step: 813, loss is 0.1449357569217682\n",
      "epoch: 16 step: 814, loss is 0.18269498646259308\n",
      "epoch: 16 step: 815, loss is 0.10980024933815002\n",
      "epoch: 16 step: 816, loss is 0.24167780578136444\n",
      "epoch: 16 step: 817, loss is 0.30761605501174927\n",
      "epoch: 16 step: 818, loss is 0.22374394536018372\n",
      "epoch: 16 step: 819, loss is 0.32565295696258545\n",
      "epoch: 16 step: 820, loss is 0.2041478157043457\n",
      "epoch: 16 step: 821, loss is 0.22174981236457825\n",
      "epoch: 16 step: 822, loss is 0.18076574802398682\n",
      "epoch: 16 step: 823, loss is 0.17373710870742798\n",
      "epoch: 16 step: 824, loss is 0.14243945479393005\n",
      "epoch: 16 step: 825, loss is 0.08334106206893921\n",
      "epoch: 16 step: 826, loss is 0.21659371256828308\n",
      "epoch: 16 step: 827, loss is 0.0897446796298027\n",
      "epoch: 16 step: 828, loss is 0.16930806636810303\n",
      "epoch: 16 step: 829, loss is 0.2014240324497223\n",
      "epoch: 16 step: 830, loss is 0.22119863331317902\n",
      "epoch: 16 step: 831, loss is 0.2867068350315094\n",
      "epoch: 16 step: 832, loss is 0.27594128251075745\n",
      "epoch: 16 step: 833, loss is 0.23698925971984863\n",
      "epoch: 16 step: 834, loss is 0.2725347578525543\n",
      "epoch: 16 step: 835, loss is 0.2010006159543991\n",
      "epoch: 16 step: 836, loss is 0.27744078636169434\n",
      "epoch: 16 step: 837, loss is 0.17327547073364258\n",
      "epoch: 16 step: 838, loss is 0.16855169832706451\n",
      "epoch: 16 step: 839, loss is 0.18782517313957214\n",
      "epoch: 16 step: 840, loss is 0.12867972254753113\n",
      "epoch: 16 step: 841, loss is 0.2410302758216858\n",
      "epoch: 16 step: 842, loss is 0.278372198343277\n",
      "epoch: 16 step: 843, loss is 0.23160085082054138\n",
      "epoch: 16 step: 844, loss is 0.2942088842391968\n",
      "epoch: 16 step: 845, loss is 0.1600005030632019\n",
      "epoch: 16 step: 846, loss is 0.14165571331977844\n",
      "epoch: 16 step: 847, loss is 0.10884547978639603\n",
      "epoch: 16 step: 848, loss is 0.17889998853206635\n",
      "epoch: 16 step: 849, loss is 0.1365300416946411\n",
      "epoch: 16 step: 850, loss is 0.19897454977035522\n",
      "epoch: 16 step: 851, loss is 0.09233821928501129\n",
      "epoch: 16 step: 852, loss is 0.2692004442214966\n",
      "epoch: 16 step: 853, loss is 0.2701680660247803\n",
      "epoch: 16 step: 854, loss is 0.23953770101070404\n",
      "epoch: 16 step: 855, loss is 0.0925954282283783\n",
      "epoch: 16 step: 856, loss is 0.1719978302717209\n",
      "epoch: 16 step: 857, loss is 0.1976669728755951\n",
      "epoch: 16 step: 858, loss is 0.18993908166885376\n",
      "epoch: 16 step: 859, loss is 0.34421634674072266\n",
      "epoch: 16 step: 860, loss is 0.19283169507980347\n",
      "epoch: 16 step: 861, loss is 0.13961368799209595\n",
      "epoch: 16 step: 862, loss is 0.2718285918235779\n",
      "epoch: 16 step: 863, loss is 0.2891336679458618\n",
      "epoch: 16 step: 864, loss is 0.47840532660484314\n",
      "epoch: 16 step: 865, loss is 0.1784670203924179\n",
      "epoch: 16 step: 866, loss is 0.2359786331653595\n",
      "epoch: 16 step: 867, loss is 0.1431483030319214\n",
      "epoch: 16 step: 868, loss is 0.18164585530757904\n",
      "epoch: 16 step: 869, loss is 0.16701486706733704\n",
      "epoch: 16 step: 870, loss is 0.18716523051261902\n",
      "epoch: 16 step: 871, loss is 0.15616068243980408\n",
      "epoch: 16 step: 872, loss is 0.10843612253665924\n",
      "epoch: 16 step: 873, loss is 0.09240324050188065\n",
      "epoch: 16 step: 874, loss is 0.25367164611816406\n",
      "epoch: 16 step: 875, loss is 0.29067859053611755\n",
      "epoch: 16 step: 876, loss is 0.13025112450122833\n",
      "epoch: 16 step: 877, loss is 0.262095183134079\n",
      "epoch: 16 step: 878, loss is 0.25028061866760254\n",
      "epoch: 16 step: 879, loss is 0.31382375955581665\n",
      "epoch: 16 step: 880, loss is 0.24536296725273132\n",
      "epoch: 16 step: 881, loss is 0.28742316365242004\n",
      "epoch: 16 step: 882, loss is 0.17756439745426178\n",
      "epoch: 16 step: 883, loss is 0.4544398784637451\n",
      "epoch: 16 step: 884, loss is 0.18814976513385773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 885, loss is 0.24264965951442719\n",
      "epoch: 16 step: 886, loss is 0.12133143097162247\n",
      "epoch: 16 step: 887, loss is 0.09033965319395065\n",
      "epoch: 16 step: 888, loss is 0.14661140739917755\n",
      "epoch: 16 step: 889, loss is 0.24399133026599884\n",
      "epoch: 16 step: 890, loss is 0.21441759169101715\n",
      "epoch: 16 step: 891, loss is 0.20698282122612\n",
      "epoch: 16 step: 892, loss is 0.22798015177249908\n",
      "epoch: 16 step: 893, loss is 0.2375199943780899\n",
      "epoch: 16 step: 894, loss is 0.2176860272884369\n",
      "epoch: 16 step: 895, loss is 0.16450656950473785\n",
      "epoch: 16 step: 896, loss is 0.31415730714797974\n",
      "epoch: 16 step: 897, loss is 0.1972377598285675\n",
      "epoch: 16 step: 898, loss is 0.32578814029693604\n",
      "epoch: 16 step: 899, loss is 0.26936012506484985\n",
      "epoch: 16 step: 900, loss is 0.2922040522098541\n",
      "epoch: 16 step: 901, loss is 0.24985285103321075\n",
      "epoch: 16 step: 902, loss is 0.3217376172542572\n",
      "epoch: 16 step: 903, loss is 0.12232539057731628\n",
      "epoch: 16 step: 904, loss is 0.42499521374702454\n",
      "epoch: 16 step: 905, loss is 0.32556989789009094\n",
      "epoch: 16 step: 906, loss is 0.28039366006851196\n",
      "epoch: 16 step: 907, loss is 0.19246649742126465\n",
      "epoch: 16 step: 908, loss is 0.19074980914592743\n",
      "epoch: 16 step: 909, loss is 0.34706151485443115\n",
      "epoch: 16 step: 910, loss is 0.2443552166223526\n",
      "epoch: 16 step: 911, loss is 0.1188107281923294\n",
      "epoch: 16 step: 912, loss is 0.27798593044281006\n",
      "epoch: 16 step: 913, loss is 0.15296117961406708\n",
      "epoch: 16 step: 914, loss is 0.13857334852218628\n",
      "epoch: 16 step: 915, loss is 0.25810956954956055\n",
      "epoch: 16 step: 916, loss is 0.33558231592178345\n",
      "epoch: 16 step: 917, loss is 0.2766190469264984\n",
      "epoch: 16 step: 918, loss is 0.15498735010623932\n",
      "epoch: 16 step: 919, loss is 0.2386999875307083\n",
      "epoch: 16 step: 920, loss is 0.15163317322731018\n",
      "epoch: 16 step: 921, loss is 0.3426820635795593\n",
      "epoch: 16 step: 922, loss is 0.12385637313127518\n",
      "epoch: 16 step: 923, loss is 0.1743755340576172\n",
      "epoch: 16 step: 924, loss is 0.15994293987751007\n",
      "epoch: 16 step: 925, loss is 0.32910585403442383\n",
      "epoch: 16 step: 926, loss is 0.3019479513168335\n",
      "epoch: 16 step: 927, loss is 0.5684251189231873\n",
      "epoch: 16 step: 928, loss is 0.12258468568325043\n",
      "epoch: 16 step: 929, loss is 0.2062443643808365\n",
      "epoch: 16 step: 930, loss is 0.1199808269739151\n",
      "epoch: 16 step: 931, loss is 0.23494799435138702\n",
      "epoch: 16 step: 932, loss is 0.17881354689598083\n",
      "epoch: 16 step: 933, loss is 0.1490957885980606\n",
      "epoch: 16 step: 934, loss is 0.21444785594940186\n",
      "epoch: 16 step: 935, loss is 0.37363338470458984\n",
      "epoch: 16 step: 936, loss is 0.12722478806972504\n",
      "epoch: 16 step: 937, loss is 0.2303866446018219\n",
      "epoch: 17 step: 1, loss is 0.25659051537513733\n",
      "epoch: 17 step: 2, loss is 0.2589360475540161\n",
      "epoch: 17 step: 3, loss is 0.27428674697875977\n",
      "epoch: 17 step: 4, loss is 0.1724368929862976\n",
      "epoch: 17 step: 5, loss is 0.1149192526936531\n",
      "epoch: 17 step: 6, loss is 0.17422570288181305\n",
      "epoch: 17 step: 7, loss is 0.2269178181886673\n",
      "epoch: 17 step: 8, loss is 0.11834298074245453\n",
      "epoch: 17 step: 9, loss is 0.10750016570091248\n",
      "epoch: 17 step: 10, loss is 0.2879224419593811\n",
      "epoch: 17 step: 11, loss is 0.17994087934494019\n",
      "epoch: 17 step: 12, loss is 0.22556594014167786\n",
      "epoch: 17 step: 13, loss is 0.2632725238800049\n",
      "epoch: 17 step: 14, loss is 0.2153117060661316\n",
      "epoch: 17 step: 15, loss is 0.23309125006198883\n",
      "epoch: 17 step: 16, loss is 0.18866515159606934\n",
      "epoch: 17 step: 17, loss is 0.1349688470363617\n",
      "epoch: 17 step: 18, loss is 0.16845716536045074\n",
      "epoch: 17 step: 19, loss is 0.18284250795841217\n",
      "epoch: 17 step: 20, loss is 0.1095380187034607\n",
      "epoch: 17 step: 21, loss is 0.2305353730916977\n",
      "epoch: 17 step: 22, loss is 0.262739360332489\n",
      "epoch: 17 step: 23, loss is 0.1478116661310196\n",
      "epoch: 17 step: 24, loss is 0.21491758525371552\n",
      "epoch: 17 step: 25, loss is 0.23396871984004974\n",
      "epoch: 17 step: 26, loss is 0.24597525596618652\n",
      "epoch: 17 step: 27, loss is 0.2246474325656891\n",
      "epoch: 17 step: 28, loss is 0.21030360460281372\n",
      "epoch: 17 step: 29, loss is 0.14939144253730774\n",
      "epoch: 17 step: 30, loss is 0.2836943566799164\n",
      "epoch: 17 step: 31, loss is 0.2046593874692917\n",
      "epoch: 17 step: 32, loss is 0.2518637478351593\n",
      "epoch: 17 step: 33, loss is 0.2027607411146164\n",
      "epoch: 17 step: 34, loss is 0.217492938041687\n",
      "epoch: 17 step: 35, loss is 0.21672576665878296\n",
      "epoch: 17 step: 36, loss is 0.28006964921951294\n",
      "epoch: 17 step: 37, loss is 0.20728778839111328\n",
      "epoch: 17 step: 38, loss is 0.14959633350372314\n",
      "epoch: 17 step: 39, loss is 0.3821503221988678\n",
      "epoch: 17 step: 40, loss is 0.2499098926782608\n",
      "epoch: 17 step: 41, loss is 0.2629838287830353\n",
      "epoch: 17 step: 42, loss is 0.3025856018066406\n",
      "epoch: 17 step: 43, loss is 0.24609282612800598\n",
      "epoch: 17 step: 44, loss is 0.3591844141483307\n",
      "epoch: 17 step: 45, loss is 0.12289004027843475\n",
      "epoch: 17 step: 46, loss is 0.2007225900888443\n",
      "epoch: 17 step: 47, loss is 0.21550890803337097\n",
      "epoch: 17 step: 48, loss is 0.3992881178855896\n",
      "epoch: 17 step: 49, loss is 0.14186124503612518\n",
      "epoch: 17 step: 50, loss is 0.17281851172447205\n",
      "epoch: 17 step: 51, loss is 0.28903454542160034\n",
      "epoch: 17 step: 52, loss is 0.239601731300354\n",
      "epoch: 17 step: 53, loss is 0.09677143394947052\n",
      "epoch: 17 step: 54, loss is 0.2101684808731079\n",
      "epoch: 17 step: 55, loss is 0.15908928215503693\n",
      "epoch: 17 step: 56, loss is 0.25967180728912354\n",
      "epoch: 17 step: 57, loss is 0.1637060046195984\n",
      "epoch: 17 step: 58, loss is 0.2059742510318756\n",
      "epoch: 17 step: 59, loss is 0.19904670119285583\n",
      "epoch: 17 step: 60, loss is 0.28194737434387207\n",
      "epoch: 17 step: 61, loss is 0.17392079532146454\n",
      "epoch: 17 step: 62, loss is 0.14955134689807892\n",
      "epoch: 17 step: 63, loss is 0.12744729220867157\n",
      "epoch: 17 step: 64, loss is 0.22584427893161774\n",
      "epoch: 17 step: 65, loss is 0.25964832305908203\n",
      "epoch: 17 step: 66, loss is 0.3678906559944153\n",
      "epoch: 17 step: 67, loss is 0.39298316836357117\n",
      "epoch: 17 step: 68, loss is 0.16135689616203308\n",
      "epoch: 17 step: 69, loss is 0.22422082722187042\n",
      "epoch: 17 step: 70, loss is 0.11473233997821808\n",
      "epoch: 17 step: 71, loss is 0.1616184413433075\n",
      "epoch: 17 step: 72, loss is 0.28911054134368896\n",
      "epoch: 17 step: 73, loss is 0.17175081372261047\n",
      "epoch: 17 step: 74, loss is 0.15578825771808624\n",
      "epoch: 17 step: 75, loss is 0.10101542621850967\n",
      "epoch: 17 step: 76, loss is 0.18241634964942932\n",
      "epoch: 17 step: 77, loss is 0.20130112767219543\n",
      "epoch: 17 step: 78, loss is 0.22836363315582275\n",
      "epoch: 17 step: 79, loss is 0.12657530605793\n",
      "epoch: 17 step: 80, loss is 0.16087380051612854\n",
      "epoch: 17 step: 81, loss is 0.2062905877828598\n",
      "epoch: 17 step: 82, loss is 0.3894006609916687\n",
      "epoch: 17 step: 83, loss is 0.2470705211162567\n",
      "epoch: 17 step: 84, loss is 0.4092111587524414\n",
      "epoch: 17 step: 85, loss is 0.20075257122516632\n",
      "epoch: 17 step: 86, loss is 0.20906281471252441\n",
      "epoch: 17 step: 87, loss is 0.20323412120342255\n",
      "epoch: 17 step: 88, loss is 0.30007776618003845\n",
      "epoch: 17 step: 89, loss is 0.2630956470966339\n",
      "epoch: 17 step: 90, loss is 0.16025623679161072\n",
      "epoch: 17 step: 91, loss is 0.1961192637681961\n",
      "epoch: 17 step: 92, loss is 0.26655545830726624\n",
      "epoch: 17 step: 93, loss is 0.2156539261341095\n",
      "epoch: 17 step: 94, loss is 0.14642280340194702\n",
      "epoch: 17 step: 95, loss is 0.20565520226955414\n",
      "epoch: 17 step: 96, loss is 0.18799659609794617\n",
      "epoch: 17 step: 97, loss is 0.22992242872714996\n",
      "epoch: 17 step: 98, loss is 0.15267516672611237\n",
      "epoch: 17 step: 99, loss is 0.3410344123840332\n",
      "epoch: 17 step: 100, loss is 0.2782869040966034\n",
      "epoch: 17 step: 101, loss is 0.17045871913433075\n",
      "epoch: 17 step: 102, loss is 0.2734130620956421\n",
      "epoch: 17 step: 103, loss is 0.2589362561702728\n",
      "epoch: 17 step: 104, loss is 0.22987717390060425\n",
      "epoch: 17 step: 105, loss is 0.2276863306760788\n",
      "epoch: 17 step: 106, loss is 0.2887718081474304\n",
      "epoch: 17 step: 107, loss is 0.17705486714839935\n",
      "epoch: 17 step: 108, loss is 0.27088385820388794\n",
      "epoch: 17 step: 109, loss is 0.19247639179229736\n",
      "epoch: 17 step: 110, loss is 0.1906542181968689\n",
      "epoch: 17 step: 111, loss is 0.2873998284339905\n",
      "epoch: 17 step: 112, loss is 0.11385445296764374\n",
      "epoch: 17 step: 113, loss is 0.2713537812232971\n",
      "epoch: 17 step: 114, loss is 0.2360001802444458\n",
      "epoch: 17 step: 115, loss is 0.0792454332113266\n",
      "epoch: 17 step: 116, loss is 0.1707044094800949\n",
      "epoch: 17 step: 117, loss is 0.1618185192346573\n",
      "epoch: 17 step: 118, loss is 0.1521056592464447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 119, loss is 0.15396025776863098\n",
      "epoch: 17 step: 120, loss is 0.2972187399864197\n",
      "epoch: 17 step: 121, loss is 0.2420356571674347\n",
      "epoch: 17 step: 122, loss is 0.16038642823696136\n",
      "epoch: 17 step: 123, loss is 0.12658356130123138\n",
      "epoch: 17 step: 124, loss is 0.2052866667509079\n",
      "epoch: 17 step: 125, loss is 0.3371700942516327\n",
      "epoch: 17 step: 126, loss is 0.3267610967159271\n",
      "epoch: 17 step: 127, loss is 0.21102504432201385\n",
      "epoch: 17 step: 128, loss is 0.16957420110702515\n",
      "epoch: 17 step: 129, loss is 0.14574021100997925\n",
      "epoch: 17 step: 130, loss is 0.27942126989364624\n",
      "epoch: 17 step: 131, loss is 0.2011931985616684\n",
      "epoch: 17 step: 132, loss is 0.2955053150653839\n",
      "epoch: 17 step: 133, loss is 0.3251972198486328\n",
      "epoch: 17 step: 134, loss is 0.07759441435337067\n",
      "epoch: 17 step: 135, loss is 0.24856910109519958\n",
      "epoch: 17 step: 136, loss is 0.24961289763450623\n",
      "epoch: 17 step: 137, loss is 0.14693614840507507\n",
      "epoch: 17 step: 138, loss is 0.2357531636953354\n",
      "epoch: 17 step: 139, loss is 0.2554134726524353\n",
      "epoch: 17 step: 140, loss is 0.2613101899623871\n",
      "epoch: 17 step: 141, loss is 0.23158009350299835\n",
      "epoch: 17 step: 142, loss is 0.13808463513851166\n",
      "epoch: 17 step: 143, loss is 0.08254368603229523\n",
      "epoch: 17 step: 144, loss is 0.1013004332780838\n",
      "epoch: 17 step: 145, loss is 0.2059255987405777\n",
      "epoch: 17 step: 146, loss is 0.1901065707206726\n",
      "epoch: 17 step: 147, loss is 0.47069844603538513\n",
      "epoch: 17 step: 148, loss is 0.25678327679634094\n",
      "epoch: 17 step: 149, loss is 0.1621713638305664\n",
      "epoch: 17 step: 150, loss is 0.39977604150772095\n",
      "epoch: 17 step: 151, loss is 0.2293211817741394\n",
      "epoch: 17 step: 152, loss is 0.24723322689533234\n",
      "epoch: 17 step: 153, loss is 0.1948094516992569\n",
      "epoch: 17 step: 154, loss is 0.19455023109912872\n",
      "epoch: 17 step: 155, loss is 0.13822904229164124\n",
      "epoch: 17 step: 156, loss is 0.17397470772266388\n",
      "epoch: 17 step: 157, loss is 0.15676750242710114\n",
      "epoch: 17 step: 158, loss is 0.21253931522369385\n",
      "epoch: 17 step: 159, loss is 0.13366827368736267\n",
      "epoch: 17 step: 160, loss is 0.14583057165145874\n",
      "epoch: 17 step: 161, loss is 0.13597926497459412\n",
      "epoch: 17 step: 162, loss is 0.17866353690624237\n",
      "epoch: 17 step: 163, loss is 0.057078517973423004\n",
      "epoch: 17 step: 164, loss is 0.16396622359752655\n",
      "epoch: 17 step: 165, loss is 0.2040279656648636\n",
      "epoch: 17 step: 166, loss is 0.24680745601654053\n",
      "epoch: 17 step: 167, loss is 0.1699015349149704\n",
      "epoch: 17 step: 168, loss is 0.15506069362163544\n",
      "epoch: 17 step: 169, loss is 0.17776666581630707\n",
      "epoch: 17 step: 170, loss is 0.2605481445789337\n",
      "epoch: 17 step: 171, loss is 0.23834092915058136\n",
      "epoch: 17 step: 172, loss is 0.20635394752025604\n",
      "epoch: 17 step: 173, loss is 0.06983299553394318\n",
      "epoch: 17 step: 174, loss is 0.2335411161184311\n",
      "epoch: 17 step: 175, loss is 0.08908677101135254\n",
      "epoch: 17 step: 176, loss is 0.2733939588069916\n",
      "epoch: 17 step: 177, loss is 0.1354675143957138\n",
      "epoch: 17 step: 178, loss is 0.15031008422374725\n",
      "epoch: 17 step: 179, loss is 0.36198821663856506\n",
      "epoch: 17 step: 180, loss is 0.22771871089935303\n",
      "epoch: 17 step: 181, loss is 0.063448965549469\n",
      "epoch: 17 step: 182, loss is 0.22255928814411163\n",
      "epoch: 17 step: 183, loss is 0.2932669222354889\n",
      "epoch: 17 step: 184, loss is 0.15160542726516724\n",
      "epoch: 17 step: 185, loss is 0.22819285094738007\n",
      "epoch: 17 step: 186, loss is 0.19316630065441132\n",
      "epoch: 17 step: 187, loss is 0.20912548899650574\n",
      "epoch: 17 step: 188, loss is 0.15236258506774902\n",
      "epoch: 17 step: 189, loss is 0.2283848375082016\n",
      "epoch: 17 step: 190, loss is 0.275313138961792\n",
      "epoch: 17 step: 191, loss is 0.3060176968574524\n",
      "epoch: 17 step: 192, loss is 0.11873441189527512\n",
      "epoch: 17 step: 193, loss is 0.24963626265525818\n",
      "epoch: 17 step: 194, loss is 0.40107765793800354\n",
      "epoch: 17 step: 195, loss is 0.1996452808380127\n",
      "epoch: 17 step: 196, loss is 0.20646288990974426\n",
      "epoch: 17 step: 197, loss is 0.276170015335083\n",
      "epoch: 17 step: 198, loss is 0.21504439413547516\n",
      "epoch: 17 step: 199, loss is 0.1812645047903061\n",
      "epoch: 17 step: 200, loss is 0.2868432402610779\n",
      "epoch: 17 step: 201, loss is 0.20940929651260376\n",
      "epoch: 17 step: 202, loss is 0.23599238693714142\n",
      "epoch: 17 step: 203, loss is 0.3301672041416168\n",
      "epoch: 17 step: 204, loss is 0.34522590041160583\n",
      "epoch: 17 step: 205, loss is 0.06294173002243042\n",
      "epoch: 17 step: 206, loss is 0.20812925696372986\n",
      "epoch: 17 step: 207, loss is 0.1733793020248413\n",
      "epoch: 17 step: 208, loss is 0.18249869346618652\n",
      "epoch: 17 step: 209, loss is 0.16840939223766327\n",
      "epoch: 17 step: 210, loss is 0.30264031887054443\n",
      "epoch: 17 step: 211, loss is 0.08215806633234024\n",
      "epoch: 17 step: 212, loss is 0.12947911024093628\n",
      "epoch: 17 step: 213, loss is 0.20079250633716583\n",
      "epoch: 17 step: 214, loss is 0.21553108096122742\n",
      "epoch: 17 step: 215, loss is 0.14962926506996155\n",
      "epoch: 17 step: 216, loss is 0.2183448225259781\n",
      "epoch: 17 step: 217, loss is 0.20663045346736908\n",
      "epoch: 17 step: 218, loss is 0.17460598051548004\n",
      "epoch: 17 step: 219, loss is 0.41761311888694763\n",
      "epoch: 17 step: 220, loss is 0.28885212540626526\n",
      "epoch: 17 step: 221, loss is 0.1164679303765297\n",
      "epoch: 17 step: 222, loss is 0.17056599259376526\n",
      "epoch: 17 step: 223, loss is 0.16853876411914825\n",
      "epoch: 17 step: 224, loss is 0.15644408762454987\n",
      "epoch: 17 step: 225, loss is 0.16686512529850006\n",
      "epoch: 17 step: 226, loss is 0.2793394923210144\n",
      "epoch: 17 step: 227, loss is 0.17393909394741058\n",
      "epoch: 17 step: 228, loss is 0.12910544872283936\n",
      "epoch: 17 step: 229, loss is 0.24512450397014618\n",
      "epoch: 17 step: 230, loss is 0.28062674403190613\n",
      "epoch: 17 step: 231, loss is 0.3348250389099121\n",
      "epoch: 17 step: 232, loss is 0.10794875770807266\n",
      "epoch: 17 step: 233, loss is 0.26908066868782043\n",
      "epoch: 17 step: 234, loss is 0.21867305040359497\n",
      "epoch: 17 step: 235, loss is 0.15979886054992676\n",
      "epoch: 17 step: 236, loss is 0.21606537699699402\n",
      "epoch: 17 step: 237, loss is 0.2161797136068344\n",
      "epoch: 17 step: 238, loss is 0.15465204417705536\n",
      "epoch: 17 step: 239, loss is 0.28261294960975647\n",
      "epoch: 17 step: 240, loss is 0.2438032329082489\n",
      "epoch: 17 step: 241, loss is 0.2859618663787842\n",
      "epoch: 17 step: 242, loss is 0.20273835957050323\n",
      "epoch: 17 step: 243, loss is 0.31976237893104553\n",
      "epoch: 17 step: 244, loss is 0.3199175298213959\n",
      "epoch: 17 step: 245, loss is 0.15765391290187836\n",
      "epoch: 17 step: 246, loss is 0.23306085169315338\n",
      "epoch: 17 step: 247, loss is 0.14767557382583618\n",
      "epoch: 17 step: 248, loss is 0.11478803306818008\n",
      "epoch: 17 step: 249, loss is 0.3646414577960968\n",
      "epoch: 17 step: 250, loss is 0.2737831473350525\n",
      "epoch: 17 step: 251, loss is 0.20911939442157745\n",
      "epoch: 17 step: 252, loss is 0.13248124718666077\n",
      "epoch: 17 step: 253, loss is 0.2662254273891449\n",
      "epoch: 17 step: 254, loss is 0.118324413895607\n",
      "epoch: 17 step: 255, loss is 0.3359496295452118\n",
      "epoch: 17 step: 256, loss is 0.13543368875980377\n",
      "epoch: 17 step: 257, loss is 0.2941772937774658\n",
      "epoch: 17 step: 258, loss is 0.13717475533485413\n",
      "epoch: 17 step: 259, loss is 0.10253310203552246\n",
      "epoch: 17 step: 260, loss is 0.25574690103530884\n",
      "epoch: 17 step: 261, loss is 0.16746686398983002\n",
      "epoch: 17 step: 262, loss is 0.28988462686538696\n",
      "epoch: 17 step: 263, loss is 0.1014605462551117\n",
      "epoch: 17 step: 264, loss is 0.17107009887695312\n",
      "epoch: 17 step: 265, loss is 0.2781631052494049\n",
      "epoch: 17 step: 266, loss is 0.31229257583618164\n",
      "epoch: 17 step: 267, loss is 0.2706342339515686\n",
      "epoch: 17 step: 268, loss is 0.10186263918876648\n",
      "epoch: 17 step: 269, loss is 0.20271964371204376\n",
      "epoch: 17 step: 270, loss is 0.15740886330604553\n",
      "epoch: 17 step: 271, loss is 0.32458263635635376\n",
      "epoch: 17 step: 272, loss is 0.1163012906908989\n",
      "epoch: 17 step: 273, loss is 0.2096356749534607\n",
      "epoch: 17 step: 274, loss is 0.23276329040527344\n",
      "epoch: 17 step: 275, loss is 0.1855214536190033\n",
      "epoch: 17 step: 276, loss is 0.17554667592048645\n",
      "epoch: 17 step: 277, loss is 0.17287234961986542\n",
      "epoch: 17 step: 278, loss is 0.0974898487329483\n",
      "epoch: 17 step: 279, loss is 0.2578850984573364\n",
      "epoch: 17 step: 280, loss is 0.26648184657096863\n",
      "epoch: 17 step: 281, loss is 0.3289031386375427\n",
      "epoch: 17 step: 282, loss is 0.22450655698776245\n",
      "epoch: 17 step: 283, loss is 0.25051969289779663\n",
      "epoch: 17 step: 284, loss is 0.3862767219543457\n",
      "epoch: 17 step: 285, loss is 0.24112889170646667\n",
      "epoch: 17 step: 286, loss is 0.34810248017311096\n",
      "epoch: 17 step: 287, loss is 0.3103421926498413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 288, loss is 0.17009980976581573\n",
      "epoch: 17 step: 289, loss is 0.2939286231994629\n",
      "epoch: 17 step: 290, loss is 0.28148844838142395\n",
      "epoch: 17 step: 291, loss is 0.23725490272045135\n",
      "epoch: 17 step: 292, loss is 0.1328306645154953\n",
      "epoch: 17 step: 293, loss is 0.1257808953523636\n",
      "epoch: 17 step: 294, loss is 0.3175874650478363\n",
      "epoch: 17 step: 295, loss is 0.34722909331321716\n",
      "epoch: 17 step: 296, loss is 0.2684916853904724\n",
      "epoch: 17 step: 297, loss is 0.29947829246520996\n",
      "epoch: 17 step: 298, loss is 0.29440832138061523\n",
      "epoch: 17 step: 299, loss is 0.09339464455842972\n",
      "epoch: 17 step: 300, loss is 0.09760187566280365\n",
      "epoch: 17 step: 301, loss is 0.08040348440408707\n",
      "epoch: 17 step: 302, loss is 0.06996643543243408\n",
      "epoch: 17 step: 303, loss is 0.18603330850601196\n",
      "epoch: 17 step: 304, loss is 0.17075809836387634\n",
      "epoch: 17 step: 305, loss is 0.12171278148889542\n",
      "epoch: 17 step: 306, loss is 0.19919776916503906\n",
      "epoch: 17 step: 307, loss is 0.16360214352607727\n",
      "epoch: 17 step: 308, loss is 0.3181464672088623\n",
      "epoch: 17 step: 309, loss is 0.09508439153432846\n",
      "epoch: 17 step: 310, loss is 0.2736181318759918\n",
      "epoch: 17 step: 311, loss is 0.18157288432121277\n",
      "epoch: 17 step: 312, loss is 0.15504798293113708\n",
      "epoch: 17 step: 313, loss is 0.12100928276777267\n",
      "epoch: 17 step: 314, loss is 0.24187421798706055\n",
      "epoch: 17 step: 315, loss is 0.15048521757125854\n",
      "epoch: 17 step: 316, loss is 0.32445695996284485\n",
      "epoch: 17 step: 317, loss is 0.230691596865654\n",
      "epoch: 17 step: 318, loss is 0.3801403045654297\n",
      "epoch: 17 step: 319, loss is 0.1313706338405609\n",
      "epoch: 17 step: 320, loss is 0.2137400060892105\n",
      "epoch: 17 step: 321, loss is 0.21328118443489075\n",
      "epoch: 17 step: 322, loss is 0.21618887782096863\n",
      "epoch: 17 step: 323, loss is 0.17002840340137482\n",
      "epoch: 17 step: 324, loss is 0.2326948195695877\n",
      "epoch: 17 step: 325, loss is 0.15180811285972595\n",
      "epoch: 17 step: 326, loss is 0.2119453251361847\n",
      "epoch: 17 step: 327, loss is 0.13647620379924774\n",
      "epoch: 17 step: 328, loss is 0.2237607091665268\n",
      "epoch: 17 step: 329, loss is 0.22102074325084686\n",
      "epoch: 17 step: 330, loss is 0.28540483117103577\n",
      "epoch: 17 step: 331, loss is 0.3102114796638489\n",
      "epoch: 17 step: 332, loss is 0.1271318644285202\n",
      "epoch: 17 step: 333, loss is 0.19442357122898102\n",
      "epoch: 17 step: 334, loss is 0.30744558572769165\n",
      "epoch: 17 step: 335, loss is 0.12028653174638748\n",
      "epoch: 17 step: 336, loss is 0.12330184876918793\n",
      "epoch: 17 step: 337, loss is 0.16446858644485474\n",
      "epoch: 17 step: 338, loss is 0.24698778986930847\n",
      "epoch: 17 step: 339, loss is 0.2460847944021225\n",
      "epoch: 17 step: 340, loss is 0.14510004222393036\n",
      "epoch: 17 step: 341, loss is 0.2867386043071747\n",
      "epoch: 17 step: 342, loss is 0.20159603655338287\n",
      "epoch: 17 step: 343, loss is 0.5426874160766602\n",
      "epoch: 17 step: 344, loss is 0.23867610096931458\n",
      "epoch: 17 step: 345, loss is 0.34584856033325195\n",
      "epoch: 17 step: 346, loss is 0.13036653399467468\n",
      "epoch: 17 step: 347, loss is 0.2748415768146515\n",
      "epoch: 17 step: 348, loss is 0.16803082823753357\n",
      "epoch: 17 step: 349, loss is 0.17529530823230743\n",
      "epoch: 17 step: 350, loss is 0.10740827023983002\n",
      "epoch: 17 step: 351, loss is 0.35039854049682617\n",
      "epoch: 17 step: 352, loss is 0.22161422669887543\n",
      "epoch: 17 step: 353, loss is 0.3230907618999481\n",
      "epoch: 17 step: 354, loss is 0.19348488748073578\n",
      "epoch: 17 step: 355, loss is 0.4172356128692627\n",
      "epoch: 17 step: 356, loss is 0.3023243844509125\n",
      "epoch: 17 step: 357, loss is 0.1657225638628006\n",
      "epoch: 17 step: 358, loss is 0.1586536467075348\n",
      "epoch: 17 step: 359, loss is 0.09860686212778091\n",
      "epoch: 17 step: 360, loss is 0.18721428513526917\n",
      "epoch: 17 step: 361, loss is 0.3150930106639862\n",
      "epoch: 17 step: 362, loss is 0.14701327681541443\n",
      "epoch: 17 step: 363, loss is 0.11867640167474747\n",
      "epoch: 17 step: 364, loss is 0.18946565687656403\n",
      "epoch: 17 step: 365, loss is 0.2507399916648865\n",
      "epoch: 17 step: 366, loss is 0.16353851556777954\n",
      "epoch: 17 step: 367, loss is 0.11133870482444763\n",
      "epoch: 17 step: 368, loss is 0.16520026326179504\n",
      "epoch: 17 step: 369, loss is 0.2140376716852188\n",
      "epoch: 17 step: 370, loss is 0.1930551677942276\n",
      "epoch: 17 step: 371, loss is 0.24304911494255066\n",
      "epoch: 17 step: 372, loss is 0.21198762953281403\n",
      "epoch: 17 step: 373, loss is 0.19708307087421417\n",
      "epoch: 17 step: 374, loss is 0.12978124618530273\n",
      "epoch: 17 step: 375, loss is 0.13839544355869293\n",
      "epoch: 17 step: 376, loss is 0.2705668807029724\n",
      "epoch: 17 step: 377, loss is 0.3215330243110657\n",
      "epoch: 17 step: 378, loss is 0.22747766971588135\n",
      "epoch: 17 step: 379, loss is 0.24299149215221405\n",
      "epoch: 17 step: 380, loss is 0.2109035849571228\n",
      "epoch: 17 step: 381, loss is 0.2096630185842514\n",
      "epoch: 17 step: 382, loss is 0.3052865266799927\n",
      "epoch: 17 step: 383, loss is 0.1609206348657608\n",
      "epoch: 17 step: 384, loss is 0.1572168916463852\n",
      "epoch: 17 step: 385, loss is 0.3553530275821686\n",
      "epoch: 17 step: 386, loss is 0.2616143822669983\n",
      "epoch: 17 step: 387, loss is 0.20998921990394592\n",
      "epoch: 17 step: 388, loss is 0.20027118921279907\n",
      "epoch: 17 step: 389, loss is 0.12384788691997528\n",
      "epoch: 17 step: 390, loss is 0.33311206102371216\n",
      "epoch: 17 step: 391, loss is 0.27189716696739197\n",
      "epoch: 17 step: 392, loss is 0.15368503332138062\n",
      "epoch: 17 step: 393, loss is 0.12474396079778671\n",
      "epoch: 17 step: 394, loss is 0.26823312044143677\n",
      "epoch: 17 step: 395, loss is 0.1143304854631424\n",
      "epoch: 17 step: 396, loss is 0.17494843900203705\n",
      "epoch: 17 step: 397, loss is 0.2949347198009491\n",
      "epoch: 17 step: 398, loss is 0.2925216555595398\n",
      "epoch: 17 step: 399, loss is 0.2076185792684555\n",
      "epoch: 17 step: 400, loss is 0.3535369336605072\n",
      "epoch: 17 step: 401, loss is 0.06758154183626175\n",
      "epoch: 17 step: 402, loss is 0.11773146688938141\n",
      "epoch: 17 step: 403, loss is 0.23987695574760437\n",
      "epoch: 17 step: 404, loss is 0.1852775514125824\n",
      "epoch: 17 step: 405, loss is 0.27600914239883423\n",
      "epoch: 17 step: 406, loss is 0.1857602447271347\n",
      "epoch: 17 step: 407, loss is 0.24207529425621033\n",
      "epoch: 17 step: 408, loss is 0.23381899297237396\n",
      "epoch: 17 step: 409, loss is 0.2178063988685608\n",
      "epoch: 17 step: 410, loss is 0.31171947717666626\n",
      "epoch: 17 step: 411, loss is 0.15992170572280884\n",
      "epoch: 17 step: 412, loss is 0.38585126399993896\n",
      "epoch: 17 step: 413, loss is 0.12317211180925369\n",
      "epoch: 17 step: 414, loss is 0.20544767379760742\n",
      "epoch: 17 step: 415, loss is 0.18857352435588837\n",
      "epoch: 17 step: 416, loss is 0.18957091867923737\n",
      "epoch: 17 step: 417, loss is 0.26577189564704895\n",
      "epoch: 17 step: 418, loss is 0.15144269168376923\n",
      "epoch: 17 step: 419, loss is 0.26032260060310364\n",
      "epoch: 17 step: 420, loss is 0.26166799664497375\n",
      "epoch: 17 step: 421, loss is 0.19276900589466095\n",
      "epoch: 17 step: 422, loss is 0.25808194279670715\n",
      "epoch: 17 step: 423, loss is 0.1957782357931137\n",
      "epoch: 17 step: 424, loss is 0.24922849237918854\n",
      "epoch: 17 step: 425, loss is 0.17998100817203522\n",
      "epoch: 17 step: 426, loss is 0.1807527095079422\n",
      "epoch: 17 step: 427, loss is 0.2893843650817871\n",
      "epoch: 17 step: 428, loss is 0.2510451078414917\n",
      "epoch: 17 step: 429, loss is 0.10353085398674011\n",
      "epoch: 17 step: 430, loss is 0.14104905724525452\n",
      "epoch: 17 step: 431, loss is 0.2557809054851532\n",
      "epoch: 17 step: 432, loss is 0.21544694900512695\n",
      "epoch: 17 step: 433, loss is 0.19625554978847504\n",
      "epoch: 17 step: 434, loss is 0.20980100333690643\n",
      "epoch: 17 step: 435, loss is 0.24656210839748383\n",
      "epoch: 17 step: 436, loss is 0.15488135814666748\n",
      "epoch: 17 step: 437, loss is 0.25101208686828613\n",
      "epoch: 17 step: 438, loss is 0.13132241368293762\n",
      "epoch: 17 step: 439, loss is 0.19225549697875977\n",
      "epoch: 17 step: 440, loss is 0.06343346834182739\n",
      "epoch: 17 step: 441, loss is 0.15192176401615143\n",
      "epoch: 17 step: 442, loss is 0.2540985345840454\n",
      "epoch: 17 step: 443, loss is 0.157873272895813\n",
      "epoch: 17 step: 444, loss is 0.25881001353263855\n",
      "epoch: 17 step: 445, loss is 0.3169851303100586\n",
      "epoch: 17 step: 446, loss is 0.2428782731294632\n",
      "epoch: 17 step: 447, loss is 0.28316888213157654\n",
      "epoch: 17 step: 448, loss is 0.12474354356527328\n",
      "epoch: 17 step: 449, loss is 0.15484105050563812\n",
      "epoch: 17 step: 450, loss is 0.15634697675704956\n",
      "epoch: 17 step: 451, loss is 0.1309930831193924\n",
      "epoch: 17 step: 452, loss is 0.3283339738845825\n",
      "epoch: 17 step: 453, loss is 0.17919091880321503\n",
      "epoch: 17 step: 454, loss is 0.1932055950164795\n",
      "epoch: 17 step: 455, loss is 0.30399999022483826\n",
      "epoch: 17 step: 456, loss is 0.18232640624046326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 457, loss is 0.17757494747638702\n",
      "epoch: 17 step: 458, loss is 0.28947317600250244\n",
      "epoch: 17 step: 459, loss is 0.12218251824378967\n",
      "epoch: 17 step: 460, loss is 0.2786915898323059\n",
      "epoch: 17 step: 461, loss is 0.3096903860569\n",
      "epoch: 17 step: 462, loss is 0.22296278178691864\n",
      "epoch: 17 step: 463, loss is 0.1579035520553589\n",
      "epoch: 17 step: 464, loss is 0.22642859816551208\n",
      "epoch: 17 step: 465, loss is 0.1533263623714447\n",
      "epoch: 17 step: 466, loss is 0.12604479491710663\n",
      "epoch: 17 step: 467, loss is 0.2378561943769455\n",
      "epoch: 17 step: 468, loss is 0.3083355724811554\n",
      "epoch: 17 step: 469, loss is 0.33300721645355225\n",
      "epoch: 17 step: 470, loss is 0.10152208060026169\n",
      "epoch: 17 step: 471, loss is 0.11781657487154007\n",
      "epoch: 17 step: 472, loss is 0.11798744648694992\n",
      "epoch: 17 step: 473, loss is 0.336534321308136\n",
      "epoch: 17 step: 474, loss is 0.3061909079551697\n",
      "epoch: 17 step: 475, loss is 0.20050659775733948\n",
      "epoch: 17 step: 476, loss is 0.10337419807910919\n",
      "epoch: 17 step: 477, loss is 0.16679464280605316\n",
      "epoch: 17 step: 478, loss is 0.18153345584869385\n",
      "epoch: 17 step: 479, loss is 0.29622626304626465\n",
      "epoch: 17 step: 480, loss is 0.2593546211719513\n",
      "epoch: 17 step: 481, loss is 0.07490942627191544\n",
      "epoch: 17 step: 482, loss is 0.22185879945755005\n",
      "epoch: 17 step: 483, loss is 0.3030979335308075\n",
      "epoch: 17 step: 484, loss is 0.29410162568092346\n",
      "epoch: 17 step: 485, loss is 0.14669424295425415\n",
      "epoch: 17 step: 486, loss is 0.12285405397415161\n",
      "epoch: 17 step: 487, loss is 0.16368286311626434\n",
      "epoch: 17 step: 488, loss is 0.2034955620765686\n",
      "epoch: 17 step: 489, loss is 0.21512140333652496\n",
      "epoch: 17 step: 490, loss is 0.22120560705661774\n",
      "epoch: 17 step: 491, loss is 0.3560435175895691\n",
      "epoch: 17 step: 492, loss is 0.14198626577854156\n",
      "epoch: 17 step: 493, loss is 0.252754271030426\n",
      "epoch: 17 step: 494, loss is 0.1031169444322586\n",
      "epoch: 17 step: 495, loss is 0.2750300467014313\n",
      "epoch: 17 step: 496, loss is 0.13080032169818878\n",
      "epoch: 17 step: 497, loss is 0.10150652378797531\n",
      "epoch: 17 step: 498, loss is 0.24633118510246277\n",
      "epoch: 17 step: 499, loss is 0.198106586933136\n",
      "epoch: 17 step: 500, loss is 0.08735834807157516\n",
      "epoch: 17 step: 501, loss is 0.19790014624595642\n",
      "epoch: 17 step: 502, loss is 0.21875590085983276\n",
      "epoch: 17 step: 503, loss is 0.22042030096054077\n",
      "epoch: 17 step: 504, loss is 0.3726600706577301\n",
      "epoch: 17 step: 505, loss is 0.18584124743938446\n",
      "epoch: 17 step: 506, loss is 0.14604324102401733\n",
      "epoch: 17 step: 507, loss is 0.2699242830276489\n",
      "epoch: 17 step: 508, loss is 0.12028581649065018\n",
      "epoch: 17 step: 509, loss is 0.17918792366981506\n",
      "epoch: 17 step: 510, loss is 0.18095195293426514\n",
      "epoch: 17 step: 511, loss is 0.21390369534492493\n",
      "epoch: 17 step: 512, loss is 0.30313438177108765\n",
      "epoch: 17 step: 513, loss is 0.20031754672527313\n",
      "epoch: 17 step: 514, loss is 0.2287515103816986\n",
      "epoch: 17 step: 515, loss is 0.10239371657371521\n",
      "epoch: 17 step: 516, loss is 0.2742455005645752\n",
      "epoch: 17 step: 517, loss is 0.26807907223701477\n",
      "epoch: 17 step: 518, loss is 0.2571248710155487\n",
      "epoch: 17 step: 519, loss is 0.20785951614379883\n",
      "epoch: 17 step: 520, loss is 0.08066499978303909\n",
      "epoch: 17 step: 521, loss is 0.2021995335817337\n",
      "epoch: 17 step: 522, loss is 0.23927032947540283\n",
      "epoch: 17 step: 523, loss is 0.17292989790439606\n",
      "epoch: 17 step: 524, loss is 0.21681015193462372\n",
      "epoch: 17 step: 525, loss is 0.18577827513217926\n",
      "epoch: 17 step: 526, loss is 0.3022005558013916\n",
      "epoch: 17 step: 527, loss is 0.15636198222637177\n",
      "epoch: 17 step: 528, loss is 0.1679680049419403\n",
      "epoch: 17 step: 529, loss is 0.15118807554244995\n",
      "epoch: 17 step: 530, loss is 0.2186611443758011\n",
      "epoch: 17 step: 531, loss is 0.20043103396892548\n",
      "epoch: 17 step: 532, loss is 0.33578240871429443\n",
      "epoch: 17 step: 533, loss is 0.2555200457572937\n",
      "epoch: 17 step: 534, loss is 0.2580294609069824\n",
      "epoch: 17 step: 535, loss is 0.21679677069187164\n",
      "epoch: 17 step: 536, loss is 0.10003823041915894\n",
      "epoch: 17 step: 537, loss is 0.3723715543746948\n",
      "epoch: 17 step: 538, loss is 0.1505320966243744\n",
      "epoch: 17 step: 539, loss is 0.25620195269584656\n",
      "epoch: 17 step: 540, loss is 0.36237633228302\n",
      "epoch: 17 step: 541, loss is 0.1772104948759079\n",
      "epoch: 17 step: 542, loss is 0.1862626075744629\n",
      "epoch: 17 step: 543, loss is 0.16653208434581757\n",
      "epoch: 17 step: 544, loss is 0.15032269060611725\n",
      "epoch: 17 step: 545, loss is 0.19802743196487427\n",
      "epoch: 17 step: 546, loss is 0.11967740952968597\n",
      "epoch: 17 step: 547, loss is 0.16356614232063293\n",
      "epoch: 17 step: 548, loss is 0.2562659978866577\n",
      "epoch: 17 step: 549, loss is 0.22238722443580627\n",
      "epoch: 17 step: 550, loss is 0.19728021323680878\n",
      "epoch: 17 step: 551, loss is 0.17489439249038696\n",
      "epoch: 17 step: 552, loss is 0.1118905171751976\n",
      "epoch: 17 step: 553, loss is 0.2879481315612793\n",
      "epoch: 17 step: 554, loss is 0.11510579288005829\n",
      "epoch: 17 step: 555, loss is 0.17159190773963928\n",
      "epoch: 17 step: 556, loss is 0.11331558972597122\n",
      "epoch: 17 step: 557, loss is 0.12635385990142822\n",
      "epoch: 17 step: 558, loss is 0.15959809720516205\n",
      "epoch: 17 step: 559, loss is 0.20385216176509857\n",
      "epoch: 17 step: 560, loss is 0.28522154688835144\n",
      "epoch: 17 step: 561, loss is 0.21246127784252167\n",
      "epoch: 17 step: 562, loss is 0.12059864401817322\n",
      "epoch: 17 step: 563, loss is 0.196448415517807\n",
      "epoch: 17 step: 564, loss is 0.33023813366889954\n",
      "epoch: 17 step: 565, loss is 0.19836312532424927\n",
      "epoch: 17 step: 566, loss is 0.19373314082622528\n",
      "epoch: 17 step: 567, loss is 0.31728529930114746\n",
      "epoch: 17 step: 568, loss is 0.2649550437927246\n",
      "epoch: 17 step: 569, loss is 0.15478730201721191\n",
      "epoch: 17 step: 570, loss is 0.21613956987857819\n",
      "epoch: 17 step: 571, loss is 0.3041642904281616\n",
      "epoch: 17 step: 572, loss is 0.1694515347480774\n",
      "epoch: 17 step: 573, loss is 0.20695361495018005\n",
      "epoch: 17 step: 574, loss is 0.3165450394153595\n",
      "epoch: 17 step: 575, loss is 0.17469272017478943\n",
      "epoch: 17 step: 576, loss is 0.1699768602848053\n",
      "epoch: 17 step: 577, loss is 0.12310893833637238\n",
      "epoch: 17 step: 578, loss is 0.257886677980423\n",
      "epoch: 17 step: 579, loss is 0.30390551686286926\n",
      "epoch: 17 step: 580, loss is 0.18264615535736084\n",
      "epoch: 17 step: 581, loss is 0.1893739104270935\n",
      "epoch: 17 step: 582, loss is 0.23496393859386444\n",
      "epoch: 17 step: 583, loss is 0.1637209951877594\n",
      "epoch: 17 step: 584, loss is 0.2426067739725113\n",
      "epoch: 17 step: 585, loss is 0.19287797808647156\n",
      "epoch: 17 step: 586, loss is 0.15648534893989563\n",
      "epoch: 17 step: 587, loss is 0.1876261830329895\n",
      "epoch: 17 step: 588, loss is 0.2809508442878723\n",
      "epoch: 17 step: 589, loss is 0.19793620705604553\n",
      "epoch: 17 step: 590, loss is 0.33882755041122437\n",
      "epoch: 17 step: 591, loss is 0.21645903587341309\n",
      "epoch: 17 step: 592, loss is 0.1840384155511856\n",
      "epoch: 17 step: 593, loss is 0.2992226779460907\n",
      "epoch: 17 step: 594, loss is 0.2354520857334137\n",
      "epoch: 17 step: 595, loss is 0.2395336925983429\n",
      "epoch: 17 step: 596, loss is 0.21438290178775787\n",
      "epoch: 17 step: 597, loss is 0.22588588297367096\n",
      "epoch: 17 step: 598, loss is 0.05574880540370941\n",
      "epoch: 17 step: 599, loss is 0.14118242263793945\n",
      "epoch: 17 step: 600, loss is 0.1614670753479004\n",
      "epoch: 17 step: 601, loss is 0.11887752264738083\n",
      "epoch: 17 step: 602, loss is 0.3058631122112274\n",
      "epoch: 17 step: 603, loss is 0.19749632477760315\n",
      "epoch: 17 step: 604, loss is 0.13497036695480347\n",
      "epoch: 17 step: 605, loss is 0.22731192409992218\n",
      "epoch: 17 step: 606, loss is 0.2712506055831909\n",
      "epoch: 17 step: 607, loss is 0.14657345414161682\n",
      "epoch: 17 step: 608, loss is 0.16513711214065552\n",
      "epoch: 17 step: 609, loss is 0.22240103781223297\n",
      "epoch: 17 step: 610, loss is 0.11336179077625275\n",
      "epoch: 17 step: 611, loss is 0.26136425137519836\n",
      "epoch: 17 step: 612, loss is 0.14335739612579346\n",
      "epoch: 17 step: 613, loss is 0.16199931502342224\n",
      "epoch: 17 step: 614, loss is 0.11404871195554733\n",
      "epoch: 17 step: 615, loss is 0.18919876217842102\n",
      "epoch: 17 step: 616, loss is 0.13579869270324707\n",
      "epoch: 17 step: 617, loss is 0.20653750002384186\n",
      "epoch: 17 step: 618, loss is 0.1617879420518875\n",
      "epoch: 17 step: 619, loss is 0.13954216241836548\n",
      "epoch: 17 step: 620, loss is 0.16287386417388916\n",
      "epoch: 17 step: 621, loss is 0.2827533483505249\n",
      "epoch: 17 step: 622, loss is 0.13432829082012177\n",
      "epoch: 17 step: 623, loss is 0.17778021097183228\n",
      "epoch: 17 step: 624, loss is 0.11689624935388565\n",
      "epoch: 17 step: 625, loss is 0.09186241030693054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 626, loss is 0.16900725662708282\n",
      "epoch: 17 step: 627, loss is 0.14489486813545227\n",
      "epoch: 17 step: 628, loss is 0.2335244119167328\n",
      "epoch: 17 step: 629, loss is 0.19666078686714172\n",
      "epoch: 17 step: 630, loss is 0.22808779776096344\n",
      "epoch: 17 step: 631, loss is 0.10040028393268585\n",
      "epoch: 17 step: 632, loss is 0.35541027784347534\n",
      "epoch: 17 step: 633, loss is 0.1552802473306656\n",
      "epoch: 17 step: 634, loss is 0.12694485485553741\n",
      "epoch: 17 step: 635, loss is 0.06693483889102936\n",
      "epoch: 17 step: 636, loss is 0.2300419956445694\n",
      "epoch: 17 step: 637, loss is 0.1498817801475525\n",
      "epoch: 17 step: 638, loss is 0.20256780087947845\n",
      "epoch: 17 step: 639, loss is 0.15376035869121552\n",
      "epoch: 17 step: 640, loss is 0.10908748209476471\n",
      "epoch: 17 step: 641, loss is 0.1485338807106018\n",
      "epoch: 17 step: 642, loss is 0.1369384378194809\n",
      "epoch: 17 step: 643, loss is 0.09296435862779617\n",
      "epoch: 17 step: 644, loss is 0.17129060626029968\n",
      "epoch: 17 step: 645, loss is 0.3482360243797302\n",
      "epoch: 17 step: 646, loss is 0.2049517184495926\n",
      "epoch: 17 step: 647, loss is 0.17463307082653046\n",
      "epoch: 17 step: 648, loss is 0.13491035997867584\n",
      "epoch: 17 step: 649, loss is 0.1424904763698578\n",
      "epoch: 17 step: 650, loss is 0.2134493887424469\n",
      "epoch: 17 step: 651, loss is 0.3108386993408203\n",
      "epoch: 17 step: 652, loss is 0.18622997403144836\n",
      "epoch: 17 step: 653, loss is 0.35380545258522034\n",
      "epoch: 17 step: 654, loss is 0.16547903418540955\n",
      "epoch: 17 step: 655, loss is 0.19694852828979492\n",
      "epoch: 17 step: 656, loss is 0.2159482091665268\n",
      "epoch: 17 step: 657, loss is 0.13016878068447113\n",
      "epoch: 17 step: 658, loss is 0.11645600199699402\n",
      "epoch: 17 step: 659, loss is 0.2135697454214096\n",
      "epoch: 17 step: 660, loss is 0.13229741156101227\n",
      "epoch: 17 step: 661, loss is 0.17810286581516266\n",
      "epoch: 17 step: 662, loss is 0.31013479828834534\n",
      "epoch: 17 step: 663, loss is 0.19042326509952545\n",
      "epoch: 17 step: 664, loss is 0.11561217904090881\n",
      "epoch: 17 step: 665, loss is 0.2032030075788498\n",
      "epoch: 17 step: 666, loss is 0.2668526768684387\n",
      "epoch: 17 step: 667, loss is 0.5315367579460144\n",
      "epoch: 17 step: 668, loss is 0.03802912309765816\n",
      "epoch: 17 step: 669, loss is 0.15088234841823578\n",
      "epoch: 17 step: 670, loss is 0.2678336501121521\n",
      "epoch: 17 step: 671, loss is 0.17672741413116455\n",
      "epoch: 17 step: 672, loss is 0.21790368854999542\n",
      "epoch: 17 step: 673, loss is 0.09379034489393234\n",
      "epoch: 17 step: 674, loss is 0.1826578825712204\n",
      "epoch: 17 step: 675, loss is 0.1571401208639145\n",
      "epoch: 17 step: 676, loss is 0.1667596846818924\n",
      "epoch: 17 step: 677, loss is 0.10582806169986725\n",
      "epoch: 17 step: 678, loss is 0.18193775415420532\n",
      "epoch: 17 step: 679, loss is 0.1761421412229538\n",
      "epoch: 17 step: 680, loss is 0.21431943774223328\n",
      "epoch: 17 step: 681, loss is 0.24888715147972107\n",
      "epoch: 17 step: 682, loss is 0.19303804636001587\n",
      "epoch: 17 step: 683, loss is 0.18930067121982574\n",
      "epoch: 17 step: 684, loss is 0.18139003217220306\n",
      "epoch: 17 step: 685, loss is 0.28553229570388794\n",
      "epoch: 17 step: 686, loss is 0.32614922523498535\n",
      "epoch: 17 step: 687, loss is 0.09399588406085968\n",
      "epoch: 17 step: 688, loss is 0.30130743980407715\n",
      "epoch: 17 step: 689, loss is 0.3219600021839142\n",
      "epoch: 17 step: 690, loss is 0.17635990679264069\n",
      "epoch: 17 step: 691, loss is 0.25386515259742737\n",
      "epoch: 17 step: 692, loss is 0.3228476941585541\n",
      "epoch: 17 step: 693, loss is 0.3145276606082916\n",
      "epoch: 17 step: 694, loss is 0.18987900018692017\n",
      "epoch: 17 step: 695, loss is 0.31940799951553345\n",
      "epoch: 17 step: 696, loss is 0.23287233710289001\n",
      "epoch: 17 step: 697, loss is 0.18802562355995178\n",
      "epoch: 17 step: 698, loss is 0.2842986285686493\n",
      "epoch: 17 step: 699, loss is 0.2623697519302368\n",
      "epoch: 17 step: 700, loss is 0.10101804882287979\n",
      "epoch: 17 step: 701, loss is 0.1718832552433014\n",
      "epoch: 17 step: 702, loss is 0.22780869901180267\n",
      "epoch: 17 step: 703, loss is 0.11006569862365723\n",
      "epoch: 17 step: 704, loss is 0.14865466952323914\n",
      "epoch: 17 step: 705, loss is 0.313689261674881\n",
      "epoch: 17 step: 706, loss is 0.18910335004329681\n",
      "epoch: 17 step: 707, loss is 0.1599881798028946\n",
      "epoch: 17 step: 708, loss is 0.21709562838077545\n",
      "epoch: 17 step: 709, loss is 0.06845341622829437\n",
      "epoch: 17 step: 710, loss is 0.30213332176208496\n",
      "epoch: 17 step: 711, loss is 0.20018181204795837\n",
      "epoch: 17 step: 712, loss is 0.23523518443107605\n",
      "epoch: 17 step: 713, loss is 0.2722611427307129\n",
      "epoch: 17 step: 714, loss is 0.3345215916633606\n",
      "epoch: 17 step: 715, loss is 0.1376895159482956\n",
      "epoch: 17 step: 716, loss is 0.14807969331741333\n",
      "epoch: 17 step: 717, loss is 0.23745957016944885\n",
      "epoch: 17 step: 718, loss is 0.25948578119277954\n",
      "epoch: 17 step: 719, loss is 0.2828451991081238\n",
      "epoch: 17 step: 720, loss is 0.11325795948505402\n",
      "epoch: 17 step: 721, loss is 0.14760026335716248\n",
      "epoch: 17 step: 722, loss is 0.3829615116119385\n",
      "epoch: 17 step: 723, loss is 0.19355785846710205\n",
      "epoch: 17 step: 724, loss is 0.359333336353302\n",
      "epoch: 17 step: 725, loss is 0.21526537835597992\n",
      "epoch: 17 step: 726, loss is 0.12246303260326385\n",
      "epoch: 17 step: 727, loss is 0.18936209380626678\n",
      "epoch: 17 step: 728, loss is 0.10683438181877136\n",
      "epoch: 17 step: 729, loss is 0.16385306417942047\n",
      "epoch: 17 step: 730, loss is 0.08937253057956696\n",
      "epoch: 17 step: 731, loss is 0.29382625222206116\n",
      "epoch: 17 step: 732, loss is 0.11075882613658905\n",
      "epoch: 17 step: 733, loss is 0.2727794945240021\n",
      "epoch: 17 step: 734, loss is 0.07742804288864136\n",
      "epoch: 17 step: 735, loss is 0.17813268303871155\n",
      "epoch: 17 step: 736, loss is 0.2863665223121643\n",
      "epoch: 17 step: 737, loss is 0.22764843702316284\n",
      "epoch: 17 step: 738, loss is 0.22710542380809784\n",
      "epoch: 17 step: 739, loss is 0.17951330542564392\n",
      "epoch: 17 step: 740, loss is 0.09117335081100464\n",
      "epoch: 17 step: 741, loss is 0.2614686191082001\n",
      "epoch: 17 step: 742, loss is 0.3182527422904968\n",
      "epoch: 17 step: 743, loss is 0.2963863015174866\n",
      "epoch: 17 step: 744, loss is 0.21917420625686646\n",
      "epoch: 17 step: 745, loss is 0.1682254672050476\n",
      "epoch: 17 step: 746, loss is 0.3272600471973419\n",
      "epoch: 17 step: 747, loss is 0.1384483128786087\n",
      "epoch: 17 step: 748, loss is 0.3568045198917389\n",
      "epoch: 17 step: 749, loss is 0.3499695360660553\n",
      "epoch: 17 step: 750, loss is 0.2363351732492447\n",
      "epoch: 17 step: 751, loss is 0.24048684537410736\n",
      "epoch: 17 step: 752, loss is 0.18466204404830933\n",
      "epoch: 17 step: 753, loss is 0.08988454937934875\n",
      "epoch: 17 step: 754, loss is 0.12441293895244598\n",
      "epoch: 17 step: 755, loss is 0.3369375467300415\n",
      "epoch: 17 step: 756, loss is 0.11267630010843277\n",
      "epoch: 17 step: 757, loss is 0.3200772702693939\n",
      "epoch: 17 step: 758, loss is 0.21102146804332733\n",
      "epoch: 17 step: 759, loss is 0.145782932639122\n",
      "epoch: 17 step: 760, loss is 0.17525100708007812\n",
      "epoch: 17 step: 761, loss is 0.34020286798477173\n",
      "epoch: 17 step: 762, loss is 0.05236620455980301\n",
      "epoch: 17 step: 763, loss is 0.3138880729675293\n",
      "epoch: 17 step: 764, loss is 0.11745806783437729\n",
      "epoch: 17 step: 765, loss is 0.17669115960597992\n",
      "epoch: 17 step: 766, loss is 0.4178543984889984\n",
      "epoch: 17 step: 767, loss is 0.15871785581111908\n",
      "epoch: 17 step: 768, loss is 0.2345450073480606\n",
      "epoch: 17 step: 769, loss is 0.1076556071639061\n",
      "epoch: 17 step: 770, loss is 0.27650997042655945\n",
      "epoch: 17 step: 771, loss is 0.16622942686080933\n",
      "epoch: 17 step: 772, loss is 0.4855344891548157\n",
      "epoch: 17 step: 773, loss is 0.36441677808761597\n",
      "epoch: 17 step: 774, loss is 0.2655327618122101\n",
      "epoch: 17 step: 775, loss is 0.1650499403476715\n",
      "epoch: 17 step: 776, loss is 0.22647017240524292\n",
      "epoch: 17 step: 777, loss is 0.23383291065692902\n",
      "epoch: 17 step: 778, loss is 0.3037894368171692\n",
      "epoch: 17 step: 779, loss is 0.07564043253660202\n",
      "epoch: 17 step: 780, loss is 0.16723735630512238\n",
      "epoch: 17 step: 781, loss is 0.09967808425426483\n",
      "epoch: 17 step: 782, loss is 0.23094236850738525\n",
      "epoch: 17 step: 783, loss is 0.20814096927642822\n",
      "epoch: 17 step: 784, loss is 0.102176234126091\n",
      "epoch: 17 step: 785, loss is 0.17841891944408417\n",
      "epoch: 17 step: 786, loss is 0.09170130640268326\n",
      "epoch: 17 step: 787, loss is 0.13375210762023926\n",
      "epoch: 17 step: 788, loss is 0.3223875164985657\n",
      "epoch: 17 step: 789, loss is 0.30512332916259766\n",
      "epoch: 17 step: 790, loss is 0.2574748992919922\n",
      "epoch: 17 step: 791, loss is 0.3840296268463135\n",
      "epoch: 17 step: 792, loss is 0.22510957717895508\n",
      "epoch: 17 step: 793, loss is 0.09937186539173126\n",
      "epoch: 17 step: 794, loss is 0.30014413595199585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 795, loss is 0.28732430934906006\n",
      "epoch: 17 step: 796, loss is 0.20310336351394653\n",
      "epoch: 17 step: 797, loss is 0.1106712743639946\n",
      "epoch: 17 step: 798, loss is 0.1943947672843933\n",
      "epoch: 17 step: 799, loss is 0.24251236021518707\n",
      "epoch: 17 step: 800, loss is 0.10111772269010544\n",
      "epoch: 17 step: 801, loss is 0.22681571543216705\n",
      "epoch: 17 step: 802, loss is 0.49995285272598267\n",
      "epoch: 17 step: 803, loss is 0.23988664150238037\n",
      "epoch: 17 step: 804, loss is 0.13435672223567963\n",
      "epoch: 17 step: 805, loss is 0.2800385355949402\n",
      "epoch: 17 step: 806, loss is 0.2390832155942917\n",
      "epoch: 17 step: 807, loss is 0.1450609415769577\n",
      "epoch: 17 step: 808, loss is 0.2815203368663788\n",
      "epoch: 17 step: 809, loss is 0.3070763945579529\n",
      "epoch: 17 step: 810, loss is 0.20100726187229156\n",
      "epoch: 17 step: 811, loss is 0.14840558171272278\n",
      "epoch: 17 step: 812, loss is 0.2560453712940216\n",
      "epoch: 17 step: 813, loss is 0.09030025452375412\n",
      "epoch: 17 step: 814, loss is 0.2737305164337158\n",
      "epoch: 17 step: 815, loss is 0.12269539386034012\n",
      "epoch: 17 step: 816, loss is 0.24561773240566254\n",
      "epoch: 17 step: 817, loss is 0.2857638895511627\n",
      "epoch: 17 step: 818, loss is 0.5237920880317688\n",
      "epoch: 17 step: 819, loss is 0.1889205276966095\n",
      "epoch: 17 step: 820, loss is 0.31772732734680176\n",
      "epoch: 17 step: 821, loss is 0.10807526111602783\n",
      "epoch: 17 step: 822, loss is 0.15550798177719116\n",
      "epoch: 17 step: 823, loss is 0.23867881298065186\n",
      "epoch: 17 step: 824, loss is 0.18051742017269135\n",
      "epoch: 17 step: 825, loss is 0.29311010241508484\n",
      "epoch: 17 step: 826, loss is 0.31933945417404175\n",
      "epoch: 17 step: 827, loss is 0.12935520708560944\n",
      "epoch: 17 step: 828, loss is 0.21123866736888885\n",
      "epoch: 17 step: 829, loss is 0.2984105944633484\n",
      "epoch: 17 step: 830, loss is 0.14888592064380646\n",
      "epoch: 17 step: 831, loss is 0.3000941574573517\n",
      "epoch: 17 step: 832, loss is 0.2102292776107788\n",
      "epoch: 17 step: 833, loss is 0.13408005237579346\n",
      "epoch: 17 step: 834, loss is 0.14475414156913757\n",
      "epoch: 17 step: 835, loss is 0.1362459808588028\n",
      "epoch: 17 step: 836, loss is 0.2200210988521576\n",
      "epoch: 17 step: 837, loss is 0.17716942727565765\n",
      "epoch: 17 step: 838, loss is 0.2700454592704773\n",
      "epoch: 17 step: 839, loss is 0.12145619094371796\n",
      "epoch: 17 step: 840, loss is 0.17380139231681824\n",
      "epoch: 17 step: 841, loss is 0.1697273999452591\n",
      "epoch: 17 step: 842, loss is 0.10985314100980759\n",
      "epoch: 17 step: 843, loss is 0.23110148310661316\n",
      "epoch: 17 step: 844, loss is 0.2633839249610901\n",
      "epoch: 17 step: 845, loss is 0.15337218344211578\n",
      "epoch: 17 step: 846, loss is 0.11139066517353058\n",
      "epoch: 17 step: 847, loss is 0.22284209728240967\n",
      "epoch: 17 step: 848, loss is 0.42820343375205994\n",
      "epoch: 17 step: 849, loss is 0.2104872465133667\n",
      "epoch: 17 step: 850, loss is 0.1275198608636856\n",
      "epoch: 17 step: 851, loss is 0.19082286953926086\n",
      "epoch: 17 step: 852, loss is 0.14455074071884155\n",
      "epoch: 17 step: 853, loss is 0.5196072459220886\n",
      "epoch: 17 step: 854, loss is 0.18589246273040771\n",
      "epoch: 17 step: 855, loss is 0.13714444637298584\n",
      "epoch: 17 step: 856, loss is 0.20846793055534363\n",
      "epoch: 17 step: 857, loss is 0.22749455273151398\n",
      "epoch: 17 step: 858, loss is 0.14926618337631226\n",
      "epoch: 17 step: 859, loss is 0.1369461864233017\n",
      "epoch: 17 step: 860, loss is 0.22561688721179962\n",
      "epoch: 17 step: 861, loss is 0.2015775889158249\n",
      "epoch: 17 step: 862, loss is 0.19382525980472565\n",
      "epoch: 17 step: 863, loss is 0.17005378007888794\n",
      "epoch: 17 step: 864, loss is 0.16078239679336548\n",
      "epoch: 17 step: 865, loss is 0.21849089860916138\n",
      "epoch: 17 step: 866, loss is 0.3489713966846466\n",
      "epoch: 17 step: 867, loss is 0.09934984147548676\n",
      "epoch: 17 step: 868, loss is 0.21762986481189728\n",
      "epoch: 17 step: 869, loss is 0.17549319565296173\n",
      "epoch: 17 step: 870, loss is 0.13546688854694366\n",
      "epoch: 17 step: 871, loss is 0.1313052624464035\n",
      "epoch: 17 step: 872, loss is 0.1395026296377182\n",
      "epoch: 17 step: 873, loss is 0.14803805947303772\n",
      "epoch: 17 step: 874, loss is 0.26338592171669006\n",
      "epoch: 17 step: 875, loss is 0.12913943827152252\n",
      "epoch: 17 step: 876, loss is 0.40773022174835205\n",
      "epoch: 17 step: 877, loss is 0.21358537673950195\n",
      "epoch: 17 step: 878, loss is 0.2367604672908783\n",
      "epoch: 17 step: 879, loss is 0.14389576017856598\n",
      "epoch: 17 step: 880, loss is 0.1088624820113182\n",
      "epoch: 17 step: 881, loss is 0.2369760125875473\n",
      "epoch: 17 step: 882, loss is 0.29092052578926086\n",
      "epoch: 17 step: 883, loss is 0.2750972807407379\n",
      "epoch: 17 step: 884, loss is 0.18361632525920868\n",
      "epoch: 17 step: 885, loss is 0.18863286077976227\n",
      "epoch: 17 step: 886, loss is 0.35052740573883057\n",
      "epoch: 17 step: 887, loss is 0.3020307719707489\n",
      "epoch: 17 step: 888, loss is 0.19791671633720398\n",
      "epoch: 17 step: 889, loss is 0.23039954900741577\n",
      "epoch: 17 step: 890, loss is 0.11744942516088486\n",
      "epoch: 17 step: 891, loss is 0.18491947650909424\n",
      "epoch: 17 step: 892, loss is 0.26314103603363037\n",
      "epoch: 17 step: 893, loss is 0.33089694380760193\n",
      "epoch: 17 step: 894, loss is 0.22159790992736816\n",
      "epoch: 17 step: 895, loss is 0.22459067404270172\n",
      "epoch: 17 step: 896, loss is 0.21745719015598297\n",
      "epoch: 17 step: 897, loss is 0.1982692927122116\n",
      "epoch: 17 step: 898, loss is 0.29441800713539124\n",
      "epoch: 17 step: 899, loss is 0.19003437459468842\n",
      "epoch: 17 step: 900, loss is 0.1804407835006714\n",
      "epoch: 17 step: 901, loss is 0.17886632680892944\n",
      "epoch: 17 step: 902, loss is 0.10004842281341553\n",
      "epoch: 17 step: 903, loss is 0.1523643583059311\n",
      "epoch: 17 step: 904, loss is 0.13573016226291656\n",
      "epoch: 17 step: 905, loss is 0.1402447372674942\n",
      "epoch: 17 step: 906, loss is 0.24500863254070282\n",
      "epoch: 17 step: 907, loss is 0.1998758316040039\n",
      "epoch: 17 step: 908, loss is 0.1439453363418579\n",
      "epoch: 17 step: 909, loss is 0.11217861622571945\n",
      "epoch: 17 step: 910, loss is 0.1583576500415802\n",
      "epoch: 17 step: 911, loss is 0.24992896616458893\n",
      "epoch: 17 step: 912, loss is 0.12695634365081787\n",
      "epoch: 17 step: 913, loss is 0.2234155237674713\n",
      "epoch: 17 step: 914, loss is 0.19962283968925476\n",
      "epoch: 17 step: 915, loss is 0.31308048963546753\n",
      "epoch: 17 step: 916, loss is 0.24319986999034882\n",
      "epoch: 17 step: 917, loss is 0.22186610102653503\n",
      "epoch: 17 step: 918, loss is 0.18286116421222687\n",
      "epoch: 17 step: 919, loss is 0.17678645253181458\n",
      "epoch: 17 step: 920, loss is 0.3742496073246002\n",
      "epoch: 17 step: 921, loss is 0.05653071776032448\n",
      "epoch: 17 step: 922, loss is 0.22447068989276886\n",
      "epoch: 17 step: 923, loss is 0.18046993017196655\n",
      "epoch: 17 step: 924, loss is 0.21594615280628204\n",
      "epoch: 17 step: 925, loss is 0.1818428784608841\n",
      "epoch: 17 step: 926, loss is 0.14540474116802216\n",
      "epoch: 17 step: 927, loss is 0.23536613583564758\n",
      "epoch: 17 step: 928, loss is 0.12241822481155396\n",
      "epoch: 17 step: 929, loss is 0.1354236602783203\n",
      "epoch: 17 step: 930, loss is 0.25087860226631165\n",
      "epoch: 17 step: 931, loss is 0.27667903900146484\n",
      "epoch: 17 step: 932, loss is 0.17389269173145294\n",
      "epoch: 17 step: 933, loss is 0.16210810840129852\n",
      "epoch: 17 step: 934, loss is 0.2434478998184204\n",
      "epoch: 17 step: 935, loss is 0.1542462706565857\n",
      "epoch: 17 step: 936, loss is 0.06345340609550476\n",
      "epoch: 17 step: 937, loss is 0.29172152280807495\n",
      "epoch: 18 step: 1, loss is 0.19107475876808167\n",
      "epoch: 18 step: 2, loss is 0.14464612305164337\n",
      "epoch: 18 step: 3, loss is 0.1179928407073021\n",
      "epoch: 18 step: 4, loss is 0.2678162455558777\n",
      "epoch: 18 step: 5, loss is 0.22347843647003174\n",
      "epoch: 18 step: 6, loss is 0.17424432933330536\n",
      "epoch: 18 step: 7, loss is 0.24708521366119385\n",
      "epoch: 18 step: 8, loss is 0.26051536202430725\n",
      "epoch: 18 step: 9, loss is 0.09334170073270798\n",
      "epoch: 18 step: 10, loss is 0.4075652062892914\n",
      "epoch: 18 step: 11, loss is 0.15675346553325653\n",
      "epoch: 18 step: 12, loss is 0.2665518522262573\n",
      "epoch: 18 step: 13, loss is 0.20723143219947815\n",
      "epoch: 18 step: 14, loss is 0.3064696192741394\n",
      "epoch: 18 step: 15, loss is 0.1666041612625122\n",
      "epoch: 18 step: 16, loss is 0.2934621572494507\n",
      "epoch: 18 step: 17, loss is 0.284174382686615\n",
      "epoch: 18 step: 18, loss is 0.11631139367818832\n",
      "epoch: 18 step: 19, loss is 0.25716346502304077\n",
      "epoch: 18 step: 20, loss is 0.15931393206119537\n",
      "epoch: 18 step: 21, loss is 0.14972615242004395\n",
      "epoch: 18 step: 22, loss is 0.28893038630485535\n",
      "epoch: 18 step: 23, loss is 0.13578274846076965\n",
      "epoch: 18 step: 24, loss is 0.15061254799365997\n",
      "epoch: 18 step: 25, loss is 0.35854992270469666\n",
      "epoch: 18 step: 26, loss is 0.3663365840911865\n",
      "epoch: 18 step: 27, loss is 0.3833913505077362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 28, loss is 0.24911795556545258\n",
      "epoch: 18 step: 29, loss is 0.17935287952423096\n",
      "epoch: 18 step: 30, loss is 0.18151503801345825\n",
      "epoch: 18 step: 31, loss is 0.1724679321050644\n",
      "epoch: 18 step: 32, loss is 0.2689536511898041\n",
      "epoch: 18 step: 33, loss is 0.28208625316619873\n",
      "epoch: 18 step: 34, loss is 0.26554906368255615\n",
      "epoch: 18 step: 35, loss is 0.18083950877189636\n",
      "epoch: 18 step: 36, loss is 0.19734211266040802\n",
      "epoch: 18 step: 37, loss is 0.1609724462032318\n",
      "epoch: 18 step: 38, loss is 0.23571065068244934\n",
      "epoch: 18 step: 39, loss is 0.11324154585599899\n",
      "epoch: 18 step: 40, loss is 0.17687281966209412\n",
      "epoch: 18 step: 41, loss is 0.13473083078861237\n",
      "epoch: 18 step: 42, loss is 0.2641414999961853\n",
      "epoch: 18 step: 43, loss is 0.22066497802734375\n",
      "epoch: 18 step: 44, loss is 0.1826595515012741\n",
      "epoch: 18 step: 45, loss is 0.16361939907073975\n",
      "epoch: 18 step: 46, loss is 0.19814734160900116\n",
      "epoch: 18 step: 47, loss is 0.23789815604686737\n",
      "epoch: 18 step: 48, loss is 0.27798736095428467\n",
      "epoch: 18 step: 49, loss is 0.24403977394104004\n",
      "epoch: 18 step: 50, loss is 0.18064984679222107\n",
      "epoch: 18 step: 51, loss is 0.17453955113887787\n",
      "epoch: 18 step: 52, loss is 0.14217807352542877\n",
      "epoch: 18 step: 53, loss is 0.16875697672367096\n",
      "epoch: 18 step: 54, loss is 0.3770131468772888\n",
      "epoch: 18 step: 55, loss is 0.1895752251148224\n",
      "epoch: 18 step: 56, loss is 0.15948674082756042\n",
      "epoch: 18 step: 57, loss is 0.17683911323547363\n",
      "epoch: 18 step: 58, loss is 0.3196355402469635\n",
      "epoch: 18 step: 59, loss is 0.13695792853832245\n",
      "epoch: 18 step: 60, loss is 0.2441285103559494\n",
      "epoch: 18 step: 61, loss is 0.33641889691352844\n",
      "epoch: 18 step: 62, loss is 0.286538302898407\n",
      "epoch: 18 step: 63, loss is 0.220408633351326\n",
      "epoch: 18 step: 64, loss is 0.14284521341323853\n",
      "epoch: 18 step: 65, loss is 0.1981274038553238\n",
      "epoch: 18 step: 66, loss is 0.21903251111507416\n",
      "epoch: 18 step: 67, loss is 0.3547532856464386\n",
      "epoch: 18 step: 68, loss is 0.2134147584438324\n",
      "epoch: 18 step: 69, loss is 0.1792391538619995\n",
      "epoch: 18 step: 70, loss is 0.15419740974903107\n",
      "epoch: 18 step: 71, loss is 0.30792489647865295\n",
      "epoch: 18 step: 72, loss is 0.110261932015419\n",
      "epoch: 18 step: 73, loss is 0.20155557990074158\n",
      "epoch: 18 step: 74, loss is 0.21938365697860718\n",
      "epoch: 18 step: 75, loss is 0.32548385858535767\n",
      "epoch: 18 step: 76, loss is 0.23402583599090576\n",
      "epoch: 18 step: 77, loss is 0.1672040820121765\n",
      "epoch: 18 step: 78, loss is 0.15285509824752808\n",
      "epoch: 18 step: 79, loss is 0.11657485365867615\n",
      "epoch: 18 step: 80, loss is 0.2626519501209259\n",
      "epoch: 18 step: 81, loss is 0.12225141376256943\n",
      "epoch: 18 step: 82, loss is 0.17734333872795105\n",
      "epoch: 18 step: 83, loss is 0.1457204967737198\n",
      "epoch: 18 step: 84, loss is 0.21103017032146454\n",
      "epoch: 18 step: 85, loss is 0.17336972057819366\n",
      "epoch: 18 step: 86, loss is 0.15746518969535828\n",
      "epoch: 18 step: 87, loss is 0.150440976023674\n",
      "epoch: 18 step: 88, loss is 0.20536747574806213\n",
      "epoch: 18 step: 89, loss is 0.10395285487174988\n",
      "epoch: 18 step: 90, loss is 0.19894829392433167\n",
      "epoch: 18 step: 91, loss is 0.34830379486083984\n",
      "epoch: 18 step: 92, loss is 0.20689800381660461\n",
      "epoch: 18 step: 93, loss is 0.23101511597633362\n",
      "epoch: 18 step: 94, loss is 0.17078520357608795\n",
      "epoch: 18 step: 95, loss is 0.13838917016983032\n",
      "epoch: 18 step: 96, loss is 0.1234457716345787\n",
      "epoch: 18 step: 97, loss is 0.22256526350975037\n",
      "epoch: 18 step: 98, loss is 0.08284123241901398\n",
      "epoch: 18 step: 99, loss is 0.1337834894657135\n",
      "epoch: 18 step: 100, loss is 0.24594005942344666\n",
      "epoch: 18 step: 101, loss is 0.16250154376029968\n",
      "epoch: 18 step: 102, loss is 0.2232707440853119\n",
      "epoch: 18 step: 103, loss is 0.34012195467948914\n",
      "epoch: 18 step: 104, loss is 0.32626163959503174\n",
      "epoch: 18 step: 105, loss is 0.11753907054662704\n",
      "epoch: 18 step: 106, loss is 0.11898039281368256\n",
      "epoch: 18 step: 107, loss is 0.20488299429416656\n",
      "epoch: 18 step: 108, loss is 0.2087819129228592\n",
      "epoch: 18 step: 109, loss is 0.1706748902797699\n",
      "epoch: 18 step: 110, loss is 0.1995874047279358\n",
      "epoch: 18 step: 111, loss is 0.25018954277038574\n",
      "epoch: 18 step: 112, loss is 0.25712937116622925\n",
      "epoch: 18 step: 113, loss is 0.2398240864276886\n",
      "epoch: 18 step: 114, loss is 0.17282651364803314\n",
      "epoch: 18 step: 115, loss is 0.19066844880580902\n",
      "epoch: 18 step: 116, loss is 0.1446804404258728\n",
      "epoch: 18 step: 117, loss is 0.1820136457681656\n",
      "epoch: 18 step: 118, loss is 0.20472168922424316\n",
      "epoch: 18 step: 119, loss is 0.16775432229042053\n",
      "epoch: 18 step: 120, loss is 0.22782167792320251\n",
      "epoch: 18 step: 121, loss is 0.07793309539556503\n",
      "epoch: 18 step: 122, loss is 0.10564322024583817\n",
      "epoch: 18 step: 123, loss is 0.20574282109737396\n",
      "epoch: 18 step: 124, loss is 0.2627379894256592\n",
      "epoch: 18 step: 125, loss is 0.22119565308094025\n",
      "epoch: 18 step: 126, loss is 0.08881395310163498\n",
      "epoch: 18 step: 127, loss is 0.24460244178771973\n",
      "epoch: 18 step: 128, loss is 0.2587886154651642\n",
      "epoch: 18 step: 129, loss is 0.2814651429653168\n",
      "epoch: 18 step: 130, loss is 0.130050390958786\n",
      "epoch: 18 step: 131, loss is 0.29312828183174133\n",
      "epoch: 18 step: 132, loss is 0.38254207372665405\n",
      "epoch: 18 step: 133, loss is 0.2738109230995178\n",
      "epoch: 18 step: 134, loss is 0.4246029257774353\n",
      "epoch: 18 step: 135, loss is 0.14496511220932007\n",
      "epoch: 18 step: 136, loss is 0.21636222302913666\n",
      "epoch: 18 step: 137, loss is 0.2226065993309021\n",
      "epoch: 18 step: 138, loss is 0.27360156178474426\n",
      "epoch: 18 step: 139, loss is 0.1414080411195755\n",
      "epoch: 18 step: 140, loss is 0.20138509571552277\n",
      "epoch: 18 step: 141, loss is 0.3046170175075531\n",
      "epoch: 18 step: 142, loss is 0.1485026478767395\n",
      "epoch: 18 step: 143, loss is 0.18601374328136444\n",
      "epoch: 18 step: 144, loss is 0.20335976779460907\n",
      "epoch: 18 step: 145, loss is 0.23684526979923248\n",
      "epoch: 18 step: 146, loss is 0.13703157007694244\n",
      "epoch: 18 step: 147, loss is 0.19183573126792908\n",
      "epoch: 18 step: 148, loss is 0.1672881692647934\n",
      "epoch: 18 step: 149, loss is 0.16512253880500793\n",
      "epoch: 18 step: 150, loss is 0.1400393396615982\n",
      "epoch: 18 step: 151, loss is 0.14878875017166138\n",
      "epoch: 18 step: 152, loss is 0.2628001570701599\n",
      "epoch: 18 step: 153, loss is 0.15651525557041168\n",
      "epoch: 18 step: 154, loss is 0.14625367522239685\n",
      "epoch: 18 step: 155, loss is 0.0885193794965744\n",
      "epoch: 18 step: 156, loss is 0.10379493236541748\n",
      "epoch: 18 step: 157, loss is 0.364840030670166\n",
      "epoch: 18 step: 158, loss is 0.08636333048343658\n",
      "epoch: 18 step: 159, loss is 0.15238279104232788\n",
      "epoch: 18 step: 160, loss is 0.08307315409183502\n",
      "epoch: 18 step: 161, loss is 0.2052178680896759\n",
      "epoch: 18 step: 162, loss is 0.14117106795310974\n",
      "epoch: 18 step: 163, loss is 0.25785359740257263\n",
      "epoch: 18 step: 164, loss is 0.12801025807857513\n",
      "epoch: 18 step: 165, loss is 0.10064023733139038\n",
      "epoch: 18 step: 166, loss is 0.18329589068889618\n",
      "epoch: 18 step: 167, loss is 0.10491982847452164\n",
      "epoch: 18 step: 168, loss is 0.17658546566963196\n",
      "epoch: 18 step: 169, loss is 0.15939274430274963\n",
      "epoch: 18 step: 170, loss is 0.1328224390745163\n",
      "epoch: 18 step: 171, loss is 0.15944786369800568\n",
      "epoch: 18 step: 172, loss is 0.1442812979221344\n",
      "epoch: 18 step: 173, loss is 0.12262093275785446\n",
      "epoch: 18 step: 174, loss is 0.14564195275306702\n",
      "epoch: 18 step: 175, loss is 0.2268182784318924\n",
      "epoch: 18 step: 176, loss is 0.2371806800365448\n",
      "epoch: 18 step: 177, loss is 0.1957789957523346\n",
      "epoch: 18 step: 178, loss is 0.2957470118999481\n",
      "epoch: 18 step: 179, loss is 0.24757510423660278\n",
      "epoch: 18 step: 180, loss is 0.10507708787918091\n",
      "epoch: 18 step: 181, loss is 0.24410229921340942\n",
      "epoch: 18 step: 182, loss is 0.2500063180923462\n",
      "epoch: 18 step: 183, loss is 0.1850472092628479\n",
      "epoch: 18 step: 184, loss is 0.16460378468036652\n",
      "epoch: 18 step: 185, loss is 0.20514488220214844\n",
      "epoch: 18 step: 186, loss is 0.18236221373081207\n",
      "epoch: 18 step: 187, loss is 0.3305659592151642\n",
      "epoch: 18 step: 188, loss is 0.2946926951408386\n",
      "epoch: 18 step: 189, loss is 0.23859375715255737\n",
      "epoch: 18 step: 190, loss is 0.3087829053401947\n",
      "epoch: 18 step: 191, loss is 0.17242494225502014\n",
      "epoch: 18 step: 192, loss is 0.23116222023963928\n",
      "epoch: 18 step: 193, loss is 0.22215957939624786\n",
      "epoch: 18 step: 194, loss is 0.13516296446323395\n",
      "epoch: 18 step: 195, loss is 0.09437474608421326\n",
      "epoch: 18 step: 196, loss is 0.10840487480163574\n",
      "epoch: 18 step: 197, loss is 0.1616632044315338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 198, loss is 0.20299400389194489\n",
      "epoch: 18 step: 199, loss is 0.14955531060695648\n",
      "epoch: 18 step: 200, loss is 0.11934999376535416\n",
      "epoch: 18 step: 201, loss is 0.10841972380876541\n",
      "epoch: 18 step: 202, loss is 0.25361883640289307\n",
      "epoch: 18 step: 203, loss is 0.13823743164539337\n",
      "epoch: 18 step: 204, loss is 0.278925359249115\n",
      "epoch: 18 step: 205, loss is 0.19927743077278137\n",
      "epoch: 18 step: 206, loss is 0.2949158847332001\n",
      "epoch: 18 step: 207, loss is 0.2742035984992981\n",
      "epoch: 18 step: 208, loss is 0.22121025621891022\n",
      "epoch: 18 step: 209, loss is 0.31104734539985657\n",
      "epoch: 18 step: 210, loss is 0.281656950712204\n",
      "epoch: 18 step: 211, loss is 0.12345650792121887\n",
      "epoch: 18 step: 212, loss is 0.26433253288269043\n",
      "epoch: 18 step: 213, loss is 0.18042483925819397\n",
      "epoch: 18 step: 214, loss is 0.18949930369853973\n",
      "epoch: 18 step: 215, loss is 0.11421962082386017\n",
      "epoch: 18 step: 216, loss is 0.16353099048137665\n",
      "epoch: 18 step: 217, loss is 0.16305819153785706\n",
      "epoch: 18 step: 218, loss is 0.3782101571559906\n",
      "epoch: 18 step: 219, loss is 0.1875583529472351\n",
      "epoch: 18 step: 220, loss is 0.12501761317253113\n",
      "epoch: 18 step: 221, loss is 0.31600499153137207\n",
      "epoch: 18 step: 222, loss is 0.09465671330690384\n",
      "epoch: 18 step: 223, loss is 0.2234710156917572\n",
      "epoch: 18 step: 224, loss is 0.264754056930542\n",
      "epoch: 18 step: 225, loss is 0.24292594194412231\n",
      "epoch: 18 step: 226, loss is 0.22180932760238647\n",
      "epoch: 18 step: 227, loss is 0.1022230014204979\n",
      "epoch: 18 step: 228, loss is 0.18220362067222595\n",
      "epoch: 18 step: 229, loss is 0.1870967596769333\n",
      "epoch: 18 step: 230, loss is 0.20109067857265472\n",
      "epoch: 18 step: 231, loss is 0.24402892589569092\n",
      "epoch: 18 step: 232, loss is 0.3103145956993103\n",
      "epoch: 18 step: 233, loss is 0.19404315948486328\n",
      "epoch: 18 step: 234, loss is 0.34639397263526917\n",
      "epoch: 18 step: 235, loss is 0.2748955488204956\n",
      "epoch: 18 step: 236, loss is 0.19114112854003906\n",
      "epoch: 18 step: 237, loss is 0.10234767943620682\n",
      "epoch: 18 step: 238, loss is 0.11243545264005661\n",
      "epoch: 18 step: 239, loss is 0.27628040313720703\n",
      "epoch: 18 step: 240, loss is 0.18356290459632874\n",
      "epoch: 18 step: 241, loss is 0.26206737756729126\n",
      "epoch: 18 step: 242, loss is 0.22933503985404968\n",
      "epoch: 18 step: 243, loss is 0.32752254605293274\n",
      "epoch: 18 step: 244, loss is 0.13162760436534882\n",
      "epoch: 18 step: 245, loss is 0.26803258061408997\n",
      "epoch: 18 step: 246, loss is 0.29884642362594604\n",
      "epoch: 18 step: 247, loss is 0.17805036902427673\n",
      "epoch: 18 step: 248, loss is 0.26657623052597046\n",
      "epoch: 18 step: 249, loss is 0.26353704929351807\n",
      "epoch: 18 step: 250, loss is 0.13866256177425385\n",
      "epoch: 18 step: 251, loss is 0.1534980982542038\n",
      "epoch: 18 step: 252, loss is 0.14585714042186737\n",
      "epoch: 18 step: 253, loss is 0.28618037700653076\n",
      "epoch: 18 step: 254, loss is 0.21464745700359344\n",
      "epoch: 18 step: 255, loss is 0.3488437235355377\n",
      "epoch: 18 step: 256, loss is 0.27643951773643494\n",
      "epoch: 18 step: 257, loss is 0.26069310307502747\n",
      "epoch: 18 step: 258, loss is 0.13776862621307373\n",
      "epoch: 18 step: 259, loss is 0.09275583922863007\n",
      "epoch: 18 step: 260, loss is 0.37805548310279846\n",
      "epoch: 18 step: 261, loss is 0.29880696535110474\n",
      "epoch: 18 step: 262, loss is 0.2519889771938324\n",
      "epoch: 18 step: 263, loss is 0.2141830176115036\n",
      "epoch: 18 step: 264, loss is 0.10181699693202972\n",
      "epoch: 18 step: 265, loss is 0.28022217750549316\n",
      "epoch: 18 step: 266, loss is 0.10178904980421066\n",
      "epoch: 18 step: 267, loss is 0.24491651356220245\n",
      "epoch: 18 step: 268, loss is 0.3292752802371979\n",
      "epoch: 18 step: 269, loss is 0.18378084897994995\n",
      "epoch: 18 step: 270, loss is 0.10349743068218231\n",
      "epoch: 18 step: 271, loss is 0.27206897735595703\n",
      "epoch: 18 step: 272, loss is 0.21348735690116882\n",
      "epoch: 18 step: 273, loss is 0.24985098838806152\n",
      "epoch: 18 step: 274, loss is 0.13552285730838776\n",
      "epoch: 18 step: 275, loss is 0.09772586822509766\n",
      "epoch: 18 step: 276, loss is 0.3453294038772583\n",
      "epoch: 18 step: 277, loss is 0.2168743908405304\n",
      "epoch: 18 step: 278, loss is 0.41092485189437866\n",
      "epoch: 18 step: 279, loss is 0.16380053758621216\n",
      "epoch: 18 step: 280, loss is 0.35379448533058167\n",
      "epoch: 18 step: 281, loss is 0.17798994481563568\n",
      "epoch: 18 step: 282, loss is 0.10864619165658951\n",
      "epoch: 18 step: 283, loss is 0.25735846161842346\n",
      "epoch: 18 step: 284, loss is 0.18079330027103424\n",
      "epoch: 18 step: 285, loss is 0.15951278805732727\n",
      "epoch: 18 step: 286, loss is 0.2385217249393463\n",
      "epoch: 18 step: 287, loss is 0.13032984733581543\n",
      "epoch: 18 step: 288, loss is 0.14761444926261902\n",
      "epoch: 18 step: 289, loss is 0.1989193707704544\n",
      "epoch: 18 step: 290, loss is 0.10798030346632004\n",
      "epoch: 18 step: 291, loss is 0.20895375311374664\n",
      "epoch: 18 step: 292, loss is 0.19756633043289185\n",
      "epoch: 18 step: 293, loss is 0.1245606541633606\n",
      "epoch: 18 step: 294, loss is 0.13720324635505676\n",
      "epoch: 18 step: 295, loss is 0.13220854103565216\n",
      "epoch: 18 step: 296, loss is 0.25981298089027405\n",
      "epoch: 18 step: 297, loss is 0.17892450094223022\n",
      "epoch: 18 step: 298, loss is 0.1656876653432846\n",
      "epoch: 18 step: 299, loss is 0.29687801003456116\n",
      "epoch: 18 step: 300, loss is 0.2496979683637619\n",
      "epoch: 18 step: 301, loss is 0.22740933299064636\n",
      "epoch: 18 step: 302, loss is 0.32426634430885315\n",
      "epoch: 18 step: 303, loss is 0.17574745416641235\n",
      "epoch: 18 step: 304, loss is 0.21144795417785645\n",
      "epoch: 18 step: 305, loss is 0.26678141951560974\n",
      "epoch: 18 step: 306, loss is 0.17158401012420654\n",
      "epoch: 18 step: 307, loss is 0.20561087131500244\n",
      "epoch: 18 step: 308, loss is 0.14101433753967285\n",
      "epoch: 18 step: 309, loss is 0.19076460599899292\n",
      "epoch: 18 step: 310, loss is 0.11000927537679672\n",
      "epoch: 18 step: 311, loss is 0.16429203748703003\n",
      "epoch: 18 step: 312, loss is 0.1617109626531601\n",
      "epoch: 18 step: 313, loss is 0.24678272008895874\n",
      "epoch: 18 step: 314, loss is 0.14252156019210815\n",
      "epoch: 18 step: 315, loss is 0.12926100194454193\n",
      "epoch: 18 step: 316, loss is 0.2376042902469635\n",
      "epoch: 18 step: 317, loss is 0.10030520707368851\n",
      "epoch: 18 step: 318, loss is 0.11622019857168198\n",
      "epoch: 18 step: 319, loss is 0.14416947960853577\n",
      "epoch: 18 step: 320, loss is 0.19560812413692474\n",
      "epoch: 18 step: 321, loss is 0.0498393289744854\n",
      "epoch: 18 step: 322, loss is 0.11804287135601044\n",
      "epoch: 18 step: 323, loss is 0.2719787359237671\n",
      "epoch: 18 step: 324, loss is 0.169886976480484\n",
      "epoch: 18 step: 325, loss is 0.15519878268241882\n",
      "epoch: 18 step: 326, loss is 0.08494739979505539\n",
      "epoch: 18 step: 327, loss is 0.08966631442308426\n",
      "epoch: 18 step: 328, loss is 0.3764816224575043\n",
      "epoch: 18 step: 329, loss is 0.07889813184738159\n",
      "epoch: 18 step: 330, loss is 0.09719669073820114\n",
      "epoch: 18 step: 331, loss is 0.13051728904247284\n",
      "epoch: 18 step: 332, loss is 0.2281714528799057\n",
      "epoch: 18 step: 333, loss is 0.16799940168857574\n",
      "epoch: 18 step: 334, loss is 0.11523710936307907\n",
      "epoch: 18 step: 335, loss is 0.16622887551784515\n",
      "epoch: 18 step: 336, loss is 0.10514827817678452\n",
      "epoch: 18 step: 337, loss is 0.2561444342136383\n",
      "epoch: 18 step: 338, loss is 0.16642539203166962\n",
      "epoch: 18 step: 339, loss is 0.2705361843109131\n",
      "epoch: 18 step: 340, loss is 0.2040218710899353\n",
      "epoch: 18 step: 341, loss is 0.17529761791229248\n",
      "epoch: 18 step: 342, loss is 0.09617585688829422\n",
      "epoch: 18 step: 343, loss is 0.11733309924602509\n",
      "epoch: 18 step: 344, loss is 0.38607683777809143\n",
      "epoch: 18 step: 345, loss is 0.12183167040348053\n",
      "epoch: 18 step: 346, loss is 0.14597851037979126\n",
      "epoch: 18 step: 347, loss is 0.18306408822536469\n",
      "epoch: 18 step: 348, loss is 0.2222713828086853\n",
      "epoch: 18 step: 349, loss is 0.37609952688217163\n",
      "epoch: 18 step: 350, loss is 0.17824359238147736\n",
      "epoch: 18 step: 351, loss is 0.19833922386169434\n",
      "epoch: 18 step: 352, loss is 0.192196324467659\n",
      "epoch: 18 step: 353, loss is 0.12309666723012924\n",
      "epoch: 18 step: 354, loss is 0.28506848216056824\n",
      "epoch: 18 step: 355, loss is 0.17235828936100006\n",
      "epoch: 18 step: 356, loss is 0.10443121194839478\n",
      "epoch: 18 step: 357, loss is 0.13375703990459442\n",
      "epoch: 18 step: 358, loss is 0.11882104724645615\n",
      "epoch: 18 step: 359, loss is 0.14979465305805206\n",
      "epoch: 18 step: 360, loss is 0.13767388463020325\n",
      "epoch: 18 step: 361, loss is 0.30573198199272156\n",
      "epoch: 18 step: 362, loss is 0.13941672444343567\n",
      "epoch: 18 step: 363, loss is 0.18511037528514862\n",
      "epoch: 18 step: 364, loss is 0.2042299211025238\n",
      "epoch: 18 step: 365, loss is 0.17795777320861816\n",
      "epoch: 18 step: 366, loss is 0.2122170478105545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 367, loss is 0.23351481556892395\n",
      "epoch: 18 step: 368, loss is 0.1191798597574234\n",
      "epoch: 18 step: 369, loss is 0.05366046354174614\n",
      "epoch: 18 step: 370, loss is 0.33892878890037537\n",
      "epoch: 18 step: 371, loss is 0.15336358547210693\n",
      "epoch: 18 step: 372, loss is 0.1389889270067215\n",
      "epoch: 18 step: 373, loss is 0.22471973299980164\n",
      "epoch: 18 step: 374, loss is 0.10815054923295975\n",
      "epoch: 18 step: 375, loss is 0.39205074310302734\n",
      "epoch: 18 step: 376, loss is 0.1665339171886444\n",
      "epoch: 18 step: 377, loss is 0.1725313514471054\n",
      "epoch: 18 step: 378, loss is 0.13939981162548065\n",
      "epoch: 18 step: 379, loss is 0.18279117345809937\n",
      "epoch: 18 step: 380, loss is 0.3231049180030823\n",
      "epoch: 18 step: 381, loss is 0.19801101088523865\n",
      "epoch: 18 step: 382, loss is 0.09055671840906143\n",
      "epoch: 18 step: 383, loss is 0.26986491680145264\n",
      "epoch: 18 step: 384, loss is 0.25065284967422485\n",
      "epoch: 18 step: 385, loss is 0.28386473655700684\n",
      "epoch: 18 step: 386, loss is 0.41711291670799255\n",
      "epoch: 18 step: 387, loss is 0.11743621528148651\n",
      "epoch: 18 step: 388, loss is 0.4254304766654968\n",
      "epoch: 18 step: 389, loss is 0.23454181849956512\n",
      "epoch: 18 step: 390, loss is 0.19442136585712433\n",
      "epoch: 18 step: 391, loss is 0.31953534483909607\n",
      "epoch: 18 step: 392, loss is 0.22105152904987335\n",
      "epoch: 18 step: 393, loss is 0.273363322019577\n",
      "epoch: 18 step: 394, loss is 0.11646715551614761\n",
      "epoch: 18 step: 395, loss is 0.24444885551929474\n",
      "epoch: 18 step: 396, loss is 0.29813820123672485\n",
      "epoch: 18 step: 397, loss is 0.21185730397701263\n",
      "epoch: 18 step: 398, loss is 0.11796781420707703\n",
      "epoch: 18 step: 399, loss is 0.20609934628009796\n",
      "epoch: 18 step: 400, loss is 0.10557305812835693\n",
      "epoch: 18 step: 401, loss is 0.1892060935497284\n",
      "epoch: 18 step: 402, loss is 0.16501875221729279\n",
      "epoch: 18 step: 403, loss is 0.14628450572490692\n",
      "epoch: 18 step: 404, loss is 0.28723371028900146\n",
      "epoch: 18 step: 405, loss is 0.12426349520683289\n",
      "epoch: 18 step: 406, loss is 0.2070235162973404\n",
      "epoch: 18 step: 407, loss is 0.1934165209531784\n",
      "epoch: 18 step: 408, loss is 0.1410341113805771\n",
      "epoch: 18 step: 409, loss is 0.17671044170856476\n",
      "epoch: 18 step: 410, loss is 0.31991180777549744\n",
      "epoch: 18 step: 411, loss is 0.10221178829669952\n",
      "epoch: 18 step: 412, loss is 0.19078446924686432\n",
      "epoch: 18 step: 413, loss is 0.1371469348669052\n",
      "epoch: 18 step: 414, loss is 0.17865276336669922\n",
      "epoch: 18 step: 415, loss is 0.17324796319007874\n",
      "epoch: 18 step: 416, loss is 0.22466127574443817\n",
      "epoch: 18 step: 417, loss is 0.18883121013641357\n",
      "epoch: 18 step: 418, loss is 0.2568771243095398\n",
      "epoch: 18 step: 419, loss is 0.1401555836200714\n",
      "epoch: 18 step: 420, loss is 0.1149408370256424\n",
      "epoch: 18 step: 421, loss is 0.2830726206302643\n",
      "epoch: 18 step: 422, loss is 0.18877266347408295\n",
      "epoch: 18 step: 423, loss is 0.15917350351810455\n",
      "epoch: 18 step: 424, loss is 0.18111048638820648\n",
      "epoch: 18 step: 425, loss is 0.1116836741566658\n",
      "epoch: 18 step: 426, loss is 0.1962764710187912\n",
      "epoch: 18 step: 427, loss is 0.1829512119293213\n",
      "epoch: 18 step: 428, loss is 0.19798697531223297\n",
      "epoch: 18 step: 429, loss is 0.263370543718338\n",
      "epoch: 18 step: 430, loss is 0.15241867303848267\n",
      "epoch: 18 step: 431, loss is 0.10325422883033752\n",
      "epoch: 18 step: 432, loss is 0.19246304035186768\n",
      "epoch: 18 step: 433, loss is 0.24072399735450745\n",
      "epoch: 18 step: 434, loss is 0.12805600464344025\n",
      "epoch: 18 step: 435, loss is 0.15544724464416504\n",
      "epoch: 18 step: 436, loss is 0.24476563930511475\n",
      "epoch: 18 step: 437, loss is 0.3296719789505005\n",
      "epoch: 18 step: 438, loss is 0.29624617099761963\n",
      "epoch: 18 step: 439, loss is 0.24184629321098328\n",
      "epoch: 18 step: 440, loss is 0.16844993829727173\n",
      "epoch: 18 step: 441, loss is 0.3521619141101837\n",
      "epoch: 18 step: 442, loss is 0.23698149621486664\n",
      "epoch: 18 step: 443, loss is 0.2730232775211334\n",
      "epoch: 18 step: 444, loss is 0.1463303565979004\n",
      "epoch: 18 step: 445, loss is 0.1632797122001648\n",
      "epoch: 18 step: 446, loss is 0.1968732476234436\n",
      "epoch: 18 step: 447, loss is 0.16910654306411743\n",
      "epoch: 18 step: 448, loss is 0.14511068165302277\n",
      "epoch: 18 step: 449, loss is 0.13749036192893982\n",
      "epoch: 18 step: 450, loss is 0.0672559067606926\n",
      "epoch: 18 step: 451, loss is 0.10477801412343979\n",
      "epoch: 18 step: 452, loss is 0.24470436573028564\n",
      "epoch: 18 step: 453, loss is 0.20308485627174377\n",
      "epoch: 18 step: 454, loss is 0.1420472264289856\n",
      "epoch: 18 step: 455, loss is 0.17376188933849335\n",
      "epoch: 18 step: 456, loss is 0.17414407432079315\n",
      "epoch: 18 step: 457, loss is 0.25014570355415344\n",
      "epoch: 18 step: 458, loss is 0.1645916849374771\n",
      "epoch: 18 step: 459, loss is 0.16615845263004303\n",
      "epoch: 18 step: 460, loss is 0.12372508645057678\n",
      "epoch: 18 step: 461, loss is 0.23636223375797272\n",
      "epoch: 18 step: 462, loss is 0.1246347725391388\n",
      "epoch: 18 step: 463, loss is 0.15608586370944977\n",
      "epoch: 18 step: 464, loss is 0.11882749944925308\n",
      "epoch: 18 step: 465, loss is 0.3293703496456146\n",
      "epoch: 18 step: 466, loss is 0.12492378801107407\n",
      "epoch: 18 step: 467, loss is 0.26476797461509705\n",
      "epoch: 18 step: 468, loss is 0.29832303524017334\n",
      "epoch: 18 step: 469, loss is 0.13942725956439972\n",
      "epoch: 18 step: 470, loss is 0.22491927444934845\n",
      "epoch: 18 step: 471, loss is 0.29853901267051697\n",
      "epoch: 18 step: 472, loss is 0.1578272432088852\n",
      "epoch: 18 step: 473, loss is 0.2162599414587021\n",
      "epoch: 18 step: 474, loss is 0.3447982966899872\n",
      "epoch: 18 step: 475, loss is 0.21134689450263977\n",
      "epoch: 18 step: 476, loss is 0.28430864214897156\n",
      "epoch: 18 step: 477, loss is 0.26402410864830017\n",
      "epoch: 18 step: 478, loss is 0.1999931037425995\n",
      "epoch: 18 step: 479, loss is 0.21761897206306458\n",
      "epoch: 18 step: 480, loss is 0.1457238793373108\n",
      "epoch: 18 step: 481, loss is 0.1276092231273651\n",
      "epoch: 18 step: 482, loss is 0.2449544370174408\n",
      "epoch: 18 step: 483, loss is 0.2313038557767868\n",
      "epoch: 18 step: 484, loss is 0.3373032510280609\n",
      "epoch: 18 step: 485, loss is 0.21566690504550934\n",
      "epoch: 18 step: 486, loss is 0.24620124697685242\n",
      "epoch: 18 step: 487, loss is 0.11445636302232742\n",
      "epoch: 18 step: 488, loss is 0.1388542652130127\n",
      "epoch: 18 step: 489, loss is 0.18881447613239288\n",
      "epoch: 18 step: 490, loss is 0.13242483139038086\n",
      "epoch: 18 step: 491, loss is 0.1334676295518875\n",
      "epoch: 18 step: 492, loss is 0.1424218714237213\n",
      "epoch: 18 step: 493, loss is 0.31117960810661316\n",
      "epoch: 18 step: 494, loss is 0.19871969521045685\n",
      "epoch: 18 step: 495, loss is 0.16445483267307281\n",
      "epoch: 18 step: 496, loss is 0.26311564445495605\n",
      "epoch: 18 step: 497, loss is 0.26951754093170166\n",
      "epoch: 18 step: 498, loss is 0.1451300084590912\n",
      "epoch: 18 step: 499, loss is 0.24173438549041748\n",
      "epoch: 18 step: 500, loss is 0.22759252786636353\n",
      "epoch: 18 step: 501, loss is 0.4223087728023529\n",
      "epoch: 18 step: 502, loss is 0.4222947955131531\n",
      "epoch: 18 step: 503, loss is 0.35478532314300537\n",
      "epoch: 18 step: 504, loss is 0.2615750730037689\n",
      "epoch: 18 step: 505, loss is 0.1414056271314621\n",
      "epoch: 18 step: 506, loss is 0.1455390751361847\n",
      "epoch: 18 step: 507, loss is 0.11182409524917603\n",
      "epoch: 18 step: 508, loss is 0.2521939277648926\n",
      "epoch: 18 step: 509, loss is 0.19206936657428741\n",
      "epoch: 18 step: 510, loss is 0.2203660011291504\n",
      "epoch: 18 step: 511, loss is 0.19561494886875153\n",
      "epoch: 18 step: 512, loss is 0.292807012796402\n",
      "epoch: 18 step: 513, loss is 0.2042330950498581\n",
      "epoch: 18 step: 514, loss is 0.11239989101886749\n",
      "epoch: 18 step: 515, loss is 0.2021239995956421\n",
      "epoch: 18 step: 516, loss is 0.14892518520355225\n",
      "epoch: 18 step: 517, loss is 0.23080404102802277\n",
      "epoch: 18 step: 518, loss is 0.23433677852153778\n",
      "epoch: 18 step: 519, loss is 0.32551467418670654\n",
      "epoch: 18 step: 520, loss is 0.23042859137058258\n",
      "epoch: 18 step: 521, loss is 0.125541090965271\n",
      "epoch: 18 step: 522, loss is 0.15444199740886688\n",
      "epoch: 18 step: 523, loss is 0.11827170103788376\n",
      "epoch: 18 step: 524, loss is 0.2581004500389099\n",
      "epoch: 18 step: 525, loss is 0.2571096122264862\n",
      "epoch: 18 step: 526, loss is 0.2163800150156021\n",
      "epoch: 18 step: 527, loss is 0.36072853207588196\n",
      "epoch: 18 step: 528, loss is 0.2678791582584381\n",
      "epoch: 18 step: 529, loss is 0.1412350833415985\n",
      "epoch: 18 step: 530, loss is 0.1839233785867691\n",
      "epoch: 18 step: 531, loss is 0.2068856656551361\n",
      "epoch: 18 step: 532, loss is 0.14014703035354614\n",
      "epoch: 18 step: 533, loss is 0.21415787935256958\n",
      "epoch: 18 step: 534, loss is 0.12270178645849228\n",
      "epoch: 18 step: 535, loss is 0.14287416636943817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 536, loss is 0.31480321288108826\n",
      "epoch: 18 step: 537, loss is 0.18745839595794678\n",
      "epoch: 18 step: 538, loss is 0.16107463836669922\n",
      "epoch: 18 step: 539, loss is 0.1329924762248993\n",
      "epoch: 18 step: 540, loss is 0.20670582354068756\n",
      "epoch: 18 step: 541, loss is 0.1691247522830963\n",
      "epoch: 18 step: 542, loss is 0.14742602407932281\n",
      "epoch: 18 step: 543, loss is 0.2921745777130127\n",
      "epoch: 18 step: 544, loss is 0.19227507710456848\n",
      "epoch: 18 step: 545, loss is 0.16514183580875397\n",
      "epoch: 18 step: 546, loss is 0.2564856708049774\n",
      "epoch: 18 step: 547, loss is 0.25656887888908386\n",
      "epoch: 18 step: 548, loss is 0.2749464213848114\n",
      "epoch: 18 step: 549, loss is 0.26763081550598145\n",
      "epoch: 18 step: 550, loss is 0.12548521161079407\n",
      "epoch: 18 step: 551, loss is 0.16549012064933777\n",
      "epoch: 18 step: 552, loss is 0.17574171721935272\n",
      "epoch: 18 step: 553, loss is 0.1270226538181305\n",
      "epoch: 18 step: 554, loss is 0.4497283399105072\n",
      "epoch: 18 step: 555, loss is 0.36184102296829224\n",
      "epoch: 18 step: 556, loss is 0.22260279953479767\n",
      "epoch: 18 step: 557, loss is 0.31381216645240784\n",
      "epoch: 18 step: 558, loss is 0.2650698721408844\n",
      "epoch: 18 step: 559, loss is 0.30180928111076355\n",
      "epoch: 18 step: 560, loss is 0.23646299540996552\n",
      "epoch: 18 step: 561, loss is 0.172896146774292\n",
      "epoch: 18 step: 562, loss is 0.42132091522216797\n",
      "epoch: 18 step: 563, loss is 0.1706627756357193\n",
      "epoch: 18 step: 564, loss is 0.48447468876838684\n",
      "epoch: 18 step: 565, loss is 0.13317765295505524\n",
      "epoch: 18 step: 566, loss is 0.23029819130897522\n",
      "epoch: 18 step: 567, loss is 0.1864210069179535\n",
      "epoch: 18 step: 568, loss is 0.16445717215538025\n",
      "epoch: 18 step: 569, loss is 0.34133049845695496\n",
      "epoch: 18 step: 570, loss is 0.1337444931268692\n",
      "epoch: 18 step: 571, loss is 0.354092538356781\n",
      "epoch: 18 step: 572, loss is 0.11682268232107162\n",
      "epoch: 18 step: 573, loss is 0.11163125932216644\n",
      "epoch: 18 step: 574, loss is 0.1411130130290985\n",
      "epoch: 18 step: 575, loss is 0.13685429096221924\n",
      "epoch: 18 step: 576, loss is 0.1816750317811966\n",
      "epoch: 18 step: 577, loss is 0.2712545692920685\n",
      "epoch: 18 step: 578, loss is 0.14594954252243042\n",
      "epoch: 18 step: 579, loss is 0.2511597275733948\n",
      "epoch: 18 step: 580, loss is 0.25521165132522583\n",
      "epoch: 18 step: 581, loss is 0.36859816312789917\n",
      "epoch: 18 step: 582, loss is 0.23703403770923615\n",
      "epoch: 18 step: 583, loss is 0.2736824154853821\n",
      "epoch: 18 step: 584, loss is 0.24864210188388824\n",
      "epoch: 18 step: 585, loss is 0.19235999882221222\n",
      "epoch: 18 step: 586, loss is 0.19750620424747467\n",
      "epoch: 18 step: 587, loss is 0.25298675894737244\n",
      "epoch: 18 step: 588, loss is 0.08282237499952316\n",
      "epoch: 18 step: 589, loss is 0.29555460810661316\n",
      "epoch: 18 step: 590, loss is 0.1316407322883606\n",
      "epoch: 18 step: 591, loss is 0.20590978860855103\n",
      "epoch: 18 step: 592, loss is 0.18035069108009338\n",
      "epoch: 18 step: 593, loss is 0.09632362425327301\n",
      "epoch: 18 step: 594, loss is 0.16973239183425903\n",
      "epoch: 18 step: 595, loss is 0.29036054015159607\n",
      "epoch: 18 step: 596, loss is 0.24581117928028107\n",
      "epoch: 18 step: 597, loss is 0.04010320454835892\n",
      "epoch: 18 step: 598, loss is 0.20877081155776978\n",
      "epoch: 18 step: 599, loss is 0.10128699243068695\n",
      "epoch: 18 step: 600, loss is 0.20820564031600952\n",
      "epoch: 18 step: 601, loss is 0.31727588176727295\n",
      "epoch: 18 step: 602, loss is 0.19738146662712097\n",
      "epoch: 18 step: 603, loss is 0.25053641200065613\n",
      "epoch: 18 step: 604, loss is 0.22881993651390076\n",
      "epoch: 18 step: 605, loss is 0.2527296543121338\n",
      "epoch: 18 step: 606, loss is 0.1835184395313263\n",
      "epoch: 18 step: 607, loss is 0.15279623866081238\n",
      "epoch: 18 step: 608, loss is 0.25744929909706116\n",
      "epoch: 18 step: 609, loss is 0.2927456796169281\n",
      "epoch: 18 step: 610, loss is 0.3498072326183319\n",
      "epoch: 18 step: 611, loss is 0.42957693338394165\n",
      "epoch: 18 step: 612, loss is 0.1662812978029251\n",
      "epoch: 18 step: 613, loss is 0.21922746300697327\n",
      "epoch: 18 step: 614, loss is 0.116356261074543\n",
      "epoch: 18 step: 615, loss is 0.07927460968494415\n",
      "epoch: 18 step: 616, loss is 0.2686997652053833\n",
      "epoch: 18 step: 617, loss is 0.17065103352069855\n",
      "epoch: 18 step: 618, loss is 0.20058251917362213\n",
      "epoch: 18 step: 619, loss is 0.1474657654762268\n",
      "epoch: 18 step: 620, loss is 0.2637324631214142\n",
      "epoch: 18 step: 621, loss is 0.2470616102218628\n",
      "epoch: 18 step: 622, loss is 0.2834518253803253\n",
      "epoch: 18 step: 623, loss is 0.2686460018157959\n",
      "epoch: 18 step: 624, loss is 0.3276042342185974\n",
      "epoch: 18 step: 625, loss is 0.21149618923664093\n",
      "epoch: 18 step: 626, loss is 0.17403388023376465\n",
      "epoch: 18 step: 627, loss is 0.16043493151664734\n",
      "epoch: 18 step: 628, loss is 0.30143046379089355\n",
      "epoch: 18 step: 629, loss is 0.13907389342784882\n",
      "epoch: 18 step: 630, loss is 0.09982657432556152\n",
      "epoch: 18 step: 631, loss is 0.14081647992134094\n",
      "epoch: 18 step: 632, loss is 0.2177741378545761\n",
      "epoch: 18 step: 633, loss is 0.2248847782611847\n",
      "epoch: 18 step: 634, loss is 0.1718815714120865\n",
      "epoch: 18 step: 635, loss is 0.15711095929145813\n",
      "epoch: 18 step: 636, loss is 0.16567616164684296\n",
      "epoch: 18 step: 637, loss is 0.22722147405147552\n",
      "epoch: 18 step: 638, loss is 0.16366447508335114\n",
      "epoch: 18 step: 639, loss is 0.06307066231966019\n",
      "epoch: 18 step: 640, loss is 0.18703927099704742\n",
      "epoch: 18 step: 641, loss is 0.25490427017211914\n",
      "epoch: 18 step: 642, loss is 0.1512906700372696\n",
      "epoch: 18 step: 643, loss is 0.19669361412525177\n",
      "epoch: 18 step: 644, loss is 0.20544375479221344\n",
      "epoch: 18 step: 645, loss is 0.1610075831413269\n",
      "epoch: 18 step: 646, loss is 0.2526533603668213\n",
      "epoch: 18 step: 647, loss is 0.2223416417837143\n",
      "epoch: 18 step: 648, loss is 0.3471350073814392\n",
      "epoch: 18 step: 649, loss is 0.20151035487651825\n",
      "epoch: 18 step: 650, loss is 0.20689113438129425\n",
      "epoch: 18 step: 651, loss is 0.19953776895999908\n",
      "epoch: 18 step: 652, loss is 0.23802024126052856\n",
      "epoch: 18 step: 653, loss is 0.1825864464044571\n",
      "epoch: 18 step: 654, loss is 0.21272197365760803\n",
      "epoch: 18 step: 655, loss is 0.17569003999233246\n",
      "epoch: 18 step: 656, loss is 0.22317063808441162\n",
      "epoch: 18 step: 657, loss is 0.2204250693321228\n",
      "epoch: 18 step: 658, loss is 0.2976294159889221\n",
      "epoch: 18 step: 659, loss is 0.3050834536552429\n",
      "epoch: 18 step: 660, loss is 0.3482876420021057\n",
      "epoch: 18 step: 661, loss is 0.2374182641506195\n",
      "epoch: 18 step: 662, loss is 0.15497927367687225\n",
      "epoch: 18 step: 663, loss is 0.2864026129245758\n",
      "epoch: 18 step: 664, loss is 0.373645544052124\n",
      "epoch: 18 step: 665, loss is 0.16824978590011597\n",
      "epoch: 18 step: 666, loss is 0.5002859830856323\n",
      "epoch: 18 step: 667, loss is 0.15126660466194153\n",
      "epoch: 18 step: 668, loss is 0.19195137917995453\n",
      "epoch: 18 step: 669, loss is 0.20293188095092773\n",
      "epoch: 18 step: 670, loss is 0.09095272421836853\n",
      "epoch: 18 step: 671, loss is 0.1133958175778389\n",
      "epoch: 18 step: 672, loss is 0.17652525007724762\n",
      "epoch: 18 step: 673, loss is 0.15483763813972473\n",
      "epoch: 18 step: 674, loss is 0.3705674409866333\n",
      "epoch: 18 step: 675, loss is 0.22991597652435303\n",
      "epoch: 18 step: 676, loss is 0.12221194058656693\n",
      "epoch: 18 step: 677, loss is 0.139824777841568\n",
      "epoch: 18 step: 678, loss is 0.36407577991485596\n",
      "epoch: 18 step: 679, loss is 0.23340684175491333\n",
      "epoch: 18 step: 680, loss is 0.15405739843845367\n",
      "epoch: 18 step: 681, loss is 0.16242119669914246\n",
      "epoch: 18 step: 682, loss is 0.27661100029945374\n",
      "epoch: 18 step: 683, loss is 0.14130614697933197\n",
      "epoch: 18 step: 684, loss is 0.1253775805234909\n",
      "epoch: 18 step: 685, loss is 0.19007177650928497\n",
      "epoch: 18 step: 686, loss is 0.3696487843990326\n",
      "epoch: 18 step: 687, loss is 0.1131032407283783\n",
      "epoch: 18 step: 688, loss is 0.13529549539089203\n",
      "epoch: 18 step: 689, loss is 0.06990078836679459\n",
      "epoch: 18 step: 690, loss is 0.2271045595407486\n",
      "epoch: 18 step: 691, loss is 0.21353845298290253\n",
      "epoch: 18 step: 692, loss is 0.22207379341125488\n",
      "epoch: 18 step: 693, loss is 0.14540615677833557\n",
      "epoch: 18 step: 694, loss is 0.2610849440097809\n",
      "epoch: 18 step: 695, loss is 0.18917568027973175\n",
      "epoch: 18 step: 696, loss is 0.2720790207386017\n",
      "epoch: 18 step: 697, loss is 0.13999007642269135\n",
      "epoch: 18 step: 698, loss is 0.20893214643001556\n",
      "epoch: 18 step: 699, loss is 0.20120635628700256\n",
      "epoch: 18 step: 700, loss is 0.1796228438615799\n",
      "epoch: 18 step: 701, loss is 0.21524807810783386\n",
      "epoch: 18 step: 702, loss is 0.3568837344646454\n",
      "epoch: 18 step: 703, loss is 0.37155571579933167\n",
      "epoch: 18 step: 704, loss is 0.22893838584423065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 705, loss is 0.22799152135849\n",
      "epoch: 18 step: 706, loss is 0.27819788455963135\n",
      "epoch: 18 step: 707, loss is 0.10877754539251328\n",
      "epoch: 18 step: 708, loss is 0.2837958037853241\n",
      "epoch: 18 step: 709, loss is 0.094541534781456\n",
      "epoch: 18 step: 710, loss is 0.1837284415960312\n",
      "epoch: 18 step: 711, loss is 0.209581196308136\n",
      "epoch: 18 step: 712, loss is 0.10544834285974503\n",
      "epoch: 18 step: 713, loss is 0.13185183703899384\n",
      "epoch: 18 step: 714, loss is 0.2625528872013092\n",
      "epoch: 18 step: 715, loss is 0.2783266305923462\n",
      "epoch: 18 step: 716, loss is 0.40925168991088867\n",
      "epoch: 18 step: 717, loss is 0.12318889796733856\n",
      "epoch: 18 step: 718, loss is 0.2847548723220825\n",
      "epoch: 18 step: 719, loss is 0.31700992584228516\n",
      "epoch: 18 step: 720, loss is 0.11646202206611633\n",
      "epoch: 18 step: 721, loss is 0.30579498410224915\n",
      "epoch: 18 step: 722, loss is 0.14906294643878937\n",
      "epoch: 18 step: 723, loss is 0.13877247273921967\n",
      "epoch: 18 step: 724, loss is 0.2061699777841568\n",
      "epoch: 18 step: 725, loss is 0.19917067885398865\n",
      "epoch: 18 step: 726, loss is 0.19214870035648346\n",
      "epoch: 18 step: 727, loss is 0.32533153891563416\n",
      "epoch: 18 step: 728, loss is 0.44951826333999634\n",
      "epoch: 18 step: 729, loss is 0.12636995315551758\n",
      "epoch: 18 step: 730, loss is 0.2153865247964859\n",
      "epoch: 18 step: 731, loss is 0.4139609932899475\n",
      "epoch: 18 step: 732, loss is 0.13331498205661774\n",
      "epoch: 18 step: 733, loss is 0.20745709538459778\n",
      "epoch: 18 step: 734, loss is 0.12923988699913025\n",
      "epoch: 18 step: 735, loss is 0.23008251190185547\n",
      "epoch: 18 step: 736, loss is 0.3073311448097229\n",
      "epoch: 18 step: 737, loss is 0.17515115439891815\n",
      "epoch: 18 step: 738, loss is 0.16258086264133453\n",
      "epoch: 18 step: 739, loss is 0.3145858645439148\n",
      "epoch: 18 step: 740, loss is 0.233765110373497\n",
      "epoch: 18 step: 741, loss is 0.2767871618270874\n",
      "epoch: 18 step: 742, loss is 0.12426893413066864\n",
      "epoch: 18 step: 743, loss is 0.19834771752357483\n",
      "epoch: 18 step: 744, loss is 0.1724267154932022\n",
      "epoch: 18 step: 745, loss is 0.2132473886013031\n",
      "epoch: 18 step: 746, loss is 0.3259773850440979\n",
      "epoch: 18 step: 747, loss is 0.4114464223384857\n",
      "epoch: 18 step: 748, loss is 0.18731841444969177\n",
      "epoch: 18 step: 749, loss is 0.1525462120771408\n",
      "epoch: 18 step: 750, loss is 0.35979852080345154\n",
      "epoch: 18 step: 751, loss is 0.14784060418605804\n",
      "epoch: 18 step: 752, loss is 0.21946407854557037\n",
      "epoch: 18 step: 753, loss is 0.2059001475572586\n",
      "epoch: 18 step: 754, loss is 0.2600967586040497\n",
      "epoch: 18 step: 755, loss is 0.21326784789562225\n",
      "epoch: 18 step: 756, loss is 0.2759747803211212\n",
      "epoch: 18 step: 757, loss is 0.3064402937889099\n",
      "epoch: 18 step: 758, loss is 0.2735384404659271\n",
      "epoch: 18 step: 759, loss is 0.16344785690307617\n",
      "epoch: 18 step: 760, loss is 0.24787503480911255\n",
      "epoch: 18 step: 761, loss is 0.07066118717193604\n",
      "epoch: 18 step: 762, loss is 0.2155439406633377\n",
      "epoch: 18 step: 763, loss is 0.18249914050102234\n",
      "epoch: 18 step: 764, loss is 0.337895929813385\n",
      "epoch: 18 step: 765, loss is 0.2296448051929474\n",
      "epoch: 18 step: 766, loss is 0.261966347694397\n",
      "epoch: 18 step: 767, loss is 0.14693371951580048\n",
      "epoch: 18 step: 768, loss is 0.23240022361278534\n",
      "epoch: 18 step: 769, loss is 0.22729648649692535\n",
      "epoch: 18 step: 770, loss is 0.16719071567058563\n",
      "epoch: 18 step: 771, loss is 0.19523657858371735\n",
      "epoch: 18 step: 772, loss is 0.2049485296010971\n",
      "epoch: 18 step: 773, loss is 0.21486574411392212\n",
      "epoch: 18 step: 774, loss is 0.18957340717315674\n",
      "epoch: 18 step: 775, loss is 0.25025689601898193\n",
      "epoch: 18 step: 776, loss is 0.11297894269227982\n",
      "epoch: 18 step: 777, loss is 0.1244133859872818\n",
      "epoch: 18 step: 778, loss is 0.1709584891796112\n",
      "epoch: 18 step: 779, loss is 0.142324760556221\n",
      "epoch: 18 step: 780, loss is 0.21671703457832336\n",
      "epoch: 18 step: 781, loss is 0.204625204205513\n",
      "epoch: 18 step: 782, loss is 0.40301084518432617\n",
      "epoch: 18 step: 783, loss is 0.2625715732574463\n",
      "epoch: 18 step: 784, loss is 0.12860743701457977\n",
      "epoch: 18 step: 785, loss is 0.192994624376297\n",
      "epoch: 18 step: 786, loss is 0.21094362437725067\n",
      "epoch: 18 step: 787, loss is 0.31932172179222107\n",
      "epoch: 18 step: 788, loss is 0.09219608455896378\n",
      "epoch: 18 step: 789, loss is 0.11931959539651871\n",
      "epoch: 18 step: 790, loss is 0.24737514555454254\n",
      "epoch: 18 step: 791, loss is 0.1571531444787979\n",
      "epoch: 18 step: 792, loss is 0.27871328592300415\n",
      "epoch: 18 step: 793, loss is 0.3061443567276001\n",
      "epoch: 18 step: 794, loss is 0.11332052946090698\n",
      "epoch: 18 step: 795, loss is 0.1352461278438568\n",
      "epoch: 18 step: 796, loss is 0.13241401314735413\n",
      "epoch: 18 step: 797, loss is 0.18127094209194183\n",
      "epoch: 18 step: 798, loss is 0.38726022839546204\n",
      "epoch: 18 step: 799, loss is 0.1658606082201004\n",
      "epoch: 18 step: 800, loss is 0.1881958395242691\n",
      "epoch: 18 step: 801, loss is 0.2565239369869232\n",
      "epoch: 18 step: 802, loss is 0.2647261917591095\n",
      "epoch: 18 step: 803, loss is 0.1990687996149063\n",
      "epoch: 18 step: 804, loss is 0.14938290417194366\n",
      "epoch: 18 step: 805, loss is 0.31873801350593567\n",
      "epoch: 18 step: 806, loss is 0.2726712226867676\n",
      "epoch: 18 step: 807, loss is 0.293319433927536\n",
      "epoch: 18 step: 808, loss is 0.23450392484664917\n",
      "epoch: 18 step: 809, loss is 0.141563281416893\n",
      "epoch: 18 step: 810, loss is 0.16178564727306366\n",
      "epoch: 18 step: 811, loss is 0.3226982057094574\n",
      "epoch: 18 step: 812, loss is 0.3852914571762085\n",
      "epoch: 18 step: 813, loss is 0.15204766392707825\n",
      "epoch: 18 step: 814, loss is 0.2147771120071411\n",
      "epoch: 18 step: 815, loss is 0.15907765924930573\n",
      "epoch: 18 step: 816, loss is 0.29438328742980957\n",
      "epoch: 18 step: 817, loss is 0.24476096034049988\n",
      "epoch: 18 step: 818, loss is 0.28189221024513245\n",
      "epoch: 18 step: 819, loss is 0.235698401927948\n",
      "epoch: 18 step: 820, loss is 0.17996828258037567\n",
      "epoch: 18 step: 821, loss is 0.15349216759204865\n",
      "epoch: 18 step: 822, loss is 0.17455050349235535\n",
      "epoch: 18 step: 823, loss is 0.14392544329166412\n",
      "epoch: 18 step: 824, loss is 0.2075168490409851\n",
      "epoch: 18 step: 825, loss is 0.16329731047153473\n",
      "epoch: 18 step: 826, loss is 0.2133367359638214\n",
      "epoch: 18 step: 827, loss is 0.2244517207145691\n",
      "epoch: 18 step: 828, loss is 0.21942566335201263\n",
      "epoch: 18 step: 829, loss is 0.3771386742591858\n",
      "epoch: 18 step: 830, loss is 0.16469325125217438\n",
      "epoch: 18 step: 831, loss is 0.11956390738487244\n",
      "epoch: 18 step: 832, loss is 0.24952861666679382\n",
      "epoch: 18 step: 833, loss is 0.21451562643051147\n",
      "epoch: 18 step: 834, loss is 0.45056360960006714\n",
      "epoch: 18 step: 835, loss is 0.2446434497833252\n",
      "epoch: 18 step: 836, loss is 0.09896252304315567\n",
      "epoch: 18 step: 837, loss is 0.08500555902719498\n",
      "epoch: 18 step: 838, loss is 0.22856903076171875\n",
      "epoch: 18 step: 839, loss is 0.16477815806865692\n",
      "epoch: 18 step: 840, loss is 0.2148941308259964\n",
      "epoch: 18 step: 841, loss is 0.21728603541851044\n",
      "epoch: 18 step: 842, loss is 0.22930148243904114\n",
      "epoch: 18 step: 843, loss is 0.14220765233039856\n",
      "epoch: 18 step: 844, loss is 0.2150053232908249\n",
      "epoch: 18 step: 845, loss is 0.2570127844810486\n",
      "epoch: 18 step: 846, loss is 0.0627594143152237\n",
      "epoch: 18 step: 847, loss is 0.1535465568304062\n",
      "epoch: 18 step: 848, loss is 0.184681236743927\n",
      "epoch: 18 step: 849, loss is 0.2464398294687271\n",
      "epoch: 18 step: 850, loss is 0.12348796427249908\n",
      "epoch: 18 step: 851, loss is 0.07765161991119385\n",
      "epoch: 18 step: 852, loss is 0.3312731385231018\n",
      "epoch: 18 step: 853, loss is 0.25952228903770447\n",
      "epoch: 18 step: 854, loss is 0.4884462058544159\n",
      "epoch: 18 step: 855, loss is 0.15584005415439606\n",
      "epoch: 18 step: 856, loss is 0.2568157911300659\n",
      "epoch: 18 step: 857, loss is 0.13729572296142578\n",
      "epoch: 18 step: 858, loss is 0.11414339393377304\n",
      "epoch: 18 step: 859, loss is 0.21750380098819733\n",
      "epoch: 18 step: 860, loss is 0.1019442081451416\n",
      "epoch: 18 step: 861, loss is 0.17729659378528595\n",
      "epoch: 18 step: 862, loss is 0.1316622942686081\n",
      "epoch: 18 step: 863, loss is 0.29354771971702576\n",
      "epoch: 18 step: 864, loss is 0.1710761934518814\n",
      "epoch: 18 step: 865, loss is 0.17905347049236298\n",
      "epoch: 18 step: 866, loss is 0.2015029937028885\n",
      "epoch: 18 step: 867, loss is 0.19947506487369537\n",
      "epoch: 18 step: 868, loss is 0.1988525092601776\n",
      "epoch: 18 step: 869, loss is 0.09369582682847977\n",
      "epoch: 18 step: 870, loss is 0.3587905764579773\n",
      "epoch: 18 step: 871, loss is 0.15780580043792725\n",
      "epoch: 18 step: 872, loss is 0.13232634961605072\n",
      "epoch: 18 step: 873, loss is 0.16152413189411163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 874, loss is 0.10107909888029099\n",
      "epoch: 18 step: 875, loss is 0.23454876244068146\n",
      "epoch: 18 step: 876, loss is 0.1473700851202011\n",
      "epoch: 18 step: 877, loss is 0.15601582825183868\n",
      "epoch: 18 step: 878, loss is 0.1732129603624344\n",
      "epoch: 18 step: 879, loss is 0.19987709820270538\n",
      "epoch: 18 step: 880, loss is 0.25514766573905945\n",
      "epoch: 18 step: 881, loss is 0.3657871186733246\n",
      "epoch: 18 step: 882, loss is 0.2028437703847885\n",
      "epoch: 18 step: 883, loss is 0.1608356237411499\n",
      "epoch: 18 step: 884, loss is 0.1374121606349945\n",
      "epoch: 18 step: 885, loss is 0.15790420770645142\n",
      "epoch: 18 step: 886, loss is 0.3294159173965454\n",
      "epoch: 18 step: 887, loss is 0.23393240571022034\n",
      "epoch: 18 step: 888, loss is 0.1360907405614853\n",
      "epoch: 18 step: 889, loss is 0.31380853056907654\n",
      "epoch: 18 step: 890, loss is 0.10620813071727753\n",
      "epoch: 18 step: 891, loss is 0.1274166852235794\n",
      "epoch: 18 step: 892, loss is 0.22253239154815674\n",
      "epoch: 18 step: 893, loss is 0.10933458060026169\n",
      "epoch: 18 step: 894, loss is 0.23418766260147095\n",
      "epoch: 18 step: 895, loss is 0.131978839635849\n",
      "epoch: 18 step: 896, loss is 0.2165542095899582\n",
      "epoch: 18 step: 897, loss is 0.2736549973487854\n",
      "epoch: 18 step: 898, loss is 0.19419631361961365\n",
      "epoch: 18 step: 899, loss is 0.28009065985679626\n",
      "epoch: 18 step: 900, loss is 0.3149806261062622\n",
      "epoch: 18 step: 901, loss is 0.19172173738479614\n",
      "epoch: 18 step: 902, loss is 0.17832012474536896\n",
      "epoch: 18 step: 903, loss is 0.20632952451705933\n",
      "epoch: 18 step: 904, loss is 0.29160088300704956\n",
      "epoch: 18 step: 905, loss is 0.2074836641550064\n",
      "epoch: 18 step: 906, loss is 0.06906677037477493\n",
      "epoch: 18 step: 907, loss is 0.1669408679008484\n",
      "epoch: 18 step: 908, loss is 0.251333624124527\n",
      "epoch: 18 step: 909, loss is 0.15712417662143707\n",
      "epoch: 18 step: 910, loss is 0.26528534293174744\n",
      "epoch: 18 step: 911, loss is 0.16716153919696808\n",
      "epoch: 18 step: 912, loss is 0.08666679263114929\n",
      "epoch: 18 step: 913, loss is 0.14253264665603638\n",
      "epoch: 18 step: 914, loss is 0.11411652714014053\n",
      "epoch: 18 step: 915, loss is 0.2204384058713913\n",
      "epoch: 18 step: 916, loss is 0.10218898952007294\n",
      "epoch: 18 step: 917, loss is 0.36754924058914185\n",
      "epoch: 18 step: 918, loss is 0.2561767101287842\n",
      "epoch: 18 step: 919, loss is 0.21754544973373413\n",
      "epoch: 18 step: 920, loss is 0.12295148521661758\n",
      "epoch: 18 step: 921, loss is 0.2757992148399353\n",
      "epoch: 18 step: 922, loss is 0.12049193680286407\n",
      "epoch: 18 step: 923, loss is 0.13002736866474152\n",
      "epoch: 18 step: 924, loss is 0.11736598610877991\n",
      "epoch: 18 step: 925, loss is 0.32732319831848145\n",
      "epoch: 18 step: 926, loss is 0.2621237337589264\n",
      "epoch: 18 step: 927, loss is 0.2282852977514267\n",
      "epoch: 18 step: 928, loss is 0.14984606206417084\n",
      "epoch: 18 step: 929, loss is 0.11522664874792099\n",
      "epoch: 18 step: 930, loss is 0.09996622800827026\n",
      "epoch: 18 step: 931, loss is 0.2374449521303177\n",
      "epoch: 18 step: 932, loss is 0.18308648467063904\n",
      "epoch: 18 step: 933, loss is 0.2180592119693756\n",
      "epoch: 18 step: 934, loss is 0.2130146473646164\n",
      "epoch: 18 step: 935, loss is 0.18036648631095886\n",
      "epoch: 18 step: 936, loss is 0.3047172725200653\n",
      "epoch: 18 step: 937, loss is 0.25666093826293945\n",
      "epoch: 19 step: 1, loss is 0.20094776153564453\n",
      "epoch: 19 step: 2, loss is 0.13427235186100006\n",
      "epoch: 19 step: 3, loss is 0.2612972855567932\n",
      "epoch: 19 step: 4, loss is 0.2010263055562973\n",
      "epoch: 19 step: 5, loss is 0.0930456593632698\n",
      "epoch: 19 step: 6, loss is 0.3833242654800415\n",
      "epoch: 19 step: 7, loss is 0.2423190027475357\n",
      "epoch: 19 step: 8, loss is 0.2711769938468933\n",
      "epoch: 19 step: 9, loss is 0.22435541450977325\n",
      "epoch: 19 step: 10, loss is 0.07783011347055435\n",
      "epoch: 19 step: 11, loss is 0.20474356412887573\n",
      "epoch: 19 step: 12, loss is 0.2119581252336502\n",
      "epoch: 19 step: 13, loss is 0.17159996926784515\n",
      "epoch: 19 step: 14, loss is 0.12911123037338257\n",
      "epoch: 19 step: 15, loss is 0.22331100702285767\n",
      "epoch: 19 step: 16, loss is 0.22350093722343445\n",
      "epoch: 19 step: 17, loss is 0.14909076690673828\n",
      "epoch: 19 step: 18, loss is 0.29862767457962036\n",
      "epoch: 19 step: 19, loss is 0.385463148355484\n",
      "epoch: 19 step: 20, loss is 0.21209856867790222\n",
      "epoch: 19 step: 21, loss is 0.21207749843597412\n",
      "epoch: 19 step: 22, loss is 0.35516220331192017\n",
      "epoch: 19 step: 23, loss is 0.1224021166563034\n",
      "epoch: 19 step: 24, loss is 0.2689938545227051\n",
      "epoch: 19 step: 25, loss is 0.12633447349071503\n",
      "epoch: 19 step: 26, loss is 0.1844780147075653\n",
      "epoch: 19 step: 27, loss is 0.14844472706317902\n",
      "epoch: 19 step: 28, loss is 0.30053597688674927\n",
      "epoch: 19 step: 29, loss is 0.40196239948272705\n",
      "epoch: 19 step: 30, loss is 0.23100806772708893\n",
      "epoch: 19 step: 31, loss is 0.13005895912647247\n",
      "epoch: 19 step: 32, loss is 0.21811147034168243\n",
      "epoch: 19 step: 33, loss is 0.1378639042377472\n",
      "epoch: 19 step: 34, loss is 0.14596280455589294\n",
      "epoch: 19 step: 35, loss is 0.2513531744480133\n",
      "epoch: 19 step: 36, loss is 0.17597422003746033\n",
      "epoch: 19 step: 37, loss is 0.28658413887023926\n",
      "epoch: 19 step: 38, loss is 0.20778122544288635\n",
      "epoch: 19 step: 39, loss is 0.2432868629693985\n",
      "epoch: 19 step: 40, loss is 0.1399601697921753\n",
      "epoch: 19 step: 41, loss is 0.16005051136016846\n",
      "epoch: 19 step: 42, loss is 0.15942412614822388\n",
      "epoch: 19 step: 43, loss is 0.19653189182281494\n",
      "epoch: 19 step: 44, loss is 0.20670545101165771\n",
      "epoch: 19 step: 45, loss is 0.19065716862678528\n",
      "epoch: 19 step: 46, loss is 0.3182719349861145\n",
      "epoch: 19 step: 47, loss is 0.2557336390018463\n",
      "epoch: 19 step: 48, loss is 0.40652912855148315\n",
      "epoch: 19 step: 49, loss is 0.2060663253068924\n",
      "epoch: 19 step: 50, loss is 0.2156396061182022\n",
      "epoch: 19 step: 51, loss is 0.14586272835731506\n",
      "epoch: 19 step: 52, loss is 0.23727111518383026\n",
      "epoch: 19 step: 53, loss is 0.27382826805114746\n",
      "epoch: 19 step: 54, loss is 0.1706300675868988\n",
      "epoch: 19 step: 55, loss is 0.17507287859916687\n",
      "epoch: 19 step: 56, loss is 0.16895508766174316\n",
      "epoch: 19 step: 57, loss is 0.21445409953594208\n",
      "epoch: 19 step: 58, loss is 0.4396524131298065\n",
      "epoch: 19 step: 59, loss is 0.3522563874721527\n",
      "epoch: 19 step: 60, loss is 0.19747579097747803\n",
      "epoch: 19 step: 61, loss is 0.11651930958032608\n",
      "epoch: 19 step: 62, loss is 0.14654576778411865\n",
      "epoch: 19 step: 63, loss is 0.1863044947385788\n",
      "epoch: 19 step: 64, loss is 0.13345758616924286\n",
      "epoch: 19 step: 65, loss is 0.0863320529460907\n",
      "epoch: 19 step: 66, loss is 0.2894304692745209\n",
      "epoch: 19 step: 67, loss is 0.2535540759563446\n",
      "epoch: 19 step: 68, loss is 0.16401417553424835\n",
      "epoch: 19 step: 69, loss is 0.20490308105945587\n",
      "epoch: 19 step: 70, loss is 0.377037912607193\n",
      "epoch: 19 step: 71, loss is 0.274392306804657\n",
      "epoch: 19 step: 72, loss is 0.1543026566505432\n",
      "epoch: 19 step: 73, loss is 0.1765403002500534\n",
      "epoch: 19 step: 74, loss is 0.14611899852752686\n",
      "epoch: 19 step: 75, loss is 0.0943504348397255\n",
      "epoch: 19 step: 76, loss is 0.12568975985050201\n",
      "epoch: 19 step: 77, loss is 0.16143833100795746\n",
      "epoch: 19 step: 78, loss is 0.15816283226013184\n",
      "epoch: 19 step: 79, loss is 0.26709720492362976\n",
      "epoch: 19 step: 80, loss is 0.13917045295238495\n",
      "epoch: 19 step: 81, loss is 0.2030320018529892\n",
      "epoch: 19 step: 82, loss is 0.26219603419303894\n",
      "epoch: 19 step: 83, loss is 0.29998698830604553\n",
      "epoch: 19 step: 84, loss is 0.35553401708602905\n",
      "epoch: 19 step: 85, loss is 0.15990784764289856\n",
      "epoch: 19 step: 86, loss is 0.23035097122192383\n",
      "epoch: 19 step: 87, loss is 0.1256568282842636\n",
      "epoch: 19 step: 88, loss is 0.20963706076145172\n",
      "epoch: 19 step: 89, loss is 0.4774680733680725\n",
      "epoch: 19 step: 90, loss is 0.23135356605052948\n",
      "epoch: 19 step: 91, loss is 0.2760998606681824\n",
      "epoch: 19 step: 92, loss is 0.21978887915611267\n",
      "epoch: 19 step: 93, loss is 0.1267211139202118\n",
      "epoch: 19 step: 94, loss is 0.15921805799007416\n",
      "epoch: 19 step: 95, loss is 0.40737783908843994\n",
      "epoch: 19 step: 96, loss is 0.20372988283634186\n",
      "epoch: 19 step: 97, loss is 0.4045926034450531\n",
      "epoch: 19 step: 98, loss is 0.16782808303833008\n",
      "epoch: 19 step: 99, loss is 0.2650199234485626\n",
      "epoch: 19 step: 100, loss is 0.244310662150383\n",
      "epoch: 19 step: 101, loss is 0.33650100231170654\n",
      "epoch: 19 step: 102, loss is 0.18452197313308716\n",
      "epoch: 19 step: 103, loss is 0.3431912958621979\n",
      "epoch: 19 step: 104, loss is 0.17455676198005676\n",
      "epoch: 19 step: 105, loss is 0.18341656029224396\n",
      "epoch: 19 step: 106, loss is 0.3181461989879608\n",
      "epoch: 19 step: 107, loss is 0.32849588990211487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 108, loss is 0.177288219332695\n",
      "epoch: 19 step: 109, loss is 0.2834245562553406\n",
      "epoch: 19 step: 110, loss is 0.06994792819023132\n",
      "epoch: 19 step: 111, loss is 0.2580631971359253\n",
      "epoch: 19 step: 112, loss is 0.40549859404563904\n",
      "epoch: 19 step: 113, loss is 0.25120899081230164\n",
      "epoch: 19 step: 114, loss is 0.17767828702926636\n",
      "epoch: 19 step: 115, loss is 0.10737854242324829\n",
      "epoch: 19 step: 116, loss is 0.15445579588413239\n",
      "epoch: 19 step: 117, loss is 0.36014723777770996\n",
      "epoch: 19 step: 118, loss is 0.22529931366443634\n",
      "epoch: 19 step: 119, loss is 0.1608247607946396\n",
      "epoch: 19 step: 120, loss is 0.13333581387996674\n",
      "epoch: 19 step: 121, loss is 0.33201053738594055\n",
      "epoch: 19 step: 122, loss is 0.17500589787960052\n",
      "epoch: 19 step: 123, loss is 0.22490404546260834\n",
      "epoch: 19 step: 124, loss is 0.14285524189472198\n",
      "epoch: 19 step: 125, loss is 0.39507177472114563\n",
      "epoch: 19 step: 126, loss is 0.3174470067024231\n",
      "epoch: 19 step: 127, loss is 0.16255687177181244\n",
      "epoch: 19 step: 128, loss is 0.215267151594162\n",
      "epoch: 19 step: 129, loss is 0.2264629304409027\n",
      "epoch: 19 step: 130, loss is 0.3400377929210663\n",
      "epoch: 19 step: 131, loss is 0.2751699984073639\n",
      "epoch: 19 step: 132, loss is 0.2360811084508896\n",
      "epoch: 19 step: 133, loss is 0.2233157604932785\n",
      "epoch: 19 step: 134, loss is 0.2070654332637787\n",
      "epoch: 19 step: 135, loss is 0.2730296552181244\n",
      "epoch: 19 step: 136, loss is 0.16201122105121613\n",
      "epoch: 19 step: 137, loss is 0.2305077761411667\n",
      "epoch: 19 step: 138, loss is 0.26445141434669495\n",
      "epoch: 19 step: 139, loss is 0.20975276827812195\n",
      "epoch: 19 step: 140, loss is 0.28725048899650574\n",
      "epoch: 19 step: 141, loss is 0.2195557802915573\n",
      "epoch: 19 step: 142, loss is 0.3032814860343933\n",
      "epoch: 19 step: 143, loss is 0.22593973577022552\n",
      "epoch: 19 step: 144, loss is 0.15487006306648254\n",
      "epoch: 19 step: 145, loss is 0.13123176991939545\n",
      "epoch: 19 step: 146, loss is 0.31554052233695984\n",
      "epoch: 19 step: 147, loss is 0.08268941193819046\n",
      "epoch: 19 step: 148, loss is 0.13993987441062927\n",
      "epoch: 19 step: 149, loss is 0.17085979878902435\n",
      "epoch: 19 step: 150, loss is 0.08036769926548004\n",
      "epoch: 19 step: 151, loss is 0.18300621211528778\n",
      "epoch: 19 step: 152, loss is 0.22891581058502197\n",
      "epoch: 19 step: 153, loss is 0.35432830452919006\n",
      "epoch: 19 step: 154, loss is 0.2533043920993805\n",
      "epoch: 19 step: 155, loss is 0.13445863127708435\n",
      "epoch: 19 step: 156, loss is 0.1706506609916687\n",
      "epoch: 19 step: 157, loss is 0.26729774475097656\n",
      "epoch: 19 step: 158, loss is 0.12478743493556976\n",
      "epoch: 19 step: 159, loss is 0.26540687680244446\n",
      "epoch: 19 step: 160, loss is 0.1731429249048233\n",
      "epoch: 19 step: 161, loss is 0.1011062040925026\n",
      "epoch: 19 step: 162, loss is 0.1982525736093521\n",
      "epoch: 19 step: 163, loss is 0.10378245264291763\n",
      "epoch: 19 step: 164, loss is 0.363237589597702\n",
      "epoch: 19 step: 165, loss is 0.29620400071144104\n",
      "epoch: 19 step: 166, loss is 0.2597697079181671\n",
      "epoch: 19 step: 167, loss is 0.15904603898525238\n",
      "epoch: 19 step: 168, loss is 0.16847576200962067\n",
      "epoch: 19 step: 169, loss is 0.1621096432209015\n",
      "epoch: 19 step: 170, loss is 0.25997138023376465\n",
      "epoch: 19 step: 171, loss is 0.5091938376426697\n",
      "epoch: 19 step: 172, loss is 0.12926706671714783\n",
      "epoch: 19 step: 173, loss is 0.28405317664146423\n",
      "epoch: 19 step: 174, loss is 0.18811576068401337\n",
      "epoch: 19 step: 175, loss is 0.17449770867824554\n",
      "epoch: 19 step: 176, loss is 0.18310688436031342\n",
      "epoch: 19 step: 177, loss is 0.1557651311159134\n",
      "epoch: 19 step: 178, loss is 0.09051129966974258\n",
      "epoch: 19 step: 179, loss is 0.2303137332201004\n",
      "epoch: 19 step: 180, loss is 0.1456596702337265\n",
      "epoch: 19 step: 181, loss is 0.187450110912323\n",
      "epoch: 19 step: 182, loss is 0.2030928134918213\n",
      "epoch: 19 step: 183, loss is 0.2086595743894577\n",
      "epoch: 19 step: 184, loss is 0.17766840755939484\n",
      "epoch: 19 step: 185, loss is 0.14480994641780853\n",
      "epoch: 19 step: 186, loss is 0.3126225173473358\n",
      "epoch: 19 step: 187, loss is 0.19749228656291962\n",
      "epoch: 19 step: 188, loss is 0.11909402906894684\n",
      "epoch: 19 step: 189, loss is 0.17647351324558258\n",
      "epoch: 19 step: 190, loss is 0.13493768870830536\n",
      "epoch: 19 step: 191, loss is 0.2082940936088562\n",
      "epoch: 19 step: 192, loss is 0.21763601899147034\n",
      "epoch: 19 step: 193, loss is 0.1966719776391983\n",
      "epoch: 19 step: 194, loss is 0.27022334933280945\n",
      "epoch: 19 step: 195, loss is 0.14356252551078796\n",
      "epoch: 19 step: 196, loss is 0.20183078944683075\n",
      "epoch: 19 step: 197, loss is 0.13881053030490875\n",
      "epoch: 19 step: 198, loss is 0.21990717947483063\n",
      "epoch: 19 step: 199, loss is 0.2811276316642761\n",
      "epoch: 19 step: 200, loss is 0.1001497283577919\n",
      "epoch: 19 step: 201, loss is 0.08730164170265198\n",
      "epoch: 19 step: 202, loss is 0.12775102257728577\n",
      "epoch: 19 step: 203, loss is 0.2263118028640747\n",
      "epoch: 19 step: 204, loss is 0.21491213142871857\n",
      "epoch: 19 step: 205, loss is 0.1320885568857193\n",
      "epoch: 19 step: 206, loss is 0.13843749463558197\n",
      "epoch: 19 step: 207, loss is 0.10211986303329468\n",
      "epoch: 19 step: 208, loss is 0.15392903983592987\n",
      "epoch: 19 step: 209, loss is 0.2325442135334015\n",
      "epoch: 19 step: 210, loss is 0.2030177116394043\n",
      "epoch: 19 step: 211, loss is 0.15361426770687103\n",
      "epoch: 19 step: 212, loss is 0.2676391303539276\n",
      "epoch: 19 step: 213, loss is 0.1647830754518509\n",
      "epoch: 19 step: 214, loss is 0.2514509856700897\n",
      "epoch: 19 step: 215, loss is 0.3231450021266937\n",
      "epoch: 19 step: 216, loss is 0.1957073211669922\n",
      "epoch: 19 step: 217, loss is 0.26199185848236084\n",
      "epoch: 19 step: 218, loss is 0.15566185116767883\n",
      "epoch: 19 step: 219, loss is 0.12633800506591797\n",
      "epoch: 19 step: 220, loss is 0.24636751413345337\n",
      "epoch: 19 step: 221, loss is 0.2130618691444397\n",
      "epoch: 19 step: 222, loss is 0.324995219707489\n",
      "epoch: 19 step: 223, loss is 0.09925568848848343\n",
      "epoch: 19 step: 224, loss is 0.18967847526073456\n",
      "epoch: 19 step: 225, loss is 0.13878579437732697\n",
      "epoch: 19 step: 226, loss is 0.29268187284469604\n",
      "epoch: 19 step: 227, loss is 0.19308221340179443\n",
      "epoch: 19 step: 228, loss is 0.16365934908390045\n",
      "epoch: 19 step: 229, loss is 0.42566531896591187\n",
      "epoch: 19 step: 230, loss is 0.21801455318927765\n",
      "epoch: 19 step: 231, loss is 0.23495319485664368\n",
      "epoch: 19 step: 232, loss is 0.22528207302093506\n",
      "epoch: 19 step: 233, loss is 0.13517338037490845\n",
      "epoch: 19 step: 234, loss is 0.1983771026134491\n",
      "epoch: 19 step: 235, loss is 0.12496694177389145\n",
      "epoch: 19 step: 236, loss is 0.18266919255256653\n",
      "epoch: 19 step: 237, loss is 0.06312857568264008\n",
      "epoch: 19 step: 238, loss is 0.17584602534770966\n",
      "epoch: 19 step: 239, loss is 0.12524661421775818\n",
      "epoch: 19 step: 240, loss is 0.25684845447540283\n",
      "epoch: 19 step: 241, loss is 0.14026573300361633\n",
      "epoch: 19 step: 242, loss is 0.14347894489765167\n",
      "epoch: 19 step: 243, loss is 0.16055043041706085\n",
      "epoch: 19 step: 244, loss is 0.2471930831670761\n",
      "epoch: 19 step: 245, loss is 0.3359157145023346\n",
      "epoch: 19 step: 246, loss is 0.14012935757637024\n",
      "epoch: 19 step: 247, loss is 0.16080251336097717\n",
      "epoch: 19 step: 248, loss is 0.23767933249473572\n",
      "epoch: 19 step: 249, loss is 0.18009331822395325\n",
      "epoch: 19 step: 250, loss is 0.05766019970178604\n",
      "epoch: 19 step: 251, loss is 0.17825689911842346\n",
      "epoch: 19 step: 252, loss is 0.12174643576145172\n",
      "epoch: 19 step: 253, loss is 0.16895341873168945\n",
      "epoch: 19 step: 254, loss is 0.17058853805065155\n",
      "epoch: 19 step: 255, loss is 0.20947818458080292\n",
      "epoch: 19 step: 256, loss is 0.10644450783729553\n",
      "epoch: 19 step: 257, loss is 0.2395627498626709\n",
      "epoch: 19 step: 258, loss is 0.18475501239299774\n",
      "epoch: 19 step: 259, loss is 0.256623774766922\n",
      "epoch: 19 step: 260, loss is 0.0863405168056488\n",
      "epoch: 19 step: 261, loss is 0.17230787873268127\n",
      "epoch: 19 step: 262, loss is 0.22877322137355804\n",
      "epoch: 19 step: 263, loss is 0.22700199484825134\n",
      "epoch: 19 step: 264, loss is 0.11510228365659714\n",
      "epoch: 19 step: 265, loss is 0.15870600938796997\n",
      "epoch: 19 step: 266, loss is 0.1758175492286682\n",
      "epoch: 19 step: 267, loss is 0.12171472609043121\n",
      "epoch: 19 step: 268, loss is 0.13539496064186096\n",
      "epoch: 19 step: 269, loss is 0.17064060270786285\n",
      "epoch: 19 step: 270, loss is 0.22432614862918854\n",
      "epoch: 19 step: 271, loss is 0.16030898690223694\n",
      "epoch: 19 step: 272, loss is 0.10950985550880432\n",
      "epoch: 19 step: 273, loss is 0.26013463735580444\n",
      "epoch: 19 step: 274, loss is 0.19545942544937134\n",
      "epoch: 19 step: 275, loss is 0.15108022093772888\n",
      "epoch: 19 step: 276, loss is 0.2782147526741028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 277, loss is 0.1900581419467926\n",
      "epoch: 19 step: 278, loss is 0.13900057971477509\n",
      "epoch: 19 step: 279, loss is 0.22057156264781952\n",
      "epoch: 19 step: 280, loss is 0.18296033143997192\n",
      "epoch: 19 step: 281, loss is 0.1588880568742752\n",
      "epoch: 19 step: 282, loss is 0.10361025482416153\n",
      "epoch: 19 step: 283, loss is 0.2927739918231964\n",
      "epoch: 19 step: 284, loss is 0.15640608966350555\n",
      "epoch: 19 step: 285, loss is 0.1969802975654602\n",
      "epoch: 19 step: 286, loss is 0.12834839522838593\n",
      "epoch: 19 step: 287, loss is 0.232215017080307\n",
      "epoch: 19 step: 288, loss is 0.19115647673606873\n",
      "epoch: 19 step: 289, loss is 0.242949441075325\n",
      "epoch: 19 step: 290, loss is 0.2768002152442932\n",
      "epoch: 19 step: 291, loss is 0.16220933198928833\n",
      "epoch: 19 step: 292, loss is 0.1813565045595169\n",
      "epoch: 19 step: 293, loss is 0.18176797032356262\n",
      "epoch: 19 step: 294, loss is 0.17392227053642273\n",
      "epoch: 19 step: 295, loss is 0.2680158019065857\n",
      "epoch: 19 step: 296, loss is 0.20666423439979553\n",
      "epoch: 19 step: 297, loss is 0.253940612077713\n",
      "epoch: 19 step: 298, loss is 0.20491692423820496\n",
      "epoch: 19 step: 299, loss is 0.3753742277622223\n",
      "epoch: 19 step: 300, loss is 0.13555502891540527\n",
      "epoch: 19 step: 301, loss is 0.12885579466819763\n",
      "epoch: 19 step: 302, loss is 0.21695652604103088\n",
      "epoch: 19 step: 303, loss is 0.14974330365657806\n",
      "epoch: 19 step: 304, loss is 0.3652501106262207\n",
      "epoch: 19 step: 305, loss is 0.11891013383865356\n",
      "epoch: 19 step: 306, loss is 0.13776560127735138\n",
      "epoch: 19 step: 307, loss is 0.23543787002563477\n",
      "epoch: 19 step: 308, loss is 0.31476444005966187\n",
      "epoch: 19 step: 309, loss is 0.20503394305706024\n",
      "epoch: 19 step: 310, loss is 0.1288333684206009\n",
      "epoch: 19 step: 311, loss is 0.1852002888917923\n",
      "epoch: 19 step: 312, loss is 0.18902716040611267\n",
      "epoch: 19 step: 313, loss is 0.16038350760936737\n",
      "epoch: 19 step: 314, loss is 0.12141433358192444\n",
      "epoch: 19 step: 315, loss is 0.11249005049467087\n",
      "epoch: 19 step: 316, loss is 0.25635719299316406\n",
      "epoch: 19 step: 317, loss is 0.24072884023189545\n",
      "epoch: 19 step: 318, loss is 0.220272496342659\n",
      "epoch: 19 step: 319, loss is 0.1551542729139328\n",
      "epoch: 19 step: 320, loss is 0.38356736302375793\n",
      "epoch: 19 step: 321, loss is 0.21335354447364807\n",
      "epoch: 19 step: 322, loss is 0.19047826528549194\n",
      "epoch: 19 step: 323, loss is 0.19178827106952667\n",
      "epoch: 19 step: 324, loss is 0.1620563268661499\n",
      "epoch: 19 step: 325, loss is 0.1763550043106079\n",
      "epoch: 19 step: 326, loss is 0.13233435153961182\n",
      "epoch: 19 step: 327, loss is 0.30003729462623596\n",
      "epoch: 19 step: 328, loss is 0.2900221347808838\n",
      "epoch: 19 step: 329, loss is 0.23722362518310547\n",
      "epoch: 19 step: 330, loss is 0.2772497832775116\n",
      "epoch: 19 step: 331, loss is 0.05390613526105881\n",
      "epoch: 19 step: 332, loss is 0.29833319783210754\n",
      "epoch: 19 step: 333, loss is 0.17339171469211578\n",
      "epoch: 19 step: 334, loss is 0.09658633172512054\n",
      "epoch: 19 step: 335, loss is 0.19778531789779663\n",
      "epoch: 19 step: 336, loss is 0.39726656675338745\n",
      "epoch: 19 step: 337, loss is 0.40233632922172546\n",
      "epoch: 19 step: 338, loss is 0.09129687398672104\n",
      "epoch: 19 step: 339, loss is 0.23617641627788544\n",
      "epoch: 19 step: 340, loss is 0.29416054487228394\n",
      "epoch: 19 step: 341, loss is 0.26156023144721985\n",
      "epoch: 19 step: 342, loss is 0.1857362538576126\n",
      "epoch: 19 step: 343, loss is 0.14010021090507507\n",
      "epoch: 19 step: 344, loss is 0.15561909973621368\n",
      "epoch: 19 step: 345, loss is 0.16369003057479858\n",
      "epoch: 19 step: 346, loss is 0.2585241198539734\n",
      "epoch: 19 step: 347, loss is 0.23436526954174042\n",
      "epoch: 19 step: 348, loss is 0.23843713104724884\n",
      "epoch: 19 step: 349, loss is 0.23107635974884033\n",
      "epoch: 19 step: 350, loss is 0.28903815150260925\n",
      "epoch: 19 step: 351, loss is 0.27097275853157043\n",
      "epoch: 19 step: 352, loss is 0.38518622517585754\n",
      "epoch: 19 step: 353, loss is 0.16869613528251648\n",
      "epoch: 19 step: 354, loss is 0.19532319903373718\n",
      "epoch: 19 step: 355, loss is 0.1427374929189682\n",
      "epoch: 19 step: 356, loss is 0.20401842892169952\n",
      "epoch: 19 step: 357, loss is 0.12095317244529724\n",
      "epoch: 19 step: 358, loss is 0.1292414516210556\n",
      "epoch: 19 step: 359, loss is 0.1487930715084076\n",
      "epoch: 19 step: 360, loss is 0.19275492429733276\n",
      "epoch: 19 step: 361, loss is 0.2512548565864563\n",
      "epoch: 19 step: 362, loss is 0.18494106829166412\n",
      "epoch: 19 step: 363, loss is 0.24636472761631012\n",
      "epoch: 19 step: 364, loss is 0.2431810200214386\n",
      "epoch: 19 step: 365, loss is 0.14872844517230988\n",
      "epoch: 19 step: 366, loss is 0.18782494962215424\n",
      "epoch: 19 step: 367, loss is 0.16935566067695618\n",
      "epoch: 19 step: 368, loss is 0.05540132522583008\n",
      "epoch: 19 step: 369, loss is 0.17383717000484467\n",
      "epoch: 19 step: 370, loss is 0.11017504334449768\n",
      "epoch: 19 step: 371, loss is 0.1502583622932434\n",
      "epoch: 19 step: 372, loss is 0.16717994213104248\n",
      "epoch: 19 step: 373, loss is 0.19117595255374908\n",
      "epoch: 19 step: 374, loss is 0.13569562137126923\n",
      "epoch: 19 step: 375, loss is 0.15463510155677795\n",
      "epoch: 19 step: 376, loss is 0.23980866372585297\n",
      "epoch: 19 step: 377, loss is 0.11510875076055527\n",
      "epoch: 19 step: 378, loss is 0.1940930187702179\n",
      "epoch: 19 step: 379, loss is 0.05297112464904785\n",
      "epoch: 19 step: 380, loss is 0.19500137865543365\n",
      "epoch: 19 step: 381, loss is 0.23680229485034943\n",
      "epoch: 19 step: 382, loss is 0.11501161009073257\n",
      "epoch: 19 step: 383, loss is 0.19400635361671448\n",
      "epoch: 19 step: 384, loss is 0.17323295772075653\n",
      "epoch: 19 step: 385, loss is 0.2085673063993454\n",
      "epoch: 19 step: 386, loss is 0.07058629393577576\n",
      "epoch: 19 step: 387, loss is 0.08992329984903336\n",
      "epoch: 19 step: 388, loss is 0.1184391900897026\n",
      "epoch: 19 step: 389, loss is 0.3532032072544098\n",
      "epoch: 19 step: 390, loss is 0.249564990401268\n",
      "epoch: 19 step: 391, loss is 0.24124109745025635\n",
      "epoch: 19 step: 392, loss is 0.14128300547599792\n",
      "epoch: 19 step: 393, loss is 0.2826930284500122\n",
      "epoch: 19 step: 394, loss is 0.10993458330631256\n",
      "epoch: 19 step: 395, loss is 0.15632066130638123\n",
      "epoch: 19 step: 396, loss is 0.21981897950172424\n",
      "epoch: 19 step: 397, loss is 0.2493649423122406\n",
      "epoch: 19 step: 398, loss is 0.26340147852897644\n",
      "epoch: 19 step: 399, loss is 0.10489681363105774\n",
      "epoch: 19 step: 400, loss is 0.22640419006347656\n",
      "epoch: 19 step: 401, loss is 0.23567883670330048\n",
      "epoch: 19 step: 402, loss is 0.4175930321216583\n",
      "epoch: 19 step: 403, loss is 0.3637484610080719\n",
      "epoch: 19 step: 404, loss is 0.14474807679653168\n",
      "epoch: 19 step: 405, loss is 0.24289573729038239\n",
      "epoch: 19 step: 406, loss is 0.158362478017807\n",
      "epoch: 19 step: 407, loss is 0.2756122648715973\n",
      "epoch: 19 step: 408, loss is 0.14382342994213104\n",
      "epoch: 19 step: 409, loss is 0.1997404396533966\n",
      "epoch: 19 step: 410, loss is 0.21508103609085083\n",
      "epoch: 19 step: 411, loss is 0.18448469042778015\n",
      "epoch: 19 step: 412, loss is 0.2792911231517792\n",
      "epoch: 19 step: 413, loss is 0.1412937492132187\n",
      "epoch: 19 step: 414, loss is 0.19964882731437683\n",
      "epoch: 19 step: 415, loss is 0.2459120899438858\n",
      "epoch: 19 step: 416, loss is 0.17277352511882782\n",
      "epoch: 19 step: 417, loss is 0.20753984153270721\n",
      "epoch: 19 step: 418, loss is 0.3171248435974121\n",
      "epoch: 19 step: 419, loss is 0.09770553559064865\n",
      "epoch: 19 step: 420, loss is 0.25921744108200073\n",
      "epoch: 19 step: 421, loss is 0.20240646600723267\n",
      "epoch: 19 step: 422, loss is 0.09377247840166092\n",
      "epoch: 19 step: 423, loss is 0.40401554107666016\n",
      "epoch: 19 step: 424, loss is 0.13190287351608276\n",
      "epoch: 19 step: 425, loss is 0.1841033399105072\n",
      "epoch: 19 step: 426, loss is 0.16835735738277435\n",
      "epoch: 19 step: 427, loss is 0.12753646075725555\n",
      "epoch: 19 step: 428, loss is 0.1525753289461136\n",
      "epoch: 19 step: 429, loss is 0.24839839339256287\n",
      "epoch: 19 step: 430, loss is 0.23524172604084015\n",
      "epoch: 19 step: 431, loss is 0.22992654144763947\n",
      "epoch: 19 step: 432, loss is 0.1922173649072647\n",
      "epoch: 19 step: 433, loss is 0.1150173544883728\n",
      "epoch: 19 step: 434, loss is 0.23777641355991364\n",
      "epoch: 19 step: 435, loss is 0.28655967116355896\n",
      "epoch: 19 step: 436, loss is 0.29330942034721375\n",
      "epoch: 19 step: 437, loss is 0.21646884083747864\n",
      "epoch: 19 step: 438, loss is 0.16119253635406494\n",
      "epoch: 19 step: 439, loss is 0.18515150249004364\n",
      "epoch: 19 step: 440, loss is 0.23071204125881195\n",
      "epoch: 19 step: 441, loss is 0.19495674967765808\n",
      "epoch: 19 step: 442, loss is 0.21159628033638\n",
      "epoch: 19 step: 443, loss is 0.19407860934734344\n",
      "epoch: 19 step: 444, loss is 0.24114084243774414\n",
      "epoch: 19 step: 445, loss is 0.5066675543785095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 446, loss is 0.19182924926280975\n",
      "epoch: 19 step: 447, loss is 0.1848025918006897\n",
      "epoch: 19 step: 448, loss is 0.24339520931243896\n",
      "epoch: 19 step: 449, loss is 0.3590787947177887\n",
      "epoch: 19 step: 450, loss is 0.12587077915668488\n",
      "epoch: 19 step: 451, loss is 0.17833518981933594\n",
      "epoch: 19 step: 452, loss is 0.2971174418926239\n",
      "epoch: 19 step: 453, loss is 0.29932069778442383\n",
      "epoch: 19 step: 454, loss is 0.2364066243171692\n",
      "epoch: 19 step: 455, loss is 0.27083325386047363\n",
      "epoch: 19 step: 456, loss is 0.36335092782974243\n",
      "epoch: 19 step: 457, loss is 0.2514805495738983\n",
      "epoch: 19 step: 458, loss is 0.35617077350616455\n",
      "epoch: 19 step: 459, loss is 0.24774622917175293\n",
      "epoch: 19 step: 460, loss is 0.1832972913980484\n",
      "epoch: 19 step: 461, loss is 0.20057325065135956\n",
      "epoch: 19 step: 462, loss is 0.14473776519298553\n",
      "epoch: 19 step: 463, loss is 0.130679190158844\n",
      "epoch: 19 step: 464, loss is 0.16110149025917053\n",
      "epoch: 19 step: 465, loss is 0.2804372012615204\n",
      "epoch: 19 step: 466, loss is 0.13424815237522125\n",
      "epoch: 19 step: 467, loss is 0.21420542895793915\n",
      "epoch: 19 step: 468, loss is 0.35741159319877625\n",
      "epoch: 19 step: 469, loss is 0.17495369911193848\n",
      "epoch: 19 step: 470, loss is 0.2136455625295639\n",
      "epoch: 19 step: 471, loss is 0.24628733098506927\n",
      "epoch: 19 step: 472, loss is 0.35624679923057556\n",
      "epoch: 19 step: 473, loss is 0.15012198686599731\n",
      "epoch: 19 step: 474, loss is 0.17411939799785614\n",
      "epoch: 19 step: 475, loss is 0.16688968241214752\n",
      "epoch: 19 step: 476, loss is 0.20339907705783844\n",
      "epoch: 19 step: 477, loss is 0.31194621324539185\n",
      "epoch: 19 step: 478, loss is 0.08903679996728897\n",
      "epoch: 19 step: 479, loss is 0.14168919622898102\n",
      "epoch: 19 step: 480, loss is 0.19408626854419708\n",
      "epoch: 19 step: 481, loss is 0.23801453411579132\n",
      "epoch: 19 step: 482, loss is 0.184882253408432\n",
      "epoch: 19 step: 483, loss is 0.3538248836994171\n",
      "epoch: 19 step: 484, loss is 0.16109983623027802\n",
      "epoch: 19 step: 485, loss is 0.3648045063018799\n",
      "epoch: 19 step: 486, loss is 0.15076357126235962\n",
      "epoch: 19 step: 487, loss is 0.27676132321357727\n",
      "epoch: 19 step: 488, loss is 0.3845228850841522\n",
      "epoch: 19 step: 489, loss is 0.11549864709377289\n",
      "epoch: 19 step: 490, loss is 0.2596779465675354\n",
      "epoch: 19 step: 491, loss is 0.28423959016799927\n",
      "epoch: 19 step: 492, loss is 0.14911171793937683\n",
      "epoch: 19 step: 493, loss is 0.22269578278064728\n",
      "epoch: 19 step: 494, loss is 0.33420005440711975\n",
      "epoch: 19 step: 495, loss is 0.32579994201660156\n",
      "epoch: 19 step: 496, loss is 0.16143745183944702\n",
      "epoch: 19 step: 497, loss is 0.15115320682525635\n",
      "epoch: 19 step: 498, loss is 0.11209681630134583\n",
      "epoch: 19 step: 499, loss is 0.23703214526176453\n",
      "epoch: 19 step: 500, loss is 0.34133097529411316\n",
      "epoch: 19 step: 501, loss is 0.3120182752609253\n",
      "epoch: 19 step: 502, loss is 0.2509690821170807\n",
      "epoch: 19 step: 503, loss is 0.17918625473976135\n",
      "epoch: 19 step: 504, loss is 0.41135379672050476\n",
      "epoch: 19 step: 505, loss is 0.2921895682811737\n",
      "epoch: 19 step: 506, loss is 0.27618059515953064\n",
      "epoch: 19 step: 507, loss is 0.1295880228281021\n",
      "epoch: 19 step: 508, loss is 0.21912798285484314\n",
      "epoch: 19 step: 509, loss is 0.176011323928833\n",
      "epoch: 19 step: 510, loss is 0.16020432114601135\n",
      "epoch: 19 step: 511, loss is 0.22794342041015625\n",
      "epoch: 19 step: 512, loss is 0.23289760947227478\n",
      "epoch: 19 step: 513, loss is 0.11464229226112366\n",
      "epoch: 19 step: 514, loss is 0.28746458888053894\n",
      "epoch: 19 step: 515, loss is 0.20883868634700775\n",
      "epoch: 19 step: 516, loss is 0.2924268841743469\n",
      "epoch: 19 step: 517, loss is 0.16736362874507904\n",
      "epoch: 19 step: 518, loss is 0.23126651346683502\n",
      "epoch: 19 step: 519, loss is 0.12839511036872864\n",
      "epoch: 19 step: 520, loss is 0.12703722715377808\n",
      "epoch: 19 step: 521, loss is 0.188958078622818\n",
      "epoch: 19 step: 522, loss is 0.20322054624557495\n",
      "epoch: 19 step: 523, loss is 0.11511486768722534\n",
      "epoch: 19 step: 524, loss is 0.28144800662994385\n",
      "epoch: 19 step: 525, loss is 0.21108737587928772\n",
      "epoch: 19 step: 526, loss is 0.11888428032398224\n",
      "epoch: 19 step: 527, loss is 0.2028730809688568\n",
      "epoch: 19 step: 528, loss is 0.11802321672439575\n",
      "epoch: 19 step: 529, loss is 0.1322013884782791\n",
      "epoch: 19 step: 530, loss is 0.2295953929424286\n",
      "epoch: 19 step: 531, loss is 0.19564610719680786\n",
      "epoch: 19 step: 532, loss is 0.07924482226371765\n",
      "epoch: 19 step: 533, loss is 0.07521238923072815\n",
      "epoch: 19 step: 534, loss is 0.10006449371576309\n",
      "epoch: 19 step: 535, loss is 0.14233756065368652\n",
      "epoch: 19 step: 536, loss is 0.2005680650472641\n",
      "epoch: 19 step: 537, loss is 0.30123481154441833\n",
      "epoch: 19 step: 538, loss is 0.20934605598449707\n",
      "epoch: 19 step: 539, loss is 0.27567625045776367\n",
      "epoch: 19 step: 540, loss is 0.22066962718963623\n",
      "epoch: 19 step: 541, loss is 0.06986993551254272\n",
      "epoch: 19 step: 542, loss is 0.22136114537715912\n",
      "epoch: 19 step: 543, loss is 0.11359111964702606\n",
      "epoch: 19 step: 544, loss is 0.20745491981506348\n",
      "epoch: 19 step: 545, loss is 0.20612750947475433\n",
      "epoch: 19 step: 546, loss is 0.23281040787696838\n",
      "epoch: 19 step: 547, loss is 0.2500525712966919\n",
      "epoch: 19 step: 548, loss is 0.1618986278772354\n",
      "epoch: 19 step: 549, loss is 0.1957213282585144\n",
      "epoch: 19 step: 550, loss is 0.18655194342136383\n",
      "epoch: 19 step: 551, loss is 0.3202335238456726\n",
      "epoch: 19 step: 552, loss is 0.24903808534145355\n",
      "epoch: 19 step: 553, loss is 0.22658906877040863\n",
      "epoch: 19 step: 554, loss is 0.10620123147964478\n",
      "epoch: 19 step: 555, loss is 0.2061758041381836\n",
      "epoch: 19 step: 556, loss is 0.24699197709560394\n",
      "epoch: 19 step: 557, loss is 0.4398248493671417\n",
      "epoch: 19 step: 558, loss is 0.2907394468784332\n",
      "epoch: 19 step: 559, loss is 0.5155788064002991\n",
      "epoch: 19 step: 560, loss is 0.06656654179096222\n",
      "epoch: 19 step: 561, loss is 0.2851688861846924\n",
      "epoch: 19 step: 562, loss is 0.16193906962871552\n",
      "epoch: 19 step: 563, loss is 0.25699594616889954\n",
      "epoch: 19 step: 564, loss is 0.1817394495010376\n",
      "epoch: 19 step: 565, loss is 0.14930713176727295\n",
      "epoch: 19 step: 566, loss is 0.31828051805496216\n",
      "epoch: 19 step: 567, loss is 0.1713622659444809\n",
      "epoch: 19 step: 568, loss is 0.17794866859912872\n",
      "epoch: 19 step: 569, loss is 0.27782320976257324\n",
      "epoch: 19 step: 570, loss is 0.3350958824157715\n",
      "epoch: 19 step: 571, loss is 0.29696905612945557\n",
      "epoch: 19 step: 572, loss is 0.12858189642429352\n",
      "epoch: 19 step: 573, loss is 0.23837195336818695\n",
      "epoch: 19 step: 574, loss is 0.20097357034683228\n",
      "epoch: 19 step: 575, loss is 0.1549382358789444\n",
      "epoch: 19 step: 576, loss is 0.16240398585796356\n",
      "epoch: 19 step: 577, loss is 0.32221198081970215\n",
      "epoch: 19 step: 578, loss is 0.2096121907234192\n",
      "epoch: 19 step: 579, loss is 0.1553429812192917\n",
      "epoch: 19 step: 580, loss is 0.18983463943004608\n",
      "epoch: 19 step: 581, loss is 0.5145592093467712\n",
      "epoch: 19 step: 582, loss is 0.11050013452768326\n",
      "epoch: 19 step: 583, loss is 0.34628647565841675\n",
      "epoch: 19 step: 584, loss is 0.12722428143024445\n",
      "epoch: 19 step: 585, loss is 0.21191003918647766\n",
      "epoch: 19 step: 586, loss is 0.22870784997940063\n",
      "epoch: 19 step: 587, loss is 0.16310067474842072\n",
      "epoch: 19 step: 588, loss is 0.12695284187793732\n",
      "epoch: 19 step: 589, loss is 0.17569565773010254\n",
      "epoch: 19 step: 590, loss is 0.2889935374259949\n",
      "epoch: 19 step: 591, loss is 0.10188457369804382\n",
      "epoch: 19 step: 592, loss is 0.13168351352214813\n",
      "epoch: 19 step: 593, loss is 0.20385079085826874\n",
      "epoch: 19 step: 594, loss is 0.1799239069223404\n",
      "epoch: 19 step: 595, loss is 0.20066271722316742\n",
      "epoch: 19 step: 596, loss is 0.22120165824890137\n",
      "epoch: 19 step: 597, loss is 0.12109379470348358\n",
      "epoch: 19 step: 598, loss is 0.18687430024147034\n",
      "epoch: 19 step: 599, loss is 0.14023908972740173\n",
      "epoch: 19 step: 600, loss is 0.2676812410354614\n",
      "epoch: 19 step: 601, loss is 0.1835746318101883\n",
      "epoch: 19 step: 602, loss is 0.34746357798576355\n",
      "epoch: 19 step: 603, loss is 0.15418174862861633\n",
      "epoch: 19 step: 604, loss is 0.323339581489563\n",
      "epoch: 19 step: 605, loss is 0.2203586995601654\n",
      "epoch: 19 step: 606, loss is 0.13284607231616974\n",
      "epoch: 19 step: 607, loss is 0.13492897152900696\n",
      "epoch: 19 step: 608, loss is 0.2507808208465576\n",
      "epoch: 19 step: 609, loss is 0.24561533331871033\n",
      "epoch: 19 step: 610, loss is 0.20657946169376373\n",
      "epoch: 19 step: 611, loss is 0.4275895655155182\n",
      "epoch: 19 step: 612, loss is 0.22622545063495636\n",
      "epoch: 19 step: 613, loss is 0.21608838438987732\n",
      "epoch: 19 step: 614, loss is 0.4699435830116272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 615, loss is 0.23184223473072052\n",
      "epoch: 19 step: 616, loss is 0.22713911533355713\n",
      "epoch: 19 step: 617, loss is 0.21773502230644226\n",
      "epoch: 19 step: 618, loss is 0.14265191555023193\n",
      "epoch: 19 step: 619, loss is 0.17476043105125427\n",
      "epoch: 19 step: 620, loss is 0.1003790870308876\n",
      "epoch: 19 step: 621, loss is 0.09790598601102829\n",
      "epoch: 19 step: 622, loss is 0.12178698182106018\n",
      "epoch: 19 step: 623, loss is 0.2585434913635254\n",
      "epoch: 19 step: 624, loss is 0.1937248706817627\n",
      "epoch: 19 step: 625, loss is 0.28161054849624634\n",
      "epoch: 19 step: 626, loss is 0.19238978624343872\n",
      "epoch: 19 step: 627, loss is 0.3285013437271118\n",
      "epoch: 19 step: 628, loss is 0.3554741144180298\n",
      "epoch: 19 step: 629, loss is 0.24910832941532135\n",
      "epoch: 19 step: 630, loss is 0.2500060200691223\n",
      "epoch: 19 step: 631, loss is 0.16751669347286224\n",
      "epoch: 19 step: 632, loss is 0.22474916279315948\n",
      "epoch: 19 step: 633, loss is 0.13734495639801025\n",
      "epoch: 19 step: 634, loss is 0.24971361458301544\n",
      "epoch: 19 step: 635, loss is 0.2192767858505249\n",
      "epoch: 19 step: 636, loss is 0.36291709542274475\n",
      "epoch: 19 step: 637, loss is 0.09896870702505112\n",
      "epoch: 19 step: 638, loss is 0.19457563757896423\n",
      "epoch: 19 step: 639, loss is 0.18618039786815643\n",
      "epoch: 19 step: 640, loss is 0.18772569298744202\n",
      "epoch: 19 step: 641, loss is 0.27205339074134827\n",
      "epoch: 19 step: 642, loss is 0.3681733012199402\n",
      "epoch: 19 step: 643, loss is 0.1910330355167389\n",
      "epoch: 19 step: 644, loss is 0.0890042781829834\n",
      "epoch: 19 step: 645, loss is 0.11281190812587738\n",
      "epoch: 19 step: 646, loss is 0.31971630454063416\n",
      "epoch: 19 step: 647, loss is 0.1845782995223999\n",
      "epoch: 19 step: 648, loss is 0.18629968166351318\n",
      "epoch: 19 step: 649, loss is 0.3238484561443329\n",
      "epoch: 19 step: 650, loss is 0.08237093687057495\n",
      "epoch: 19 step: 651, loss is 0.179985910654068\n",
      "epoch: 19 step: 652, loss is 0.15918567776679993\n",
      "epoch: 19 step: 653, loss is 0.24923965334892273\n",
      "epoch: 19 step: 654, loss is 0.13040871918201447\n",
      "epoch: 19 step: 655, loss is 0.172252357006073\n",
      "epoch: 19 step: 656, loss is 0.20628540217876434\n",
      "epoch: 19 step: 657, loss is 0.07563283294439316\n",
      "epoch: 19 step: 658, loss is 0.16480039060115814\n",
      "epoch: 19 step: 659, loss is 0.20478342473506927\n",
      "epoch: 19 step: 660, loss is 0.2506733536720276\n",
      "epoch: 19 step: 661, loss is 0.14723458886146545\n",
      "epoch: 19 step: 662, loss is 0.21382679045200348\n",
      "epoch: 19 step: 663, loss is 0.17433889210224152\n",
      "epoch: 19 step: 664, loss is 0.18339528143405914\n",
      "epoch: 19 step: 665, loss is 0.21410898864269257\n",
      "epoch: 19 step: 666, loss is 0.21210025250911713\n",
      "epoch: 19 step: 667, loss is 0.11892295628786087\n",
      "epoch: 19 step: 668, loss is 0.09147922694683075\n",
      "epoch: 19 step: 669, loss is 0.23850323259830475\n",
      "epoch: 19 step: 670, loss is 0.31478872895240784\n",
      "epoch: 19 step: 671, loss is 0.08646585047245026\n",
      "epoch: 19 step: 672, loss is 0.2105444222688675\n",
      "epoch: 19 step: 673, loss is 0.2154676914215088\n",
      "epoch: 19 step: 674, loss is 0.20335210859775543\n",
      "epoch: 19 step: 675, loss is 0.16044288873672485\n",
      "epoch: 19 step: 676, loss is 0.27069273591041565\n",
      "epoch: 19 step: 677, loss is 0.19827719032764435\n",
      "epoch: 19 step: 678, loss is 0.25790169835090637\n",
      "epoch: 19 step: 679, loss is 0.2854403555393219\n",
      "epoch: 19 step: 680, loss is 0.16018499433994293\n",
      "epoch: 19 step: 681, loss is 0.12091054767370224\n",
      "epoch: 19 step: 682, loss is 0.18740786612033844\n",
      "epoch: 19 step: 683, loss is 0.28763192892074585\n",
      "epoch: 19 step: 684, loss is 0.20671674609184265\n",
      "epoch: 19 step: 685, loss is 0.07961667329072952\n",
      "epoch: 19 step: 686, loss is 0.38642606139183044\n",
      "epoch: 19 step: 687, loss is 0.1172008141875267\n",
      "epoch: 19 step: 688, loss is 0.17486155033111572\n",
      "epoch: 19 step: 689, loss is 0.19118614494800568\n",
      "epoch: 19 step: 690, loss is 0.30425623059272766\n",
      "epoch: 19 step: 691, loss is 0.2468826323747635\n",
      "epoch: 19 step: 692, loss is 0.19314974546432495\n",
      "epoch: 19 step: 693, loss is 0.07818374782800674\n",
      "epoch: 19 step: 694, loss is 0.223613902926445\n",
      "epoch: 19 step: 695, loss is 0.10903947800397873\n",
      "epoch: 19 step: 696, loss is 0.14217261970043182\n",
      "epoch: 19 step: 697, loss is 0.1735021024942398\n",
      "epoch: 19 step: 698, loss is 0.12849946320056915\n",
      "epoch: 19 step: 699, loss is 0.223430797457695\n",
      "epoch: 19 step: 700, loss is 0.3263365924358368\n",
      "epoch: 19 step: 701, loss is 0.23838332295417786\n",
      "epoch: 19 step: 702, loss is 0.31352442502975464\n",
      "epoch: 19 step: 703, loss is 0.2350863665342331\n",
      "epoch: 19 step: 704, loss is 0.13226205110549927\n",
      "epoch: 19 step: 705, loss is 0.16785451769828796\n",
      "epoch: 19 step: 706, loss is 0.07529672980308533\n",
      "epoch: 19 step: 707, loss is 0.25819218158721924\n",
      "epoch: 19 step: 708, loss is 0.2953658401966095\n",
      "epoch: 19 step: 709, loss is 0.19324201345443726\n",
      "epoch: 19 step: 710, loss is 0.19618767499923706\n",
      "epoch: 19 step: 711, loss is 0.22902558743953705\n",
      "epoch: 19 step: 712, loss is 0.23377130925655365\n",
      "epoch: 19 step: 713, loss is 0.40566593408584595\n",
      "epoch: 19 step: 714, loss is 0.32800763845443726\n",
      "epoch: 19 step: 715, loss is 0.2012130618095398\n",
      "epoch: 19 step: 716, loss is 0.1594933122396469\n",
      "epoch: 19 step: 717, loss is 0.1480235606431961\n",
      "epoch: 19 step: 718, loss is 0.2738613784313202\n",
      "epoch: 19 step: 719, loss is 0.14484953880310059\n",
      "epoch: 19 step: 720, loss is 0.4668387770652771\n",
      "epoch: 19 step: 721, loss is 0.178537979722023\n",
      "epoch: 19 step: 722, loss is 0.06176484376192093\n",
      "epoch: 19 step: 723, loss is 0.32477739453315735\n",
      "epoch: 19 step: 724, loss is 0.20011356472969055\n",
      "epoch: 19 step: 725, loss is 0.1868886947631836\n",
      "epoch: 19 step: 726, loss is 0.24101054668426514\n",
      "epoch: 19 step: 727, loss is 0.12304995954036713\n",
      "epoch: 19 step: 728, loss is 0.09105388820171356\n",
      "epoch: 19 step: 729, loss is 0.26594051718711853\n",
      "epoch: 19 step: 730, loss is 0.2662924528121948\n",
      "epoch: 19 step: 731, loss is 0.09443165361881256\n",
      "epoch: 19 step: 732, loss is 0.23592162132263184\n",
      "epoch: 19 step: 733, loss is 0.14333689212799072\n",
      "epoch: 19 step: 734, loss is 0.35098880529403687\n",
      "epoch: 19 step: 735, loss is 0.08399450033903122\n",
      "epoch: 19 step: 736, loss is 0.19077518582344055\n",
      "epoch: 19 step: 737, loss is 0.07234005630016327\n",
      "epoch: 19 step: 738, loss is 0.11474354565143585\n",
      "epoch: 19 step: 739, loss is 0.2844773530960083\n",
      "epoch: 19 step: 740, loss is 0.1384122222661972\n",
      "epoch: 19 step: 741, loss is 0.24245686829090118\n",
      "epoch: 19 step: 742, loss is 0.15884314477443695\n",
      "epoch: 19 step: 743, loss is 0.40810567140579224\n",
      "epoch: 19 step: 744, loss is 0.3380971848964691\n",
      "epoch: 19 step: 745, loss is 0.1869601011276245\n",
      "epoch: 19 step: 746, loss is 0.16791456937789917\n",
      "epoch: 19 step: 747, loss is 0.12983965873718262\n",
      "epoch: 19 step: 748, loss is 0.11964456737041473\n",
      "epoch: 19 step: 749, loss is 0.2345472276210785\n",
      "epoch: 19 step: 750, loss is 0.42842432856559753\n",
      "epoch: 19 step: 751, loss is 0.13568440079689026\n",
      "epoch: 19 step: 752, loss is 0.30726492404937744\n",
      "epoch: 19 step: 753, loss is 0.22782878577709198\n",
      "epoch: 19 step: 754, loss is 0.24385114014148712\n",
      "epoch: 19 step: 755, loss is 0.28816163539886475\n",
      "epoch: 19 step: 756, loss is 0.20193572342395782\n",
      "epoch: 19 step: 757, loss is 0.24085955321788788\n",
      "epoch: 19 step: 758, loss is 0.259384423494339\n",
      "epoch: 19 step: 759, loss is 0.26162198185920715\n",
      "epoch: 19 step: 760, loss is 0.20190919935703278\n",
      "epoch: 19 step: 761, loss is 0.13302554190158844\n",
      "epoch: 19 step: 762, loss is 0.1878078430891037\n",
      "epoch: 19 step: 763, loss is 0.23153632879257202\n",
      "epoch: 19 step: 764, loss is 0.12554334104061127\n",
      "epoch: 19 step: 765, loss is 0.1135345995426178\n",
      "epoch: 19 step: 766, loss is 0.2350253164768219\n",
      "epoch: 19 step: 767, loss is 0.16384665668010712\n",
      "epoch: 19 step: 768, loss is 0.25618600845336914\n",
      "epoch: 19 step: 769, loss is 0.14713838696479797\n",
      "epoch: 19 step: 770, loss is 0.22346927225589752\n",
      "epoch: 19 step: 771, loss is 0.20768488943576813\n",
      "epoch: 19 step: 772, loss is 0.2456655651330948\n",
      "epoch: 19 step: 773, loss is 0.24139639735221863\n",
      "epoch: 19 step: 774, loss is 0.0815897136926651\n",
      "epoch: 19 step: 775, loss is 0.11403501033782959\n",
      "epoch: 19 step: 776, loss is 0.17852449417114258\n",
      "epoch: 19 step: 777, loss is 0.20136721432209015\n",
      "epoch: 19 step: 778, loss is 0.3467619717121124\n",
      "epoch: 19 step: 779, loss is 0.13534492254257202\n",
      "epoch: 19 step: 780, loss is 0.33947885036468506\n",
      "epoch: 19 step: 781, loss is 0.22812053561210632\n",
      "epoch: 19 step: 782, loss is 0.13743697106838226\n",
      "epoch: 19 step: 783, loss is 0.3159762918949127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 784, loss is 0.15640264749526978\n",
      "epoch: 19 step: 785, loss is 0.21868589520454407\n",
      "epoch: 19 step: 786, loss is 0.11289592832326889\n",
      "epoch: 19 step: 787, loss is 0.2459077537059784\n",
      "epoch: 19 step: 788, loss is 0.15549437701702118\n",
      "epoch: 19 step: 789, loss is 0.24177145957946777\n",
      "epoch: 19 step: 790, loss is 0.15286926925182343\n",
      "epoch: 19 step: 791, loss is 0.34035131335258484\n",
      "epoch: 19 step: 792, loss is 0.15524466335773468\n",
      "epoch: 19 step: 793, loss is 0.21538753807544708\n",
      "epoch: 19 step: 794, loss is 0.20098185539245605\n",
      "epoch: 19 step: 795, loss is 0.23088572919368744\n",
      "epoch: 19 step: 796, loss is 0.22937053442001343\n",
      "epoch: 19 step: 797, loss is 0.3690548241138458\n",
      "epoch: 19 step: 798, loss is 0.21490883827209473\n",
      "epoch: 19 step: 799, loss is 0.13343404233455658\n",
      "epoch: 19 step: 800, loss is 0.22135381400585175\n",
      "epoch: 19 step: 801, loss is 0.28777503967285156\n",
      "epoch: 19 step: 802, loss is 0.25393104553222656\n",
      "epoch: 19 step: 803, loss is 0.23790596425533295\n",
      "epoch: 19 step: 804, loss is 0.1597873419523239\n",
      "epoch: 19 step: 805, loss is 0.33597010374069214\n",
      "epoch: 19 step: 806, loss is 0.24150802195072174\n",
      "epoch: 19 step: 807, loss is 0.23030275106430054\n",
      "epoch: 19 step: 808, loss is 0.15016981959342957\n",
      "epoch: 19 step: 809, loss is 0.28076839447021484\n",
      "epoch: 19 step: 810, loss is 0.1694803684949875\n",
      "epoch: 19 step: 811, loss is 0.14124585688114166\n",
      "epoch: 19 step: 812, loss is 0.11943729966878891\n",
      "epoch: 19 step: 813, loss is 0.21607917547225952\n",
      "epoch: 19 step: 814, loss is 0.13846807181835175\n",
      "epoch: 19 step: 815, loss is 0.09928420931100845\n",
      "epoch: 19 step: 816, loss is 0.09527204930782318\n",
      "epoch: 19 step: 817, loss is 0.25409629940986633\n",
      "epoch: 19 step: 818, loss is 0.23452018201351166\n",
      "epoch: 19 step: 819, loss is 0.08510050922632217\n",
      "epoch: 19 step: 820, loss is 0.13225851953029633\n",
      "epoch: 19 step: 821, loss is 0.33684152364730835\n",
      "epoch: 19 step: 822, loss is 0.21266281604766846\n",
      "epoch: 19 step: 823, loss is 0.19619978964328766\n",
      "epoch: 19 step: 824, loss is 0.18293970823287964\n",
      "epoch: 19 step: 825, loss is 0.12944509088993073\n",
      "epoch: 19 step: 826, loss is 0.18204282224178314\n",
      "epoch: 19 step: 827, loss is 0.18511271476745605\n",
      "epoch: 19 step: 828, loss is 0.21463604271411896\n",
      "epoch: 19 step: 829, loss is 0.28244876861572266\n",
      "epoch: 19 step: 830, loss is 0.17163650691509247\n",
      "epoch: 19 step: 831, loss is 0.14716625213623047\n",
      "epoch: 19 step: 832, loss is 0.24208234250545502\n",
      "epoch: 19 step: 833, loss is 0.14762699604034424\n",
      "epoch: 19 step: 834, loss is 0.13599425554275513\n",
      "epoch: 19 step: 835, loss is 0.09117420762777328\n",
      "epoch: 19 step: 836, loss is 0.10211475938558578\n",
      "epoch: 19 step: 837, loss is 0.20552973449230194\n",
      "epoch: 19 step: 838, loss is 0.18296095728874207\n",
      "epoch: 19 step: 839, loss is 0.28007540106773376\n",
      "epoch: 19 step: 840, loss is 0.11207154393196106\n",
      "epoch: 19 step: 841, loss is 0.28471729159355164\n",
      "epoch: 19 step: 842, loss is 0.17790603637695312\n",
      "epoch: 19 step: 843, loss is 0.14172160625457764\n",
      "epoch: 19 step: 844, loss is 0.16260138154029846\n",
      "epoch: 19 step: 845, loss is 0.14526785910129547\n",
      "epoch: 19 step: 846, loss is 0.24490895867347717\n",
      "epoch: 19 step: 847, loss is 0.240348219871521\n",
      "epoch: 19 step: 848, loss is 0.3071659207344055\n",
      "epoch: 19 step: 849, loss is 0.21802759170532227\n",
      "epoch: 19 step: 850, loss is 0.09181130677461624\n",
      "epoch: 19 step: 851, loss is 0.26608511805534363\n",
      "epoch: 19 step: 852, loss is 0.12104963511228561\n",
      "epoch: 19 step: 853, loss is 0.11911571025848389\n",
      "epoch: 19 step: 854, loss is 0.2236979603767395\n",
      "epoch: 19 step: 855, loss is 0.16686441004276276\n",
      "epoch: 19 step: 856, loss is 0.19389455020427704\n",
      "epoch: 19 step: 857, loss is 0.10761571675539017\n",
      "epoch: 19 step: 858, loss is 0.1979457288980484\n",
      "epoch: 19 step: 859, loss is 0.11504895985126495\n",
      "epoch: 19 step: 860, loss is 0.26199352741241455\n",
      "epoch: 19 step: 861, loss is 0.2568811774253845\n",
      "epoch: 19 step: 862, loss is 0.10844938457012177\n",
      "epoch: 19 step: 863, loss is 0.164845272898674\n",
      "epoch: 19 step: 864, loss is 0.28773510456085205\n",
      "epoch: 19 step: 865, loss is 0.10472904145717621\n",
      "epoch: 19 step: 866, loss is 0.19283902645111084\n",
      "epoch: 19 step: 867, loss is 0.122896209359169\n",
      "epoch: 19 step: 868, loss is 0.21850009262561798\n",
      "epoch: 19 step: 869, loss is 0.20494478940963745\n",
      "epoch: 19 step: 870, loss is 0.2858293056488037\n",
      "epoch: 19 step: 871, loss is 0.17499224841594696\n",
      "epoch: 19 step: 872, loss is 0.25566092133522034\n",
      "epoch: 19 step: 873, loss is 0.19517438113689423\n",
      "epoch: 19 step: 874, loss is 0.2136305421590805\n",
      "epoch: 19 step: 875, loss is 0.3501923680305481\n",
      "epoch: 19 step: 876, loss is 0.1552269011735916\n",
      "epoch: 19 step: 877, loss is 0.21294493973255157\n",
      "epoch: 19 step: 878, loss is 0.2625395357608795\n",
      "epoch: 19 step: 879, loss is 0.19888919591903687\n",
      "epoch: 19 step: 880, loss is 0.2416054904460907\n",
      "epoch: 19 step: 881, loss is 0.3204147219657898\n",
      "epoch: 19 step: 882, loss is 0.06352841854095459\n",
      "epoch: 19 step: 883, loss is 0.20806148648262024\n",
      "epoch: 19 step: 884, loss is 0.24961945414543152\n",
      "epoch: 19 step: 885, loss is 0.26234325766563416\n",
      "epoch: 19 step: 886, loss is 0.2343633770942688\n",
      "epoch: 19 step: 887, loss is 0.26047641038894653\n",
      "epoch: 19 step: 888, loss is 0.3545830249786377\n",
      "epoch: 19 step: 889, loss is 0.14455272257328033\n",
      "epoch: 19 step: 890, loss is 0.1143033504486084\n",
      "epoch: 19 step: 891, loss is 0.1642766147851944\n",
      "epoch: 19 step: 892, loss is 0.27541932463645935\n",
      "epoch: 19 step: 893, loss is 0.22011594474315643\n",
      "epoch: 19 step: 894, loss is 0.22129879891872406\n",
      "epoch: 19 step: 895, loss is 0.3861066699028015\n",
      "epoch: 19 step: 896, loss is 0.19145695865154266\n",
      "epoch: 19 step: 897, loss is 0.2775786519050598\n",
      "epoch: 19 step: 898, loss is 0.13752855360507965\n",
      "epoch: 19 step: 899, loss is 0.3030988276004791\n",
      "epoch: 19 step: 900, loss is 0.192798912525177\n",
      "epoch: 19 step: 901, loss is 0.2114274799823761\n",
      "epoch: 19 step: 902, loss is 0.11376354098320007\n",
      "epoch: 19 step: 903, loss is 0.1850425750017166\n",
      "epoch: 19 step: 904, loss is 0.24932703375816345\n",
      "epoch: 19 step: 905, loss is 0.21248383820056915\n",
      "epoch: 19 step: 906, loss is 0.1284661889076233\n",
      "epoch: 19 step: 907, loss is 0.12764054536819458\n",
      "epoch: 19 step: 908, loss is 0.12532947957515717\n",
      "epoch: 19 step: 909, loss is 0.37024733424186707\n",
      "epoch: 19 step: 910, loss is 0.13005204498767853\n",
      "epoch: 19 step: 911, loss is 0.18317577242851257\n",
      "epoch: 19 step: 912, loss is 0.318328857421875\n",
      "epoch: 19 step: 913, loss is 0.1919020563364029\n",
      "epoch: 19 step: 914, loss is 0.20463182032108307\n",
      "epoch: 19 step: 915, loss is 0.19718089699745178\n",
      "epoch: 19 step: 916, loss is 0.15620484948158264\n",
      "epoch: 19 step: 917, loss is 0.11989358067512512\n",
      "epoch: 19 step: 918, loss is 0.2438354343175888\n",
      "epoch: 19 step: 919, loss is 0.1573311984539032\n",
      "epoch: 19 step: 920, loss is 0.2938160300254822\n",
      "epoch: 19 step: 921, loss is 0.16661402583122253\n",
      "epoch: 19 step: 922, loss is 0.3070220947265625\n",
      "epoch: 19 step: 923, loss is 0.18389250338077545\n",
      "epoch: 19 step: 924, loss is 0.17233118414878845\n",
      "epoch: 19 step: 925, loss is 0.27766403555870056\n",
      "epoch: 19 step: 926, loss is 0.10955461114645004\n",
      "epoch: 19 step: 927, loss is 0.22690556943416595\n",
      "epoch: 19 step: 928, loss is 0.32522258162498474\n",
      "epoch: 19 step: 929, loss is 0.18698938190937042\n",
      "epoch: 19 step: 930, loss is 0.155945286154747\n",
      "epoch: 19 step: 931, loss is 0.48054492473602295\n",
      "epoch: 19 step: 932, loss is 0.2709793150424957\n",
      "epoch: 19 step: 933, loss is 0.14050564169883728\n",
      "epoch: 19 step: 934, loss is 0.17174705862998962\n",
      "epoch: 19 step: 935, loss is 0.23213209211826324\n",
      "epoch: 19 step: 936, loss is 0.18470077216625214\n",
      "epoch: 19 step: 937, loss is 0.23485128581523895\n",
      "epoch: 20 step: 1, loss is 0.2322268784046173\n",
      "epoch: 20 step: 2, loss is 0.16691280901432037\n",
      "epoch: 20 step: 3, loss is 0.1994073987007141\n",
      "epoch: 20 step: 4, loss is 0.23700399696826935\n",
      "epoch: 20 step: 5, loss is 0.13249915838241577\n",
      "epoch: 20 step: 6, loss is 0.2630631923675537\n",
      "epoch: 20 step: 7, loss is 0.3080841898918152\n",
      "epoch: 20 step: 8, loss is 0.3098154664039612\n",
      "epoch: 20 step: 9, loss is 0.3313867151737213\n",
      "epoch: 20 step: 10, loss is 0.21007683873176575\n",
      "epoch: 20 step: 11, loss is 0.15488746762275696\n",
      "epoch: 20 step: 12, loss is 0.32326462864875793\n",
      "epoch: 20 step: 13, loss is 0.3336007297039032\n",
      "epoch: 20 step: 14, loss is 0.33790260553359985\n",
      "epoch: 20 step: 15, loss is 0.2929593026638031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 16, loss is 0.1542109102010727\n",
      "epoch: 20 step: 17, loss is 0.1599046289920807\n",
      "epoch: 20 step: 18, loss is 0.14623519778251648\n",
      "epoch: 20 step: 19, loss is 0.3109564185142517\n",
      "epoch: 20 step: 20, loss is 0.3356126844882965\n",
      "epoch: 20 step: 21, loss is 0.3383311331272125\n",
      "epoch: 20 step: 22, loss is 0.19301337003707886\n",
      "epoch: 20 step: 23, loss is 0.1526690572500229\n",
      "epoch: 20 step: 24, loss is 0.09113431721925735\n",
      "epoch: 20 step: 25, loss is 0.16113820672035217\n",
      "epoch: 20 step: 26, loss is 0.23646551370620728\n",
      "epoch: 20 step: 27, loss is 0.2565914988517761\n",
      "epoch: 20 step: 28, loss is 0.08178877085447311\n",
      "epoch: 20 step: 29, loss is 0.3961317539215088\n",
      "epoch: 20 step: 30, loss is 0.2930240035057068\n",
      "epoch: 20 step: 31, loss is 0.10371274501085281\n",
      "epoch: 20 step: 32, loss is 0.18809406459331512\n",
      "epoch: 20 step: 33, loss is 0.20928163826465607\n",
      "epoch: 20 step: 34, loss is 0.11962421238422394\n",
      "epoch: 20 step: 35, loss is 0.20639412105083466\n",
      "epoch: 20 step: 36, loss is 0.16340701282024384\n",
      "epoch: 20 step: 37, loss is 0.17584674060344696\n",
      "epoch: 20 step: 38, loss is 0.11973019689321518\n",
      "epoch: 20 step: 39, loss is 0.28082001209259033\n",
      "epoch: 20 step: 40, loss is 0.2648226320743561\n",
      "epoch: 20 step: 41, loss is 0.13106653094291687\n",
      "epoch: 20 step: 42, loss is 0.18772344291210175\n",
      "epoch: 20 step: 43, loss is 0.12704573571681976\n",
      "epoch: 20 step: 44, loss is 0.23138396441936493\n",
      "epoch: 20 step: 45, loss is 0.11199390143156052\n",
      "epoch: 20 step: 46, loss is 0.19231392443180084\n",
      "epoch: 20 step: 47, loss is 0.12136195600032806\n",
      "epoch: 20 step: 48, loss is 0.3035731911659241\n",
      "epoch: 20 step: 49, loss is 0.3038317561149597\n",
      "epoch: 20 step: 50, loss is 0.20928195118904114\n",
      "epoch: 20 step: 51, loss is 0.1430116444826126\n",
      "epoch: 20 step: 52, loss is 0.19140273332595825\n",
      "epoch: 20 step: 53, loss is 0.16719429194927216\n",
      "epoch: 20 step: 54, loss is 0.20456372201442719\n",
      "epoch: 20 step: 55, loss is 0.10303789377212524\n",
      "epoch: 20 step: 56, loss is 0.2753905653953552\n",
      "epoch: 20 step: 57, loss is 0.2465023249387741\n",
      "epoch: 20 step: 58, loss is 0.03906853497028351\n",
      "epoch: 20 step: 59, loss is 0.07609235495328903\n",
      "epoch: 20 step: 60, loss is 0.13963429629802704\n",
      "epoch: 20 step: 61, loss is 0.0928172692656517\n",
      "epoch: 20 step: 62, loss is 0.15252090990543365\n",
      "epoch: 20 step: 63, loss is 0.30258020758628845\n",
      "epoch: 20 step: 64, loss is 0.3080619275569916\n",
      "epoch: 20 step: 65, loss is 0.23467904329299927\n",
      "epoch: 20 step: 66, loss is 0.14988918602466583\n",
      "epoch: 20 step: 67, loss is 0.17546561360359192\n",
      "epoch: 20 step: 68, loss is 0.27343669533729553\n",
      "epoch: 20 step: 69, loss is 0.1590365320444107\n",
      "epoch: 20 step: 70, loss is 0.2610695958137512\n",
      "epoch: 20 step: 71, loss is 0.2000785917043686\n",
      "epoch: 20 step: 72, loss is 0.08835309743881226\n",
      "epoch: 20 step: 73, loss is 0.15354730188846588\n",
      "epoch: 20 step: 74, loss is 0.1480146199464798\n",
      "epoch: 20 step: 75, loss is 0.12041813880205154\n",
      "epoch: 20 step: 76, loss is 0.14845849573612213\n",
      "epoch: 20 step: 77, loss is 0.13357853889465332\n",
      "epoch: 20 step: 78, loss is 0.20017577707767487\n",
      "epoch: 20 step: 79, loss is 0.129171222448349\n",
      "epoch: 20 step: 80, loss is 0.09809516370296478\n",
      "epoch: 20 step: 81, loss is 0.1756286472082138\n",
      "epoch: 20 step: 82, loss is 0.1259213387966156\n",
      "epoch: 20 step: 83, loss is 0.17402233183383942\n",
      "epoch: 20 step: 84, loss is 0.2637035548686981\n",
      "epoch: 20 step: 85, loss is 0.14116781949996948\n",
      "epoch: 20 step: 86, loss is 0.23722931742668152\n",
      "epoch: 20 step: 87, loss is 0.36882707476615906\n",
      "epoch: 20 step: 88, loss is 0.3005618155002594\n",
      "epoch: 20 step: 89, loss is 0.18034058809280396\n",
      "epoch: 20 step: 90, loss is 0.3403262794017792\n",
      "epoch: 20 step: 91, loss is 0.11763783544301987\n",
      "epoch: 20 step: 92, loss is 0.14982736110687256\n",
      "epoch: 20 step: 93, loss is 0.22589240968227386\n",
      "epoch: 20 step: 94, loss is 0.19529156386852264\n",
      "epoch: 20 step: 95, loss is 0.30633488297462463\n",
      "epoch: 20 step: 96, loss is 0.1604706346988678\n",
      "epoch: 20 step: 97, loss is 0.13837899267673492\n",
      "epoch: 20 step: 98, loss is 0.21772757172584534\n",
      "epoch: 20 step: 99, loss is 0.29197338223457336\n",
      "epoch: 20 step: 100, loss is 0.22857555747032166\n",
      "epoch: 20 step: 101, loss is 0.23251672089099884\n",
      "epoch: 20 step: 102, loss is 0.24770338833332062\n",
      "epoch: 20 step: 103, loss is 0.13151249289512634\n",
      "epoch: 20 step: 104, loss is 0.27805179357528687\n",
      "epoch: 20 step: 105, loss is 0.2607648968696594\n",
      "epoch: 20 step: 106, loss is 0.16486908495426178\n",
      "epoch: 20 step: 107, loss is 0.2604561150074005\n",
      "epoch: 20 step: 108, loss is 0.07159441709518433\n",
      "epoch: 20 step: 109, loss is 0.13697579503059387\n",
      "epoch: 20 step: 110, loss is 0.29161009192466736\n",
      "epoch: 20 step: 111, loss is 0.14552362263202667\n",
      "epoch: 20 step: 112, loss is 0.19605585932731628\n",
      "epoch: 20 step: 113, loss is 0.13363218307495117\n",
      "epoch: 20 step: 114, loss is 0.21292516589164734\n",
      "epoch: 20 step: 115, loss is 0.14295880496501923\n",
      "epoch: 20 step: 116, loss is 0.1997700035572052\n",
      "epoch: 20 step: 117, loss is 0.17328807711601257\n",
      "epoch: 20 step: 118, loss is 0.2425466775894165\n",
      "epoch: 20 step: 119, loss is 0.11108901351690292\n",
      "epoch: 20 step: 120, loss is 0.23658090829849243\n",
      "epoch: 20 step: 121, loss is 0.23297417163848877\n",
      "epoch: 20 step: 122, loss is 0.18315251171588898\n",
      "epoch: 20 step: 123, loss is 0.3185768127441406\n",
      "epoch: 20 step: 124, loss is 0.26140034198760986\n",
      "epoch: 20 step: 125, loss is 0.14071673154830933\n",
      "epoch: 20 step: 126, loss is 0.20656515657901764\n",
      "epoch: 20 step: 127, loss is 0.1893254518508911\n",
      "epoch: 20 step: 128, loss is 0.13633190095424652\n",
      "epoch: 20 step: 129, loss is 0.1206677258014679\n",
      "epoch: 20 step: 130, loss is 0.13899053633213043\n",
      "epoch: 20 step: 131, loss is 0.23759493231773376\n",
      "epoch: 20 step: 132, loss is 0.20261317491531372\n",
      "epoch: 20 step: 133, loss is 0.06264279782772064\n",
      "epoch: 20 step: 134, loss is 0.2205161303281784\n",
      "epoch: 20 step: 135, loss is 0.10918201506137848\n",
      "epoch: 20 step: 136, loss is 0.15163244307041168\n",
      "epoch: 20 step: 137, loss is 0.13157279789447784\n",
      "epoch: 20 step: 138, loss is 0.143230602145195\n",
      "epoch: 20 step: 139, loss is 0.3379787504673004\n",
      "epoch: 20 step: 140, loss is 0.16951683163642883\n",
      "epoch: 20 step: 141, loss is 0.0743374302983284\n",
      "epoch: 20 step: 142, loss is 0.0920831635594368\n",
      "epoch: 20 step: 143, loss is 0.14066031575202942\n",
      "epoch: 20 step: 144, loss is 0.10946948826313019\n",
      "epoch: 20 step: 145, loss is 0.0786600261926651\n",
      "epoch: 20 step: 146, loss is 0.2554352283477783\n",
      "epoch: 20 step: 147, loss is 0.10290323197841644\n",
      "epoch: 20 step: 148, loss is 0.08848171681165695\n",
      "epoch: 20 step: 149, loss is 0.19296994805335999\n",
      "epoch: 20 step: 150, loss is 0.152041494846344\n",
      "epoch: 20 step: 151, loss is 0.1964462697505951\n",
      "epoch: 20 step: 152, loss is 0.20327073335647583\n",
      "epoch: 20 step: 153, loss is 0.19024237990379333\n",
      "epoch: 20 step: 154, loss is 0.19512233138084412\n",
      "epoch: 20 step: 155, loss is 0.20451360940933228\n",
      "epoch: 20 step: 156, loss is 0.295414537191391\n",
      "epoch: 20 step: 157, loss is 0.0765521228313446\n",
      "epoch: 20 step: 158, loss is 0.33707284927368164\n",
      "epoch: 20 step: 159, loss is 0.2593093514442444\n",
      "epoch: 20 step: 160, loss is 0.13876599073410034\n",
      "epoch: 20 step: 161, loss is 0.143708735704422\n",
      "epoch: 20 step: 162, loss is 0.2702046036720276\n",
      "epoch: 20 step: 163, loss is 0.1714877188205719\n",
      "epoch: 20 step: 164, loss is 0.21578755974769592\n",
      "epoch: 20 step: 165, loss is 0.27726465463638306\n",
      "epoch: 20 step: 166, loss is 0.1986556053161621\n",
      "epoch: 20 step: 167, loss is 0.2853436768054962\n",
      "epoch: 20 step: 168, loss is 0.4265265166759491\n",
      "epoch: 20 step: 169, loss is 0.14752203226089478\n",
      "epoch: 20 step: 170, loss is 0.2523672878742218\n",
      "epoch: 20 step: 171, loss is 0.3290635645389557\n",
      "epoch: 20 step: 172, loss is 0.17977161705493927\n",
      "epoch: 20 step: 173, loss is 0.13644197583198547\n",
      "epoch: 20 step: 174, loss is 0.14273886382579803\n",
      "epoch: 20 step: 175, loss is 0.23884695768356323\n",
      "epoch: 20 step: 176, loss is 0.18866311013698578\n",
      "epoch: 20 step: 177, loss is 0.16155239939689636\n",
      "epoch: 20 step: 178, loss is 0.21855852007865906\n",
      "epoch: 20 step: 179, loss is 0.1709970235824585\n",
      "epoch: 20 step: 180, loss is 0.14505989849567413\n",
      "epoch: 20 step: 181, loss is 0.1999923586845398\n",
      "epoch: 20 step: 182, loss is 0.2808130979537964\n",
      "epoch: 20 step: 183, loss is 0.1546921581029892\n",
      "epoch: 20 step: 184, loss is 0.3092963993549347\n",
      "epoch: 20 step: 185, loss is 0.20034176111221313\n",
      "epoch: 20 step: 186, loss is 0.2101237177848816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 187, loss is 0.1899355947971344\n",
      "epoch: 20 step: 188, loss is 0.19177977740764618\n",
      "epoch: 20 step: 189, loss is 0.258516401052475\n",
      "epoch: 20 step: 190, loss is 0.16534556448459625\n",
      "epoch: 20 step: 191, loss is 0.19440799951553345\n",
      "epoch: 20 step: 192, loss is 0.2581714391708374\n",
      "epoch: 20 step: 193, loss is 0.1703418642282486\n",
      "epoch: 20 step: 194, loss is 0.23502953350543976\n",
      "epoch: 20 step: 195, loss is 0.27248838543891907\n",
      "epoch: 20 step: 196, loss is 0.1438731700181961\n",
      "epoch: 20 step: 197, loss is 0.15541289746761322\n",
      "epoch: 20 step: 198, loss is 0.20086254179477692\n",
      "epoch: 20 step: 199, loss is 0.2740343511104584\n",
      "epoch: 20 step: 200, loss is 0.1798502653837204\n",
      "epoch: 20 step: 201, loss is 0.08334249258041382\n",
      "epoch: 20 step: 202, loss is 0.07859847694635391\n",
      "epoch: 20 step: 203, loss is 0.20682060718536377\n",
      "epoch: 20 step: 204, loss is 0.2308502048254013\n",
      "epoch: 20 step: 205, loss is 0.407939612865448\n",
      "epoch: 20 step: 206, loss is 0.20136511325836182\n",
      "epoch: 20 step: 207, loss is 0.20993846654891968\n",
      "epoch: 20 step: 208, loss is 0.1570587456226349\n",
      "epoch: 20 step: 209, loss is 0.1842840611934662\n",
      "epoch: 20 step: 210, loss is 0.1679784208536148\n",
      "epoch: 20 step: 211, loss is 0.32355520129203796\n",
      "epoch: 20 step: 212, loss is 0.13892240822315216\n",
      "epoch: 20 step: 213, loss is 0.28850340843200684\n",
      "epoch: 20 step: 214, loss is 0.18402937054634094\n",
      "epoch: 20 step: 215, loss is 0.13455119729042053\n",
      "epoch: 20 step: 216, loss is 0.25603148341178894\n",
      "epoch: 20 step: 217, loss is 0.21317598223686218\n",
      "epoch: 20 step: 218, loss is 0.1664656549692154\n",
      "epoch: 20 step: 219, loss is 0.21929842233657837\n",
      "epoch: 20 step: 220, loss is 0.20513831079006195\n",
      "epoch: 20 step: 221, loss is 0.13950766623020172\n",
      "epoch: 20 step: 222, loss is 0.28708040714263916\n",
      "epoch: 20 step: 223, loss is 0.21753916144371033\n",
      "epoch: 20 step: 224, loss is 0.12633375823497772\n",
      "epoch: 20 step: 225, loss is 0.09730685502290726\n",
      "epoch: 20 step: 226, loss is 0.18491536378860474\n",
      "epoch: 20 step: 227, loss is 0.10879655927419662\n",
      "epoch: 20 step: 228, loss is 0.20851661264896393\n",
      "epoch: 20 step: 229, loss is 0.2585921883583069\n",
      "epoch: 20 step: 230, loss is 0.2560259699821472\n",
      "epoch: 20 step: 231, loss is 0.21428978443145752\n",
      "epoch: 20 step: 232, loss is 0.19786053895950317\n",
      "epoch: 20 step: 233, loss is 0.26313158869743347\n",
      "epoch: 20 step: 234, loss is 0.11992238461971283\n",
      "epoch: 20 step: 235, loss is 0.19184450805187225\n",
      "epoch: 20 step: 236, loss is 0.14417888224124908\n",
      "epoch: 20 step: 237, loss is 0.44567441940307617\n",
      "epoch: 20 step: 238, loss is 0.248795747756958\n",
      "epoch: 20 step: 239, loss is 0.15238335728645325\n",
      "epoch: 20 step: 240, loss is 0.2555159330368042\n",
      "epoch: 20 step: 241, loss is 0.3189978301525116\n",
      "epoch: 20 step: 242, loss is 0.08814088255167007\n",
      "epoch: 20 step: 243, loss is 0.13874025642871857\n",
      "epoch: 20 step: 244, loss is 0.2598097622394562\n",
      "epoch: 20 step: 245, loss is 0.22881703078746796\n",
      "epoch: 20 step: 246, loss is 0.34660574793815613\n",
      "epoch: 20 step: 247, loss is 0.1253378987312317\n",
      "epoch: 20 step: 248, loss is 0.2987256646156311\n",
      "epoch: 20 step: 249, loss is 0.13846854865550995\n",
      "epoch: 20 step: 250, loss is 0.31692394614219666\n",
      "epoch: 20 step: 251, loss is 0.2983517050743103\n",
      "epoch: 20 step: 252, loss is 0.11695968359708786\n",
      "epoch: 20 step: 253, loss is 0.15943273901939392\n",
      "epoch: 20 step: 254, loss is 0.15133871138095856\n",
      "epoch: 20 step: 255, loss is 0.10429750382900238\n",
      "epoch: 20 step: 256, loss is 0.21836495399475098\n",
      "epoch: 20 step: 257, loss is 0.15404832363128662\n",
      "epoch: 20 step: 258, loss is 0.20415039360523224\n",
      "epoch: 20 step: 259, loss is 0.6784467101097107\n",
      "epoch: 20 step: 260, loss is 0.22454442083835602\n",
      "epoch: 20 step: 261, loss is 0.20238740742206573\n",
      "epoch: 20 step: 262, loss is 0.16697624325752258\n",
      "epoch: 20 step: 263, loss is 0.1036323755979538\n",
      "epoch: 20 step: 264, loss is 0.1664808988571167\n",
      "epoch: 20 step: 265, loss is 0.1674846112728119\n",
      "epoch: 20 step: 266, loss is 0.3049013018608093\n",
      "epoch: 20 step: 267, loss is 0.25751909613609314\n",
      "epoch: 20 step: 268, loss is 0.16255640983581543\n",
      "epoch: 20 step: 269, loss is 0.08927224576473236\n",
      "epoch: 20 step: 270, loss is 0.20538468658924103\n",
      "epoch: 20 step: 271, loss is 0.08981112390756607\n",
      "epoch: 20 step: 272, loss is 0.4116305112838745\n",
      "epoch: 20 step: 273, loss is 0.20399492979049683\n",
      "epoch: 20 step: 274, loss is 0.09266911447048187\n",
      "epoch: 20 step: 275, loss is 0.18947681784629822\n",
      "epoch: 20 step: 276, loss is 0.08027445524930954\n",
      "epoch: 20 step: 277, loss is 0.2159239649772644\n",
      "epoch: 20 step: 278, loss is 0.1679304540157318\n",
      "epoch: 20 step: 279, loss is 0.1755780577659607\n",
      "epoch: 20 step: 280, loss is 0.2772231996059418\n",
      "epoch: 20 step: 281, loss is 0.3180457353591919\n",
      "epoch: 20 step: 282, loss is 0.16667035222053528\n",
      "epoch: 20 step: 283, loss is 0.1619824916124344\n",
      "epoch: 20 step: 284, loss is 0.22519735991954803\n",
      "epoch: 20 step: 285, loss is 0.20302145183086395\n",
      "epoch: 20 step: 286, loss is 0.16359686851501465\n",
      "epoch: 20 step: 287, loss is 0.10994748771190643\n",
      "epoch: 20 step: 288, loss is 0.26825013756752014\n",
      "epoch: 20 step: 289, loss is 0.16355165839195251\n",
      "epoch: 20 step: 290, loss is 0.20925985276699066\n",
      "epoch: 20 step: 291, loss is 0.3193502426147461\n",
      "epoch: 20 step: 292, loss is 0.2933436632156372\n",
      "epoch: 20 step: 293, loss is 0.193687304854393\n",
      "epoch: 20 step: 294, loss is 0.24633023142814636\n",
      "epoch: 20 step: 295, loss is 0.11830412596464157\n",
      "epoch: 20 step: 296, loss is 0.17802947759628296\n",
      "epoch: 20 step: 297, loss is 0.15618020296096802\n",
      "epoch: 20 step: 298, loss is 0.16430555284023285\n",
      "epoch: 20 step: 299, loss is 0.1269093155860901\n",
      "epoch: 20 step: 300, loss is 0.20036855340003967\n",
      "epoch: 20 step: 301, loss is 0.08276063948869705\n",
      "epoch: 20 step: 302, loss is 0.11540690064430237\n",
      "epoch: 20 step: 303, loss is 0.1543312668800354\n",
      "epoch: 20 step: 304, loss is 0.20416142046451569\n",
      "epoch: 20 step: 305, loss is 0.14695747196674347\n",
      "epoch: 20 step: 306, loss is 0.17709723114967346\n",
      "epoch: 20 step: 307, loss is 0.16468752920627594\n",
      "epoch: 20 step: 308, loss is 0.07239900529384613\n",
      "epoch: 20 step: 309, loss is 0.3765236437320709\n",
      "epoch: 20 step: 310, loss is 0.18484583497047424\n",
      "epoch: 20 step: 311, loss is 0.20649169385433197\n",
      "epoch: 20 step: 312, loss is 0.2795965075492859\n",
      "epoch: 20 step: 313, loss is 0.30202925205230713\n",
      "epoch: 20 step: 314, loss is 0.21025395393371582\n",
      "epoch: 20 step: 315, loss is 0.20114704966545105\n",
      "epoch: 20 step: 316, loss is 0.04754861444234848\n",
      "epoch: 20 step: 317, loss is 0.14951419830322266\n",
      "epoch: 20 step: 318, loss is 0.34344160556793213\n",
      "epoch: 20 step: 319, loss is 0.21576502919197083\n",
      "epoch: 20 step: 320, loss is 0.15990151464939117\n",
      "epoch: 20 step: 321, loss is 0.17011387646198273\n",
      "epoch: 20 step: 322, loss is 0.3270643651485443\n",
      "epoch: 20 step: 323, loss is 0.13642409443855286\n",
      "epoch: 20 step: 324, loss is 0.2713025212287903\n",
      "epoch: 20 step: 325, loss is 0.2675340175628662\n",
      "epoch: 20 step: 326, loss is 0.35625675320625305\n",
      "epoch: 20 step: 327, loss is 0.1299503594636917\n",
      "epoch: 20 step: 328, loss is 0.2325955629348755\n",
      "epoch: 20 step: 329, loss is 0.2089565098285675\n",
      "epoch: 20 step: 330, loss is 0.11929964274168015\n",
      "epoch: 20 step: 331, loss is 0.21446199715137482\n",
      "epoch: 20 step: 332, loss is 0.12649646401405334\n",
      "epoch: 20 step: 333, loss is 0.13176245987415314\n",
      "epoch: 20 step: 334, loss is 0.18456892669200897\n",
      "epoch: 20 step: 335, loss is 0.09146979451179504\n",
      "epoch: 20 step: 336, loss is 0.12996147572994232\n",
      "epoch: 20 step: 337, loss is 0.12615101039409637\n",
      "epoch: 20 step: 338, loss is 0.20838563144207\n",
      "epoch: 20 step: 339, loss is 0.3447878360748291\n",
      "epoch: 20 step: 340, loss is 0.15309318900108337\n",
      "epoch: 20 step: 341, loss is 0.056121647357940674\n",
      "epoch: 20 step: 342, loss is 0.1706007421016693\n",
      "epoch: 20 step: 343, loss is 0.259536474943161\n",
      "epoch: 20 step: 344, loss is 0.10009510815143585\n",
      "epoch: 20 step: 345, loss is 0.12666434049606323\n",
      "epoch: 20 step: 346, loss is 0.151434987783432\n",
      "epoch: 20 step: 347, loss is 0.1865130066871643\n",
      "epoch: 20 step: 348, loss is 0.09875989705324173\n",
      "epoch: 20 step: 349, loss is 0.21736352145671844\n",
      "epoch: 20 step: 350, loss is 0.21684032678604126\n",
      "epoch: 20 step: 351, loss is 0.19095680117607117\n",
      "epoch: 20 step: 352, loss is 0.17040979862213135\n",
      "epoch: 20 step: 353, loss is 0.11497169733047485\n",
      "epoch: 20 step: 354, loss is 0.06706570833921432\n",
      "epoch: 20 step: 355, loss is 0.18059735000133514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 356, loss is 0.1312430053949356\n",
      "epoch: 20 step: 357, loss is 0.19877731800079346\n",
      "epoch: 20 step: 358, loss is 0.1465897411108017\n",
      "epoch: 20 step: 359, loss is 0.10509432107210159\n",
      "epoch: 20 step: 360, loss is 0.2549262046813965\n",
      "epoch: 20 step: 361, loss is 0.23978960514068604\n",
      "epoch: 20 step: 362, loss is 0.2101033329963684\n",
      "epoch: 20 step: 363, loss is 0.23790231347084045\n",
      "epoch: 20 step: 364, loss is 0.29880648851394653\n",
      "epoch: 20 step: 365, loss is 0.19502952694892883\n",
      "epoch: 20 step: 366, loss is 0.31676051020622253\n",
      "epoch: 20 step: 367, loss is 0.30631959438323975\n",
      "epoch: 20 step: 368, loss is 0.24504008889198303\n",
      "epoch: 20 step: 369, loss is 0.3675306737422943\n",
      "epoch: 20 step: 370, loss is 0.1378633677959442\n",
      "epoch: 20 step: 371, loss is 0.18404270708560944\n",
      "epoch: 20 step: 372, loss is 0.21606044471263885\n",
      "epoch: 20 step: 373, loss is 0.1419268101453781\n",
      "epoch: 20 step: 374, loss is 0.2032841145992279\n",
      "epoch: 20 step: 375, loss is 0.1509770154953003\n",
      "epoch: 20 step: 376, loss is 0.16074417531490326\n",
      "epoch: 20 step: 377, loss is 0.12523731589317322\n",
      "epoch: 20 step: 378, loss is 0.10543128103017807\n",
      "epoch: 20 step: 379, loss is 0.39940401911735535\n",
      "epoch: 20 step: 380, loss is 0.1704934686422348\n",
      "epoch: 20 step: 381, loss is 0.18119803071022034\n",
      "epoch: 20 step: 382, loss is 0.1664225161075592\n",
      "epoch: 20 step: 383, loss is 0.23073430359363556\n",
      "epoch: 20 step: 384, loss is 0.17255574464797974\n",
      "epoch: 20 step: 385, loss is 0.13469859957695007\n",
      "epoch: 20 step: 386, loss is 0.1637752205133438\n",
      "epoch: 20 step: 387, loss is 0.21981148421764374\n",
      "epoch: 20 step: 388, loss is 0.23729728162288666\n",
      "epoch: 20 step: 389, loss is 0.1601632982492447\n",
      "epoch: 20 step: 390, loss is 0.12261627614498138\n",
      "epoch: 20 step: 391, loss is 0.19675271213054657\n",
      "epoch: 20 step: 392, loss is 0.2549249529838562\n",
      "epoch: 20 step: 393, loss is 0.21074065566062927\n",
      "epoch: 20 step: 394, loss is 0.19417625665664673\n",
      "epoch: 20 step: 395, loss is 0.12748970091342926\n",
      "epoch: 20 step: 396, loss is 0.1496109664440155\n",
      "epoch: 20 step: 397, loss is 0.3747723698616028\n",
      "epoch: 20 step: 398, loss is 0.17898713052272797\n",
      "epoch: 20 step: 399, loss is 0.1775272637605667\n",
      "epoch: 20 step: 400, loss is 0.16519948840141296\n",
      "epoch: 20 step: 401, loss is 0.18545983731746674\n",
      "epoch: 20 step: 402, loss is 0.11899785697460175\n",
      "epoch: 20 step: 403, loss is 0.16875138878822327\n",
      "epoch: 20 step: 404, loss is 0.2707555294036865\n",
      "epoch: 20 step: 405, loss is 0.2026047557592392\n",
      "epoch: 20 step: 406, loss is 0.27896931767463684\n",
      "epoch: 20 step: 407, loss is 0.2806231677532196\n",
      "epoch: 20 step: 408, loss is 0.3650881350040436\n",
      "epoch: 20 step: 409, loss is 0.17172542214393616\n",
      "epoch: 20 step: 410, loss is 0.18976233899593353\n",
      "epoch: 20 step: 411, loss is 0.30119460821151733\n",
      "epoch: 20 step: 412, loss is 0.14914032816886902\n",
      "epoch: 20 step: 413, loss is 0.21625424921512604\n",
      "epoch: 20 step: 414, loss is 0.14399810135364532\n",
      "epoch: 20 step: 415, loss is 0.13704726099967957\n",
      "epoch: 20 step: 416, loss is 0.11724025756120682\n",
      "epoch: 20 step: 417, loss is 0.30514129996299744\n",
      "epoch: 20 step: 418, loss is 0.09814192354679108\n",
      "epoch: 20 step: 419, loss is 0.1102452278137207\n",
      "epoch: 20 step: 420, loss is 0.17065174877643585\n",
      "epoch: 20 step: 421, loss is 0.16682825982570648\n",
      "epoch: 20 step: 422, loss is 0.07633237540721893\n",
      "epoch: 20 step: 423, loss is 0.10935445129871368\n",
      "epoch: 20 step: 424, loss is 0.18603000044822693\n",
      "epoch: 20 step: 425, loss is 0.1068844348192215\n",
      "epoch: 20 step: 426, loss is 0.2780957818031311\n",
      "epoch: 20 step: 427, loss is 0.13616785407066345\n",
      "epoch: 20 step: 428, loss is 0.24418741464614868\n",
      "epoch: 20 step: 429, loss is 0.20018383860588074\n",
      "epoch: 20 step: 430, loss is 0.3133603036403656\n",
      "epoch: 20 step: 431, loss is 0.1830912083387375\n",
      "epoch: 20 step: 432, loss is 0.36065438389778137\n",
      "epoch: 20 step: 433, loss is 0.22490884363651276\n",
      "epoch: 20 step: 434, loss is 0.2346843183040619\n",
      "epoch: 20 step: 435, loss is 0.11836858838796616\n",
      "epoch: 20 step: 436, loss is 0.20533689856529236\n",
      "epoch: 20 step: 437, loss is 0.15035611391067505\n",
      "epoch: 20 step: 438, loss is 0.13092467188835144\n",
      "epoch: 20 step: 439, loss is 0.2065763622522354\n",
      "epoch: 20 step: 440, loss is 0.3035517632961273\n",
      "epoch: 20 step: 441, loss is 0.13805830478668213\n",
      "epoch: 20 step: 442, loss is 0.15768250823020935\n",
      "epoch: 20 step: 443, loss is 0.1510918289422989\n",
      "epoch: 20 step: 444, loss is 0.35294637084007263\n",
      "epoch: 20 step: 445, loss is 0.1311536580324173\n",
      "epoch: 20 step: 446, loss is 0.12673179805278778\n",
      "epoch: 20 step: 447, loss is 0.09493574500083923\n",
      "epoch: 20 step: 448, loss is 0.1178799718618393\n",
      "epoch: 20 step: 449, loss is 0.1436830759048462\n",
      "epoch: 20 step: 450, loss is 0.30261027812957764\n",
      "epoch: 20 step: 451, loss is 0.2430390566587448\n",
      "epoch: 20 step: 452, loss is 0.11126120388507843\n",
      "epoch: 20 step: 453, loss is 0.21775460243225098\n",
      "epoch: 20 step: 454, loss is 0.3182167708873749\n",
      "epoch: 20 step: 455, loss is 0.23721589148044586\n",
      "epoch: 20 step: 456, loss is 0.15780450403690338\n",
      "epoch: 20 step: 457, loss is 0.2491399496793747\n",
      "epoch: 20 step: 458, loss is 0.22659233212471008\n",
      "epoch: 20 step: 459, loss is 0.20266151428222656\n",
      "epoch: 20 step: 460, loss is 0.08523336797952652\n",
      "epoch: 20 step: 461, loss is 0.2627429664134979\n",
      "epoch: 20 step: 462, loss is 0.3037175238132477\n",
      "epoch: 20 step: 463, loss is 0.11031998693943024\n",
      "epoch: 20 step: 464, loss is 0.19499468803405762\n",
      "epoch: 20 step: 465, loss is 0.1509351283311844\n",
      "epoch: 20 step: 466, loss is 0.08070901781320572\n",
      "epoch: 20 step: 467, loss is 0.1255597472190857\n",
      "epoch: 20 step: 468, loss is 0.36700737476348877\n",
      "epoch: 20 step: 469, loss is 0.182816743850708\n",
      "epoch: 20 step: 470, loss is 0.19576197862625122\n",
      "epoch: 20 step: 471, loss is 0.1784161478281021\n",
      "epoch: 20 step: 472, loss is 0.2519964277744293\n",
      "epoch: 20 step: 473, loss is 0.3098779618740082\n",
      "epoch: 20 step: 474, loss is 0.17143523693084717\n",
      "epoch: 20 step: 475, loss is 0.08904236555099487\n",
      "epoch: 20 step: 476, loss is 0.18392989039421082\n",
      "epoch: 20 step: 477, loss is 0.22649025917053223\n",
      "epoch: 20 step: 478, loss is 0.14827439188957214\n",
      "epoch: 20 step: 479, loss is 0.19000160694122314\n",
      "epoch: 20 step: 480, loss is 0.08082827925682068\n",
      "epoch: 20 step: 481, loss is 0.11661893129348755\n",
      "epoch: 20 step: 482, loss is 0.25940266251564026\n",
      "epoch: 20 step: 483, loss is 0.17329438030719757\n",
      "epoch: 20 step: 484, loss is 0.1892932504415512\n",
      "epoch: 20 step: 485, loss is 0.16263821721076965\n",
      "epoch: 20 step: 486, loss is 0.1048453226685524\n",
      "epoch: 20 step: 487, loss is 0.26083654165267944\n",
      "epoch: 20 step: 488, loss is 0.31458336114883423\n",
      "epoch: 20 step: 489, loss is 0.2388889193534851\n",
      "epoch: 20 step: 490, loss is 0.13563784956932068\n",
      "epoch: 20 step: 491, loss is 0.10163743793964386\n",
      "epoch: 20 step: 492, loss is 0.15049856901168823\n",
      "epoch: 20 step: 493, loss is 0.14981068670749664\n",
      "epoch: 20 step: 494, loss is 0.28442540764808655\n",
      "epoch: 20 step: 495, loss is 0.22077688574790955\n",
      "epoch: 20 step: 496, loss is 0.20606978237628937\n",
      "epoch: 20 step: 497, loss is 0.20720377564430237\n",
      "epoch: 20 step: 498, loss is 0.359652042388916\n",
      "epoch: 20 step: 499, loss is 0.2144300639629364\n",
      "epoch: 20 step: 500, loss is 0.35408830642700195\n",
      "epoch: 20 step: 501, loss is 0.09649110585451126\n",
      "epoch: 20 step: 502, loss is 0.2100481390953064\n",
      "epoch: 20 step: 503, loss is 0.3100442588329315\n",
      "epoch: 20 step: 504, loss is 0.22999443113803864\n",
      "epoch: 20 step: 505, loss is 0.10123041272163391\n",
      "epoch: 20 step: 506, loss is 0.11251965165138245\n",
      "epoch: 20 step: 507, loss is 0.2537948191165924\n",
      "epoch: 20 step: 508, loss is 0.10823667794466019\n",
      "epoch: 20 step: 509, loss is 0.07343734800815582\n",
      "epoch: 20 step: 510, loss is 0.190609872341156\n",
      "epoch: 20 step: 511, loss is 0.2332107573747635\n",
      "epoch: 20 step: 512, loss is 0.35682785511016846\n",
      "epoch: 20 step: 513, loss is 0.14686770737171173\n",
      "epoch: 20 step: 514, loss is 0.1313004493713379\n",
      "epoch: 20 step: 515, loss is 0.2746073305606842\n",
      "epoch: 20 step: 516, loss is 0.2416924238204956\n",
      "epoch: 20 step: 517, loss is 0.20440179109573364\n",
      "epoch: 20 step: 518, loss is 0.21372966468334198\n",
      "epoch: 20 step: 519, loss is 0.10256817936897278\n",
      "epoch: 20 step: 520, loss is 0.1752961128950119\n",
      "epoch: 20 step: 521, loss is 0.17023508250713348\n",
      "epoch: 20 step: 522, loss is 0.2797757387161255\n",
      "epoch: 20 step: 523, loss is 0.14074507355690002\n",
      "epoch: 20 step: 524, loss is 0.23978924751281738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 525, loss is 0.27305930852890015\n",
      "epoch: 20 step: 526, loss is 0.25348034501075745\n",
      "epoch: 20 step: 527, loss is 0.25934624671936035\n",
      "epoch: 20 step: 528, loss is 0.19867688417434692\n",
      "epoch: 20 step: 529, loss is 0.3199078440666199\n",
      "epoch: 20 step: 530, loss is 0.3198760747909546\n",
      "epoch: 20 step: 531, loss is 0.28331008553504944\n",
      "epoch: 20 step: 532, loss is 0.1741936206817627\n",
      "epoch: 20 step: 533, loss is 0.24306713044643402\n",
      "epoch: 20 step: 534, loss is 0.11610626429319382\n",
      "epoch: 20 step: 535, loss is 0.18775801360607147\n",
      "epoch: 20 step: 536, loss is 0.2520580291748047\n",
      "epoch: 20 step: 537, loss is 0.24466446042060852\n",
      "epoch: 20 step: 538, loss is 0.2946176528930664\n",
      "epoch: 20 step: 539, loss is 0.1530374437570572\n",
      "epoch: 20 step: 540, loss is 0.2879236042499542\n",
      "epoch: 20 step: 541, loss is 0.2480158805847168\n",
      "epoch: 20 step: 542, loss is 0.39970293641090393\n",
      "epoch: 20 step: 543, loss is 0.28775280714035034\n",
      "epoch: 20 step: 544, loss is 0.3590456545352936\n",
      "epoch: 20 step: 545, loss is 0.30161142349243164\n",
      "epoch: 20 step: 546, loss is 0.10591846704483032\n",
      "epoch: 20 step: 547, loss is 0.277835488319397\n",
      "epoch: 20 step: 548, loss is 0.11535923928022385\n",
      "epoch: 20 step: 549, loss is 0.16694845259189606\n",
      "epoch: 20 step: 550, loss is 0.24362778663635254\n",
      "epoch: 20 step: 551, loss is 0.1901027262210846\n",
      "epoch: 20 step: 552, loss is 0.1554243117570877\n",
      "epoch: 20 step: 553, loss is 0.20797199010849\n",
      "epoch: 20 step: 554, loss is 0.18908990919589996\n",
      "epoch: 20 step: 555, loss is 0.19631364941596985\n",
      "epoch: 20 step: 556, loss is 0.3085421323776245\n",
      "epoch: 20 step: 557, loss is 0.25209927558898926\n",
      "epoch: 20 step: 558, loss is 0.09661301970481873\n",
      "epoch: 20 step: 559, loss is 0.32563626766204834\n",
      "epoch: 20 step: 560, loss is 0.2145586758852005\n",
      "epoch: 20 step: 561, loss is 0.167985737323761\n",
      "epoch: 20 step: 562, loss is 0.41601118445396423\n",
      "epoch: 20 step: 563, loss is 0.2236141711473465\n",
      "epoch: 20 step: 564, loss is 0.18362444639205933\n",
      "epoch: 20 step: 565, loss is 0.19758357107639313\n",
      "epoch: 20 step: 566, loss is 0.2555639445781708\n",
      "epoch: 20 step: 567, loss is 0.17224672436714172\n",
      "epoch: 20 step: 568, loss is 0.22073182463645935\n",
      "epoch: 20 step: 569, loss is 0.24659962952136993\n",
      "epoch: 20 step: 570, loss is 0.14509356021881104\n",
      "epoch: 20 step: 571, loss is 0.1711074858903885\n",
      "epoch: 20 step: 572, loss is 0.1918099969625473\n",
      "epoch: 20 step: 573, loss is 0.13281874358654022\n",
      "epoch: 20 step: 574, loss is 0.12765228748321533\n",
      "epoch: 20 step: 575, loss is 0.1322484016418457\n",
      "epoch: 20 step: 576, loss is 0.1119663268327713\n",
      "epoch: 20 step: 577, loss is 0.3315353989601135\n",
      "epoch: 20 step: 578, loss is 0.19830480217933655\n",
      "epoch: 20 step: 579, loss is 0.1467728614807129\n",
      "epoch: 20 step: 580, loss is 0.1188601553440094\n",
      "epoch: 20 step: 581, loss is 0.22587890923023224\n",
      "epoch: 20 step: 582, loss is 0.17646746337413788\n",
      "epoch: 20 step: 583, loss is 0.24768832325935364\n",
      "epoch: 20 step: 584, loss is 0.1963348090648651\n",
      "epoch: 20 step: 585, loss is 0.22938799858093262\n",
      "epoch: 20 step: 586, loss is 0.2176823765039444\n",
      "epoch: 20 step: 587, loss is 0.34401509165763855\n",
      "epoch: 20 step: 588, loss is 0.41424939036369324\n",
      "epoch: 20 step: 589, loss is 0.20436985790729523\n",
      "epoch: 20 step: 590, loss is 0.17572139203548431\n",
      "epoch: 20 step: 591, loss is 0.2408594787120819\n",
      "epoch: 20 step: 592, loss is 0.17574085295200348\n",
      "epoch: 20 step: 593, loss is 0.16875265538692474\n",
      "epoch: 20 step: 594, loss is 0.18651315569877625\n",
      "epoch: 20 step: 595, loss is 0.25790685415267944\n",
      "epoch: 20 step: 596, loss is 0.1921127736568451\n",
      "epoch: 20 step: 597, loss is 0.2128264158964157\n",
      "epoch: 20 step: 598, loss is 0.1936865597963333\n",
      "epoch: 20 step: 599, loss is 0.3355633616447449\n",
      "epoch: 20 step: 600, loss is 0.22984960675239563\n",
      "epoch: 20 step: 601, loss is 0.2807283103466034\n",
      "epoch: 20 step: 602, loss is 0.23727497458457947\n",
      "epoch: 20 step: 603, loss is 0.2436758428812027\n",
      "epoch: 20 step: 604, loss is 0.1254129260778427\n",
      "epoch: 20 step: 605, loss is 0.22742074728012085\n",
      "epoch: 20 step: 606, loss is 0.23275354504585266\n",
      "epoch: 20 step: 607, loss is 0.16810747981071472\n",
      "epoch: 20 step: 608, loss is 0.15438589453697205\n",
      "epoch: 20 step: 609, loss is 0.09141059219837189\n",
      "epoch: 20 step: 610, loss is 0.09504612535238266\n",
      "epoch: 20 step: 611, loss is 0.10443191230297089\n",
      "epoch: 20 step: 612, loss is 0.16699396073818207\n",
      "epoch: 20 step: 613, loss is 0.15847764909267426\n",
      "epoch: 20 step: 614, loss is 0.31329450011253357\n",
      "epoch: 20 step: 615, loss is 0.2908819615840912\n",
      "epoch: 20 step: 616, loss is 0.1213010922074318\n",
      "epoch: 20 step: 617, loss is 0.3272968530654907\n",
      "epoch: 20 step: 618, loss is 0.1909528523683548\n",
      "epoch: 20 step: 619, loss is 0.4024107754230499\n",
      "epoch: 20 step: 620, loss is 0.12775997817516327\n",
      "epoch: 20 step: 621, loss is 0.14963412284851074\n",
      "epoch: 20 step: 622, loss is 0.07794076204299927\n",
      "epoch: 20 step: 623, loss is 0.13582289218902588\n",
      "epoch: 20 step: 624, loss is 0.2509436011314392\n",
      "epoch: 20 step: 625, loss is 0.21446557343006134\n",
      "epoch: 20 step: 626, loss is 0.2503044009208679\n",
      "epoch: 20 step: 627, loss is 0.2601722478866577\n",
      "epoch: 20 step: 628, loss is 0.4269717037677765\n",
      "epoch: 20 step: 629, loss is 0.29821324348449707\n",
      "epoch: 20 step: 630, loss is 0.1370791792869568\n",
      "epoch: 20 step: 631, loss is 0.1631781905889511\n",
      "epoch: 20 step: 632, loss is 0.07927795499563217\n",
      "epoch: 20 step: 633, loss is 0.32874467968940735\n",
      "epoch: 20 step: 634, loss is 0.15373347699642181\n",
      "epoch: 20 step: 635, loss is 0.31639763712882996\n",
      "epoch: 20 step: 636, loss is 0.2398485243320465\n",
      "epoch: 20 step: 637, loss is 0.17391455173492432\n",
      "epoch: 20 step: 638, loss is 0.1230267807841301\n",
      "epoch: 20 step: 639, loss is 0.23474383354187012\n",
      "epoch: 20 step: 640, loss is 0.1821409910917282\n",
      "epoch: 20 step: 641, loss is 0.20562048256397247\n",
      "epoch: 20 step: 642, loss is 0.2608107626438141\n",
      "epoch: 20 step: 643, loss is 0.10194885730743408\n",
      "epoch: 20 step: 644, loss is 0.1929098516702652\n",
      "epoch: 20 step: 645, loss is 0.16525870561599731\n",
      "epoch: 20 step: 646, loss is 0.24541214108467102\n",
      "epoch: 20 step: 647, loss is 0.15052403509616852\n",
      "epoch: 20 step: 648, loss is 0.2488231658935547\n",
      "epoch: 20 step: 649, loss is 0.19385594129562378\n",
      "epoch: 20 step: 650, loss is 0.12157164514064789\n",
      "epoch: 20 step: 651, loss is 0.18131023645401\n",
      "epoch: 20 step: 652, loss is 0.08245988935232162\n",
      "epoch: 20 step: 653, loss is 0.2347189486026764\n",
      "epoch: 20 step: 654, loss is 0.22108961641788483\n",
      "epoch: 20 step: 655, loss is 0.23539821803569794\n",
      "epoch: 20 step: 656, loss is 0.22328339517116547\n",
      "epoch: 20 step: 657, loss is 0.20749212801456451\n",
      "epoch: 20 step: 658, loss is 0.1657952219247818\n",
      "epoch: 20 step: 659, loss is 0.19470582902431488\n",
      "epoch: 20 step: 660, loss is 0.25786924362182617\n",
      "epoch: 20 step: 661, loss is 0.15802446007728577\n",
      "epoch: 20 step: 662, loss is 0.15453653037548065\n",
      "epoch: 20 step: 663, loss is 0.15413643419742584\n",
      "epoch: 20 step: 664, loss is 0.1458922177553177\n",
      "epoch: 20 step: 665, loss is 0.18760383129119873\n",
      "epoch: 20 step: 666, loss is 0.15220479667186737\n",
      "epoch: 20 step: 667, loss is 0.289793998003006\n",
      "epoch: 20 step: 668, loss is 0.21065448224544525\n",
      "epoch: 20 step: 669, loss is 0.18163037300109863\n",
      "epoch: 20 step: 670, loss is 0.19851402938365936\n",
      "epoch: 20 step: 671, loss is 0.20369692146778107\n",
      "epoch: 20 step: 672, loss is 0.31708472967147827\n",
      "epoch: 20 step: 673, loss is 0.38086366653442383\n",
      "epoch: 20 step: 674, loss is 0.14330551028251648\n",
      "epoch: 20 step: 675, loss is 0.16061186790466309\n",
      "epoch: 20 step: 676, loss is 0.09829945117235184\n",
      "epoch: 20 step: 677, loss is 0.22234748303890228\n",
      "epoch: 20 step: 678, loss is 0.19468210637569427\n",
      "epoch: 20 step: 679, loss is 0.19011461734771729\n",
      "epoch: 20 step: 680, loss is 0.1270250380039215\n",
      "epoch: 20 step: 681, loss is 0.2740587592124939\n",
      "epoch: 20 step: 682, loss is 0.20219877362251282\n",
      "epoch: 20 step: 683, loss is 0.19203439354896545\n",
      "epoch: 20 step: 684, loss is 0.2636623978614807\n",
      "epoch: 20 step: 685, loss is 0.3690337836742401\n",
      "epoch: 20 step: 686, loss is 0.318839967250824\n",
      "epoch: 20 step: 687, loss is 0.14369165897369385\n",
      "epoch: 20 step: 688, loss is 0.258354514837265\n",
      "epoch: 20 step: 689, loss is 0.3121047914028168\n",
      "epoch: 20 step: 690, loss is 0.3689268231391907\n",
      "epoch: 20 step: 691, loss is 0.11789500713348389\n",
      "epoch: 20 step: 692, loss is 0.19552497565746307\n",
      "epoch: 20 step: 693, loss is 0.21149255335330963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 694, loss is 0.2882804274559021\n",
      "epoch: 20 step: 695, loss is 0.2531603276729584\n",
      "epoch: 20 step: 696, loss is 0.1020985096693039\n",
      "epoch: 20 step: 697, loss is 0.2148064523935318\n",
      "epoch: 20 step: 698, loss is 0.20092128217220306\n",
      "epoch: 20 step: 699, loss is 0.22735323011875153\n",
      "epoch: 20 step: 700, loss is 0.21169088780879974\n",
      "epoch: 20 step: 701, loss is 0.3699912130832672\n",
      "epoch: 20 step: 702, loss is 0.4163155257701874\n",
      "epoch: 20 step: 703, loss is 0.24771833419799805\n",
      "epoch: 20 step: 704, loss is 0.22581632435321808\n",
      "epoch: 20 step: 705, loss is 0.20595687627792358\n",
      "epoch: 20 step: 706, loss is 0.18170961737632751\n",
      "epoch: 20 step: 707, loss is 0.0764392539858818\n",
      "epoch: 20 step: 708, loss is 0.12022455781698227\n",
      "epoch: 20 step: 709, loss is 0.30532124638557434\n",
      "epoch: 20 step: 710, loss is 0.09168373793363571\n",
      "epoch: 20 step: 711, loss is 0.1575678288936615\n",
      "epoch: 20 step: 712, loss is 0.2916749119758606\n",
      "epoch: 20 step: 713, loss is 0.1120418980717659\n",
      "epoch: 20 step: 714, loss is 0.14371278882026672\n",
      "epoch: 20 step: 715, loss is 0.19248317182064056\n",
      "epoch: 20 step: 716, loss is 0.10386408865451813\n",
      "epoch: 20 step: 717, loss is 0.07328050583600998\n",
      "epoch: 20 step: 718, loss is 0.3215588331222534\n",
      "epoch: 20 step: 719, loss is 0.0852942168712616\n",
      "epoch: 20 step: 720, loss is 0.2078292816877365\n",
      "epoch: 20 step: 721, loss is 0.07009490579366684\n",
      "epoch: 20 step: 722, loss is 0.14378927648067474\n",
      "epoch: 20 step: 723, loss is 0.3821941912174225\n",
      "epoch: 20 step: 724, loss is 0.22377856075763702\n",
      "epoch: 20 step: 725, loss is 0.26834163069725037\n",
      "epoch: 20 step: 726, loss is 0.35726043581962585\n",
      "epoch: 20 step: 727, loss is 0.2710033357143402\n",
      "epoch: 20 step: 728, loss is 0.16479244828224182\n",
      "epoch: 20 step: 729, loss is 0.3053898811340332\n",
      "epoch: 20 step: 730, loss is 0.1659465879201889\n",
      "epoch: 20 step: 731, loss is 0.16124072670936584\n",
      "epoch: 20 step: 732, loss is 0.2565825283527374\n",
      "epoch: 20 step: 733, loss is 0.18814586102962494\n",
      "epoch: 20 step: 734, loss is 0.28186580538749695\n",
      "epoch: 20 step: 735, loss is 0.1271284967660904\n",
      "epoch: 20 step: 736, loss is 0.170850470662117\n",
      "epoch: 20 step: 737, loss is 0.17928937077522278\n",
      "epoch: 20 step: 738, loss is 0.29525282979011536\n",
      "epoch: 20 step: 739, loss is 0.23276269435882568\n",
      "epoch: 20 step: 740, loss is 0.22166000306606293\n",
      "epoch: 20 step: 741, loss is 0.2330678552389145\n",
      "epoch: 20 step: 742, loss is 0.1533108800649643\n",
      "epoch: 20 step: 743, loss is 0.2597593069076538\n",
      "epoch: 20 step: 744, loss is 0.16088607907295227\n",
      "epoch: 20 step: 745, loss is 0.3702106475830078\n",
      "epoch: 20 step: 746, loss is 0.16876181960105896\n",
      "epoch: 20 step: 747, loss is 0.1888074278831482\n",
      "epoch: 20 step: 748, loss is 0.09484915435314178\n",
      "epoch: 20 step: 749, loss is 0.18498435616493225\n",
      "epoch: 20 step: 750, loss is 0.1138196587562561\n",
      "epoch: 20 step: 751, loss is 0.200084388256073\n",
      "epoch: 20 step: 752, loss is 0.17052976787090302\n",
      "epoch: 20 step: 753, loss is 0.2650752365589142\n",
      "epoch: 20 step: 754, loss is 0.27819687128067017\n",
      "epoch: 20 step: 755, loss is 0.1809196174144745\n",
      "epoch: 20 step: 756, loss is 0.4363071322441101\n",
      "epoch: 20 step: 757, loss is 0.13347221910953522\n",
      "epoch: 20 step: 758, loss is 0.11811365187168121\n",
      "epoch: 20 step: 759, loss is 0.1755150407552719\n",
      "epoch: 20 step: 760, loss is 0.1628536432981491\n",
      "epoch: 20 step: 761, loss is 0.33689501881599426\n",
      "epoch: 20 step: 762, loss is 0.11515578627586365\n",
      "epoch: 20 step: 763, loss is 0.2105633020401001\n",
      "epoch: 20 step: 764, loss is 0.17620806396007538\n",
      "epoch: 20 step: 765, loss is 0.4030698239803314\n",
      "epoch: 20 step: 766, loss is 0.0744141936302185\n",
      "epoch: 20 step: 767, loss is 0.15728631615638733\n",
      "epoch: 20 step: 768, loss is 0.26569631695747375\n",
      "epoch: 20 step: 769, loss is 0.23007814586162567\n",
      "epoch: 20 step: 770, loss is 0.23702530562877655\n",
      "epoch: 20 step: 771, loss is 0.16686636209487915\n",
      "epoch: 20 step: 772, loss is 0.12059537321329117\n",
      "epoch: 20 step: 773, loss is 0.2539292871952057\n",
      "epoch: 20 step: 774, loss is 0.15590448677539825\n",
      "epoch: 20 step: 775, loss is 0.13704389333724976\n",
      "epoch: 20 step: 776, loss is 0.28995999693870544\n",
      "epoch: 20 step: 777, loss is 0.26686036586761475\n",
      "epoch: 20 step: 778, loss is 0.4071616232395172\n",
      "epoch: 20 step: 779, loss is 0.20806647837162018\n",
      "epoch: 20 step: 780, loss is 0.24712017178535461\n",
      "epoch: 20 step: 781, loss is 0.25259968638420105\n",
      "epoch: 20 step: 782, loss is 0.25216394662857056\n",
      "epoch: 20 step: 783, loss is 0.2568049132823944\n",
      "epoch: 20 step: 784, loss is 0.06020530313253403\n",
      "epoch: 20 step: 785, loss is 0.22302351891994476\n",
      "epoch: 20 step: 786, loss is 0.24186904728412628\n",
      "epoch: 20 step: 787, loss is 0.09560753405094147\n",
      "epoch: 20 step: 788, loss is 0.2574579417705536\n",
      "epoch: 20 step: 789, loss is 0.09411567449569702\n",
      "epoch: 20 step: 790, loss is 0.16932861506938934\n",
      "epoch: 20 step: 791, loss is 0.2072010487318039\n",
      "epoch: 20 step: 792, loss is 0.09841190278530121\n",
      "epoch: 20 step: 793, loss is 0.1331314891576767\n",
      "epoch: 20 step: 794, loss is 0.26766887307167053\n",
      "epoch: 20 step: 795, loss is 0.16973130404949188\n",
      "epoch: 20 step: 796, loss is 0.2940433919429779\n",
      "epoch: 20 step: 797, loss is 0.1394527554512024\n",
      "epoch: 20 step: 798, loss is 0.2831975817680359\n",
      "epoch: 20 step: 799, loss is 0.28708481788635254\n",
      "epoch: 20 step: 800, loss is 0.14963041245937347\n",
      "epoch: 20 step: 801, loss is 0.2514398992061615\n",
      "epoch: 20 step: 802, loss is 0.1906694620847702\n",
      "epoch: 20 step: 803, loss is 0.17154461145401\n",
      "epoch: 20 step: 804, loss is 0.15708805620670319\n",
      "epoch: 20 step: 805, loss is 0.27881157398223877\n",
      "epoch: 20 step: 806, loss is 0.08594098687171936\n",
      "epoch: 20 step: 807, loss is 0.10584312677383423\n",
      "epoch: 20 step: 808, loss is 0.2485237717628479\n",
      "epoch: 20 step: 809, loss is 0.22052700817584991\n",
      "epoch: 20 step: 810, loss is 0.1957775503396988\n",
      "epoch: 20 step: 811, loss is 0.17199024558067322\n",
      "epoch: 20 step: 812, loss is 0.10781580954790115\n",
      "epoch: 20 step: 813, loss is 0.07156320661306381\n",
      "epoch: 20 step: 814, loss is 0.39681756496429443\n",
      "epoch: 20 step: 815, loss is 0.12676072120666504\n",
      "epoch: 20 step: 816, loss is 0.16552458703517914\n",
      "epoch: 20 step: 817, loss is 0.17190812528133392\n",
      "epoch: 20 step: 818, loss is 0.2003972828388214\n",
      "epoch: 20 step: 819, loss is 0.06056810915470123\n",
      "epoch: 20 step: 820, loss is 0.21242770552635193\n",
      "epoch: 20 step: 821, loss is 0.2103421986103058\n",
      "epoch: 20 step: 822, loss is 0.08397675305604935\n",
      "epoch: 20 step: 823, loss is 0.19812534749507904\n",
      "epoch: 20 step: 824, loss is 0.12196468561887741\n",
      "epoch: 20 step: 825, loss is 0.21554327011108398\n",
      "epoch: 20 step: 826, loss is 0.21395544707775116\n",
      "epoch: 20 step: 827, loss is 0.24615569412708282\n",
      "epoch: 20 step: 828, loss is 0.2653122842311859\n",
      "epoch: 20 step: 829, loss is 0.12312818318605423\n",
      "epoch: 20 step: 830, loss is 0.2640119791030884\n",
      "epoch: 20 step: 831, loss is 0.30507344007492065\n",
      "epoch: 20 step: 832, loss is 0.1581893414258957\n",
      "epoch: 20 step: 833, loss is 0.2351027876138687\n",
      "epoch: 20 step: 834, loss is 0.2001846432685852\n",
      "epoch: 20 step: 835, loss is 0.24955014884471893\n",
      "epoch: 20 step: 836, loss is 0.09656143933534622\n",
      "epoch: 20 step: 837, loss is 0.2703712284564972\n",
      "epoch: 20 step: 838, loss is 0.23275476694107056\n",
      "epoch: 20 step: 839, loss is 0.09256762266159058\n",
      "epoch: 20 step: 840, loss is 0.27315157651901245\n",
      "epoch: 20 step: 841, loss is 0.20798653364181519\n",
      "epoch: 20 step: 842, loss is 0.16594335436820984\n",
      "epoch: 20 step: 843, loss is 0.2735549211502075\n",
      "epoch: 20 step: 844, loss is 0.2800603210926056\n",
      "epoch: 20 step: 845, loss is 0.17455455660820007\n",
      "epoch: 20 step: 846, loss is 0.20304706692695618\n",
      "epoch: 20 step: 847, loss is 0.13520807027816772\n",
      "epoch: 20 step: 848, loss is 0.22479170560836792\n",
      "epoch: 20 step: 849, loss is 0.16619448363780975\n",
      "epoch: 20 step: 850, loss is 0.18398484587669373\n",
      "epoch: 20 step: 851, loss is 0.20449787378311157\n",
      "epoch: 20 step: 852, loss is 0.23540464043617249\n",
      "epoch: 20 step: 853, loss is 0.18943119049072266\n",
      "epoch: 20 step: 854, loss is 0.3541141748428345\n",
      "epoch: 20 step: 855, loss is 0.20840588212013245\n",
      "epoch: 20 step: 856, loss is 0.23219792544841766\n",
      "epoch: 20 step: 857, loss is 0.08774678409099579\n",
      "epoch: 20 step: 858, loss is 0.3186047077178955\n",
      "epoch: 20 step: 859, loss is 0.21294155716896057\n",
      "epoch: 20 step: 860, loss is 0.1069202572107315\n",
      "epoch: 20 step: 861, loss is 0.2593524158000946\n",
      "epoch: 20 step: 862, loss is 0.18022578954696655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 863, loss is 0.13380558788776398\n",
      "epoch: 20 step: 864, loss is 0.25417450070381165\n",
      "epoch: 20 step: 865, loss is 0.38421082496643066\n",
      "epoch: 20 step: 866, loss is 0.26057636737823486\n",
      "epoch: 20 step: 867, loss is 0.0717705562710762\n",
      "epoch: 20 step: 868, loss is 0.1357269436120987\n",
      "epoch: 20 step: 869, loss is 0.21545615792274475\n",
      "epoch: 20 step: 870, loss is 0.17535558342933655\n",
      "epoch: 20 step: 871, loss is 0.22533318400382996\n",
      "epoch: 20 step: 872, loss is 0.14385370910167694\n",
      "epoch: 20 step: 873, loss is 0.153976172208786\n",
      "epoch: 20 step: 874, loss is 0.10033123195171356\n",
      "epoch: 20 step: 875, loss is 0.20957453548908234\n",
      "epoch: 20 step: 876, loss is 0.35087525844573975\n",
      "epoch: 20 step: 877, loss is 0.2926742434501648\n",
      "epoch: 20 step: 878, loss is 0.36059460043907166\n",
      "epoch: 20 step: 879, loss is 0.22376948595046997\n",
      "epoch: 20 step: 880, loss is 0.14189745485782623\n",
      "epoch: 20 step: 881, loss is 0.20516371726989746\n",
      "epoch: 20 step: 882, loss is 0.3206746280193329\n",
      "epoch: 20 step: 883, loss is 0.17450003325939178\n",
      "epoch: 20 step: 884, loss is 0.12457907199859619\n",
      "epoch: 20 step: 885, loss is 0.2862953543663025\n",
      "epoch: 20 step: 886, loss is 0.1875474452972412\n",
      "epoch: 20 step: 887, loss is 0.2486318051815033\n",
      "epoch: 20 step: 888, loss is 0.45847344398498535\n",
      "epoch: 20 step: 889, loss is 0.21334080398082733\n",
      "epoch: 20 step: 890, loss is 0.11934040486812592\n",
      "epoch: 20 step: 891, loss is 0.20325303077697754\n",
      "epoch: 20 step: 892, loss is 0.21188458800315857\n",
      "epoch: 20 step: 893, loss is 0.23384228348731995\n",
      "epoch: 20 step: 894, loss is 0.3279587924480438\n",
      "epoch: 20 step: 895, loss is 0.20248553156852722\n",
      "epoch: 20 step: 896, loss is 0.19832095503807068\n",
      "epoch: 20 step: 897, loss is 0.1567980945110321\n",
      "epoch: 20 step: 898, loss is 0.19014036655426025\n",
      "epoch: 20 step: 899, loss is 0.14608852565288544\n",
      "epoch: 20 step: 900, loss is 0.14915381371974945\n",
      "epoch: 20 step: 901, loss is 0.26810675859451294\n",
      "epoch: 20 step: 902, loss is 0.1243889331817627\n",
      "epoch: 20 step: 903, loss is 0.1804039478302002\n",
      "epoch: 20 step: 904, loss is 0.24408261477947235\n",
      "epoch: 20 step: 905, loss is 0.12164754420518875\n",
      "epoch: 20 step: 906, loss is 0.14793457090854645\n",
      "epoch: 20 step: 907, loss is 0.18269948661327362\n",
      "epoch: 20 step: 908, loss is 0.18377144634723663\n",
      "epoch: 20 step: 909, loss is 0.12160829454660416\n",
      "epoch: 20 step: 910, loss is 0.08457742631435394\n",
      "epoch: 20 step: 911, loss is 0.2967384159564972\n",
      "epoch: 20 step: 912, loss is 0.0565158948302269\n",
      "epoch: 20 step: 913, loss is 0.2667787969112396\n",
      "epoch: 20 step: 914, loss is 0.05907006189227104\n",
      "epoch: 20 step: 915, loss is 0.2529911994934082\n",
      "epoch: 20 step: 916, loss is 0.2544434666633606\n",
      "epoch: 20 step: 917, loss is 0.0781150683760643\n",
      "epoch: 20 step: 918, loss is 0.13999757170677185\n",
      "epoch: 20 step: 919, loss is 0.23311366140842438\n",
      "epoch: 20 step: 920, loss is 0.32883745431900024\n",
      "epoch: 20 step: 921, loss is 0.28568077087402344\n",
      "epoch: 20 step: 922, loss is 0.1935354322195053\n",
      "epoch: 20 step: 923, loss is 0.25796380639076233\n",
      "epoch: 20 step: 924, loss is 0.22210288047790527\n",
      "epoch: 20 step: 925, loss is 0.18914465606212616\n",
      "epoch: 20 step: 926, loss is 0.17424438893795013\n",
      "epoch: 20 step: 927, loss is 0.12860526144504547\n",
      "epoch: 20 step: 928, loss is 0.26625287532806396\n",
      "epoch: 20 step: 929, loss is 0.24304921925067902\n",
      "epoch: 20 step: 930, loss is 0.19174028933048248\n",
      "epoch: 20 step: 931, loss is 0.1293080896139145\n",
      "epoch: 20 step: 932, loss is 0.3136025369167328\n",
      "epoch: 20 step: 933, loss is 0.1688670516014099\n",
      "epoch: 20 step: 934, loss is 0.09309494495391846\n",
      "epoch: 20 step: 935, loss is 0.091140516102314\n",
      "epoch: 20 step: 936, loss is 0.2665250301361084\n",
      "epoch: 20 step: 937, loss is 0.1687374860048294\n",
      "epoch: 21 step: 1, loss is 0.2562035024166107\n",
      "epoch: 21 step: 2, loss is 0.090337835252285\n",
      "epoch: 21 step: 3, loss is 0.2220367044210434\n",
      "epoch: 21 step: 4, loss is 0.2972942888736725\n",
      "epoch: 21 step: 5, loss is 0.2194603532552719\n",
      "epoch: 21 step: 6, loss is 0.11149099469184875\n",
      "epoch: 21 step: 7, loss is 0.130533829331398\n",
      "epoch: 21 step: 8, loss is 0.22725623846054077\n",
      "epoch: 21 step: 9, loss is 0.1310468167066574\n",
      "epoch: 21 step: 10, loss is 0.07417051494121552\n",
      "epoch: 21 step: 11, loss is 0.1302831918001175\n",
      "epoch: 21 step: 12, loss is 0.1108691543340683\n",
      "epoch: 21 step: 13, loss is 0.145822674036026\n",
      "epoch: 21 step: 14, loss is 0.25677627325057983\n",
      "epoch: 21 step: 15, loss is 0.27987608313560486\n",
      "epoch: 21 step: 16, loss is 0.17508670687675476\n",
      "epoch: 21 step: 17, loss is 0.2257428616285324\n",
      "epoch: 21 step: 18, loss is 0.17606526613235474\n",
      "epoch: 21 step: 19, loss is 0.22345446050167084\n",
      "epoch: 21 step: 20, loss is 0.20388098061084747\n",
      "epoch: 21 step: 21, loss is 0.0930858701467514\n",
      "epoch: 21 step: 22, loss is 0.35378578305244446\n",
      "epoch: 21 step: 23, loss is 0.14831392467021942\n",
      "epoch: 21 step: 24, loss is 0.17092230916023254\n",
      "epoch: 21 step: 25, loss is 0.128565713763237\n",
      "epoch: 21 step: 26, loss is 0.1230965256690979\n",
      "epoch: 21 step: 27, loss is 0.24221354722976685\n",
      "epoch: 21 step: 28, loss is 0.4197186231613159\n",
      "epoch: 21 step: 29, loss is 0.3496209383010864\n",
      "epoch: 21 step: 30, loss is 0.20924405753612518\n",
      "epoch: 21 step: 31, loss is 0.05332345888018608\n",
      "epoch: 21 step: 32, loss is 0.09537024796009064\n",
      "epoch: 21 step: 33, loss is 0.31680116057395935\n",
      "epoch: 21 step: 34, loss is 0.2004590630531311\n",
      "epoch: 21 step: 35, loss is 0.2867926359176636\n",
      "epoch: 21 step: 36, loss is 0.3005681335926056\n",
      "epoch: 21 step: 37, loss is 0.2589060664176941\n",
      "epoch: 21 step: 38, loss is 0.08049089461565018\n",
      "epoch: 21 step: 39, loss is 0.2034035623073578\n",
      "epoch: 21 step: 40, loss is 0.31394729018211365\n",
      "epoch: 21 step: 41, loss is 0.2289050966501236\n",
      "epoch: 21 step: 42, loss is 0.1881813257932663\n",
      "epoch: 21 step: 43, loss is 0.23931781947612762\n",
      "epoch: 21 step: 44, loss is 0.25703632831573486\n",
      "epoch: 21 step: 45, loss is 0.24013666808605194\n",
      "epoch: 21 step: 46, loss is 0.25491300225257874\n",
      "epoch: 21 step: 47, loss is 0.13300393521785736\n",
      "epoch: 21 step: 48, loss is 0.3609756529331207\n",
      "epoch: 21 step: 49, loss is 0.28840675950050354\n",
      "epoch: 21 step: 50, loss is 0.2558962404727936\n",
      "epoch: 21 step: 51, loss is 0.1589382290840149\n",
      "epoch: 21 step: 52, loss is 0.11844929307699203\n",
      "epoch: 21 step: 53, loss is 0.17368635535240173\n",
      "epoch: 21 step: 54, loss is 0.1266976296901703\n",
      "epoch: 21 step: 55, loss is 0.2392832636833191\n",
      "epoch: 21 step: 56, loss is 0.22338111698627472\n",
      "epoch: 21 step: 57, loss is 0.294526070356369\n",
      "epoch: 21 step: 58, loss is 0.19982196390628815\n",
      "epoch: 21 step: 59, loss is 0.2118629515171051\n",
      "epoch: 21 step: 60, loss is 0.31100139021873474\n",
      "epoch: 21 step: 61, loss is 0.2468249797821045\n",
      "epoch: 21 step: 62, loss is 0.26065340638160706\n",
      "epoch: 21 step: 63, loss is 0.1462811678647995\n",
      "epoch: 21 step: 64, loss is 0.2447095364332199\n",
      "epoch: 21 step: 65, loss is 0.19158253073692322\n",
      "epoch: 21 step: 66, loss is 0.17829343676567078\n",
      "epoch: 21 step: 67, loss is 0.09135407209396362\n",
      "epoch: 21 step: 68, loss is 0.2993086874485016\n",
      "epoch: 21 step: 69, loss is 0.19587451219558716\n",
      "epoch: 21 step: 70, loss is 0.17256686091423035\n",
      "epoch: 21 step: 71, loss is 0.161114901304245\n",
      "epoch: 21 step: 72, loss is 0.37255117297172546\n",
      "epoch: 21 step: 73, loss is 0.13028927147388458\n",
      "epoch: 21 step: 74, loss is 0.17607469856739044\n",
      "epoch: 21 step: 75, loss is 0.11035982519388199\n",
      "epoch: 21 step: 76, loss is 0.11461973190307617\n",
      "epoch: 21 step: 77, loss is 0.18268044292926788\n",
      "epoch: 21 step: 78, loss is 0.2240680754184723\n",
      "epoch: 21 step: 79, loss is 0.2114941030740738\n",
      "epoch: 21 step: 80, loss is 0.10379191488027573\n",
      "epoch: 21 step: 81, loss is 0.22006988525390625\n",
      "epoch: 21 step: 82, loss is 0.24942240118980408\n",
      "epoch: 21 step: 83, loss is 0.15784817934036255\n",
      "epoch: 21 step: 84, loss is 0.12961754202842712\n",
      "epoch: 21 step: 85, loss is 0.1666482836008072\n",
      "epoch: 21 step: 86, loss is 0.14268513023853302\n",
      "epoch: 21 step: 87, loss is 0.1832975447177887\n",
      "epoch: 21 step: 88, loss is 0.15953780710697174\n",
      "epoch: 21 step: 89, loss is 0.13232670724391937\n",
      "epoch: 21 step: 90, loss is 0.1696675419807434\n",
      "epoch: 21 step: 91, loss is 0.14337490499019623\n",
      "epoch: 21 step: 92, loss is 0.3160046339035034\n",
      "epoch: 21 step: 93, loss is 0.1781562864780426\n",
      "epoch: 21 step: 94, loss is 0.21980643272399902\n",
      "epoch: 21 step: 95, loss is 0.27619802951812744\n",
      "epoch: 21 step: 96, loss is 0.2855865955352783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 97, loss is 0.19255562126636505\n",
      "epoch: 21 step: 98, loss is 0.12925635278224945\n",
      "epoch: 21 step: 99, loss is 0.17297881841659546\n",
      "epoch: 21 step: 100, loss is 0.30484819412231445\n",
      "epoch: 21 step: 101, loss is 0.15232113003730774\n",
      "epoch: 21 step: 102, loss is 0.13471318781375885\n",
      "epoch: 21 step: 103, loss is 0.24499373137950897\n",
      "epoch: 21 step: 104, loss is 0.356336385011673\n",
      "epoch: 21 step: 105, loss is 0.16899622976779938\n",
      "epoch: 21 step: 106, loss is 0.2403637170791626\n",
      "epoch: 21 step: 107, loss is 0.1954294890165329\n",
      "epoch: 21 step: 108, loss is 0.3688697814941406\n",
      "epoch: 21 step: 109, loss is 0.19909483194351196\n",
      "epoch: 21 step: 110, loss is 0.262160986661911\n",
      "epoch: 21 step: 111, loss is 0.17736037075519562\n",
      "epoch: 21 step: 112, loss is 0.15598228573799133\n",
      "epoch: 21 step: 113, loss is 0.14686214923858643\n",
      "epoch: 21 step: 114, loss is 0.23935334384441376\n",
      "epoch: 21 step: 115, loss is 0.1679028570652008\n",
      "epoch: 21 step: 116, loss is 0.14657016098499298\n",
      "epoch: 21 step: 117, loss is 0.25497063994407654\n",
      "epoch: 21 step: 118, loss is 0.21296000480651855\n",
      "epoch: 21 step: 119, loss is 0.14255212247371674\n",
      "epoch: 21 step: 120, loss is 0.32610780000686646\n",
      "epoch: 21 step: 121, loss is 0.08223414421081543\n",
      "epoch: 21 step: 122, loss is 0.22737760841846466\n",
      "epoch: 21 step: 123, loss is 0.17286305129528046\n",
      "epoch: 21 step: 124, loss is 0.2323092222213745\n",
      "epoch: 21 step: 125, loss is 0.24950648844242096\n",
      "epoch: 21 step: 126, loss is 0.1682273894548416\n",
      "epoch: 21 step: 127, loss is 0.14012283086776733\n",
      "epoch: 21 step: 128, loss is 0.2503790259361267\n",
      "epoch: 21 step: 129, loss is 0.14954634010791779\n",
      "epoch: 21 step: 130, loss is 0.20296329259872437\n",
      "epoch: 21 step: 131, loss is 0.27136388421058655\n",
      "epoch: 21 step: 132, loss is 0.14295276999473572\n",
      "epoch: 21 step: 133, loss is 0.15250366926193237\n",
      "epoch: 21 step: 134, loss is 0.15749868750572205\n",
      "epoch: 21 step: 135, loss is 0.23322747647762299\n",
      "epoch: 21 step: 136, loss is 0.23979997634887695\n",
      "epoch: 21 step: 137, loss is 0.20296524465084076\n",
      "epoch: 21 step: 138, loss is 0.21187420189380646\n",
      "epoch: 21 step: 139, loss is 0.20728899538516998\n",
      "epoch: 21 step: 140, loss is 0.24236707389354706\n",
      "epoch: 21 step: 141, loss is 0.21628698706626892\n",
      "epoch: 21 step: 142, loss is 0.22721779346466064\n",
      "epoch: 21 step: 143, loss is 0.1607278436422348\n",
      "epoch: 21 step: 144, loss is 0.24588878452777863\n",
      "epoch: 21 step: 145, loss is 0.21111498773097992\n",
      "epoch: 21 step: 146, loss is 0.2195630520582199\n",
      "epoch: 21 step: 147, loss is 0.08535116910934448\n",
      "epoch: 21 step: 148, loss is 0.13787926733493805\n",
      "epoch: 21 step: 149, loss is 0.16519436240196228\n",
      "epoch: 21 step: 150, loss is 0.11150702089071274\n",
      "epoch: 21 step: 151, loss is 0.4337233304977417\n",
      "epoch: 21 step: 152, loss is 0.211483433842659\n",
      "epoch: 21 step: 153, loss is 0.24658282101154327\n",
      "epoch: 21 step: 154, loss is 0.13382534682750702\n",
      "epoch: 21 step: 155, loss is 0.13278891146183014\n",
      "epoch: 21 step: 156, loss is 0.17039111256599426\n",
      "epoch: 21 step: 157, loss is 0.11582572758197784\n",
      "epoch: 21 step: 158, loss is 0.30126598477363586\n",
      "epoch: 21 step: 159, loss is 0.08958763629198074\n",
      "epoch: 21 step: 160, loss is 0.2058226466178894\n",
      "epoch: 21 step: 161, loss is 0.13772661983966827\n",
      "epoch: 21 step: 162, loss is 0.18783831596374512\n",
      "epoch: 21 step: 163, loss is 0.2870872914791107\n",
      "epoch: 21 step: 164, loss is 0.1980856955051422\n",
      "epoch: 21 step: 165, loss is 0.13974852859973907\n",
      "epoch: 21 step: 166, loss is 0.23651134967803955\n",
      "epoch: 21 step: 167, loss is 0.1542612761259079\n",
      "epoch: 21 step: 168, loss is 0.1902117282152176\n",
      "epoch: 21 step: 169, loss is 0.13469406962394714\n",
      "epoch: 21 step: 170, loss is 0.1810557246208191\n",
      "epoch: 21 step: 171, loss is 0.09824904054403305\n",
      "epoch: 21 step: 172, loss is 0.2670442461967468\n",
      "epoch: 21 step: 173, loss is 0.25772276520729065\n",
      "epoch: 21 step: 174, loss is 0.26306262612342834\n",
      "epoch: 21 step: 175, loss is 0.21933111548423767\n",
      "epoch: 21 step: 176, loss is 0.08739902079105377\n",
      "epoch: 21 step: 177, loss is 0.20439140498638153\n",
      "epoch: 21 step: 178, loss is 0.17639169096946716\n",
      "epoch: 21 step: 179, loss is 0.19906622171401978\n",
      "epoch: 21 step: 180, loss is 0.08810203522443771\n",
      "epoch: 21 step: 181, loss is 0.18328934907913208\n",
      "epoch: 21 step: 182, loss is 0.2647266685962677\n",
      "epoch: 21 step: 183, loss is 0.15196600556373596\n",
      "epoch: 21 step: 184, loss is 0.08478087931871414\n",
      "epoch: 21 step: 185, loss is 0.12261562049388885\n",
      "epoch: 21 step: 186, loss is 0.15657258033752441\n",
      "epoch: 21 step: 187, loss is 0.16617904603481293\n",
      "epoch: 21 step: 188, loss is 0.2828156650066376\n",
      "epoch: 21 step: 189, loss is 0.08943494409322739\n",
      "epoch: 21 step: 190, loss is 0.10865869373083115\n",
      "epoch: 21 step: 191, loss is 0.19824077188968658\n",
      "epoch: 21 step: 192, loss is 0.20988696813583374\n",
      "epoch: 21 step: 193, loss is 0.13855910301208496\n",
      "epoch: 21 step: 194, loss is 0.08218416571617126\n",
      "epoch: 21 step: 195, loss is 0.24069657921791077\n",
      "epoch: 21 step: 196, loss is 0.11485561728477478\n",
      "epoch: 21 step: 197, loss is 0.05638713389635086\n",
      "epoch: 21 step: 198, loss is 0.1119135245680809\n",
      "epoch: 21 step: 199, loss is 0.10557714849710464\n",
      "epoch: 21 step: 200, loss is 0.17705318331718445\n",
      "epoch: 21 step: 201, loss is 0.223453551530838\n",
      "epoch: 21 step: 202, loss is 0.46285802125930786\n",
      "epoch: 21 step: 203, loss is 0.34600910544395447\n",
      "epoch: 21 step: 204, loss is 0.11588321626186371\n",
      "epoch: 21 step: 205, loss is 0.26920947432518005\n",
      "epoch: 21 step: 206, loss is 0.32437771558761597\n",
      "epoch: 21 step: 207, loss is 0.15760689973831177\n",
      "epoch: 21 step: 208, loss is 0.2769133150577545\n",
      "epoch: 21 step: 209, loss is 0.12155218422412872\n",
      "epoch: 21 step: 210, loss is 0.07602638006210327\n",
      "epoch: 21 step: 211, loss is 0.1086876168847084\n",
      "epoch: 21 step: 212, loss is 0.2084430605173111\n",
      "epoch: 21 step: 213, loss is 0.17502263188362122\n",
      "epoch: 21 step: 214, loss is 0.22148531675338745\n",
      "epoch: 21 step: 215, loss is 0.16267544031143188\n",
      "epoch: 21 step: 216, loss is 0.19614137709140778\n",
      "epoch: 21 step: 217, loss is 0.17475196719169617\n",
      "epoch: 21 step: 218, loss is 0.24143604934215546\n",
      "epoch: 21 step: 219, loss is 0.16335110366344452\n",
      "epoch: 21 step: 220, loss is 0.25402742624282837\n",
      "epoch: 21 step: 221, loss is 0.22352707386016846\n",
      "epoch: 21 step: 222, loss is 0.2744617164134979\n",
      "epoch: 21 step: 223, loss is 0.16491909325122833\n",
      "epoch: 21 step: 224, loss is 0.19531957805156708\n",
      "epoch: 21 step: 225, loss is 0.20713019371032715\n",
      "epoch: 21 step: 226, loss is 0.1251152604818344\n",
      "epoch: 21 step: 227, loss is 0.20219247043132782\n",
      "epoch: 21 step: 228, loss is 0.14462003111839294\n",
      "epoch: 21 step: 229, loss is 0.17746368050575256\n",
      "epoch: 21 step: 230, loss is 0.22001086175441742\n",
      "epoch: 21 step: 231, loss is 0.2603694796562195\n",
      "epoch: 21 step: 232, loss is 0.205185204744339\n",
      "epoch: 21 step: 233, loss is 0.27160054445266724\n",
      "epoch: 21 step: 234, loss is 0.25303399562835693\n",
      "epoch: 21 step: 235, loss is 0.15109631419181824\n",
      "epoch: 21 step: 236, loss is 0.1971534788608551\n",
      "epoch: 21 step: 237, loss is 0.1340375393629074\n",
      "epoch: 21 step: 238, loss is 0.17832417786121368\n",
      "epoch: 21 step: 239, loss is 0.13947349786758423\n",
      "epoch: 21 step: 240, loss is 0.21174754202365875\n",
      "epoch: 21 step: 241, loss is 0.08728817105293274\n",
      "epoch: 21 step: 242, loss is 0.13112442195415497\n",
      "epoch: 21 step: 243, loss is 0.16768679022789001\n",
      "epoch: 21 step: 244, loss is 0.2401479184627533\n",
      "epoch: 21 step: 245, loss is 0.10487984120845795\n",
      "epoch: 21 step: 246, loss is 0.18780463933944702\n",
      "epoch: 21 step: 247, loss is 0.16259153187274933\n",
      "epoch: 21 step: 248, loss is 0.139138862490654\n",
      "epoch: 21 step: 249, loss is 0.12402250617742538\n",
      "epoch: 21 step: 250, loss is 0.21493622660636902\n",
      "epoch: 21 step: 251, loss is 0.08867838978767395\n",
      "epoch: 21 step: 252, loss is 0.15638583898544312\n",
      "epoch: 21 step: 253, loss is 0.32210642099380493\n",
      "epoch: 21 step: 254, loss is 0.17074082791805267\n",
      "epoch: 21 step: 255, loss is 0.21538223326206207\n",
      "epoch: 21 step: 256, loss is 0.1384013295173645\n",
      "epoch: 21 step: 257, loss is 0.1438884288072586\n",
      "epoch: 21 step: 258, loss is 0.25293198227882385\n",
      "epoch: 21 step: 259, loss is 0.16099153459072113\n",
      "epoch: 21 step: 260, loss is 0.12930159270763397\n",
      "epoch: 21 step: 261, loss is 0.12134115397930145\n",
      "epoch: 21 step: 262, loss is 0.16813921928405762\n",
      "epoch: 21 step: 263, loss is 0.23767420649528503\n",
      "epoch: 21 step: 264, loss is 0.40074947476387024\n",
      "epoch: 21 step: 265, loss is 0.3258225917816162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 266, loss is 0.18651503324508667\n",
      "epoch: 21 step: 267, loss is 0.13341081142425537\n",
      "epoch: 21 step: 268, loss is 0.16661781072616577\n",
      "epoch: 21 step: 269, loss is 0.18789245188236237\n",
      "epoch: 21 step: 270, loss is 0.177987203001976\n",
      "epoch: 21 step: 271, loss is 0.16798794269561768\n",
      "epoch: 21 step: 272, loss is 0.18379342555999756\n",
      "epoch: 21 step: 273, loss is 0.052895694971084595\n",
      "epoch: 21 step: 274, loss is 0.11795929074287415\n",
      "epoch: 21 step: 275, loss is 0.13837122917175293\n",
      "epoch: 21 step: 276, loss is 0.2593223452568054\n",
      "epoch: 21 step: 277, loss is 0.18193106353282928\n",
      "epoch: 21 step: 278, loss is 0.15552620589733124\n",
      "epoch: 21 step: 279, loss is 0.17915432155132294\n",
      "epoch: 21 step: 280, loss is 0.17713510990142822\n",
      "epoch: 21 step: 281, loss is 0.07897646725177765\n",
      "epoch: 21 step: 282, loss is 0.12044043093919754\n",
      "epoch: 21 step: 283, loss is 0.23651401698589325\n",
      "epoch: 21 step: 284, loss is 0.08959770202636719\n",
      "epoch: 21 step: 285, loss is 0.14206385612487793\n",
      "epoch: 21 step: 286, loss is 0.1616496741771698\n",
      "epoch: 21 step: 287, loss is 0.2735908031463623\n",
      "epoch: 21 step: 288, loss is 0.23045572638511658\n",
      "epoch: 21 step: 289, loss is 0.24603599309921265\n",
      "epoch: 21 step: 290, loss is 0.2035704255104065\n",
      "epoch: 21 step: 291, loss is 0.13446512818336487\n",
      "epoch: 21 step: 292, loss is 0.15408585965633392\n",
      "epoch: 21 step: 293, loss is 0.15135951340198517\n",
      "epoch: 21 step: 294, loss is 0.16865567862987518\n",
      "epoch: 21 step: 295, loss is 0.144156351685524\n",
      "epoch: 21 step: 296, loss is 0.17619812488555908\n",
      "epoch: 21 step: 297, loss is 0.1903689056634903\n",
      "epoch: 21 step: 298, loss is 0.15711534023284912\n",
      "epoch: 21 step: 299, loss is 0.26383262872695923\n",
      "epoch: 21 step: 300, loss is 0.233273446559906\n",
      "epoch: 21 step: 301, loss is 0.20286600291728973\n",
      "epoch: 21 step: 302, loss is 0.1737535297870636\n",
      "epoch: 21 step: 303, loss is 0.47389668226242065\n",
      "epoch: 21 step: 304, loss is 0.2993677258491516\n",
      "epoch: 21 step: 305, loss is 0.3139772117137909\n",
      "epoch: 21 step: 306, loss is 0.11103467643260956\n",
      "epoch: 21 step: 307, loss is 0.2072901576757431\n",
      "epoch: 21 step: 308, loss is 0.28760358691215515\n",
      "epoch: 21 step: 309, loss is 0.28602102398872375\n",
      "epoch: 21 step: 310, loss is 0.41161802411079407\n",
      "epoch: 21 step: 311, loss is 0.25292062759399414\n",
      "epoch: 21 step: 312, loss is 0.1375003606081009\n",
      "epoch: 21 step: 313, loss is 0.3249223232269287\n",
      "epoch: 21 step: 314, loss is 0.14336447417736053\n",
      "epoch: 21 step: 315, loss is 0.11022960394620895\n",
      "epoch: 21 step: 316, loss is 0.1850677877664566\n",
      "epoch: 21 step: 317, loss is 0.29003697633743286\n",
      "epoch: 21 step: 318, loss is 0.10896963626146317\n",
      "epoch: 21 step: 319, loss is 0.12290701270103455\n",
      "epoch: 21 step: 320, loss is 0.24405726790428162\n",
      "epoch: 21 step: 321, loss is 0.17151890695095062\n",
      "epoch: 21 step: 322, loss is 0.21205517649650574\n",
      "epoch: 21 step: 323, loss is 0.10751271992921829\n",
      "epoch: 21 step: 324, loss is 0.1348182111978531\n",
      "epoch: 21 step: 325, loss is 0.21971026062965393\n",
      "epoch: 21 step: 326, loss is 0.2864275872707367\n",
      "epoch: 21 step: 327, loss is 0.18001845479011536\n",
      "epoch: 21 step: 328, loss is 0.2350408285856247\n",
      "epoch: 21 step: 329, loss is 0.06132335215806961\n",
      "epoch: 21 step: 330, loss is 0.35122695565223694\n",
      "epoch: 21 step: 331, loss is 0.30862438678741455\n",
      "epoch: 21 step: 332, loss is 0.2333468347787857\n",
      "epoch: 21 step: 333, loss is 0.2929788827896118\n",
      "epoch: 21 step: 334, loss is 0.17672909796237946\n",
      "epoch: 21 step: 335, loss is 0.11595596373081207\n",
      "epoch: 21 step: 336, loss is 0.07201652228832245\n",
      "epoch: 21 step: 337, loss is 0.19290713965892792\n",
      "epoch: 21 step: 338, loss is 0.21443696320056915\n",
      "epoch: 21 step: 339, loss is 0.20877376198768616\n",
      "epoch: 21 step: 340, loss is 0.12060289829969406\n",
      "epoch: 21 step: 341, loss is 0.08404514193534851\n",
      "epoch: 21 step: 342, loss is 0.18188226222991943\n",
      "epoch: 21 step: 343, loss is 0.11387462913990021\n",
      "epoch: 21 step: 344, loss is 0.0686727985739708\n",
      "epoch: 21 step: 345, loss is 0.20972512662410736\n",
      "epoch: 21 step: 346, loss is 0.1839650571346283\n",
      "epoch: 21 step: 347, loss is 0.25033122301101685\n",
      "epoch: 21 step: 348, loss is 0.2607405483722687\n",
      "epoch: 21 step: 349, loss is 0.2562878131866455\n",
      "epoch: 21 step: 350, loss is 0.11330952495336533\n",
      "epoch: 21 step: 351, loss is 0.44056135416030884\n",
      "epoch: 21 step: 352, loss is 0.16165101528167725\n",
      "epoch: 21 step: 353, loss is 0.10629061609506607\n",
      "epoch: 21 step: 354, loss is 0.23470371961593628\n",
      "epoch: 21 step: 355, loss is 0.10643567144870758\n",
      "epoch: 21 step: 356, loss is 0.1365874707698822\n",
      "epoch: 21 step: 357, loss is 0.23192232847213745\n",
      "epoch: 21 step: 358, loss is 0.14542707800865173\n",
      "epoch: 21 step: 359, loss is 0.14876258373260498\n",
      "epoch: 21 step: 360, loss is 0.21180163323879242\n",
      "epoch: 21 step: 361, loss is 0.13293449580669403\n",
      "epoch: 21 step: 362, loss is 0.1519790142774582\n",
      "epoch: 21 step: 363, loss is 0.18270327150821686\n",
      "epoch: 21 step: 364, loss is 0.3208600878715515\n",
      "epoch: 21 step: 365, loss is 0.14763067662715912\n",
      "epoch: 21 step: 366, loss is 0.22085818648338318\n",
      "epoch: 21 step: 367, loss is 0.12015821784734726\n",
      "epoch: 21 step: 368, loss is 0.15180878341197968\n",
      "epoch: 21 step: 369, loss is 0.19007807970046997\n",
      "epoch: 21 step: 370, loss is 0.16507427394390106\n",
      "epoch: 21 step: 371, loss is 0.0816439539194107\n",
      "epoch: 21 step: 372, loss is 0.22739726305007935\n",
      "epoch: 21 step: 373, loss is 0.1033758744597435\n",
      "epoch: 21 step: 374, loss is 0.18161894381046295\n",
      "epoch: 21 step: 375, loss is 0.09431245923042297\n",
      "epoch: 21 step: 376, loss is 0.46656283736228943\n",
      "epoch: 21 step: 377, loss is 0.28393465280532837\n",
      "epoch: 21 step: 378, loss is 0.10360513627529144\n",
      "epoch: 21 step: 379, loss is 0.2721509635448456\n",
      "epoch: 21 step: 380, loss is 0.13222788274288177\n",
      "epoch: 21 step: 381, loss is 0.36595383286476135\n",
      "epoch: 21 step: 382, loss is 0.2021004855632782\n",
      "epoch: 21 step: 383, loss is 0.18453672528266907\n",
      "epoch: 21 step: 384, loss is 0.10993297398090363\n",
      "epoch: 21 step: 385, loss is 0.18006053566932678\n",
      "epoch: 21 step: 386, loss is 0.1653454750776291\n",
      "epoch: 21 step: 387, loss is 0.1887189894914627\n",
      "epoch: 21 step: 388, loss is 0.4668922424316406\n",
      "epoch: 21 step: 389, loss is 0.1404258757829666\n",
      "epoch: 21 step: 390, loss is 0.11114994436502457\n",
      "epoch: 21 step: 391, loss is 0.22117136418819427\n",
      "epoch: 21 step: 392, loss is 0.22436931729316711\n",
      "epoch: 21 step: 393, loss is 0.1654895842075348\n",
      "epoch: 21 step: 394, loss is 0.1668199896812439\n",
      "epoch: 21 step: 395, loss is 0.12025927007198334\n",
      "epoch: 21 step: 396, loss is 0.11756455153226852\n",
      "epoch: 21 step: 397, loss is 0.08765800297260284\n",
      "epoch: 21 step: 398, loss is 0.2203129529953003\n",
      "epoch: 21 step: 399, loss is 0.048310019075870514\n",
      "epoch: 21 step: 400, loss is 0.2077074646949768\n",
      "epoch: 21 step: 401, loss is 0.085268035531044\n",
      "epoch: 21 step: 402, loss is 0.2214082032442093\n",
      "epoch: 21 step: 403, loss is 0.2152254432439804\n",
      "epoch: 21 step: 404, loss is 0.13950954377651215\n",
      "epoch: 21 step: 405, loss is 0.06831018626689911\n",
      "epoch: 21 step: 406, loss is 0.25533658266067505\n",
      "epoch: 21 step: 407, loss is 0.3611106276512146\n",
      "epoch: 21 step: 408, loss is 0.2250034064054489\n",
      "epoch: 21 step: 409, loss is 0.14522267878055573\n",
      "epoch: 21 step: 410, loss is 0.18804776668548584\n",
      "epoch: 21 step: 411, loss is 0.16649724543094635\n",
      "epoch: 21 step: 412, loss is 0.13046158850193024\n",
      "epoch: 21 step: 413, loss is 0.1995287984609604\n",
      "epoch: 21 step: 414, loss is 0.1529073864221573\n",
      "epoch: 21 step: 415, loss is 0.1996801346540451\n",
      "epoch: 21 step: 416, loss is 0.14474454522132874\n",
      "epoch: 21 step: 417, loss is 0.3718528151512146\n",
      "epoch: 21 step: 418, loss is 0.2598530054092407\n",
      "epoch: 21 step: 419, loss is 0.12104865163564682\n",
      "epoch: 21 step: 420, loss is 0.2844182252883911\n",
      "epoch: 21 step: 421, loss is 0.19886420667171478\n",
      "epoch: 21 step: 422, loss is 0.3723367750644684\n",
      "epoch: 21 step: 423, loss is 0.22629570960998535\n",
      "epoch: 21 step: 424, loss is 0.1460045576095581\n",
      "epoch: 21 step: 425, loss is 0.27654245495796204\n",
      "epoch: 21 step: 426, loss is 0.2351016104221344\n",
      "epoch: 21 step: 427, loss is 0.12839065492153168\n",
      "epoch: 21 step: 428, loss is 0.2263471633195877\n",
      "epoch: 21 step: 429, loss is 0.2417299449443817\n",
      "epoch: 21 step: 430, loss is 0.1799529641866684\n",
      "epoch: 21 step: 431, loss is 0.10831545293331146\n",
      "epoch: 21 step: 432, loss is 0.11395256221294403\n",
      "epoch: 21 step: 433, loss is 0.22288638353347778\n",
      "epoch: 21 step: 434, loss is 0.15155328810214996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 435, loss is 0.08830215781927109\n",
      "epoch: 21 step: 436, loss is 0.19117377698421478\n",
      "epoch: 21 step: 437, loss is 0.09600688517093658\n",
      "epoch: 21 step: 438, loss is 0.24832135438919067\n",
      "epoch: 21 step: 439, loss is 0.2613929212093353\n",
      "epoch: 21 step: 440, loss is 0.06862357258796692\n",
      "epoch: 21 step: 441, loss is 0.2786790728569031\n",
      "epoch: 21 step: 442, loss is 0.1669723391532898\n",
      "epoch: 21 step: 443, loss is 0.08313491940498352\n",
      "epoch: 21 step: 444, loss is 0.3616102933883667\n",
      "epoch: 21 step: 445, loss is 0.07748203724622726\n",
      "epoch: 21 step: 446, loss is 0.3126221001148224\n",
      "epoch: 21 step: 447, loss is 0.27334675192832947\n",
      "epoch: 21 step: 448, loss is 0.2758161425590515\n",
      "epoch: 21 step: 449, loss is 0.18926896154880524\n",
      "epoch: 21 step: 450, loss is 0.18221107125282288\n",
      "epoch: 21 step: 451, loss is 0.22412364184856415\n",
      "epoch: 21 step: 452, loss is 0.1679762601852417\n",
      "epoch: 21 step: 453, loss is 0.1420963704586029\n",
      "epoch: 21 step: 454, loss is 0.1475445032119751\n",
      "epoch: 21 step: 455, loss is 0.19875521957874298\n",
      "epoch: 21 step: 456, loss is 0.16097109019756317\n",
      "epoch: 21 step: 457, loss is 0.1160583347082138\n",
      "epoch: 21 step: 458, loss is 0.16645869612693787\n",
      "epoch: 21 step: 459, loss is 0.1091947928071022\n",
      "epoch: 21 step: 460, loss is 0.20578533411026\n",
      "epoch: 21 step: 461, loss is 0.15340794622898102\n",
      "epoch: 21 step: 462, loss is 0.12050649523735046\n",
      "epoch: 21 step: 463, loss is 0.27686911821365356\n",
      "epoch: 21 step: 464, loss is 0.3691416382789612\n",
      "epoch: 21 step: 465, loss is 0.1891283541917801\n",
      "epoch: 21 step: 466, loss is 0.20197737216949463\n",
      "epoch: 21 step: 467, loss is 0.20505520701408386\n",
      "epoch: 21 step: 468, loss is 0.12374652922153473\n",
      "epoch: 21 step: 469, loss is 0.14410361647605896\n",
      "epoch: 21 step: 470, loss is 0.21233780682086945\n",
      "epoch: 21 step: 471, loss is 0.20435521006584167\n",
      "epoch: 21 step: 472, loss is 0.27029743790626526\n",
      "epoch: 21 step: 473, loss is 0.4208572506904602\n",
      "epoch: 21 step: 474, loss is 0.19407238066196442\n",
      "epoch: 21 step: 475, loss is 0.12359698116779327\n",
      "epoch: 21 step: 476, loss is 0.23884865641593933\n",
      "epoch: 21 step: 477, loss is 0.20106075704097748\n",
      "epoch: 21 step: 478, loss is 0.320928692817688\n",
      "epoch: 21 step: 479, loss is 0.29197266697883606\n",
      "epoch: 21 step: 480, loss is 0.35647010803222656\n",
      "epoch: 21 step: 481, loss is 0.17599612474441528\n",
      "epoch: 21 step: 482, loss is 0.16922563314437866\n",
      "epoch: 21 step: 483, loss is 0.2965101897716522\n",
      "epoch: 21 step: 484, loss is 0.31563127040863037\n",
      "epoch: 21 step: 485, loss is 0.1450250744819641\n",
      "epoch: 21 step: 486, loss is 0.19797015190124512\n",
      "epoch: 21 step: 487, loss is 0.16459806263446808\n",
      "epoch: 21 step: 488, loss is 0.2616492807865143\n",
      "epoch: 21 step: 489, loss is 0.17998358607292175\n",
      "epoch: 21 step: 490, loss is 0.1153159812092781\n",
      "epoch: 21 step: 491, loss is 0.3423570394515991\n",
      "epoch: 21 step: 492, loss is 0.1750270575284958\n",
      "epoch: 21 step: 493, loss is 0.21899239718914032\n",
      "epoch: 21 step: 494, loss is 0.16904540359973907\n",
      "epoch: 21 step: 495, loss is 0.17658106982707977\n",
      "epoch: 21 step: 496, loss is 0.22919617593288422\n",
      "epoch: 21 step: 497, loss is 0.16292832791805267\n",
      "epoch: 21 step: 498, loss is 0.253010630607605\n",
      "epoch: 21 step: 499, loss is 0.10232502967119217\n",
      "epoch: 21 step: 500, loss is 0.05775744840502739\n",
      "epoch: 21 step: 501, loss is 0.15591520071029663\n",
      "epoch: 21 step: 502, loss is 0.19857726991176605\n",
      "epoch: 21 step: 503, loss is 0.09039793163537979\n",
      "epoch: 21 step: 504, loss is 0.23013436794281006\n",
      "epoch: 21 step: 505, loss is 0.19625215232372284\n",
      "epoch: 21 step: 506, loss is 0.3073429465293884\n",
      "epoch: 21 step: 507, loss is 0.24199645221233368\n",
      "epoch: 21 step: 508, loss is 0.2157568484544754\n",
      "epoch: 21 step: 509, loss is 0.23749171197414398\n",
      "epoch: 21 step: 510, loss is 0.09680790454149246\n",
      "epoch: 21 step: 511, loss is 0.13205944001674652\n",
      "epoch: 21 step: 512, loss is 0.2583802044391632\n",
      "epoch: 21 step: 513, loss is 0.29080989956855774\n",
      "epoch: 21 step: 514, loss is 0.10359461605548859\n",
      "epoch: 21 step: 515, loss is 0.25205767154693604\n",
      "epoch: 21 step: 516, loss is 0.0983370691537857\n",
      "epoch: 21 step: 517, loss is 0.32167112827301025\n",
      "epoch: 21 step: 518, loss is 0.1535099744796753\n",
      "epoch: 21 step: 519, loss is 0.19838263094425201\n",
      "epoch: 21 step: 520, loss is 0.1705688238143921\n",
      "epoch: 21 step: 521, loss is 0.13468597829341888\n",
      "epoch: 21 step: 522, loss is 0.214265376329422\n",
      "epoch: 21 step: 523, loss is 0.3106655180454254\n",
      "epoch: 21 step: 524, loss is 0.17091915011405945\n",
      "epoch: 21 step: 525, loss is 0.30393093824386597\n",
      "epoch: 21 step: 526, loss is 0.3491528332233429\n",
      "epoch: 21 step: 527, loss is 0.16061854362487793\n",
      "epoch: 21 step: 528, loss is 0.16920143365859985\n",
      "epoch: 21 step: 529, loss is 0.08505254238843918\n",
      "epoch: 21 step: 530, loss is 0.16097719967365265\n",
      "epoch: 21 step: 531, loss is 0.17903870344161987\n",
      "epoch: 21 step: 532, loss is 0.2651326656341553\n",
      "epoch: 21 step: 533, loss is 0.19168224930763245\n",
      "epoch: 21 step: 534, loss is 0.11696361750364304\n",
      "epoch: 21 step: 535, loss is 0.24025648832321167\n",
      "epoch: 21 step: 536, loss is 0.11434420943260193\n",
      "epoch: 21 step: 537, loss is 0.11576654016971588\n",
      "epoch: 21 step: 538, loss is 0.22529414296150208\n",
      "epoch: 21 step: 539, loss is 0.216605544090271\n",
      "epoch: 21 step: 540, loss is 0.1539164036512375\n",
      "epoch: 21 step: 541, loss is 0.13731344044208527\n",
      "epoch: 21 step: 542, loss is 0.22732196748256683\n",
      "epoch: 21 step: 543, loss is 0.06260184198617935\n",
      "epoch: 21 step: 544, loss is 0.2210567593574524\n",
      "epoch: 21 step: 545, loss is 0.2212865948677063\n",
      "epoch: 21 step: 546, loss is 0.2576715052127838\n",
      "epoch: 21 step: 547, loss is 0.15999318659305573\n",
      "epoch: 21 step: 548, loss is 0.3148883879184723\n",
      "epoch: 21 step: 549, loss is 0.1369086354970932\n",
      "epoch: 21 step: 550, loss is 0.17884621024131775\n",
      "epoch: 21 step: 551, loss is 0.36434248089790344\n",
      "epoch: 21 step: 552, loss is 0.21030966937541962\n",
      "epoch: 21 step: 553, loss is 0.1574867218732834\n",
      "epoch: 21 step: 554, loss is 0.32129570841789246\n",
      "epoch: 21 step: 555, loss is 0.09670954197645187\n",
      "epoch: 21 step: 556, loss is 0.2884874939918518\n",
      "epoch: 21 step: 557, loss is 0.16462069749832153\n",
      "epoch: 21 step: 558, loss is 0.13235415518283844\n",
      "epoch: 21 step: 559, loss is 0.2643711268901825\n",
      "epoch: 21 step: 560, loss is 0.2170613408088684\n",
      "epoch: 21 step: 561, loss is 0.1456425040960312\n",
      "epoch: 21 step: 562, loss is 0.2073412388563156\n",
      "epoch: 21 step: 563, loss is 0.2311505228281021\n",
      "epoch: 21 step: 564, loss is 0.2506813108921051\n",
      "epoch: 21 step: 565, loss is 0.2646368145942688\n",
      "epoch: 21 step: 566, loss is 0.1544254571199417\n",
      "epoch: 21 step: 567, loss is 0.22101208567619324\n",
      "epoch: 21 step: 568, loss is 0.18408630788326263\n",
      "epoch: 21 step: 569, loss is 0.2439681887626648\n",
      "epoch: 21 step: 570, loss is 0.23054517805576324\n",
      "epoch: 21 step: 571, loss is 0.19783098995685577\n",
      "epoch: 21 step: 572, loss is 0.12000385671854019\n",
      "epoch: 21 step: 573, loss is 0.2902611792087555\n",
      "epoch: 21 step: 574, loss is 0.2047688513994217\n",
      "epoch: 21 step: 575, loss is 0.13276678323745728\n",
      "epoch: 21 step: 576, loss is 0.21474221348762512\n",
      "epoch: 21 step: 577, loss is 0.12732386589050293\n",
      "epoch: 21 step: 578, loss is 0.19748906791210175\n",
      "epoch: 21 step: 579, loss is 0.10283828526735306\n",
      "epoch: 21 step: 580, loss is 0.1951918751001358\n",
      "epoch: 21 step: 581, loss is 0.1512451469898224\n",
      "epoch: 21 step: 582, loss is 0.21371380984783173\n",
      "epoch: 21 step: 583, loss is 0.06446580588817596\n",
      "epoch: 21 step: 584, loss is 0.07776141911745071\n",
      "epoch: 21 step: 585, loss is 0.10118257999420166\n",
      "epoch: 21 step: 586, loss is 0.10019004344940186\n",
      "epoch: 21 step: 587, loss is 0.22644060850143433\n",
      "epoch: 21 step: 588, loss is 0.28925544023513794\n",
      "epoch: 21 step: 589, loss is 0.1071464866399765\n",
      "epoch: 21 step: 590, loss is 0.28909000754356384\n",
      "epoch: 21 step: 591, loss is 0.3063993752002716\n",
      "epoch: 21 step: 592, loss is 0.24619321525096893\n",
      "epoch: 21 step: 593, loss is 0.38694992661476135\n",
      "epoch: 21 step: 594, loss is 0.19945232570171356\n",
      "epoch: 21 step: 595, loss is 0.3247254192829132\n",
      "epoch: 21 step: 596, loss is 0.1250426322221756\n",
      "epoch: 21 step: 597, loss is 0.1667778193950653\n",
      "epoch: 21 step: 598, loss is 0.1487019807100296\n",
      "epoch: 21 step: 599, loss is 0.2461429238319397\n",
      "epoch: 21 step: 600, loss is 0.2342262864112854\n",
      "epoch: 21 step: 601, loss is 0.2230045050382614\n",
      "epoch: 21 step: 602, loss is 0.1478588879108429\n",
      "epoch: 21 step: 603, loss is 0.3028711676597595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 604, loss is 0.16418984532356262\n",
      "epoch: 21 step: 605, loss is 0.043032363057136536\n",
      "epoch: 21 step: 606, loss is 0.1635850965976715\n",
      "epoch: 21 step: 607, loss is 0.1981486678123474\n",
      "epoch: 21 step: 608, loss is 0.13318967819213867\n",
      "epoch: 21 step: 609, loss is 0.18221935629844666\n",
      "epoch: 21 step: 610, loss is 0.23645442724227905\n",
      "epoch: 21 step: 611, loss is 0.1669079214334488\n",
      "epoch: 21 step: 612, loss is 0.14769606292247772\n",
      "epoch: 21 step: 613, loss is 0.09373875707387924\n",
      "epoch: 21 step: 614, loss is 0.36031752824783325\n",
      "epoch: 21 step: 615, loss is 0.3236905336380005\n",
      "epoch: 21 step: 616, loss is 0.2721298933029175\n",
      "epoch: 21 step: 617, loss is 0.19239704310894012\n",
      "epoch: 21 step: 618, loss is 0.19940884411334991\n",
      "epoch: 21 step: 619, loss is 0.2654324471950531\n",
      "epoch: 21 step: 620, loss is 0.2360619753599167\n",
      "epoch: 21 step: 621, loss is 0.29991060495376587\n",
      "epoch: 21 step: 622, loss is 0.20426127314567566\n",
      "epoch: 21 step: 623, loss is 0.23093168437480927\n",
      "epoch: 21 step: 624, loss is 0.2029203623533249\n",
      "epoch: 21 step: 625, loss is 0.1181914359331131\n",
      "epoch: 21 step: 626, loss is 0.17300266027450562\n",
      "epoch: 21 step: 627, loss is 0.39877238869667053\n",
      "epoch: 21 step: 628, loss is 0.27740129828453064\n",
      "epoch: 21 step: 629, loss is 0.345804899930954\n",
      "epoch: 21 step: 630, loss is 0.21108825504779816\n",
      "epoch: 21 step: 631, loss is 0.3042770326137543\n",
      "epoch: 21 step: 632, loss is 0.13224059343338013\n",
      "epoch: 21 step: 633, loss is 0.1602621078491211\n",
      "epoch: 21 step: 634, loss is 0.1569112241268158\n",
      "epoch: 21 step: 635, loss is 0.2690489590167999\n",
      "epoch: 21 step: 636, loss is 0.23895607888698578\n",
      "epoch: 21 step: 637, loss is 0.19032526016235352\n",
      "epoch: 21 step: 638, loss is 0.06728385388851166\n",
      "epoch: 21 step: 639, loss is 0.22423185408115387\n",
      "epoch: 21 step: 640, loss is 0.2407931685447693\n",
      "epoch: 21 step: 641, loss is 0.14156053960323334\n",
      "epoch: 21 step: 642, loss is 0.10222502052783966\n",
      "epoch: 21 step: 643, loss is 0.12460575997829437\n",
      "epoch: 21 step: 644, loss is 0.13848558068275452\n",
      "epoch: 21 step: 645, loss is 0.19099336862564087\n",
      "epoch: 21 step: 646, loss is 0.12915299832820892\n",
      "epoch: 21 step: 647, loss is 0.2472025752067566\n",
      "epoch: 21 step: 648, loss is 0.14052437245845795\n",
      "epoch: 21 step: 649, loss is 0.1523970067501068\n",
      "epoch: 21 step: 650, loss is 0.20253504812717438\n",
      "epoch: 21 step: 651, loss is 0.3634544909000397\n",
      "epoch: 21 step: 652, loss is 0.13426288962364197\n",
      "epoch: 21 step: 653, loss is 0.27246275544166565\n",
      "epoch: 21 step: 654, loss is 0.1666073501110077\n",
      "epoch: 21 step: 655, loss is 0.2510541081428528\n",
      "epoch: 21 step: 656, loss is 0.29145896434783936\n",
      "epoch: 21 step: 657, loss is 0.3806544542312622\n",
      "epoch: 21 step: 658, loss is 0.2529407739639282\n",
      "epoch: 21 step: 659, loss is 0.1262424737215042\n",
      "epoch: 21 step: 660, loss is 0.2394966185092926\n",
      "epoch: 21 step: 661, loss is 0.18092019855976105\n",
      "epoch: 21 step: 662, loss is 0.2570061683654785\n",
      "epoch: 21 step: 663, loss is 0.2539668381214142\n",
      "epoch: 21 step: 664, loss is 0.3031107783317566\n",
      "epoch: 21 step: 665, loss is 0.1757778376340866\n",
      "epoch: 21 step: 666, loss is 0.20898735523223877\n",
      "epoch: 21 step: 667, loss is 0.18818701803684235\n",
      "epoch: 21 step: 668, loss is 0.13928429782390594\n",
      "epoch: 21 step: 669, loss is 0.17368201911449432\n",
      "epoch: 21 step: 670, loss is 0.1834419220685959\n",
      "epoch: 21 step: 671, loss is 0.17465411126613617\n",
      "epoch: 21 step: 672, loss is 0.19008409976959229\n",
      "epoch: 21 step: 673, loss is 0.18807315826416016\n",
      "epoch: 21 step: 674, loss is 0.12118670344352722\n",
      "epoch: 21 step: 675, loss is 0.06366834044456482\n",
      "epoch: 21 step: 676, loss is 0.2723298668861389\n",
      "epoch: 21 step: 677, loss is 0.2061019092798233\n",
      "epoch: 21 step: 678, loss is 0.19305263459682465\n",
      "epoch: 21 step: 679, loss is 0.2207227498292923\n",
      "epoch: 21 step: 680, loss is 0.30154338479042053\n",
      "epoch: 21 step: 681, loss is 0.28488650918006897\n",
      "epoch: 21 step: 682, loss is 0.19942165911197662\n",
      "epoch: 21 step: 683, loss is 0.17680995166301727\n",
      "epoch: 21 step: 684, loss is 0.1586415320634842\n",
      "epoch: 21 step: 685, loss is 0.1622367650270462\n",
      "epoch: 21 step: 686, loss is 0.20454902946949005\n",
      "epoch: 21 step: 687, loss is 0.3954118490219116\n",
      "epoch: 21 step: 688, loss is 0.2153925895690918\n",
      "epoch: 21 step: 689, loss is 0.3329804539680481\n",
      "epoch: 21 step: 690, loss is 0.22925786674022675\n",
      "epoch: 21 step: 691, loss is 0.46780699491500854\n",
      "epoch: 21 step: 692, loss is 0.2401704490184784\n",
      "epoch: 21 step: 693, loss is 0.2686443030834198\n",
      "epoch: 21 step: 694, loss is 0.13179361820220947\n",
      "epoch: 21 step: 695, loss is 0.2328096181154251\n",
      "epoch: 21 step: 696, loss is 0.14191995561122894\n",
      "epoch: 21 step: 697, loss is 0.1850261092185974\n",
      "epoch: 21 step: 698, loss is 0.16668188571929932\n",
      "epoch: 21 step: 699, loss is 0.19960960745811462\n",
      "epoch: 21 step: 700, loss is 0.13888022303581238\n",
      "epoch: 21 step: 701, loss is 0.26836562156677246\n",
      "epoch: 21 step: 702, loss is 0.29378142952919006\n",
      "epoch: 21 step: 703, loss is 0.3698979318141937\n",
      "epoch: 21 step: 704, loss is 0.09265265613794327\n",
      "epoch: 21 step: 705, loss is 0.12901006639003754\n",
      "epoch: 21 step: 706, loss is 0.1309245228767395\n",
      "epoch: 21 step: 707, loss is 0.11686351150274277\n",
      "epoch: 21 step: 708, loss is 0.25507083535194397\n",
      "epoch: 21 step: 709, loss is 0.23514479398727417\n",
      "epoch: 21 step: 710, loss is 0.09720159322023392\n",
      "epoch: 21 step: 711, loss is 0.20269574224948883\n",
      "epoch: 21 step: 712, loss is 0.16736634075641632\n",
      "epoch: 21 step: 713, loss is 0.0804237425327301\n",
      "epoch: 21 step: 714, loss is 0.1886087954044342\n",
      "epoch: 21 step: 715, loss is 0.13238854706287384\n",
      "epoch: 21 step: 716, loss is 0.19184903800487518\n",
      "epoch: 21 step: 717, loss is 0.11346761882305145\n",
      "epoch: 21 step: 718, loss is 0.2616395652294159\n",
      "epoch: 21 step: 719, loss is 0.19226980209350586\n",
      "epoch: 21 step: 720, loss is 0.35499387979507446\n",
      "epoch: 21 step: 721, loss is 0.17800836265087128\n",
      "epoch: 21 step: 722, loss is 0.19596613943576813\n",
      "epoch: 21 step: 723, loss is 0.3430895209312439\n",
      "epoch: 21 step: 724, loss is 0.18357928097248077\n",
      "epoch: 21 step: 725, loss is 0.17177404463291168\n",
      "epoch: 21 step: 726, loss is 0.16162684559822083\n",
      "epoch: 21 step: 727, loss is 0.24459323287010193\n",
      "epoch: 21 step: 728, loss is 0.2584880292415619\n",
      "epoch: 21 step: 729, loss is 0.1264144778251648\n",
      "epoch: 21 step: 730, loss is 0.13432331383228302\n",
      "epoch: 21 step: 731, loss is 0.12081731855869293\n",
      "epoch: 21 step: 732, loss is 0.18168118596076965\n",
      "epoch: 21 step: 733, loss is 0.23815155029296875\n",
      "epoch: 21 step: 734, loss is 0.16322524845600128\n",
      "epoch: 21 step: 735, loss is 0.1182744950056076\n",
      "epoch: 21 step: 736, loss is 0.07528672367334366\n",
      "epoch: 21 step: 737, loss is 0.17495542764663696\n",
      "epoch: 21 step: 738, loss is 0.2194526642560959\n",
      "epoch: 21 step: 739, loss is 0.18546059727668762\n",
      "epoch: 21 step: 740, loss is 0.16941799223423004\n",
      "epoch: 21 step: 741, loss is 0.3109086751937866\n",
      "epoch: 21 step: 742, loss is 0.21108096837997437\n",
      "epoch: 21 step: 743, loss is 0.2904796004295349\n",
      "epoch: 21 step: 744, loss is 0.1981251984834671\n",
      "epoch: 21 step: 745, loss is 0.15705107152462006\n",
      "epoch: 21 step: 746, loss is 0.19806772470474243\n",
      "epoch: 21 step: 747, loss is 0.08939194679260254\n",
      "epoch: 21 step: 748, loss is 0.15328970551490784\n",
      "epoch: 21 step: 749, loss is 0.29610496759414673\n",
      "epoch: 21 step: 750, loss is 0.2535802125930786\n",
      "epoch: 21 step: 751, loss is 0.13786591589450836\n",
      "epoch: 21 step: 752, loss is 0.21655449271202087\n",
      "epoch: 21 step: 753, loss is 0.1910293847322464\n",
      "epoch: 21 step: 754, loss is 0.334221750497818\n",
      "epoch: 21 step: 755, loss is 0.2424110472202301\n",
      "epoch: 21 step: 756, loss is 0.1550871580839157\n",
      "epoch: 21 step: 757, loss is 0.23514394462108612\n",
      "epoch: 21 step: 758, loss is 0.3374861776828766\n",
      "epoch: 21 step: 759, loss is 0.20117759704589844\n",
      "epoch: 21 step: 760, loss is 0.12899939715862274\n",
      "epoch: 21 step: 761, loss is 0.26956960558891296\n",
      "epoch: 21 step: 762, loss is 0.22880762815475464\n",
      "epoch: 21 step: 763, loss is 0.2010900229215622\n",
      "epoch: 21 step: 764, loss is 0.19966186583042145\n",
      "epoch: 21 step: 765, loss is 0.25163426995277405\n",
      "epoch: 21 step: 766, loss is 0.1664426177740097\n",
      "epoch: 21 step: 767, loss is 0.24656327068805695\n",
      "epoch: 21 step: 768, loss is 0.2654818296432495\n",
      "epoch: 21 step: 769, loss is 0.19914062321186066\n",
      "epoch: 21 step: 770, loss is 0.16258621215820312\n",
      "epoch: 21 step: 771, loss is 0.1832198053598404\n",
      "epoch: 21 step: 772, loss is 0.1690189242362976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 773, loss is 0.20629461109638214\n",
      "epoch: 21 step: 774, loss is 0.17224521934986115\n",
      "epoch: 21 step: 775, loss is 0.23131582140922546\n",
      "epoch: 21 step: 776, loss is 0.10139933228492737\n",
      "epoch: 21 step: 777, loss is 0.2600109279155731\n",
      "epoch: 21 step: 778, loss is 0.15702258050441742\n",
      "epoch: 21 step: 779, loss is 0.2505941689014435\n",
      "epoch: 21 step: 780, loss is 0.2570365369319916\n",
      "epoch: 21 step: 781, loss is 0.2111545354127884\n",
      "epoch: 21 step: 782, loss is 0.10466859489679337\n",
      "epoch: 21 step: 783, loss is 0.30274224281311035\n",
      "epoch: 21 step: 784, loss is 0.1786966174840927\n",
      "epoch: 21 step: 785, loss is 0.07749032229185104\n",
      "epoch: 21 step: 786, loss is 0.16144345700740814\n",
      "epoch: 21 step: 787, loss is 0.2257399559020996\n",
      "epoch: 21 step: 788, loss is 0.21330665051937103\n",
      "epoch: 21 step: 789, loss is 0.24055063724517822\n",
      "epoch: 21 step: 790, loss is 0.17563144862651825\n",
      "epoch: 21 step: 791, loss is 0.3741598129272461\n",
      "epoch: 21 step: 792, loss is 0.13233411312103271\n",
      "epoch: 21 step: 793, loss is 0.2957831025123596\n",
      "epoch: 21 step: 794, loss is 0.2997073531150818\n",
      "epoch: 21 step: 795, loss is 0.2989999055862427\n",
      "epoch: 21 step: 796, loss is 0.12168143689632416\n",
      "epoch: 21 step: 797, loss is 0.1794159710407257\n",
      "epoch: 21 step: 798, loss is 0.12384205311536789\n",
      "epoch: 21 step: 799, loss is 0.1872987449169159\n",
      "epoch: 21 step: 800, loss is 0.12928186357021332\n",
      "epoch: 21 step: 801, loss is 0.24251529574394226\n",
      "epoch: 21 step: 802, loss is 0.1251716911792755\n",
      "epoch: 21 step: 803, loss is 0.2206723988056183\n",
      "epoch: 21 step: 804, loss is 0.16097623109817505\n",
      "epoch: 21 step: 805, loss is 0.11253519356250763\n",
      "epoch: 21 step: 806, loss is 0.1715943068265915\n",
      "epoch: 21 step: 807, loss is 0.2996552884578705\n",
      "epoch: 21 step: 808, loss is 0.24240146577358246\n",
      "epoch: 21 step: 809, loss is 0.09467848390340805\n",
      "epoch: 21 step: 810, loss is 0.22246244549751282\n",
      "epoch: 21 step: 811, loss is 0.2418069690465927\n",
      "epoch: 21 step: 812, loss is 0.19348832964897156\n",
      "epoch: 21 step: 813, loss is 0.18267452716827393\n",
      "epoch: 21 step: 814, loss is 0.27359041571617126\n",
      "epoch: 21 step: 815, loss is 0.29990795254707336\n",
      "epoch: 21 step: 816, loss is 0.10773259401321411\n",
      "epoch: 21 step: 817, loss is 0.40946778655052185\n",
      "epoch: 21 step: 818, loss is 0.17878572642803192\n",
      "epoch: 21 step: 819, loss is 0.16357366740703583\n",
      "epoch: 21 step: 820, loss is 0.18415690958499908\n",
      "epoch: 21 step: 821, loss is 0.3059272766113281\n",
      "epoch: 21 step: 822, loss is 0.14777728915214539\n",
      "epoch: 21 step: 823, loss is 0.1688234955072403\n",
      "epoch: 21 step: 824, loss is 0.27785205841064453\n",
      "epoch: 21 step: 825, loss is 0.3666640818119049\n",
      "epoch: 21 step: 826, loss is 0.2766411602497101\n",
      "epoch: 21 step: 827, loss is 0.15266381204128265\n",
      "epoch: 21 step: 828, loss is 0.17755462229251862\n",
      "epoch: 21 step: 829, loss is 0.06930556893348694\n",
      "epoch: 21 step: 830, loss is 0.20051898062229156\n",
      "epoch: 21 step: 831, loss is 0.29259181022644043\n",
      "epoch: 21 step: 832, loss is 0.2338542491197586\n",
      "epoch: 21 step: 833, loss is 0.14374807476997375\n",
      "epoch: 21 step: 834, loss is 0.26296567916870117\n",
      "epoch: 21 step: 835, loss is 0.18653257191181183\n",
      "epoch: 21 step: 836, loss is 0.15082913637161255\n",
      "epoch: 21 step: 837, loss is 0.3366639316082001\n",
      "epoch: 21 step: 838, loss is 0.0852443054318428\n",
      "epoch: 21 step: 839, loss is 0.1393078863620758\n",
      "epoch: 21 step: 840, loss is 0.10062311589717865\n",
      "epoch: 21 step: 841, loss is 0.1704222857952118\n",
      "epoch: 21 step: 842, loss is 0.24428068101406097\n",
      "epoch: 21 step: 843, loss is 0.1318136304616928\n",
      "epoch: 21 step: 844, loss is 0.19911566376686096\n",
      "epoch: 21 step: 845, loss is 0.23632846772670746\n",
      "epoch: 21 step: 846, loss is 0.21768198907375336\n",
      "epoch: 21 step: 847, loss is 0.19735728204250336\n",
      "epoch: 21 step: 848, loss is 0.16036148369312286\n",
      "epoch: 21 step: 849, loss is 0.1709820181131363\n",
      "epoch: 21 step: 850, loss is 0.2159343659877777\n",
      "epoch: 21 step: 851, loss is 0.09394829720258713\n",
      "epoch: 21 step: 852, loss is 0.20593827962875366\n",
      "epoch: 21 step: 853, loss is 0.186564102768898\n",
      "epoch: 21 step: 854, loss is 0.21942725777626038\n",
      "epoch: 21 step: 855, loss is 0.09081083536148071\n",
      "epoch: 21 step: 856, loss is 0.2505272924900055\n",
      "epoch: 21 step: 857, loss is 0.23116159439086914\n",
      "epoch: 21 step: 858, loss is 0.07941096276044846\n",
      "epoch: 21 step: 859, loss is 0.2126757949590683\n",
      "epoch: 21 step: 860, loss is 0.23595380783081055\n",
      "epoch: 21 step: 861, loss is 0.1619817167520523\n",
      "epoch: 21 step: 862, loss is 0.1641915738582611\n",
      "epoch: 21 step: 863, loss is 0.18041469156742096\n",
      "epoch: 21 step: 864, loss is 0.16535374522209167\n",
      "epoch: 21 step: 865, loss is 0.17750564217567444\n",
      "epoch: 21 step: 866, loss is 0.23684461414813995\n",
      "epoch: 21 step: 867, loss is 0.15078182518482208\n",
      "epoch: 21 step: 868, loss is 0.2863483726978302\n",
      "epoch: 21 step: 869, loss is 0.18776923418045044\n",
      "epoch: 21 step: 870, loss is 0.14745815098285675\n",
      "epoch: 21 step: 871, loss is 0.22617799043655396\n",
      "epoch: 21 step: 872, loss is 0.10472290962934494\n",
      "epoch: 21 step: 873, loss is 0.16784630715847015\n",
      "epoch: 21 step: 874, loss is 0.25216788053512573\n",
      "epoch: 21 step: 875, loss is 0.16845008730888367\n",
      "epoch: 21 step: 876, loss is 0.12404567748308182\n",
      "epoch: 21 step: 877, loss is 0.1439875215291977\n",
      "epoch: 21 step: 878, loss is 0.10032141208648682\n",
      "epoch: 21 step: 879, loss is 0.16554582118988037\n",
      "epoch: 21 step: 880, loss is 0.07806725800037384\n",
      "epoch: 21 step: 881, loss is 0.12198768556118011\n",
      "epoch: 21 step: 882, loss is 0.4042211174964905\n",
      "epoch: 21 step: 883, loss is 0.26380011439323425\n",
      "epoch: 21 step: 884, loss is 0.20805281400680542\n",
      "epoch: 21 step: 885, loss is 0.3140852153301239\n",
      "epoch: 21 step: 886, loss is 0.09988190233707428\n",
      "epoch: 21 step: 887, loss is 0.1309131234884262\n",
      "epoch: 21 step: 888, loss is 0.2148239016532898\n",
      "epoch: 21 step: 889, loss is 0.16827607154846191\n",
      "epoch: 21 step: 890, loss is 0.2549576163291931\n",
      "epoch: 21 step: 891, loss is 0.12693005800247192\n",
      "epoch: 21 step: 892, loss is 0.20600315928459167\n",
      "epoch: 21 step: 893, loss is 0.30075234174728394\n",
      "epoch: 21 step: 894, loss is 0.13045544922351837\n",
      "epoch: 21 step: 895, loss is 0.06929372996091843\n",
      "epoch: 21 step: 896, loss is 0.1263706088066101\n",
      "epoch: 21 step: 897, loss is 0.198072150349617\n",
      "epoch: 21 step: 898, loss is 0.21817998588085175\n",
      "epoch: 21 step: 899, loss is 0.2856605648994446\n",
      "epoch: 21 step: 900, loss is 0.11516720801591873\n",
      "epoch: 21 step: 901, loss is 0.19318416714668274\n",
      "epoch: 21 step: 902, loss is 0.10817322134971619\n",
      "epoch: 21 step: 903, loss is 0.28173643350601196\n",
      "epoch: 21 step: 904, loss is 0.17366695404052734\n",
      "epoch: 21 step: 905, loss is 0.19290949404239655\n",
      "epoch: 21 step: 906, loss is 0.16748416423797607\n",
      "epoch: 21 step: 907, loss is 0.2062394767999649\n",
      "epoch: 21 step: 908, loss is 0.06492287665605545\n",
      "epoch: 21 step: 909, loss is 0.1643603891134262\n",
      "epoch: 21 step: 910, loss is 0.28258177638053894\n",
      "epoch: 21 step: 911, loss is 0.17821116745471954\n",
      "epoch: 21 step: 912, loss is 0.14727814495563507\n",
      "epoch: 21 step: 913, loss is 0.23859231173992157\n",
      "epoch: 21 step: 914, loss is 0.11337128281593323\n",
      "epoch: 21 step: 915, loss is 0.3884986937046051\n",
      "epoch: 21 step: 916, loss is 0.12416598200798035\n",
      "epoch: 21 step: 917, loss is 0.20737585425376892\n",
      "epoch: 21 step: 918, loss is 0.20177192986011505\n",
      "epoch: 21 step: 919, loss is 0.2975408136844635\n",
      "epoch: 21 step: 920, loss is 0.24458876252174377\n",
      "epoch: 21 step: 921, loss is 0.3058532476425171\n",
      "epoch: 21 step: 922, loss is 0.28425246477127075\n",
      "epoch: 21 step: 923, loss is 0.29191678762435913\n",
      "epoch: 21 step: 924, loss is 0.10021879523992538\n",
      "epoch: 21 step: 925, loss is 0.16222946345806122\n",
      "epoch: 21 step: 926, loss is 0.10627839714288712\n",
      "epoch: 21 step: 927, loss is 0.3911578357219696\n",
      "epoch: 21 step: 928, loss is 0.17325004935264587\n",
      "epoch: 21 step: 929, loss is 0.1574515700340271\n",
      "epoch: 21 step: 930, loss is 0.2286452054977417\n",
      "epoch: 21 step: 931, loss is 0.12744061648845673\n",
      "epoch: 21 step: 932, loss is 0.22203657031059265\n",
      "epoch: 21 step: 933, loss is 0.11274079978466034\n",
      "epoch: 21 step: 934, loss is 0.2809699773788452\n",
      "epoch: 21 step: 935, loss is 0.1454055905342102\n",
      "epoch: 21 step: 936, loss is 0.24391251802444458\n",
      "epoch: 21 step: 937, loss is 0.2243075966835022\n",
      "epoch: 22 step: 1, loss is 0.1762576848268509\n",
      "epoch: 22 step: 2, loss is 0.16589497029781342\n",
      "epoch: 22 step: 3, loss is 0.24617569148540497\n",
      "epoch: 22 step: 4, loss is 0.2783653736114502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 5, loss is 0.09637968242168427\n",
      "epoch: 22 step: 6, loss is 0.19519616663455963\n",
      "epoch: 22 step: 7, loss is 0.32794684171676636\n",
      "epoch: 22 step: 8, loss is 0.14438271522521973\n",
      "epoch: 22 step: 9, loss is 0.29591116309165955\n",
      "epoch: 22 step: 10, loss is 0.09306128323078156\n",
      "epoch: 22 step: 11, loss is 0.22248625755310059\n",
      "epoch: 22 step: 12, loss is 0.17326503992080688\n",
      "epoch: 22 step: 13, loss is 0.15249192714691162\n",
      "epoch: 22 step: 14, loss is 0.09466717392206192\n",
      "epoch: 22 step: 15, loss is 0.1623508185148239\n",
      "epoch: 22 step: 16, loss is 0.23623399436473846\n",
      "epoch: 22 step: 17, loss is 0.23594297468662262\n",
      "epoch: 22 step: 18, loss is 0.07098069041967392\n",
      "epoch: 22 step: 19, loss is 0.20373007655143738\n",
      "epoch: 22 step: 20, loss is 0.18539342284202576\n",
      "epoch: 22 step: 21, loss is 0.16167587041854858\n",
      "epoch: 22 step: 22, loss is 0.21840038895606995\n",
      "epoch: 22 step: 23, loss is 0.13527293503284454\n",
      "epoch: 22 step: 24, loss is 0.1568736732006073\n",
      "epoch: 22 step: 25, loss is 0.10513731837272644\n",
      "epoch: 22 step: 26, loss is 0.2771197259426117\n",
      "epoch: 22 step: 27, loss is 0.19626952707767487\n",
      "epoch: 22 step: 28, loss is 0.18026916682720184\n",
      "epoch: 22 step: 29, loss is 0.15015871822834015\n",
      "epoch: 22 step: 30, loss is 0.4592630863189697\n",
      "epoch: 22 step: 31, loss is 0.09842951595783234\n",
      "epoch: 22 step: 32, loss is 0.14972248673439026\n",
      "epoch: 22 step: 33, loss is 0.18641014397144318\n",
      "epoch: 22 step: 34, loss is 0.3087211549282074\n",
      "epoch: 22 step: 35, loss is 0.13828489184379578\n",
      "epoch: 22 step: 36, loss is 0.2394464910030365\n",
      "epoch: 22 step: 37, loss is 0.2914011776447296\n",
      "epoch: 22 step: 38, loss is 0.1561392992734909\n",
      "epoch: 22 step: 39, loss is 0.4311268627643585\n",
      "epoch: 22 step: 40, loss is 0.12452933192253113\n",
      "epoch: 22 step: 41, loss is 0.36345210671424866\n",
      "epoch: 22 step: 42, loss is 0.167341411113739\n",
      "epoch: 22 step: 43, loss is 0.25989314913749695\n",
      "epoch: 22 step: 44, loss is 0.1901208907365799\n",
      "epoch: 22 step: 45, loss is 0.10995130985975266\n",
      "epoch: 22 step: 46, loss is 0.10982273519039154\n",
      "epoch: 22 step: 47, loss is 0.10595882683992386\n",
      "epoch: 22 step: 48, loss is 0.10329165309667587\n",
      "epoch: 22 step: 49, loss is 0.166449174284935\n",
      "epoch: 22 step: 50, loss is 0.1997576504945755\n",
      "epoch: 22 step: 51, loss is 0.17477069795131683\n",
      "epoch: 22 step: 52, loss is 0.19373728334903717\n",
      "epoch: 22 step: 53, loss is 0.3530789315700531\n",
      "epoch: 22 step: 54, loss is 0.23283126950263977\n",
      "epoch: 22 step: 55, loss is 0.24528542160987854\n",
      "epoch: 22 step: 56, loss is 0.17654328048229218\n",
      "epoch: 22 step: 57, loss is 0.24158044159412384\n",
      "epoch: 22 step: 58, loss is 0.22509734332561493\n",
      "epoch: 22 step: 59, loss is 0.30209261178970337\n",
      "epoch: 22 step: 60, loss is 0.22012990713119507\n",
      "epoch: 22 step: 61, loss is 0.17397354543209076\n",
      "epoch: 22 step: 62, loss is 0.1261298507452011\n",
      "epoch: 22 step: 63, loss is 0.24394428730010986\n",
      "epoch: 22 step: 64, loss is 0.15711715817451477\n",
      "epoch: 22 step: 65, loss is 0.24581778049468994\n",
      "epoch: 22 step: 66, loss is 0.15902218222618103\n",
      "epoch: 22 step: 67, loss is 0.13570521771907806\n",
      "epoch: 22 step: 68, loss is 0.2779098153114319\n",
      "epoch: 22 step: 69, loss is 0.17562440037727356\n",
      "epoch: 22 step: 70, loss is 0.0854124054312706\n",
      "epoch: 22 step: 71, loss is 0.10825379192829132\n",
      "epoch: 22 step: 72, loss is 0.291033536195755\n",
      "epoch: 22 step: 73, loss is 0.11842165887355804\n",
      "epoch: 22 step: 74, loss is 0.11617110669612885\n",
      "epoch: 22 step: 75, loss is 0.18566854298114777\n",
      "epoch: 22 step: 76, loss is 0.15125450491905212\n",
      "epoch: 22 step: 77, loss is 0.12508511543273926\n",
      "epoch: 22 step: 78, loss is 0.15027932822704315\n",
      "epoch: 22 step: 79, loss is 0.2156468778848648\n",
      "epoch: 22 step: 80, loss is 0.11703450232744217\n",
      "epoch: 22 step: 81, loss is 0.14563189446926117\n",
      "epoch: 22 step: 82, loss is 0.2184647023677826\n",
      "epoch: 22 step: 83, loss is 0.1220923513174057\n",
      "epoch: 22 step: 84, loss is 0.10778079926967621\n",
      "epoch: 22 step: 85, loss is 0.16942347586154938\n",
      "epoch: 22 step: 86, loss is 0.1961173117160797\n",
      "epoch: 22 step: 87, loss is 0.15123037993907928\n",
      "epoch: 22 step: 88, loss is 0.24202774465084076\n",
      "epoch: 22 step: 89, loss is 0.18843595683574677\n",
      "epoch: 22 step: 90, loss is 0.24895885586738586\n",
      "epoch: 22 step: 91, loss is 0.1575852483510971\n",
      "epoch: 22 step: 92, loss is 0.1887262612581253\n",
      "epoch: 22 step: 93, loss is 0.2804577946662903\n",
      "epoch: 22 step: 94, loss is 0.21187236905097961\n",
      "epoch: 22 step: 95, loss is 0.15183278918266296\n",
      "epoch: 22 step: 96, loss is 0.25474515557289124\n",
      "epoch: 22 step: 97, loss is 0.16390414535999298\n",
      "epoch: 22 step: 98, loss is 0.18120603263378143\n",
      "epoch: 22 step: 99, loss is 0.21925151348114014\n",
      "epoch: 22 step: 100, loss is 0.122608982026577\n",
      "epoch: 22 step: 101, loss is 0.13649775087833405\n",
      "epoch: 22 step: 102, loss is 0.17385222017765045\n",
      "epoch: 22 step: 103, loss is 0.14958059787750244\n",
      "epoch: 22 step: 104, loss is 0.12542413175106049\n",
      "epoch: 22 step: 105, loss is 0.16720689833164215\n",
      "epoch: 22 step: 106, loss is 0.31694865226745605\n",
      "epoch: 22 step: 107, loss is 0.23597651720046997\n",
      "epoch: 22 step: 108, loss is 0.2656635642051697\n",
      "epoch: 22 step: 109, loss is 0.2813122272491455\n",
      "epoch: 22 step: 110, loss is 0.23729746043682098\n",
      "epoch: 22 step: 111, loss is 0.25337129831314087\n",
      "epoch: 22 step: 112, loss is 0.29819804430007935\n",
      "epoch: 22 step: 113, loss is 0.10366004705429077\n",
      "epoch: 22 step: 114, loss is 0.20148710906505585\n",
      "epoch: 22 step: 115, loss is 0.20598438382148743\n",
      "epoch: 22 step: 116, loss is 0.21214839816093445\n",
      "epoch: 22 step: 117, loss is 0.24474725127220154\n",
      "epoch: 22 step: 118, loss is 0.3890289068222046\n",
      "epoch: 22 step: 119, loss is 0.1896861344575882\n",
      "epoch: 22 step: 120, loss is 0.23944182693958282\n",
      "epoch: 22 step: 121, loss is 0.16652491688728333\n",
      "epoch: 22 step: 122, loss is 0.08010854572057724\n",
      "epoch: 22 step: 123, loss is 0.17218248546123505\n",
      "epoch: 22 step: 124, loss is 0.18471218645572662\n",
      "epoch: 22 step: 125, loss is 0.24993500113487244\n",
      "epoch: 22 step: 126, loss is 0.1832994967699051\n",
      "epoch: 22 step: 127, loss is 0.1843990534543991\n",
      "epoch: 22 step: 128, loss is 0.08711785078048706\n",
      "epoch: 22 step: 129, loss is 0.16238686442375183\n",
      "epoch: 22 step: 130, loss is 0.24627552926540375\n",
      "epoch: 22 step: 131, loss is 0.15946726500988007\n",
      "epoch: 22 step: 132, loss is 0.2013571709394455\n",
      "epoch: 22 step: 133, loss is 0.16626738011837006\n",
      "epoch: 22 step: 134, loss is 0.1809953898191452\n",
      "epoch: 22 step: 135, loss is 0.16465680301189423\n",
      "epoch: 22 step: 136, loss is 0.2584565579891205\n",
      "epoch: 22 step: 137, loss is 0.17763634026050568\n",
      "epoch: 22 step: 138, loss is 0.1326320469379425\n",
      "epoch: 22 step: 139, loss is 0.17137028276920319\n",
      "epoch: 22 step: 140, loss is 0.3158705234527588\n",
      "epoch: 22 step: 141, loss is 0.12275667488574982\n",
      "epoch: 22 step: 142, loss is 0.19040893018245697\n",
      "epoch: 22 step: 143, loss is 0.08001604676246643\n",
      "epoch: 22 step: 144, loss is 0.13624653220176697\n",
      "epoch: 22 step: 145, loss is 0.23200762271881104\n",
      "epoch: 22 step: 146, loss is 0.09103363752365112\n",
      "epoch: 22 step: 147, loss is 0.2745298743247986\n",
      "epoch: 22 step: 148, loss is 0.30811452865600586\n",
      "epoch: 22 step: 149, loss is 0.24121443927288055\n",
      "epoch: 22 step: 150, loss is 0.07174790650606155\n",
      "epoch: 22 step: 151, loss is 0.19951744377613068\n",
      "epoch: 22 step: 152, loss is 0.1825399398803711\n",
      "epoch: 22 step: 153, loss is 0.18736757338047028\n",
      "epoch: 22 step: 154, loss is 0.2791188359260559\n",
      "epoch: 22 step: 155, loss is 0.1879207193851471\n",
      "epoch: 22 step: 156, loss is 0.49335017800331116\n",
      "epoch: 22 step: 157, loss is 0.21142323315143585\n",
      "epoch: 22 step: 158, loss is 0.2292662262916565\n",
      "epoch: 22 step: 159, loss is 0.16803689301013947\n",
      "epoch: 22 step: 160, loss is 0.14265672862529755\n",
      "epoch: 22 step: 161, loss is 0.26279160380363464\n",
      "epoch: 22 step: 162, loss is 0.2708759605884552\n",
      "epoch: 22 step: 163, loss is 0.0814383327960968\n",
      "epoch: 22 step: 164, loss is 0.16070453822612762\n",
      "epoch: 22 step: 165, loss is 0.34065061807632446\n",
      "epoch: 22 step: 166, loss is 0.17004291713237762\n",
      "epoch: 22 step: 167, loss is 0.3024502694606781\n",
      "epoch: 22 step: 168, loss is 0.14788185060024261\n",
      "epoch: 22 step: 169, loss is 0.22513309121131897\n",
      "epoch: 22 step: 170, loss is 0.14094968140125275\n",
      "epoch: 22 step: 171, loss is 0.17789791524410248\n",
      "epoch: 22 step: 172, loss is 0.14477260410785675\n",
      "epoch: 22 step: 173, loss is 0.10780930519104004\n",
      "epoch: 22 step: 174, loss is 0.1864786595106125\n",
      "epoch: 22 step: 175, loss is 0.14639252424240112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 176, loss is 0.19115489721298218\n",
      "epoch: 22 step: 177, loss is 0.16239604353904724\n",
      "epoch: 22 step: 178, loss is 0.16352693736553192\n",
      "epoch: 22 step: 179, loss is 0.16988341510295868\n",
      "epoch: 22 step: 180, loss is 0.32109734416007996\n",
      "epoch: 22 step: 181, loss is 0.22255001962184906\n",
      "epoch: 22 step: 182, loss is 0.25386860966682434\n",
      "epoch: 22 step: 183, loss is 0.1360635608434677\n",
      "epoch: 22 step: 184, loss is 0.35472121834754944\n",
      "epoch: 22 step: 185, loss is 0.1989772468805313\n",
      "epoch: 22 step: 186, loss is 0.1790403127670288\n",
      "epoch: 22 step: 187, loss is 0.07679615169763565\n",
      "epoch: 22 step: 188, loss is 0.22110743820667267\n",
      "epoch: 22 step: 189, loss is 0.24419048428535461\n",
      "epoch: 22 step: 190, loss is 0.18093381822109222\n",
      "epoch: 22 step: 191, loss is 0.1873592734336853\n",
      "epoch: 22 step: 192, loss is 0.33138903975486755\n",
      "epoch: 22 step: 193, loss is 0.25069087743759155\n",
      "epoch: 22 step: 194, loss is 0.2033787965774536\n",
      "epoch: 22 step: 195, loss is 0.22818079590797424\n",
      "epoch: 22 step: 196, loss is 0.13021323084831238\n",
      "epoch: 22 step: 197, loss is 0.24628685414791107\n",
      "epoch: 22 step: 198, loss is 0.10448673367500305\n",
      "epoch: 22 step: 199, loss is 0.2679305374622345\n",
      "epoch: 22 step: 200, loss is 0.14649325609207153\n",
      "epoch: 22 step: 201, loss is 0.16651955246925354\n",
      "epoch: 22 step: 202, loss is 0.2609478235244751\n",
      "epoch: 22 step: 203, loss is 0.31341010332107544\n",
      "epoch: 22 step: 204, loss is 0.1214369684457779\n",
      "epoch: 22 step: 205, loss is 0.12052737921476364\n",
      "epoch: 22 step: 206, loss is 0.18875114619731903\n",
      "epoch: 22 step: 207, loss is 0.28148698806762695\n",
      "epoch: 22 step: 208, loss is 0.24809981882572174\n",
      "epoch: 22 step: 209, loss is 0.05810978263616562\n",
      "epoch: 22 step: 210, loss is 0.1789630949497223\n",
      "epoch: 22 step: 211, loss is 0.13736513257026672\n",
      "epoch: 22 step: 212, loss is 0.17230260372161865\n",
      "epoch: 22 step: 213, loss is 0.20730766654014587\n",
      "epoch: 22 step: 214, loss is 0.14393626153469086\n",
      "epoch: 22 step: 215, loss is 0.2063697725534439\n",
      "epoch: 22 step: 216, loss is 0.14842446148395538\n",
      "epoch: 22 step: 217, loss is 0.21912351250648499\n",
      "epoch: 22 step: 218, loss is 0.13332699239253998\n",
      "epoch: 22 step: 219, loss is 0.1028982624411583\n",
      "epoch: 22 step: 220, loss is 0.0828532800078392\n",
      "epoch: 22 step: 221, loss is 0.21288354694843292\n",
      "epoch: 22 step: 222, loss is 0.18825450539588928\n",
      "epoch: 22 step: 223, loss is 0.15770940482616425\n",
      "epoch: 22 step: 224, loss is 0.22679927945137024\n",
      "epoch: 22 step: 225, loss is 0.09745246171951294\n",
      "epoch: 22 step: 226, loss is 0.2575434744358063\n",
      "epoch: 22 step: 227, loss is 0.15758292376995087\n",
      "epoch: 22 step: 228, loss is 0.2066047638654709\n",
      "epoch: 22 step: 229, loss is 0.3349733054637909\n",
      "epoch: 22 step: 230, loss is 0.2458237260580063\n",
      "epoch: 22 step: 231, loss is 0.12319514900445938\n",
      "epoch: 22 step: 232, loss is 0.43775370717048645\n",
      "epoch: 22 step: 233, loss is 0.38736334443092346\n",
      "epoch: 22 step: 234, loss is 0.24028077721595764\n",
      "epoch: 22 step: 235, loss is 0.09650368988513947\n",
      "epoch: 22 step: 236, loss is 0.19383688271045685\n",
      "epoch: 22 step: 237, loss is 0.12297437340021133\n",
      "epoch: 22 step: 238, loss is 0.15224891901016235\n",
      "epoch: 22 step: 239, loss is 0.18629677593708038\n",
      "epoch: 22 step: 240, loss is 0.24761952459812164\n",
      "epoch: 22 step: 241, loss is 0.19780279695987701\n",
      "epoch: 22 step: 242, loss is 0.38658761978149414\n",
      "epoch: 22 step: 243, loss is 0.3501119017601013\n",
      "epoch: 22 step: 244, loss is 0.16522178053855896\n",
      "epoch: 22 step: 245, loss is 0.1762983500957489\n",
      "epoch: 22 step: 246, loss is 0.21518361568450928\n",
      "epoch: 22 step: 247, loss is 0.23989391326904297\n",
      "epoch: 22 step: 248, loss is 0.20583811402320862\n",
      "epoch: 22 step: 249, loss is 0.30110281705856323\n",
      "epoch: 22 step: 250, loss is 0.12047548592090607\n",
      "epoch: 22 step: 251, loss is 0.19696635007858276\n",
      "epoch: 22 step: 252, loss is 0.19111713767051697\n",
      "epoch: 22 step: 253, loss is 0.09584391117095947\n",
      "epoch: 22 step: 254, loss is 0.12788397073745728\n",
      "epoch: 22 step: 255, loss is 0.2145618051290512\n",
      "epoch: 22 step: 256, loss is 0.16729645431041718\n",
      "epoch: 22 step: 257, loss is 0.2156943827867508\n",
      "epoch: 22 step: 258, loss is 0.21938811242580414\n",
      "epoch: 22 step: 259, loss is 0.17185471951961517\n",
      "epoch: 22 step: 260, loss is 0.1852046549320221\n",
      "epoch: 22 step: 261, loss is 0.183378666639328\n",
      "epoch: 22 step: 262, loss is 0.04825258255004883\n",
      "epoch: 22 step: 263, loss is 0.19898565113544464\n",
      "epoch: 22 step: 264, loss is 0.1514471471309662\n",
      "epoch: 22 step: 265, loss is 0.12228194624185562\n",
      "epoch: 22 step: 266, loss is 0.22856885194778442\n",
      "epoch: 22 step: 267, loss is 0.10116418451070786\n",
      "epoch: 22 step: 268, loss is 0.17332875728607178\n",
      "epoch: 22 step: 269, loss is 0.1921340525150299\n",
      "epoch: 22 step: 270, loss is 0.21372807025909424\n",
      "epoch: 22 step: 271, loss is 0.08832771331071854\n",
      "epoch: 22 step: 272, loss is 0.21609774231910706\n",
      "epoch: 22 step: 273, loss is 0.19718343019485474\n",
      "epoch: 22 step: 274, loss is 0.142907977104187\n",
      "epoch: 22 step: 275, loss is 0.1757643073797226\n",
      "epoch: 22 step: 276, loss is 0.29527661204338074\n",
      "epoch: 22 step: 277, loss is 0.14515456557273865\n",
      "epoch: 22 step: 278, loss is 0.15109972655773163\n",
      "epoch: 22 step: 279, loss is 0.1914636641740799\n",
      "epoch: 22 step: 280, loss is 0.26132720708847046\n",
      "epoch: 22 step: 281, loss is 0.1918467879295349\n",
      "epoch: 22 step: 282, loss is 0.23729704320430756\n",
      "epoch: 22 step: 283, loss is 0.15666286647319794\n",
      "epoch: 22 step: 284, loss is 0.27626773715019226\n",
      "epoch: 22 step: 285, loss is 0.34226542711257935\n",
      "epoch: 22 step: 286, loss is 0.2518593668937683\n",
      "epoch: 22 step: 287, loss is 0.189412921667099\n",
      "epoch: 22 step: 288, loss is 0.2810375690460205\n",
      "epoch: 22 step: 289, loss is 0.2264610379934311\n",
      "epoch: 22 step: 290, loss is 0.1313519924879074\n",
      "epoch: 22 step: 291, loss is 0.12102685123682022\n",
      "epoch: 22 step: 292, loss is 0.13030241429805756\n",
      "epoch: 22 step: 293, loss is 0.13175731897354126\n",
      "epoch: 22 step: 294, loss is 0.13314542174339294\n",
      "epoch: 22 step: 295, loss is 0.17745563387870789\n",
      "epoch: 22 step: 296, loss is 0.14650265872478485\n",
      "epoch: 22 step: 297, loss is 0.15098688006401062\n",
      "epoch: 22 step: 298, loss is 0.16075287759304047\n",
      "epoch: 22 step: 299, loss is 0.13486026227474213\n",
      "epoch: 22 step: 300, loss is 0.35607507824897766\n",
      "epoch: 22 step: 301, loss is 0.17050856351852417\n",
      "epoch: 22 step: 302, loss is 0.1837744563817978\n",
      "epoch: 22 step: 303, loss is 0.10905499011278152\n",
      "epoch: 22 step: 304, loss is 0.111776202917099\n",
      "epoch: 22 step: 305, loss is 0.14915496110916138\n",
      "epoch: 22 step: 306, loss is 0.17796017229557037\n",
      "epoch: 22 step: 307, loss is 0.19875819981098175\n",
      "epoch: 22 step: 308, loss is 0.23905353248119354\n",
      "epoch: 22 step: 309, loss is 0.234264075756073\n",
      "epoch: 22 step: 310, loss is 0.22911937534809113\n",
      "epoch: 22 step: 311, loss is 0.15496355295181274\n",
      "epoch: 22 step: 312, loss is 0.14374281466007233\n",
      "epoch: 22 step: 313, loss is 0.21366454660892487\n",
      "epoch: 22 step: 314, loss is 0.3911043107509613\n",
      "epoch: 22 step: 315, loss is 0.11468233168125153\n",
      "epoch: 22 step: 316, loss is 0.22725950181484222\n",
      "epoch: 22 step: 317, loss is 0.16655078530311584\n",
      "epoch: 22 step: 318, loss is 0.24519990384578705\n",
      "epoch: 22 step: 319, loss is 0.29923388361930847\n",
      "epoch: 22 step: 320, loss is 0.22224073112010956\n",
      "epoch: 22 step: 321, loss is 0.2671869397163391\n",
      "epoch: 22 step: 322, loss is 0.10779305547475815\n",
      "epoch: 22 step: 323, loss is 0.08084909617900848\n",
      "epoch: 22 step: 324, loss is 0.217577263712883\n",
      "epoch: 22 step: 325, loss is 0.19809061288833618\n",
      "epoch: 22 step: 326, loss is 0.11471052467823029\n",
      "epoch: 22 step: 327, loss is 0.31848257780075073\n",
      "epoch: 22 step: 328, loss is 0.21947792172431946\n",
      "epoch: 22 step: 329, loss is 0.09900590032339096\n",
      "epoch: 22 step: 330, loss is 0.16051851212978363\n",
      "epoch: 22 step: 331, loss is 0.1410493701696396\n",
      "epoch: 22 step: 332, loss is 0.226395383477211\n",
      "epoch: 22 step: 333, loss is 0.21205541491508484\n",
      "epoch: 22 step: 334, loss is 0.181036576628685\n",
      "epoch: 22 step: 335, loss is 0.3635748624801636\n",
      "epoch: 22 step: 336, loss is 0.18412211537361145\n",
      "epoch: 22 step: 337, loss is 0.17750747501850128\n",
      "epoch: 22 step: 338, loss is 0.14226455986499786\n",
      "epoch: 22 step: 339, loss is 0.07020840793848038\n",
      "epoch: 22 step: 340, loss is 0.15547601878643036\n",
      "epoch: 22 step: 341, loss is 0.2853444516658783\n",
      "epoch: 22 step: 342, loss is 0.13589714467525482\n",
      "epoch: 22 step: 343, loss is 0.22321638464927673\n",
      "epoch: 22 step: 344, loss is 0.12040908634662628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 345, loss is 0.10146104544401169\n",
      "epoch: 22 step: 346, loss is 0.43175262212753296\n",
      "epoch: 22 step: 347, loss is 0.15712983906269073\n",
      "epoch: 22 step: 348, loss is 0.27170127630233765\n",
      "epoch: 22 step: 349, loss is 0.14370578527450562\n",
      "epoch: 22 step: 350, loss is 0.3699832558631897\n",
      "epoch: 22 step: 351, loss is 0.2536769509315491\n",
      "epoch: 22 step: 352, loss is 0.21514348685741425\n",
      "epoch: 22 step: 353, loss is 0.289915531873703\n",
      "epoch: 22 step: 354, loss is 0.2698154151439667\n",
      "epoch: 22 step: 355, loss is 0.15712809562683105\n",
      "epoch: 22 step: 356, loss is 0.20047523081302643\n",
      "epoch: 22 step: 357, loss is 0.14781233668327332\n",
      "epoch: 22 step: 358, loss is 0.10012540221214294\n",
      "epoch: 22 step: 359, loss is 0.2111445665359497\n",
      "epoch: 22 step: 360, loss is 0.30452898144721985\n",
      "epoch: 22 step: 361, loss is 0.17312657833099365\n",
      "epoch: 22 step: 362, loss is 0.23080359399318695\n",
      "epoch: 22 step: 363, loss is 0.2865457832813263\n",
      "epoch: 22 step: 364, loss is 0.10570800304412842\n",
      "epoch: 22 step: 365, loss is 0.16576384007930756\n",
      "epoch: 22 step: 366, loss is 0.2955627143383026\n",
      "epoch: 22 step: 367, loss is 0.2193402349948883\n",
      "epoch: 22 step: 368, loss is 0.23037441074848175\n",
      "epoch: 22 step: 369, loss is 0.20968036353588104\n",
      "epoch: 22 step: 370, loss is 0.08842163532972336\n",
      "epoch: 22 step: 371, loss is 0.14371269941329956\n",
      "epoch: 22 step: 372, loss is 0.2084108293056488\n",
      "epoch: 22 step: 373, loss is 0.19233007729053497\n",
      "epoch: 22 step: 374, loss is 0.13592509925365448\n",
      "epoch: 22 step: 375, loss is 0.2268141806125641\n",
      "epoch: 22 step: 376, loss is 0.15717299282550812\n",
      "epoch: 22 step: 377, loss is 0.17339548468589783\n",
      "epoch: 22 step: 378, loss is 0.1521938145160675\n",
      "epoch: 22 step: 379, loss is 0.1865972876548767\n",
      "epoch: 22 step: 380, loss is 0.1795884370803833\n",
      "epoch: 22 step: 381, loss is 0.18115736544132233\n",
      "epoch: 22 step: 382, loss is 0.1523507535457611\n",
      "epoch: 22 step: 383, loss is 0.1421833336353302\n",
      "epoch: 22 step: 384, loss is 0.1869862675666809\n",
      "epoch: 22 step: 385, loss is 0.16453611850738525\n",
      "epoch: 22 step: 386, loss is 0.1328890323638916\n",
      "epoch: 22 step: 387, loss is 0.21454563736915588\n",
      "epoch: 22 step: 388, loss is 0.22521482408046722\n",
      "epoch: 22 step: 389, loss is 0.21662656962871552\n",
      "epoch: 22 step: 390, loss is 0.2064705640077591\n",
      "epoch: 22 step: 391, loss is 0.28662577271461487\n",
      "epoch: 22 step: 392, loss is 0.3860878646373749\n",
      "epoch: 22 step: 393, loss is 0.20365341007709503\n",
      "epoch: 22 step: 394, loss is 0.1336769014596939\n",
      "epoch: 22 step: 395, loss is 0.12178655713796616\n",
      "epoch: 22 step: 396, loss is 0.15250976383686066\n",
      "epoch: 22 step: 397, loss is 0.20418420433998108\n",
      "epoch: 22 step: 398, loss is 0.22856055200099945\n",
      "epoch: 22 step: 399, loss is 0.34565088152885437\n",
      "epoch: 22 step: 400, loss is 0.14459176361560822\n",
      "epoch: 22 step: 401, loss is 0.3272319436073303\n",
      "epoch: 22 step: 402, loss is 0.21717406809329987\n",
      "epoch: 22 step: 403, loss is 0.2643744945526123\n",
      "epoch: 22 step: 404, loss is 0.18670734763145447\n",
      "epoch: 22 step: 405, loss is 0.29910627007484436\n",
      "epoch: 22 step: 406, loss is 0.35576125979423523\n",
      "epoch: 22 step: 407, loss is 0.2520509958267212\n",
      "epoch: 22 step: 408, loss is 0.21660654246807098\n",
      "epoch: 22 step: 409, loss is 0.19577179849147797\n",
      "epoch: 22 step: 410, loss is 0.42511728405952454\n",
      "epoch: 22 step: 411, loss is 0.1378045231103897\n",
      "epoch: 22 step: 412, loss is 0.4017600417137146\n",
      "epoch: 22 step: 413, loss is 0.19282320141792297\n",
      "epoch: 22 step: 414, loss is 0.07700158655643463\n",
      "epoch: 22 step: 415, loss is 0.12026774138212204\n",
      "epoch: 22 step: 416, loss is 0.10268425941467285\n",
      "epoch: 22 step: 417, loss is 0.17469750344753265\n",
      "epoch: 22 step: 418, loss is 0.1948721557855606\n",
      "epoch: 22 step: 419, loss is 0.21790869534015656\n",
      "epoch: 22 step: 420, loss is 0.2410607933998108\n",
      "epoch: 22 step: 421, loss is 0.20601744949817657\n",
      "epoch: 22 step: 422, loss is 0.27164700627326965\n",
      "epoch: 22 step: 423, loss is 0.14117510616779327\n",
      "epoch: 22 step: 424, loss is 0.19200055301189423\n",
      "epoch: 22 step: 425, loss is 0.22380822896957397\n",
      "epoch: 22 step: 426, loss is 0.14019545912742615\n",
      "epoch: 22 step: 427, loss is 0.3263661861419678\n",
      "epoch: 22 step: 428, loss is 0.21964138746261597\n",
      "epoch: 22 step: 429, loss is 0.13676728308200836\n",
      "epoch: 22 step: 430, loss is 0.2618110477924347\n",
      "epoch: 22 step: 431, loss is 0.43478673696517944\n",
      "epoch: 22 step: 432, loss is 0.19808979332447052\n",
      "epoch: 22 step: 433, loss is 0.13353043794631958\n",
      "epoch: 22 step: 434, loss is 0.15125887095928192\n",
      "epoch: 22 step: 435, loss is 0.22820527851581573\n",
      "epoch: 22 step: 436, loss is 0.184365913271904\n",
      "epoch: 22 step: 437, loss is 0.16367612779140472\n",
      "epoch: 22 step: 438, loss is 0.3568572998046875\n",
      "epoch: 22 step: 439, loss is 0.2366388589143753\n",
      "epoch: 22 step: 440, loss is 0.2039780616760254\n",
      "epoch: 22 step: 441, loss is 0.1597726047039032\n",
      "epoch: 22 step: 442, loss is 0.12027281522750854\n",
      "epoch: 22 step: 443, loss is 0.22891190648078918\n",
      "epoch: 22 step: 444, loss is 0.22739990055561066\n",
      "epoch: 22 step: 445, loss is 0.19018986821174622\n",
      "epoch: 22 step: 446, loss is 0.22192631661891937\n",
      "epoch: 22 step: 447, loss is 0.20840969681739807\n",
      "epoch: 22 step: 448, loss is 0.16174404323101044\n",
      "epoch: 22 step: 449, loss is 0.31438708305358887\n",
      "epoch: 22 step: 450, loss is 0.23739440739154816\n",
      "epoch: 22 step: 451, loss is 0.3804149031639099\n",
      "epoch: 22 step: 452, loss is 0.1299288421869278\n",
      "epoch: 22 step: 453, loss is 0.1556624472141266\n",
      "epoch: 22 step: 454, loss is 0.17766974866390228\n",
      "epoch: 22 step: 455, loss is 0.17794474959373474\n",
      "epoch: 22 step: 456, loss is 0.19753991067409515\n",
      "epoch: 22 step: 457, loss is 0.17347437143325806\n",
      "epoch: 22 step: 458, loss is 0.31408214569091797\n",
      "epoch: 22 step: 459, loss is 0.14920111000537872\n",
      "epoch: 22 step: 460, loss is 0.14662262797355652\n",
      "epoch: 22 step: 461, loss is 0.20065967738628387\n",
      "epoch: 22 step: 462, loss is 0.19290007650852203\n",
      "epoch: 22 step: 463, loss is 0.1721159964799881\n",
      "epoch: 22 step: 464, loss is 0.26064231991767883\n",
      "epoch: 22 step: 465, loss is 0.15848112106323242\n",
      "epoch: 22 step: 466, loss is 0.21513009071350098\n",
      "epoch: 22 step: 467, loss is 0.15733768045902252\n",
      "epoch: 22 step: 468, loss is 0.27263012528419495\n",
      "epoch: 22 step: 469, loss is 0.1426643580198288\n",
      "epoch: 22 step: 470, loss is 0.2318831980228424\n",
      "epoch: 22 step: 471, loss is 0.10191769152879715\n",
      "epoch: 22 step: 472, loss is 0.12356924265623093\n",
      "epoch: 22 step: 473, loss is 0.2640398442745209\n",
      "epoch: 22 step: 474, loss is 0.14953087270259857\n",
      "epoch: 22 step: 475, loss is 0.1971920132637024\n",
      "epoch: 22 step: 476, loss is 0.13487738370895386\n",
      "epoch: 22 step: 477, loss is 0.4156891703605652\n",
      "epoch: 22 step: 478, loss is 0.1349642425775528\n",
      "epoch: 22 step: 479, loss is 0.22735095024108887\n",
      "epoch: 22 step: 480, loss is 0.23143695294857025\n",
      "epoch: 22 step: 481, loss is 0.15857470035552979\n",
      "epoch: 22 step: 482, loss is 0.29512494802474976\n",
      "epoch: 22 step: 483, loss is 0.09539341926574707\n",
      "epoch: 22 step: 484, loss is 0.15650337934494019\n",
      "epoch: 22 step: 485, loss is 0.18894587457180023\n",
      "epoch: 22 step: 486, loss is 0.35398927330970764\n",
      "epoch: 22 step: 487, loss is 0.19492140412330627\n",
      "epoch: 22 step: 488, loss is 0.3373734652996063\n",
      "epoch: 22 step: 489, loss is 0.39772260189056396\n",
      "epoch: 22 step: 490, loss is 0.1301886886358261\n",
      "epoch: 22 step: 491, loss is 0.17328014969825745\n",
      "epoch: 22 step: 492, loss is 0.19834977388381958\n",
      "epoch: 22 step: 493, loss is 0.10905729979276657\n",
      "epoch: 22 step: 494, loss is 0.12446924299001694\n",
      "epoch: 22 step: 495, loss is 0.16579896211624146\n",
      "epoch: 22 step: 496, loss is 0.07475215196609497\n",
      "epoch: 22 step: 497, loss is 0.12464170902967453\n",
      "epoch: 22 step: 498, loss is 0.15727964043617249\n",
      "epoch: 22 step: 499, loss is 0.1265784054994583\n",
      "epoch: 22 step: 500, loss is 0.1509317308664322\n",
      "epoch: 22 step: 501, loss is 0.13859409093856812\n",
      "epoch: 22 step: 502, loss is 0.36273446679115295\n",
      "epoch: 22 step: 503, loss is 0.1308600902557373\n",
      "epoch: 22 step: 504, loss is 0.15977288782596588\n",
      "epoch: 22 step: 505, loss is 0.18390406668186188\n",
      "epoch: 22 step: 506, loss is 0.14140437543392181\n",
      "epoch: 22 step: 507, loss is 0.31103700399398804\n",
      "epoch: 22 step: 508, loss is 0.13471373915672302\n",
      "epoch: 22 step: 509, loss is 0.1295824497938156\n",
      "epoch: 22 step: 510, loss is 0.22721867263317108\n",
      "epoch: 22 step: 511, loss is 0.18756192922592163\n",
      "epoch: 22 step: 512, loss is 0.16134709119796753\n",
      "epoch: 22 step: 513, loss is 0.1295389086008072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 514, loss is 0.23777121305465698\n",
      "epoch: 22 step: 515, loss is 0.06560899317264557\n",
      "epoch: 22 step: 516, loss is 0.11664288491010666\n",
      "epoch: 22 step: 517, loss is 0.055227089673280716\n",
      "epoch: 22 step: 518, loss is 0.2464095801115036\n",
      "epoch: 22 step: 519, loss is 0.17531609535217285\n",
      "epoch: 22 step: 520, loss is 0.1998538225889206\n",
      "epoch: 22 step: 521, loss is 0.1658003181219101\n",
      "epoch: 22 step: 522, loss is 0.23555110394954681\n",
      "epoch: 22 step: 523, loss is 0.17262804508209229\n",
      "epoch: 22 step: 524, loss is 0.15606589615345\n",
      "epoch: 22 step: 525, loss is 0.09051766246557236\n",
      "epoch: 22 step: 526, loss is 0.07309281826019287\n",
      "epoch: 22 step: 527, loss is 0.07489094883203506\n",
      "epoch: 22 step: 528, loss is 0.29265472292900085\n",
      "epoch: 22 step: 529, loss is 0.04630427435040474\n",
      "epoch: 22 step: 530, loss is 0.16640086472034454\n",
      "epoch: 22 step: 531, loss is 0.13576436042785645\n",
      "epoch: 22 step: 532, loss is 0.24133287370204926\n",
      "epoch: 22 step: 533, loss is 0.29711484909057617\n",
      "epoch: 22 step: 534, loss is 0.2497888058423996\n",
      "epoch: 22 step: 535, loss is 0.4098983407020569\n",
      "epoch: 22 step: 536, loss is 0.1419295072555542\n",
      "epoch: 22 step: 537, loss is 0.30242711305618286\n",
      "epoch: 22 step: 538, loss is 0.37167972326278687\n",
      "epoch: 22 step: 539, loss is 0.18766294419765472\n",
      "epoch: 22 step: 540, loss is 0.21485278010368347\n",
      "epoch: 22 step: 541, loss is 0.1260591596364975\n",
      "epoch: 22 step: 542, loss is 0.10808733105659485\n",
      "epoch: 22 step: 543, loss is 0.2457898110151291\n",
      "epoch: 22 step: 544, loss is 0.1577913910150528\n",
      "epoch: 22 step: 545, loss is 0.22682157158851624\n",
      "epoch: 22 step: 546, loss is 0.19303308427333832\n",
      "epoch: 22 step: 547, loss is 0.19356206059455872\n",
      "epoch: 22 step: 548, loss is 0.06737789511680603\n",
      "epoch: 22 step: 549, loss is 0.2511293590068817\n",
      "epoch: 22 step: 550, loss is 0.13167475163936615\n",
      "epoch: 22 step: 551, loss is 0.15214815735816956\n",
      "epoch: 22 step: 552, loss is 0.1288433074951172\n",
      "epoch: 22 step: 553, loss is 0.1357957124710083\n",
      "epoch: 22 step: 554, loss is 0.12531468272209167\n",
      "epoch: 22 step: 555, loss is 0.11524603515863419\n",
      "epoch: 22 step: 556, loss is 0.1027418002486229\n",
      "epoch: 22 step: 557, loss is 0.21561826765537262\n",
      "epoch: 22 step: 558, loss is 0.1956348717212677\n",
      "epoch: 22 step: 559, loss is 0.19898194074630737\n",
      "epoch: 22 step: 560, loss is 0.11351022124290466\n",
      "epoch: 22 step: 561, loss is 0.3615034520626068\n",
      "epoch: 22 step: 562, loss is 0.33790358901023865\n",
      "epoch: 22 step: 563, loss is 0.2715735137462616\n",
      "epoch: 22 step: 564, loss is 0.08470388501882553\n",
      "epoch: 22 step: 565, loss is 0.16607370972633362\n",
      "epoch: 22 step: 566, loss is 0.25472867488861084\n",
      "epoch: 22 step: 567, loss is 0.19735673069953918\n",
      "epoch: 22 step: 568, loss is 0.12957067787647247\n",
      "epoch: 22 step: 569, loss is 0.16666628420352936\n",
      "epoch: 22 step: 570, loss is 0.30133017897605896\n",
      "epoch: 22 step: 571, loss is 0.08623319119215012\n",
      "epoch: 22 step: 572, loss is 0.21029922366142273\n",
      "epoch: 22 step: 573, loss is 0.33329445123672485\n",
      "epoch: 22 step: 574, loss is 0.23985379934310913\n",
      "epoch: 22 step: 575, loss is 0.11108283698558807\n",
      "epoch: 22 step: 576, loss is 0.08594960719347\n",
      "epoch: 22 step: 577, loss is 0.12550505995750427\n",
      "epoch: 22 step: 578, loss is 0.2280002385377884\n",
      "epoch: 22 step: 579, loss is 0.10859805345535278\n",
      "epoch: 22 step: 580, loss is 0.11959335952997208\n",
      "epoch: 22 step: 581, loss is 0.25886887311935425\n",
      "epoch: 22 step: 582, loss is 0.1683046668767929\n",
      "epoch: 22 step: 583, loss is 0.10980067402124405\n",
      "epoch: 22 step: 584, loss is 0.10174182802438736\n",
      "epoch: 22 step: 585, loss is 0.1604006588459015\n",
      "epoch: 22 step: 586, loss is 0.25645971298217773\n",
      "epoch: 22 step: 587, loss is 0.22820179164409637\n",
      "epoch: 22 step: 588, loss is 0.31895911693573\n",
      "epoch: 22 step: 589, loss is 0.35095325112342834\n",
      "epoch: 22 step: 590, loss is 0.30795225501060486\n",
      "epoch: 22 step: 591, loss is 0.17601895332336426\n",
      "epoch: 22 step: 592, loss is 0.21602261066436768\n",
      "epoch: 22 step: 593, loss is 0.20563410222530365\n",
      "epoch: 22 step: 594, loss is 0.15381793677806854\n",
      "epoch: 22 step: 595, loss is 0.25086361169815063\n",
      "epoch: 22 step: 596, loss is 0.17719124257564545\n",
      "epoch: 22 step: 597, loss is 0.14358973503112793\n",
      "epoch: 22 step: 598, loss is 0.38707464933395386\n",
      "epoch: 22 step: 599, loss is 0.19522134959697723\n",
      "epoch: 22 step: 600, loss is 0.23335567116737366\n",
      "epoch: 22 step: 601, loss is 0.19448067247867584\n",
      "epoch: 22 step: 602, loss is 0.06417890638113022\n",
      "epoch: 22 step: 603, loss is 0.32100480794906616\n",
      "epoch: 22 step: 604, loss is 0.22830259799957275\n",
      "epoch: 22 step: 605, loss is 0.1698291152715683\n",
      "epoch: 22 step: 606, loss is 0.29798710346221924\n",
      "epoch: 22 step: 607, loss is 0.1422176957130432\n",
      "epoch: 22 step: 608, loss is 0.24340613186359406\n",
      "epoch: 22 step: 609, loss is 0.3360636234283447\n",
      "epoch: 22 step: 610, loss is 0.308148056268692\n",
      "epoch: 22 step: 611, loss is 0.14491447806358337\n",
      "epoch: 22 step: 612, loss is 0.27484390139579773\n",
      "epoch: 22 step: 613, loss is 0.1539922058582306\n",
      "epoch: 22 step: 614, loss is 0.09225206077098846\n",
      "epoch: 22 step: 615, loss is 0.14884114265441895\n",
      "epoch: 22 step: 616, loss is 0.23954442143440247\n",
      "epoch: 22 step: 617, loss is 0.20787787437438965\n",
      "epoch: 22 step: 618, loss is 0.20181699097156525\n",
      "epoch: 22 step: 619, loss is 0.1740003228187561\n",
      "epoch: 22 step: 620, loss is 0.16962850093841553\n",
      "epoch: 22 step: 621, loss is 0.14364251494407654\n",
      "epoch: 22 step: 622, loss is 0.20839497447013855\n",
      "epoch: 22 step: 623, loss is 0.21564671397209167\n",
      "epoch: 22 step: 624, loss is 0.03633521869778633\n",
      "epoch: 22 step: 625, loss is 0.21119770407676697\n",
      "epoch: 22 step: 626, loss is 0.1862352192401886\n",
      "epoch: 22 step: 627, loss is 0.12588435411453247\n",
      "epoch: 22 step: 628, loss is 0.33921319246292114\n",
      "epoch: 22 step: 629, loss is 0.24876758456230164\n",
      "epoch: 22 step: 630, loss is 0.15770582854747772\n",
      "epoch: 22 step: 631, loss is 0.20015807449817657\n",
      "epoch: 22 step: 632, loss is 0.14143264293670654\n",
      "epoch: 22 step: 633, loss is 0.34600239992141724\n",
      "epoch: 22 step: 634, loss is 0.05832535773515701\n",
      "epoch: 22 step: 635, loss is 0.25239601731300354\n",
      "epoch: 22 step: 636, loss is 0.27594977617263794\n",
      "epoch: 22 step: 637, loss is 0.14447352290153503\n",
      "epoch: 22 step: 638, loss is 0.22166144847869873\n",
      "epoch: 22 step: 639, loss is 0.3172411024570465\n",
      "epoch: 22 step: 640, loss is 0.19435983896255493\n",
      "epoch: 22 step: 641, loss is 0.20043131709098816\n",
      "epoch: 22 step: 642, loss is 0.14462001621723175\n",
      "epoch: 22 step: 643, loss is 0.4772217869758606\n",
      "epoch: 22 step: 644, loss is 0.18936872482299805\n",
      "epoch: 22 step: 645, loss is 0.1503763645887375\n",
      "epoch: 22 step: 646, loss is 0.24208660423755646\n",
      "epoch: 22 step: 647, loss is 0.2945981025695801\n",
      "epoch: 22 step: 648, loss is 0.15033721923828125\n",
      "epoch: 22 step: 649, loss is 0.17093424499034882\n",
      "epoch: 22 step: 650, loss is 0.19051799178123474\n",
      "epoch: 22 step: 651, loss is 0.21897977590560913\n",
      "epoch: 22 step: 652, loss is 0.2187223881483078\n",
      "epoch: 22 step: 653, loss is 0.17198778688907623\n",
      "epoch: 22 step: 654, loss is 0.17600597441196442\n",
      "epoch: 22 step: 655, loss is 0.18650837242603302\n",
      "epoch: 22 step: 656, loss is 0.21950934827327728\n",
      "epoch: 22 step: 657, loss is 0.1579761952161789\n",
      "epoch: 22 step: 658, loss is 0.1756109893321991\n",
      "epoch: 22 step: 659, loss is 0.23689457774162292\n",
      "epoch: 22 step: 660, loss is 0.22765745222568512\n",
      "epoch: 22 step: 661, loss is 0.2598268985748291\n",
      "epoch: 22 step: 662, loss is 0.10323192179203033\n",
      "epoch: 22 step: 663, loss is 0.20316483080387115\n",
      "epoch: 22 step: 664, loss is 0.10939577221870422\n",
      "epoch: 22 step: 665, loss is 0.22270795702934265\n",
      "epoch: 22 step: 666, loss is 0.23043259978294373\n",
      "epoch: 22 step: 667, loss is 0.3673452138900757\n",
      "epoch: 22 step: 668, loss is 0.5109095573425293\n",
      "epoch: 22 step: 669, loss is 0.1772102266550064\n",
      "epoch: 22 step: 670, loss is 0.19288906455039978\n",
      "epoch: 22 step: 671, loss is 0.2281161993741989\n",
      "epoch: 22 step: 672, loss is 0.19692350924015045\n",
      "epoch: 22 step: 673, loss is 0.17393459379673004\n",
      "epoch: 22 step: 674, loss is 0.20388107001781464\n",
      "epoch: 22 step: 675, loss is 0.2743876278400421\n",
      "epoch: 22 step: 676, loss is 0.34351134300231934\n",
      "epoch: 22 step: 677, loss is 0.28131335973739624\n",
      "epoch: 22 step: 678, loss is 0.24058662354946136\n",
      "epoch: 22 step: 679, loss is 0.15019497275352478\n",
      "epoch: 22 step: 680, loss is 0.13758453726768494\n",
      "epoch: 22 step: 681, loss is 0.07718079537153244\n",
      "epoch: 22 step: 682, loss is 0.11666031926870346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 683, loss is 0.12470468133687973\n",
      "epoch: 22 step: 684, loss is 0.42712369561195374\n",
      "epoch: 22 step: 685, loss is 0.19885343313217163\n",
      "epoch: 22 step: 686, loss is 0.133092999458313\n",
      "epoch: 22 step: 687, loss is 0.2471717894077301\n",
      "epoch: 22 step: 688, loss is 0.13032497465610504\n",
      "epoch: 22 step: 689, loss is 0.2245129644870758\n",
      "epoch: 22 step: 690, loss is 0.12964333593845367\n",
      "epoch: 22 step: 691, loss is 0.10346944630146027\n",
      "epoch: 22 step: 692, loss is 0.21250467002391815\n",
      "epoch: 22 step: 693, loss is 0.13094761967658997\n",
      "epoch: 22 step: 694, loss is 0.2711327373981476\n",
      "epoch: 22 step: 695, loss is 0.22349858283996582\n",
      "epoch: 22 step: 696, loss is 0.22691725194454193\n",
      "epoch: 22 step: 697, loss is 0.18597465753555298\n",
      "epoch: 22 step: 698, loss is 0.10818373411893845\n",
      "epoch: 22 step: 699, loss is 0.2402350753545761\n",
      "epoch: 22 step: 700, loss is 0.2768600881099701\n",
      "epoch: 22 step: 701, loss is 0.16042084991931915\n",
      "epoch: 22 step: 702, loss is 0.30609411001205444\n",
      "epoch: 22 step: 703, loss is 0.15090489387512207\n",
      "epoch: 22 step: 704, loss is 0.14078809320926666\n",
      "epoch: 22 step: 705, loss is 0.16187630593776703\n",
      "epoch: 22 step: 706, loss is 0.3081432282924652\n",
      "epoch: 22 step: 707, loss is 0.23543483018875122\n",
      "epoch: 22 step: 708, loss is 0.1374492049217224\n",
      "epoch: 22 step: 709, loss is 0.28962019085884094\n",
      "epoch: 22 step: 710, loss is 0.12447027117013931\n",
      "epoch: 22 step: 711, loss is 0.22044742107391357\n",
      "epoch: 22 step: 712, loss is 0.26740211248397827\n",
      "epoch: 22 step: 713, loss is 0.136048823595047\n",
      "epoch: 22 step: 714, loss is 0.21176934242248535\n",
      "epoch: 22 step: 715, loss is 0.2560790181159973\n",
      "epoch: 22 step: 716, loss is 0.2583373188972473\n",
      "epoch: 22 step: 717, loss is 0.06344602257013321\n",
      "epoch: 22 step: 718, loss is 0.19058647751808167\n",
      "epoch: 22 step: 719, loss is 0.30333444476127625\n",
      "epoch: 22 step: 720, loss is 0.13373905420303345\n",
      "epoch: 22 step: 721, loss is 0.21696718037128448\n",
      "epoch: 22 step: 722, loss is 0.09484370052814484\n",
      "epoch: 22 step: 723, loss is 0.15958130359649658\n",
      "epoch: 22 step: 724, loss is 0.2624834477901459\n",
      "epoch: 22 step: 725, loss is 0.20666521787643433\n",
      "epoch: 22 step: 726, loss is 0.15699973702430725\n",
      "epoch: 22 step: 727, loss is 0.1424872726202011\n",
      "epoch: 22 step: 728, loss is 0.20312872529029846\n",
      "epoch: 22 step: 729, loss is 0.17049260437488556\n",
      "epoch: 22 step: 730, loss is 0.2883877456188202\n",
      "epoch: 22 step: 731, loss is 0.10406484454870224\n",
      "epoch: 22 step: 732, loss is 0.2400064617395401\n",
      "epoch: 22 step: 733, loss is 0.17697152495384216\n",
      "epoch: 22 step: 734, loss is 0.13825280964374542\n",
      "epoch: 22 step: 735, loss is 0.07600869983434677\n",
      "epoch: 22 step: 736, loss is 0.25791433453559875\n",
      "epoch: 22 step: 737, loss is 0.23262546956539154\n",
      "epoch: 22 step: 738, loss is 0.2442145049571991\n",
      "epoch: 22 step: 739, loss is 0.11457948386669159\n",
      "epoch: 22 step: 740, loss is 0.20872174203395844\n",
      "epoch: 22 step: 741, loss is 0.30494794249534607\n",
      "epoch: 22 step: 742, loss is 0.2792764902114868\n",
      "epoch: 22 step: 743, loss is 0.22673508524894714\n",
      "epoch: 22 step: 744, loss is 0.1957017183303833\n",
      "epoch: 22 step: 745, loss is 0.23185394704341888\n",
      "epoch: 22 step: 746, loss is 0.30197131633758545\n",
      "epoch: 22 step: 747, loss is 0.27998560667037964\n",
      "epoch: 22 step: 748, loss is 0.1519862860441208\n",
      "epoch: 22 step: 749, loss is 0.2515588700771332\n",
      "epoch: 22 step: 750, loss is 0.29514560103416443\n",
      "epoch: 22 step: 751, loss is 0.26344868540763855\n",
      "epoch: 22 step: 752, loss is 0.2823821008205414\n",
      "epoch: 22 step: 753, loss is 0.08830240368843079\n",
      "epoch: 22 step: 754, loss is 0.16027753055095673\n",
      "epoch: 22 step: 755, loss is 0.1368809938430786\n",
      "epoch: 22 step: 756, loss is 0.1633165180683136\n",
      "epoch: 22 step: 757, loss is 0.11494650691747665\n",
      "epoch: 22 step: 758, loss is 0.209853395819664\n",
      "epoch: 22 step: 759, loss is 0.11809568107128143\n",
      "epoch: 22 step: 760, loss is 0.13965463638305664\n",
      "epoch: 22 step: 761, loss is 0.18601828813552856\n",
      "epoch: 22 step: 762, loss is 0.17003928124904633\n",
      "epoch: 22 step: 763, loss is 0.1786157339811325\n",
      "epoch: 22 step: 764, loss is 0.17037715017795563\n",
      "epoch: 22 step: 765, loss is 0.2856397032737732\n",
      "epoch: 22 step: 766, loss is 0.21627753973007202\n",
      "epoch: 22 step: 767, loss is 0.16500379145145416\n",
      "epoch: 22 step: 768, loss is 0.3054411709308624\n",
      "epoch: 22 step: 769, loss is 0.20326289534568787\n",
      "epoch: 22 step: 770, loss is 0.048991452902555466\n",
      "epoch: 22 step: 771, loss is 0.33197376132011414\n",
      "epoch: 22 step: 772, loss is 0.1610431671142578\n",
      "epoch: 22 step: 773, loss is 0.09742359817028046\n",
      "epoch: 22 step: 774, loss is 0.261332243680954\n",
      "epoch: 22 step: 775, loss is 0.1976812332868576\n",
      "epoch: 22 step: 776, loss is 0.10510499030351639\n",
      "epoch: 22 step: 777, loss is 0.22851085662841797\n",
      "epoch: 22 step: 778, loss is 0.1426599621772766\n",
      "epoch: 22 step: 779, loss is 0.07905732840299606\n",
      "epoch: 22 step: 780, loss is 0.16260933876037598\n",
      "epoch: 22 step: 781, loss is 0.11487863212823868\n",
      "epoch: 22 step: 782, loss is 0.1292043924331665\n",
      "epoch: 22 step: 783, loss is 0.10741962492465973\n",
      "epoch: 22 step: 784, loss is 0.10928361862897873\n",
      "epoch: 22 step: 785, loss is 0.3224962651729584\n",
      "epoch: 22 step: 786, loss is 0.2554361820220947\n",
      "epoch: 22 step: 787, loss is 0.2802213430404663\n",
      "epoch: 22 step: 788, loss is 0.20646654069423676\n",
      "epoch: 22 step: 789, loss is 0.17770850658416748\n",
      "epoch: 22 step: 790, loss is 0.21826106309890747\n",
      "epoch: 22 step: 791, loss is 0.3079667389392853\n",
      "epoch: 22 step: 792, loss is 0.2214008867740631\n",
      "epoch: 22 step: 793, loss is 0.16976138949394226\n",
      "epoch: 22 step: 794, loss is 0.11976686865091324\n",
      "epoch: 22 step: 795, loss is 0.16388486325740814\n",
      "epoch: 22 step: 796, loss is 0.0968322679400444\n",
      "epoch: 22 step: 797, loss is 0.13954061269760132\n",
      "epoch: 22 step: 798, loss is 0.18794110417366028\n",
      "epoch: 22 step: 799, loss is 0.16217106580734253\n",
      "epoch: 22 step: 800, loss is 0.10593876987695694\n",
      "epoch: 22 step: 801, loss is 0.1755114048719406\n",
      "epoch: 22 step: 802, loss is 0.24152931571006775\n",
      "epoch: 22 step: 803, loss is 0.2440766543149948\n",
      "epoch: 22 step: 804, loss is 0.2524118423461914\n",
      "epoch: 22 step: 805, loss is 0.08359985798597336\n",
      "epoch: 22 step: 806, loss is 0.29927217960357666\n",
      "epoch: 22 step: 807, loss is 0.12477932125329971\n",
      "epoch: 22 step: 808, loss is 0.07516271620988846\n",
      "epoch: 22 step: 809, loss is 0.09053647518157959\n",
      "epoch: 22 step: 810, loss is 0.16317425668239594\n",
      "epoch: 22 step: 811, loss is 0.18805712461471558\n",
      "epoch: 22 step: 812, loss is 0.18891698122024536\n",
      "epoch: 22 step: 813, loss is 0.16975809633731842\n",
      "epoch: 22 step: 814, loss is 0.15216290950775146\n",
      "epoch: 22 step: 815, loss is 0.19633713364601135\n",
      "epoch: 22 step: 816, loss is 0.11339546740055084\n",
      "epoch: 22 step: 817, loss is 0.17827819287776947\n",
      "epoch: 22 step: 818, loss is 0.16318751871585846\n",
      "epoch: 22 step: 819, loss is 0.12103335559368134\n",
      "epoch: 22 step: 820, loss is 0.24576039612293243\n",
      "epoch: 22 step: 821, loss is 0.21778546273708344\n",
      "epoch: 22 step: 822, loss is 0.3627839982509613\n",
      "epoch: 22 step: 823, loss is 0.1734214723110199\n",
      "epoch: 22 step: 824, loss is 0.10500603914260864\n",
      "epoch: 22 step: 825, loss is 0.36643823981285095\n",
      "epoch: 22 step: 826, loss is 0.2018631100654602\n",
      "epoch: 22 step: 827, loss is 0.227857306599617\n",
      "epoch: 22 step: 828, loss is 0.27471354603767395\n",
      "epoch: 22 step: 829, loss is 0.20487980544567108\n",
      "epoch: 22 step: 830, loss is 0.2558877766132355\n",
      "epoch: 22 step: 831, loss is 0.23157702386379242\n",
      "epoch: 22 step: 832, loss is 0.17804007232189178\n",
      "epoch: 22 step: 833, loss is 0.18625949323177338\n",
      "epoch: 22 step: 834, loss is 0.1390896737575531\n",
      "epoch: 22 step: 835, loss is 0.24692745506763458\n",
      "epoch: 22 step: 836, loss is 0.2024819701910019\n",
      "epoch: 22 step: 837, loss is 0.14669053256511688\n",
      "epoch: 22 step: 838, loss is 0.09560295194387436\n",
      "epoch: 22 step: 839, loss is 0.2928411364555359\n",
      "epoch: 22 step: 840, loss is 0.20416177809238434\n",
      "epoch: 22 step: 841, loss is 0.06410123407840729\n",
      "epoch: 22 step: 842, loss is 0.1841701865196228\n",
      "epoch: 22 step: 843, loss is 0.21927021443843842\n",
      "epoch: 22 step: 844, loss is 0.13231749832630157\n",
      "epoch: 22 step: 845, loss is 0.2844741642475128\n",
      "epoch: 22 step: 846, loss is 0.19148525595664978\n",
      "epoch: 22 step: 847, loss is 0.24857905507087708\n",
      "epoch: 22 step: 848, loss is 0.1011580377817154\n",
      "epoch: 22 step: 849, loss is 0.39081642031669617\n",
      "epoch: 22 step: 850, loss is 0.3100740611553192\n",
      "epoch: 22 step: 851, loss is 0.17864570021629333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 852, loss is 0.20822633802890778\n",
      "epoch: 22 step: 853, loss is 0.1956772357225418\n",
      "epoch: 22 step: 854, loss is 0.20680712163448334\n",
      "epoch: 22 step: 855, loss is 0.20017226040363312\n",
      "epoch: 22 step: 856, loss is 0.15653397142887115\n",
      "epoch: 22 step: 857, loss is 0.5234708786010742\n",
      "epoch: 22 step: 858, loss is 0.20315173268318176\n",
      "epoch: 22 step: 859, loss is 0.23087725043296814\n",
      "epoch: 22 step: 860, loss is 0.2518165409564972\n",
      "epoch: 22 step: 861, loss is 0.10512147843837738\n",
      "epoch: 22 step: 862, loss is 0.1717822253704071\n",
      "epoch: 22 step: 863, loss is 0.17178428173065186\n",
      "epoch: 22 step: 864, loss is 0.17155925929546356\n",
      "epoch: 22 step: 865, loss is 0.13790155947208405\n",
      "epoch: 22 step: 866, loss is 0.17801067233085632\n",
      "epoch: 22 step: 867, loss is 0.1532866507768631\n",
      "epoch: 22 step: 868, loss is 0.1843831092119217\n",
      "epoch: 22 step: 869, loss is 0.24178555607795715\n",
      "epoch: 22 step: 870, loss is 0.13732922077178955\n",
      "epoch: 22 step: 871, loss is 0.07616004347801208\n",
      "epoch: 22 step: 872, loss is 0.1643563210964203\n",
      "epoch: 22 step: 873, loss is 0.1408628672361374\n",
      "epoch: 22 step: 874, loss is 0.24204309284687042\n",
      "epoch: 22 step: 875, loss is 0.14148758351802826\n",
      "epoch: 22 step: 876, loss is 0.16238513588905334\n",
      "epoch: 22 step: 877, loss is 0.17028482258319855\n",
      "epoch: 22 step: 878, loss is 0.18205037713050842\n",
      "epoch: 22 step: 879, loss is 0.2951914966106415\n",
      "epoch: 22 step: 880, loss is 0.1069636419415474\n",
      "epoch: 22 step: 881, loss is 0.12395048141479492\n",
      "epoch: 22 step: 882, loss is 0.1780049055814743\n",
      "epoch: 22 step: 883, loss is 0.16210684180259705\n",
      "epoch: 22 step: 884, loss is 0.16511838138103485\n",
      "epoch: 22 step: 885, loss is 0.11177994310855865\n",
      "epoch: 22 step: 886, loss is 0.28725719451904297\n",
      "epoch: 22 step: 887, loss is 0.36143186688423157\n",
      "epoch: 22 step: 888, loss is 0.21808098256587982\n",
      "epoch: 22 step: 889, loss is 0.10853942483663559\n",
      "epoch: 22 step: 890, loss is 0.3360353112220764\n",
      "epoch: 22 step: 891, loss is 0.22035230696201324\n",
      "epoch: 22 step: 892, loss is 0.11350792646408081\n",
      "epoch: 22 step: 893, loss is 0.1696942001581192\n",
      "epoch: 22 step: 894, loss is 0.2942461371421814\n",
      "epoch: 22 step: 895, loss is 0.18842774629592896\n",
      "epoch: 22 step: 896, loss is 0.17084945738315582\n",
      "epoch: 22 step: 897, loss is 0.05741732567548752\n",
      "epoch: 22 step: 898, loss is 0.22027648985385895\n",
      "epoch: 22 step: 899, loss is 0.3921082019805908\n",
      "epoch: 22 step: 900, loss is 0.06475137919187546\n",
      "epoch: 22 step: 901, loss is 0.09979835152626038\n",
      "epoch: 22 step: 902, loss is 0.18751955032348633\n",
      "epoch: 22 step: 903, loss is 0.14698383212089539\n",
      "epoch: 22 step: 904, loss is 0.16370663046836853\n",
      "epoch: 22 step: 905, loss is 0.26070353388786316\n",
      "epoch: 22 step: 906, loss is 0.25791987776756287\n",
      "epoch: 22 step: 907, loss is 0.2039121836423874\n",
      "epoch: 22 step: 908, loss is 0.25080594420433044\n",
      "epoch: 22 step: 909, loss is 0.2764987647533417\n",
      "epoch: 22 step: 910, loss is 0.33611607551574707\n",
      "epoch: 22 step: 911, loss is 0.257365882396698\n",
      "epoch: 22 step: 912, loss is 0.2740378975868225\n",
      "epoch: 22 step: 913, loss is 0.23364484310150146\n",
      "epoch: 22 step: 914, loss is 0.29991215467453003\n",
      "epoch: 22 step: 915, loss is 0.1277640163898468\n",
      "epoch: 22 step: 916, loss is 0.23680363595485687\n",
      "epoch: 22 step: 917, loss is 0.23883210122585297\n",
      "epoch: 22 step: 918, loss is 0.19226481020450592\n",
      "epoch: 22 step: 919, loss is 0.3096473217010498\n",
      "epoch: 22 step: 920, loss is 0.16487713158130646\n",
      "epoch: 22 step: 921, loss is 0.10414142161607742\n",
      "epoch: 22 step: 922, loss is 0.2354721575975418\n",
      "epoch: 22 step: 923, loss is 0.3286181688308716\n",
      "epoch: 22 step: 924, loss is 0.34132418036460876\n",
      "epoch: 22 step: 925, loss is 0.1447761058807373\n",
      "epoch: 22 step: 926, loss is 0.2920238673686981\n",
      "epoch: 22 step: 927, loss is 0.21847748756408691\n",
      "epoch: 22 step: 928, loss is 0.20476359128952026\n",
      "epoch: 22 step: 929, loss is 0.2107875496149063\n",
      "epoch: 22 step: 930, loss is 0.1333903819322586\n",
      "epoch: 22 step: 931, loss is 0.2577276825904846\n",
      "epoch: 22 step: 932, loss is 0.15105457603931427\n",
      "epoch: 22 step: 933, loss is 0.12573084235191345\n",
      "epoch: 22 step: 934, loss is 0.11832920461893082\n",
      "epoch: 22 step: 935, loss is 0.1882881075143814\n",
      "epoch: 22 step: 936, loss is 0.12987424433231354\n",
      "epoch: 22 step: 937, loss is 0.11828792840242386\n",
      "epoch: 23 step: 1, loss is 0.1438814401626587\n",
      "epoch: 23 step: 2, loss is 0.17296147346496582\n",
      "epoch: 23 step: 3, loss is 0.15896835923194885\n",
      "epoch: 23 step: 4, loss is 0.2008492350578308\n",
      "epoch: 23 step: 5, loss is 0.20466232299804688\n",
      "epoch: 23 step: 6, loss is 0.20603182911872864\n",
      "epoch: 23 step: 7, loss is 0.1158696785569191\n",
      "epoch: 23 step: 8, loss is 0.18214210867881775\n",
      "epoch: 23 step: 9, loss is 0.2624007761478424\n",
      "epoch: 23 step: 10, loss is 0.1346929967403412\n",
      "epoch: 23 step: 11, loss is 0.20776130259037018\n",
      "epoch: 23 step: 12, loss is 0.18867246806621552\n",
      "epoch: 23 step: 13, loss is 0.11845533549785614\n",
      "epoch: 23 step: 14, loss is 0.1599949449300766\n",
      "epoch: 23 step: 15, loss is 0.11962362378835678\n",
      "epoch: 23 step: 16, loss is 0.16553005576133728\n",
      "epoch: 23 step: 17, loss is 0.0840231329202652\n",
      "epoch: 23 step: 18, loss is 0.12155244499444962\n",
      "epoch: 23 step: 19, loss is 0.13583269715309143\n",
      "epoch: 23 step: 20, loss is 0.14473456144332886\n",
      "epoch: 23 step: 21, loss is 0.2383226454257965\n",
      "epoch: 23 step: 22, loss is 0.23641720414161682\n",
      "epoch: 23 step: 23, loss is 0.15580123662948608\n",
      "epoch: 23 step: 24, loss is 0.14729690551757812\n",
      "epoch: 23 step: 25, loss is 0.23421239852905273\n",
      "epoch: 23 step: 26, loss is 0.20291917026042938\n",
      "epoch: 23 step: 27, loss is 0.1245773583650589\n",
      "epoch: 23 step: 28, loss is 0.08395034074783325\n",
      "epoch: 23 step: 29, loss is 0.13215358555316925\n",
      "epoch: 23 step: 30, loss is 0.22112682461738586\n",
      "epoch: 23 step: 31, loss is 0.1994180828332901\n",
      "epoch: 23 step: 32, loss is 0.24876731634140015\n",
      "epoch: 23 step: 33, loss is 0.10366209596395493\n",
      "epoch: 23 step: 34, loss is 0.25676313042640686\n",
      "epoch: 23 step: 35, loss is 0.09669926762580872\n",
      "epoch: 23 step: 36, loss is 0.33993828296661377\n",
      "epoch: 23 step: 37, loss is 0.17952314019203186\n",
      "epoch: 23 step: 38, loss is 0.279702365398407\n",
      "epoch: 23 step: 39, loss is 0.1389215588569641\n",
      "epoch: 23 step: 40, loss is 0.24585023522377014\n",
      "epoch: 23 step: 41, loss is 0.2550334334373474\n",
      "epoch: 23 step: 42, loss is 0.16341958940029144\n",
      "epoch: 23 step: 43, loss is 0.23383836448192596\n",
      "epoch: 23 step: 44, loss is 0.13113854825496674\n",
      "epoch: 23 step: 45, loss is 0.1823582947254181\n",
      "epoch: 23 step: 46, loss is 0.14924821257591248\n",
      "epoch: 23 step: 47, loss is 0.23513557016849518\n",
      "epoch: 23 step: 48, loss is 0.18824175000190735\n",
      "epoch: 23 step: 49, loss is 0.21070629358291626\n",
      "epoch: 23 step: 50, loss is 0.07571793347597122\n",
      "epoch: 23 step: 51, loss is 0.21035325527191162\n",
      "epoch: 23 step: 52, loss is 0.11987712979316711\n",
      "epoch: 23 step: 53, loss is 0.19375842809677124\n",
      "epoch: 23 step: 54, loss is 0.16514883935451508\n",
      "epoch: 23 step: 55, loss is 0.2387876659631729\n",
      "epoch: 23 step: 56, loss is 0.0879027396440506\n",
      "epoch: 23 step: 57, loss is 0.14263612031936646\n",
      "epoch: 23 step: 58, loss is 0.15827038884162903\n",
      "epoch: 23 step: 59, loss is 0.10714113712310791\n",
      "epoch: 23 step: 60, loss is 0.35967737436294556\n",
      "epoch: 23 step: 61, loss is 0.23313888907432556\n",
      "epoch: 23 step: 62, loss is 0.21585305035114288\n",
      "epoch: 23 step: 63, loss is 0.12585891783237457\n",
      "epoch: 23 step: 64, loss is 0.15407593548297882\n",
      "epoch: 23 step: 65, loss is 0.12786182761192322\n",
      "epoch: 23 step: 66, loss is 0.27907535433769226\n",
      "epoch: 23 step: 67, loss is 0.19285792112350464\n",
      "epoch: 23 step: 68, loss is 0.18769869208335876\n",
      "epoch: 23 step: 69, loss is 0.1787269562482834\n",
      "epoch: 23 step: 70, loss is 0.1630895435810089\n",
      "epoch: 23 step: 71, loss is 0.24101874232292175\n",
      "epoch: 23 step: 72, loss is 0.11195223778486252\n",
      "epoch: 23 step: 73, loss is 0.19683882594108582\n",
      "epoch: 23 step: 74, loss is 0.25099045038223267\n",
      "epoch: 23 step: 75, loss is 0.17064110934734344\n",
      "epoch: 23 step: 76, loss is 0.24259835481643677\n",
      "epoch: 23 step: 77, loss is 0.17137576639652252\n",
      "epoch: 23 step: 78, loss is 0.15255896747112274\n",
      "epoch: 23 step: 79, loss is 0.19706392288208008\n",
      "epoch: 23 step: 80, loss is 0.11547284573316574\n",
      "epoch: 23 step: 81, loss is 0.1176900640130043\n",
      "epoch: 23 step: 82, loss is 0.22527030110359192\n",
      "epoch: 23 step: 83, loss is 0.12494059652090073\n",
      "epoch: 23 step: 84, loss is 0.2058933675289154\n",
      "epoch: 23 step: 85, loss is 0.07403959333896637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 86, loss is 0.302580326795578\n",
      "epoch: 23 step: 87, loss is 0.21876852214336395\n",
      "epoch: 23 step: 88, loss is 0.24941325187683105\n",
      "epoch: 23 step: 89, loss is 0.20304112136363983\n",
      "epoch: 23 step: 90, loss is 0.1493915617465973\n",
      "epoch: 23 step: 91, loss is 0.19261984527111053\n",
      "epoch: 23 step: 92, loss is 0.2533280551433563\n",
      "epoch: 23 step: 93, loss is 0.23553626239299774\n",
      "epoch: 23 step: 94, loss is 0.08387278020381927\n",
      "epoch: 23 step: 95, loss is 0.13921132683753967\n",
      "epoch: 23 step: 96, loss is 0.3377484083175659\n",
      "epoch: 23 step: 97, loss is 0.17671626806259155\n",
      "epoch: 23 step: 98, loss is 0.25453758239746094\n",
      "epoch: 23 step: 99, loss is 0.1145486906170845\n",
      "epoch: 23 step: 100, loss is 0.14493541419506073\n",
      "epoch: 23 step: 101, loss is 0.16071900725364685\n",
      "epoch: 23 step: 102, loss is 0.19265815615653992\n",
      "epoch: 23 step: 103, loss is 0.15843459963798523\n",
      "epoch: 23 step: 104, loss is 0.18871648609638214\n",
      "epoch: 23 step: 105, loss is 0.12806275486946106\n",
      "epoch: 23 step: 106, loss is 0.22891829907894135\n",
      "epoch: 23 step: 107, loss is 0.13475406169891357\n",
      "epoch: 23 step: 108, loss is 0.29070988297462463\n",
      "epoch: 23 step: 109, loss is 0.07602692395448685\n",
      "epoch: 23 step: 110, loss is 0.3530190885066986\n",
      "epoch: 23 step: 111, loss is 0.07647720724344254\n",
      "epoch: 23 step: 112, loss is 0.16464677453041077\n",
      "epoch: 23 step: 113, loss is 0.2514020502567291\n",
      "epoch: 23 step: 114, loss is 0.17185594141483307\n",
      "epoch: 23 step: 115, loss is 0.16014282405376434\n",
      "epoch: 23 step: 116, loss is 0.22647443413734436\n",
      "epoch: 23 step: 117, loss is 0.17785444855690002\n",
      "epoch: 23 step: 118, loss is 0.3918105363845825\n",
      "epoch: 23 step: 119, loss is 0.1400572955608368\n",
      "epoch: 23 step: 120, loss is 0.15964826941490173\n",
      "epoch: 23 step: 121, loss is 0.24649639427661896\n",
      "epoch: 23 step: 122, loss is 0.14313265681266785\n",
      "epoch: 23 step: 123, loss is 0.1615665704011917\n",
      "epoch: 23 step: 124, loss is 0.2283036857843399\n",
      "epoch: 23 step: 125, loss is 0.27694928646087646\n",
      "epoch: 23 step: 126, loss is 0.241577610373497\n",
      "epoch: 23 step: 127, loss is 0.10808995366096497\n",
      "epoch: 23 step: 128, loss is 0.37415942549705505\n",
      "epoch: 23 step: 129, loss is 0.16271939873695374\n",
      "epoch: 23 step: 130, loss is 0.2839028835296631\n",
      "epoch: 23 step: 131, loss is 0.1560143530368805\n",
      "epoch: 23 step: 132, loss is 0.2076740562915802\n",
      "epoch: 23 step: 133, loss is 0.20892708003520966\n",
      "epoch: 23 step: 134, loss is 0.2653864026069641\n",
      "epoch: 23 step: 135, loss is 0.17960195243358612\n",
      "epoch: 23 step: 136, loss is 0.2678094804286957\n",
      "epoch: 23 step: 137, loss is 0.217412531375885\n",
      "epoch: 23 step: 138, loss is 0.20494312047958374\n",
      "epoch: 23 step: 139, loss is 0.386685311794281\n",
      "epoch: 23 step: 140, loss is 0.18815909326076508\n",
      "epoch: 23 step: 141, loss is 0.4288472533226013\n",
      "epoch: 23 step: 142, loss is 0.2388009876012802\n",
      "epoch: 23 step: 143, loss is 0.20876282453536987\n",
      "epoch: 23 step: 144, loss is 0.16130748391151428\n",
      "epoch: 23 step: 145, loss is 0.08813519775867462\n",
      "epoch: 23 step: 146, loss is 0.3148387670516968\n",
      "epoch: 23 step: 147, loss is 0.25454628467559814\n",
      "epoch: 23 step: 148, loss is 0.3842454254627228\n",
      "epoch: 23 step: 149, loss is 0.0724036917090416\n",
      "epoch: 23 step: 150, loss is 0.14420169591903687\n",
      "epoch: 23 step: 151, loss is 0.05657235160470009\n",
      "epoch: 23 step: 152, loss is 0.19688506424427032\n",
      "epoch: 23 step: 153, loss is 0.19555561244487762\n",
      "epoch: 23 step: 154, loss is 0.49016180634498596\n",
      "epoch: 23 step: 155, loss is 0.17514783143997192\n",
      "epoch: 23 step: 156, loss is 0.1786019653081894\n",
      "epoch: 23 step: 157, loss is 0.07994338124990463\n",
      "epoch: 23 step: 158, loss is 0.14871619641780853\n",
      "epoch: 23 step: 159, loss is 0.14689496159553528\n",
      "epoch: 23 step: 160, loss is 0.1186886727809906\n",
      "epoch: 23 step: 161, loss is 0.17975221574306488\n",
      "epoch: 23 step: 162, loss is 0.29540160298347473\n",
      "epoch: 23 step: 163, loss is 0.13906200230121613\n",
      "epoch: 23 step: 164, loss is 0.09557896107435226\n",
      "epoch: 23 step: 165, loss is 0.2622963786125183\n",
      "epoch: 23 step: 166, loss is 0.16460491716861725\n",
      "epoch: 23 step: 167, loss is 0.12745147943496704\n",
      "epoch: 23 step: 168, loss is 0.08382054418325424\n",
      "epoch: 23 step: 169, loss is 0.3364076018333435\n",
      "epoch: 23 step: 170, loss is 0.31691962480545044\n",
      "epoch: 23 step: 171, loss is 0.19094663858413696\n",
      "epoch: 23 step: 172, loss is 0.20048163831233978\n",
      "epoch: 23 step: 173, loss is 0.19036968052387238\n",
      "epoch: 23 step: 174, loss is 0.1943160891532898\n",
      "epoch: 23 step: 175, loss is 0.11453959345817566\n",
      "epoch: 23 step: 176, loss is 0.10803346335887909\n",
      "epoch: 23 step: 177, loss is 0.18069304525852203\n",
      "epoch: 23 step: 178, loss is 0.28921329975128174\n",
      "epoch: 23 step: 179, loss is 0.05724155902862549\n",
      "epoch: 23 step: 180, loss is 0.09624182432889938\n",
      "epoch: 23 step: 181, loss is 0.2651199400424957\n",
      "epoch: 23 step: 182, loss is 0.27902767062187195\n",
      "epoch: 23 step: 183, loss is 0.1651836782693863\n",
      "epoch: 23 step: 184, loss is 0.13075603544712067\n",
      "epoch: 23 step: 185, loss is 0.2286914885044098\n",
      "epoch: 23 step: 186, loss is 0.1229865550994873\n",
      "epoch: 23 step: 187, loss is 0.14470167458057404\n",
      "epoch: 23 step: 188, loss is 0.2886650562286377\n",
      "epoch: 23 step: 189, loss is 0.3067986071109772\n",
      "epoch: 23 step: 190, loss is 0.23305204510688782\n",
      "epoch: 23 step: 191, loss is 0.3708459734916687\n",
      "epoch: 23 step: 192, loss is 0.3032163083553314\n",
      "epoch: 23 step: 193, loss is 0.31396758556365967\n",
      "epoch: 23 step: 194, loss is 0.08849450945854187\n",
      "epoch: 23 step: 195, loss is 0.07869233936071396\n",
      "epoch: 23 step: 196, loss is 0.21360741555690765\n",
      "epoch: 23 step: 197, loss is 0.18749751150608063\n",
      "epoch: 23 step: 198, loss is 0.2075634002685547\n",
      "epoch: 23 step: 199, loss is 0.15805332362651825\n",
      "epoch: 23 step: 200, loss is 0.17102603614330292\n",
      "epoch: 23 step: 201, loss is 0.17669589817523956\n",
      "epoch: 23 step: 202, loss is 0.17817559838294983\n",
      "epoch: 23 step: 203, loss is 0.19022448360919952\n",
      "epoch: 23 step: 204, loss is 0.2289714515209198\n",
      "epoch: 23 step: 205, loss is 0.22176121175289154\n",
      "epoch: 23 step: 206, loss is 0.12593956291675568\n",
      "epoch: 23 step: 207, loss is 0.28514739871025085\n",
      "epoch: 23 step: 208, loss is 0.25425994396209717\n",
      "epoch: 23 step: 209, loss is 0.2677449882030487\n",
      "epoch: 23 step: 210, loss is 0.05872653052210808\n",
      "epoch: 23 step: 211, loss is 0.1954195499420166\n",
      "epoch: 23 step: 212, loss is 0.2676764726638794\n",
      "epoch: 23 step: 213, loss is 0.3139905631542206\n",
      "epoch: 23 step: 214, loss is 0.08282666653394699\n",
      "epoch: 23 step: 215, loss is 0.16974462568759918\n",
      "epoch: 23 step: 216, loss is 0.16873252391815186\n",
      "epoch: 23 step: 217, loss is 0.14709696173667908\n",
      "epoch: 23 step: 218, loss is 0.2692348062992096\n",
      "epoch: 23 step: 219, loss is 0.29268360137939453\n",
      "epoch: 23 step: 220, loss is 0.23955245316028595\n",
      "epoch: 23 step: 221, loss is 0.15103332698345184\n",
      "epoch: 23 step: 222, loss is 0.1924172192811966\n",
      "epoch: 23 step: 223, loss is 0.14836600422859192\n",
      "epoch: 23 step: 224, loss is 0.17274776101112366\n",
      "epoch: 23 step: 225, loss is 0.27411413192749023\n",
      "epoch: 23 step: 226, loss is 0.3472173511981964\n",
      "epoch: 23 step: 227, loss is 0.23209451138973236\n",
      "epoch: 23 step: 228, loss is 0.19701161980628967\n",
      "epoch: 23 step: 229, loss is 0.10562165081501007\n",
      "epoch: 23 step: 230, loss is 0.11623572558164597\n",
      "epoch: 23 step: 231, loss is 0.164567232131958\n",
      "epoch: 23 step: 232, loss is 0.29932901263237\n",
      "epoch: 23 step: 233, loss is 0.27044036984443665\n",
      "epoch: 23 step: 234, loss is 0.29525890946388245\n",
      "epoch: 23 step: 235, loss is 0.09326435625553131\n",
      "epoch: 23 step: 236, loss is 0.20254194736480713\n",
      "epoch: 23 step: 237, loss is 0.0836474746465683\n",
      "epoch: 23 step: 238, loss is 0.23096291720867157\n",
      "epoch: 23 step: 239, loss is 0.22086381912231445\n",
      "epoch: 23 step: 240, loss is 0.2081500142812729\n",
      "epoch: 23 step: 241, loss is 0.39373597502708435\n",
      "epoch: 23 step: 242, loss is 0.11686301231384277\n",
      "epoch: 23 step: 243, loss is 0.17296938598155975\n",
      "epoch: 23 step: 244, loss is 0.1468636691570282\n",
      "epoch: 23 step: 245, loss is 0.18288156390190125\n",
      "epoch: 23 step: 246, loss is 0.16510136425495148\n",
      "epoch: 23 step: 247, loss is 0.1419549584388733\n",
      "epoch: 23 step: 248, loss is 0.09705020487308502\n",
      "epoch: 23 step: 249, loss is 0.12914854288101196\n",
      "epoch: 23 step: 250, loss is 0.18778277933597565\n",
      "epoch: 23 step: 251, loss is 0.12437686324119568\n",
      "epoch: 23 step: 252, loss is 0.17660000920295715\n",
      "epoch: 23 step: 253, loss is 0.09769140183925629\n",
      "epoch: 23 step: 254, loss is 0.2247651070356369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 255, loss is 0.1865474432706833\n",
      "epoch: 23 step: 256, loss is 0.0986270010471344\n",
      "epoch: 23 step: 257, loss is 0.10462116450071335\n",
      "epoch: 23 step: 258, loss is 0.09532430022954941\n",
      "epoch: 23 step: 259, loss is 0.16374148428440094\n",
      "epoch: 23 step: 260, loss is 0.14480409026145935\n",
      "epoch: 23 step: 261, loss is 0.18451184034347534\n",
      "epoch: 23 step: 262, loss is 0.23866046965122223\n",
      "epoch: 23 step: 263, loss is 0.23459024727344513\n",
      "epoch: 23 step: 264, loss is 0.23651523888111115\n",
      "epoch: 23 step: 265, loss is 0.16997452080249786\n",
      "epoch: 23 step: 266, loss is 0.13862581551074982\n",
      "epoch: 23 step: 267, loss is 0.19604890048503876\n",
      "epoch: 23 step: 268, loss is 0.0732521340250969\n",
      "epoch: 23 step: 269, loss is 0.29888278245925903\n",
      "epoch: 23 step: 270, loss is 0.3043319880962372\n",
      "epoch: 23 step: 271, loss is 0.22841587662696838\n",
      "epoch: 23 step: 272, loss is 0.15097767114639282\n",
      "epoch: 23 step: 273, loss is 0.09818831086158752\n",
      "epoch: 23 step: 274, loss is 0.1374957114458084\n",
      "epoch: 23 step: 275, loss is 0.17456763982772827\n",
      "epoch: 23 step: 276, loss is 0.2823486328125\n",
      "epoch: 23 step: 277, loss is 0.1848313957452774\n",
      "epoch: 23 step: 278, loss is 0.1094919741153717\n",
      "epoch: 23 step: 279, loss is 0.18411466479301453\n",
      "epoch: 23 step: 280, loss is 0.26281771063804626\n",
      "epoch: 23 step: 281, loss is 0.2022557556629181\n",
      "epoch: 23 step: 282, loss is 0.12618248164653778\n",
      "epoch: 23 step: 283, loss is 0.15932317078113556\n",
      "epoch: 23 step: 284, loss is 0.17038558423519135\n",
      "epoch: 23 step: 285, loss is 0.2558741867542267\n",
      "epoch: 23 step: 286, loss is 0.13382433354854584\n",
      "epoch: 23 step: 287, loss is 0.22950589656829834\n",
      "epoch: 23 step: 288, loss is 0.17299684882164001\n",
      "epoch: 23 step: 289, loss is 0.197551429271698\n",
      "epoch: 23 step: 290, loss is 0.11390282213687897\n",
      "epoch: 23 step: 291, loss is 0.05130491778254509\n",
      "epoch: 23 step: 292, loss is 0.4977244436740875\n",
      "epoch: 23 step: 293, loss is 0.31357428431510925\n",
      "epoch: 23 step: 294, loss is 0.1020745113492012\n",
      "epoch: 23 step: 295, loss is 0.11246578395366669\n",
      "epoch: 23 step: 296, loss is 0.15003834664821625\n",
      "epoch: 23 step: 297, loss is 0.2616775929927826\n",
      "epoch: 23 step: 298, loss is 0.12644337117671967\n",
      "epoch: 23 step: 299, loss is 0.21456128358840942\n",
      "epoch: 23 step: 300, loss is 0.1356625258922577\n",
      "epoch: 23 step: 301, loss is 0.31584182381629944\n",
      "epoch: 23 step: 302, loss is 0.16486485302448273\n",
      "epoch: 23 step: 303, loss is 0.10339926928281784\n",
      "epoch: 23 step: 304, loss is 0.16826018691062927\n",
      "epoch: 23 step: 305, loss is 0.18342798948287964\n",
      "epoch: 23 step: 306, loss is 0.21608059108257294\n",
      "epoch: 23 step: 307, loss is 0.22347138822078705\n",
      "epoch: 23 step: 308, loss is 0.14729155600070953\n",
      "epoch: 23 step: 309, loss is 0.1779545247554779\n",
      "epoch: 23 step: 310, loss is 0.2463313341140747\n",
      "epoch: 23 step: 311, loss is 0.24278154969215393\n",
      "epoch: 23 step: 312, loss is 0.1877179741859436\n",
      "epoch: 23 step: 313, loss is 0.31349435448646545\n",
      "epoch: 23 step: 314, loss is 0.1859775334596634\n",
      "epoch: 23 step: 315, loss is 0.16190087795257568\n",
      "epoch: 23 step: 316, loss is 0.24470464885234833\n",
      "epoch: 23 step: 317, loss is 0.16516360640525818\n",
      "epoch: 23 step: 318, loss is 0.38420209288597107\n",
      "epoch: 23 step: 319, loss is 0.12495662271976471\n",
      "epoch: 23 step: 320, loss is 0.2139977216720581\n",
      "epoch: 23 step: 321, loss is 0.07485349476337433\n",
      "epoch: 23 step: 322, loss is 0.12427626550197601\n",
      "epoch: 23 step: 323, loss is 0.16208644211292267\n",
      "epoch: 23 step: 324, loss is 0.22572866082191467\n",
      "epoch: 23 step: 325, loss is 0.11988916993141174\n",
      "epoch: 23 step: 326, loss is 0.13237862288951874\n",
      "epoch: 23 step: 327, loss is 0.15673241019248962\n",
      "epoch: 23 step: 328, loss is 0.20388437807559967\n",
      "epoch: 23 step: 329, loss is 0.374967098236084\n",
      "epoch: 23 step: 330, loss is 0.18949392437934875\n",
      "epoch: 23 step: 331, loss is 0.30558398365974426\n",
      "epoch: 23 step: 332, loss is 0.18117912113666534\n",
      "epoch: 23 step: 333, loss is 0.15286293625831604\n",
      "epoch: 23 step: 334, loss is 0.2397688627243042\n",
      "epoch: 23 step: 335, loss is 0.09467604011297226\n",
      "epoch: 23 step: 336, loss is 0.3584855794906616\n",
      "epoch: 23 step: 337, loss is 0.21924255788326263\n",
      "epoch: 23 step: 338, loss is 0.09234140068292618\n",
      "epoch: 23 step: 339, loss is 0.13762855529785156\n",
      "epoch: 23 step: 340, loss is 0.10681531578302383\n",
      "epoch: 23 step: 341, loss is 0.15748538076877594\n",
      "epoch: 23 step: 342, loss is 0.19374559819698334\n",
      "epoch: 23 step: 343, loss is 0.16523584723472595\n",
      "epoch: 23 step: 344, loss is 0.29868194460868835\n",
      "epoch: 23 step: 345, loss is 0.1843143105506897\n",
      "epoch: 23 step: 346, loss is 0.14418500661849976\n",
      "epoch: 23 step: 347, loss is 0.36614561080932617\n",
      "epoch: 23 step: 348, loss is 0.2598085105419159\n",
      "epoch: 23 step: 349, loss is 0.17253310978412628\n",
      "epoch: 23 step: 350, loss is 0.1558365523815155\n",
      "epoch: 23 step: 351, loss is 0.1950754076242447\n",
      "epoch: 23 step: 352, loss is 0.424480676651001\n",
      "epoch: 23 step: 353, loss is 0.22085154056549072\n",
      "epoch: 23 step: 354, loss is 0.16162200272083282\n",
      "epoch: 23 step: 355, loss is 0.23707455396652222\n",
      "epoch: 23 step: 356, loss is 0.25667604804039\n",
      "epoch: 23 step: 357, loss is 0.1963786780834198\n",
      "epoch: 23 step: 358, loss is 0.32046109437942505\n",
      "epoch: 23 step: 359, loss is 0.23205727338790894\n",
      "epoch: 23 step: 360, loss is 0.1918308436870575\n",
      "epoch: 23 step: 361, loss is 0.18722455203533173\n",
      "epoch: 23 step: 362, loss is 0.23961131274700165\n",
      "epoch: 23 step: 363, loss is 0.17955170571804047\n",
      "epoch: 23 step: 364, loss is 0.10993298143148422\n",
      "epoch: 23 step: 365, loss is 0.15967704355716705\n",
      "epoch: 23 step: 366, loss is 0.11797880381345749\n",
      "epoch: 23 step: 367, loss is 0.17867203056812286\n",
      "epoch: 23 step: 368, loss is 0.17438197135925293\n",
      "epoch: 23 step: 369, loss is 0.3672362267971039\n",
      "epoch: 23 step: 370, loss is 0.059217941015958786\n",
      "epoch: 23 step: 371, loss is 0.050100021064281464\n",
      "epoch: 23 step: 372, loss is 0.27666357159614563\n",
      "epoch: 23 step: 373, loss is 0.05883590877056122\n",
      "epoch: 23 step: 374, loss is 0.14955852925777435\n",
      "epoch: 23 step: 375, loss is 0.1829470694065094\n",
      "epoch: 23 step: 376, loss is 0.20157332718372345\n",
      "epoch: 23 step: 377, loss is 0.20585188269615173\n",
      "epoch: 23 step: 378, loss is 0.18534015119075775\n",
      "epoch: 23 step: 379, loss is 0.19033482670783997\n",
      "epoch: 23 step: 380, loss is 0.15865673124790192\n",
      "epoch: 23 step: 381, loss is 0.11364927142858505\n",
      "epoch: 23 step: 382, loss is 0.233847975730896\n",
      "epoch: 23 step: 383, loss is 0.17222662270069122\n",
      "epoch: 23 step: 384, loss is 0.24747277796268463\n",
      "epoch: 23 step: 385, loss is 0.16505646705627441\n",
      "epoch: 23 step: 386, loss is 0.20402728021144867\n",
      "epoch: 23 step: 387, loss is 0.1760329157114029\n",
      "epoch: 23 step: 388, loss is 0.17628487944602966\n",
      "epoch: 23 step: 389, loss is 0.2554846704006195\n",
      "epoch: 23 step: 390, loss is 0.2440529614686966\n",
      "epoch: 23 step: 391, loss is 0.18330515921115875\n",
      "epoch: 23 step: 392, loss is 0.2141260802745819\n",
      "epoch: 23 step: 393, loss is 0.07815143465995789\n",
      "epoch: 23 step: 394, loss is 0.12843270599842072\n",
      "epoch: 23 step: 395, loss is 0.10352664440870285\n",
      "epoch: 23 step: 396, loss is 0.09237262606620789\n",
      "epoch: 23 step: 397, loss is 0.16548605263233185\n",
      "epoch: 23 step: 398, loss is 0.12400627881288528\n",
      "epoch: 23 step: 399, loss is 0.13348498940467834\n",
      "epoch: 23 step: 400, loss is 0.3862990438938141\n",
      "epoch: 23 step: 401, loss is 0.22034917771816254\n",
      "epoch: 23 step: 402, loss is 0.13048377633094788\n",
      "epoch: 23 step: 403, loss is 0.13747338950634003\n",
      "epoch: 23 step: 404, loss is 0.27169331908226013\n",
      "epoch: 23 step: 405, loss is 0.34175509214401245\n",
      "epoch: 23 step: 406, loss is 0.18693025410175323\n",
      "epoch: 23 step: 407, loss is 0.15102364122867584\n",
      "epoch: 23 step: 408, loss is 0.16415324807167053\n",
      "epoch: 23 step: 409, loss is 0.13506457209587097\n",
      "epoch: 23 step: 410, loss is 0.14918212592601776\n",
      "epoch: 23 step: 411, loss is 0.18469177186489105\n",
      "epoch: 23 step: 412, loss is 0.28294554352760315\n",
      "epoch: 23 step: 413, loss is 0.3086608350276947\n",
      "epoch: 23 step: 414, loss is 0.2167343944311142\n",
      "epoch: 23 step: 415, loss is 0.2172592282295227\n",
      "epoch: 23 step: 416, loss is 0.235114187002182\n",
      "epoch: 23 step: 417, loss is 0.29121237993240356\n",
      "epoch: 23 step: 418, loss is 0.14483028650283813\n",
      "epoch: 23 step: 419, loss is 0.0933794155716896\n",
      "epoch: 23 step: 420, loss is 0.24579709768295288\n",
      "epoch: 23 step: 421, loss is 0.11873909085988998\n",
      "epoch: 23 step: 422, loss is 0.2484719455242157\n",
      "epoch: 23 step: 423, loss is 0.15673835575580597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 424, loss is 0.21284906566143036\n",
      "epoch: 23 step: 425, loss is 0.23915383219718933\n",
      "epoch: 23 step: 426, loss is 0.20228002965450287\n",
      "epoch: 23 step: 427, loss is 0.19898834824562073\n",
      "epoch: 23 step: 428, loss is 0.11524409800767899\n",
      "epoch: 23 step: 429, loss is 0.18975616991519928\n",
      "epoch: 23 step: 430, loss is 0.18472521007061005\n",
      "epoch: 23 step: 431, loss is 0.15464237332344055\n",
      "epoch: 23 step: 432, loss is 0.11494391411542892\n",
      "epoch: 23 step: 433, loss is 0.229139044880867\n",
      "epoch: 23 step: 434, loss is 0.15984539687633514\n",
      "epoch: 23 step: 435, loss is 0.16634270548820496\n",
      "epoch: 23 step: 436, loss is 0.2408275008201599\n",
      "epoch: 23 step: 437, loss is 0.2175016701221466\n",
      "epoch: 23 step: 438, loss is 0.1617308408021927\n",
      "epoch: 23 step: 439, loss is 0.19415979087352753\n",
      "epoch: 23 step: 440, loss is 0.12104837596416473\n",
      "epoch: 23 step: 441, loss is 0.2516862154006958\n",
      "epoch: 23 step: 442, loss is 0.15399402379989624\n",
      "epoch: 23 step: 443, loss is 0.17299948632717133\n",
      "epoch: 23 step: 444, loss is 0.18259043991565704\n",
      "epoch: 23 step: 445, loss is 0.23763208091259003\n",
      "epoch: 23 step: 446, loss is 0.19628848135471344\n",
      "epoch: 23 step: 447, loss is 0.22618822753429413\n",
      "epoch: 23 step: 448, loss is 0.13691572844982147\n",
      "epoch: 23 step: 449, loss is 0.14993375539779663\n",
      "epoch: 23 step: 450, loss is 0.15356706082820892\n",
      "epoch: 23 step: 451, loss is 0.23016130924224854\n",
      "epoch: 23 step: 452, loss is 0.19795677065849304\n",
      "epoch: 23 step: 453, loss is 0.12595249712467194\n",
      "epoch: 23 step: 454, loss is 0.13353805243968964\n",
      "epoch: 23 step: 455, loss is 0.16635921597480774\n",
      "epoch: 23 step: 456, loss is 0.30944758653640747\n",
      "epoch: 23 step: 457, loss is 0.22568978369235992\n",
      "epoch: 23 step: 458, loss is 0.20091305673122406\n",
      "epoch: 23 step: 459, loss is 0.19544348120689392\n",
      "epoch: 23 step: 460, loss is 0.1591651439666748\n",
      "epoch: 23 step: 461, loss is 0.114846371114254\n",
      "epoch: 23 step: 462, loss is 0.11118120700120926\n",
      "epoch: 23 step: 463, loss is 0.17633162438869476\n",
      "epoch: 23 step: 464, loss is 0.3978492319583893\n",
      "epoch: 23 step: 465, loss is 0.10793282836675644\n",
      "epoch: 23 step: 466, loss is 0.11387855559587479\n",
      "epoch: 23 step: 467, loss is 0.12423796206712723\n",
      "epoch: 23 step: 468, loss is 0.13937941193580627\n",
      "epoch: 23 step: 469, loss is 0.175844207406044\n",
      "epoch: 23 step: 470, loss is 0.3432711064815521\n",
      "epoch: 23 step: 471, loss is 0.14145232737064362\n",
      "epoch: 23 step: 472, loss is 0.22513122856616974\n",
      "epoch: 23 step: 473, loss is 0.10355009138584137\n",
      "epoch: 23 step: 474, loss is 0.080964595079422\n",
      "epoch: 23 step: 475, loss is 0.30976203083992004\n",
      "epoch: 23 step: 476, loss is 0.16385935246944427\n",
      "epoch: 23 step: 477, loss is 0.12770645320415497\n",
      "epoch: 23 step: 478, loss is 0.19332432746887207\n",
      "epoch: 23 step: 479, loss is 0.09769010543823242\n",
      "epoch: 23 step: 480, loss is 0.20018574595451355\n",
      "epoch: 23 step: 481, loss is 0.16122297942638397\n",
      "epoch: 23 step: 482, loss is 0.1632387340068817\n",
      "epoch: 23 step: 483, loss is 0.1384538859128952\n",
      "epoch: 23 step: 484, loss is 0.21898604929447174\n",
      "epoch: 23 step: 485, loss is 0.19363778829574585\n",
      "epoch: 23 step: 486, loss is 0.1969725787639618\n",
      "epoch: 23 step: 487, loss is 0.2590014934539795\n",
      "epoch: 23 step: 488, loss is 0.23652853071689606\n",
      "epoch: 23 step: 489, loss is 0.21827389299869537\n",
      "epoch: 23 step: 490, loss is 0.20462024211883545\n",
      "epoch: 23 step: 491, loss is 0.1995437741279602\n",
      "epoch: 23 step: 492, loss is 0.2552514970302582\n",
      "epoch: 23 step: 493, loss is 0.1595917046070099\n",
      "epoch: 23 step: 494, loss is 0.17542295157909393\n",
      "epoch: 23 step: 495, loss is 0.2846006453037262\n",
      "epoch: 23 step: 496, loss is 0.11546816676855087\n",
      "epoch: 23 step: 497, loss is 0.19069968163967133\n",
      "epoch: 23 step: 498, loss is 0.09327930212020874\n",
      "epoch: 23 step: 499, loss is 0.29158321022987366\n",
      "epoch: 23 step: 500, loss is 0.13724373281002045\n",
      "epoch: 23 step: 501, loss is 0.23457218706607819\n",
      "epoch: 23 step: 502, loss is 0.14959031343460083\n",
      "epoch: 23 step: 503, loss is 0.20972940325737\n",
      "epoch: 23 step: 504, loss is 0.26683109998703003\n",
      "epoch: 23 step: 505, loss is 0.21786002814769745\n",
      "epoch: 23 step: 506, loss is 0.42831116914749146\n",
      "epoch: 23 step: 507, loss is 0.19519096612930298\n",
      "epoch: 23 step: 508, loss is 0.1333734393119812\n",
      "epoch: 23 step: 509, loss is 0.30222317576408386\n",
      "epoch: 23 step: 510, loss is 0.149952694773674\n",
      "epoch: 23 step: 511, loss is 0.11677909642457962\n",
      "epoch: 23 step: 512, loss is 0.21646325290203094\n",
      "epoch: 23 step: 513, loss is 0.16299769282341003\n",
      "epoch: 23 step: 514, loss is 0.1589372754096985\n",
      "epoch: 23 step: 515, loss is 0.12913474440574646\n",
      "epoch: 23 step: 516, loss is 0.21100975573062897\n",
      "epoch: 23 step: 517, loss is 0.3633577227592468\n",
      "epoch: 23 step: 518, loss is 0.2591652274131775\n",
      "epoch: 23 step: 519, loss is 0.1999078094959259\n",
      "epoch: 23 step: 520, loss is 0.14980880916118622\n",
      "epoch: 23 step: 521, loss is 0.3066750466823578\n",
      "epoch: 23 step: 522, loss is 0.1666083037853241\n",
      "epoch: 23 step: 523, loss is 0.08745861053466797\n",
      "epoch: 23 step: 524, loss is 0.16222257912158966\n",
      "epoch: 23 step: 525, loss is 0.2257022261619568\n",
      "epoch: 23 step: 526, loss is 0.290833055973053\n",
      "epoch: 23 step: 527, loss is 0.21461188793182373\n",
      "epoch: 23 step: 528, loss is 0.19845235347747803\n",
      "epoch: 23 step: 529, loss is 0.25262171030044556\n",
      "epoch: 23 step: 530, loss is 0.3202304244041443\n",
      "epoch: 23 step: 531, loss is 0.11108715832233429\n",
      "epoch: 23 step: 532, loss is 0.1445464938879013\n",
      "epoch: 23 step: 533, loss is 0.08475892245769501\n",
      "epoch: 23 step: 534, loss is 0.06558013707399368\n",
      "epoch: 23 step: 535, loss is 0.09443417191505432\n",
      "epoch: 23 step: 536, loss is 0.1235009953379631\n",
      "epoch: 23 step: 537, loss is 0.27966997027397156\n",
      "epoch: 23 step: 538, loss is 0.15834194421768188\n",
      "epoch: 23 step: 539, loss is 0.09666476398706436\n",
      "epoch: 23 step: 540, loss is 0.1630600392818451\n",
      "epoch: 23 step: 541, loss is 0.25802889466285706\n",
      "epoch: 23 step: 542, loss is 0.1840924322605133\n",
      "epoch: 23 step: 543, loss is 0.15858007967472076\n",
      "epoch: 23 step: 544, loss is 0.10975185036659241\n",
      "epoch: 23 step: 545, loss is 0.1312439739704132\n",
      "epoch: 23 step: 546, loss is 0.2910579442977905\n",
      "epoch: 23 step: 547, loss is 0.18109847605228424\n",
      "epoch: 23 step: 548, loss is 0.17719075083732605\n",
      "epoch: 23 step: 549, loss is 0.26374000310897827\n",
      "epoch: 23 step: 550, loss is 0.19702394306659698\n",
      "epoch: 23 step: 551, loss is 0.23131069540977478\n",
      "epoch: 23 step: 552, loss is 0.28774505853652954\n",
      "epoch: 23 step: 553, loss is 0.1628175973892212\n",
      "epoch: 23 step: 554, loss is 0.3250553607940674\n",
      "epoch: 23 step: 555, loss is 0.10104487836360931\n",
      "epoch: 23 step: 556, loss is 0.3069663643836975\n",
      "epoch: 23 step: 557, loss is 0.1950397491455078\n",
      "epoch: 23 step: 558, loss is 0.22348962724208832\n",
      "epoch: 23 step: 559, loss is 0.09263787418603897\n",
      "epoch: 23 step: 560, loss is 0.17713069915771484\n",
      "epoch: 23 step: 561, loss is 0.14862853288650513\n",
      "epoch: 23 step: 562, loss is 0.09422086179256439\n",
      "epoch: 23 step: 563, loss is 0.1661282777786255\n",
      "epoch: 23 step: 564, loss is 0.2778209447860718\n",
      "epoch: 23 step: 565, loss is 0.18854941427707672\n",
      "epoch: 23 step: 566, loss is 0.22821223735809326\n",
      "epoch: 23 step: 567, loss is 0.05107715353369713\n",
      "epoch: 23 step: 568, loss is 0.1719561517238617\n",
      "epoch: 23 step: 569, loss is 0.17252354323863983\n",
      "epoch: 23 step: 570, loss is 0.10313640534877777\n",
      "epoch: 23 step: 571, loss is 0.10008066147565842\n",
      "epoch: 23 step: 572, loss is 0.16677416861057281\n",
      "epoch: 23 step: 573, loss is 0.3296739459037781\n",
      "epoch: 23 step: 574, loss is 0.15017150342464447\n",
      "epoch: 23 step: 575, loss is 0.2672194242477417\n",
      "epoch: 23 step: 576, loss is 0.16229262948036194\n",
      "epoch: 23 step: 577, loss is 0.16598331928253174\n",
      "epoch: 23 step: 578, loss is 0.10427992790937424\n",
      "epoch: 23 step: 579, loss is 0.2037145495414734\n",
      "epoch: 23 step: 580, loss is 0.11797568947076797\n",
      "epoch: 23 step: 581, loss is 0.2473747730255127\n",
      "epoch: 23 step: 582, loss is 0.18019871413707733\n",
      "epoch: 23 step: 583, loss is 0.18986967206001282\n",
      "epoch: 23 step: 584, loss is 0.14220210909843445\n",
      "epoch: 23 step: 585, loss is 0.1468961089849472\n",
      "epoch: 23 step: 586, loss is 0.11664120852947235\n",
      "epoch: 23 step: 587, loss is 0.2865540087223053\n",
      "epoch: 23 step: 588, loss is 0.11687009036540985\n",
      "epoch: 23 step: 589, loss is 0.1819511502981186\n",
      "epoch: 23 step: 590, loss is 0.12367311865091324\n",
      "epoch: 23 step: 591, loss is 0.25112926959991455\n",
      "epoch: 23 step: 592, loss is 0.15826775133609772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 593, loss is 0.12443314492702484\n",
      "epoch: 23 step: 594, loss is 0.3685563802719116\n",
      "epoch: 23 step: 595, loss is 0.29212653636932373\n",
      "epoch: 23 step: 596, loss is 0.09733722358942032\n",
      "epoch: 23 step: 597, loss is 0.0976557657122612\n",
      "epoch: 23 step: 598, loss is 0.11470669507980347\n",
      "epoch: 23 step: 599, loss is 0.17289572954177856\n",
      "epoch: 23 step: 600, loss is 0.08517765253782272\n",
      "epoch: 23 step: 601, loss is 0.20891259610652924\n",
      "epoch: 23 step: 602, loss is 0.2234044224023819\n",
      "epoch: 23 step: 603, loss is 0.2730720341205597\n",
      "epoch: 23 step: 604, loss is 0.32494956254959106\n",
      "epoch: 23 step: 605, loss is 0.20669014751911163\n",
      "epoch: 23 step: 606, loss is 0.19066579639911652\n",
      "epoch: 23 step: 607, loss is 0.1452927589416504\n",
      "epoch: 23 step: 608, loss is 0.18553470075130463\n",
      "epoch: 23 step: 609, loss is 0.14065317809581757\n",
      "epoch: 23 step: 610, loss is 0.19362246990203857\n",
      "epoch: 23 step: 611, loss is 0.15433526039123535\n",
      "epoch: 23 step: 612, loss is 0.16155555844306946\n",
      "epoch: 23 step: 613, loss is 0.10742459446191788\n",
      "epoch: 23 step: 614, loss is 0.16564232110977173\n",
      "epoch: 23 step: 615, loss is 0.3256278336048126\n",
      "epoch: 23 step: 616, loss is 0.12781868875026703\n",
      "epoch: 23 step: 617, loss is 0.1965065449476242\n",
      "epoch: 23 step: 618, loss is 0.30330562591552734\n",
      "epoch: 23 step: 619, loss is 0.08358132839202881\n",
      "epoch: 23 step: 620, loss is 0.18507465720176697\n",
      "epoch: 23 step: 621, loss is 0.2122317999601364\n",
      "epoch: 23 step: 622, loss is 0.21697400510311127\n",
      "epoch: 23 step: 623, loss is 0.12747612595558167\n",
      "epoch: 23 step: 624, loss is 0.19947011768817902\n",
      "epoch: 23 step: 625, loss is 0.10574740916490555\n",
      "epoch: 23 step: 626, loss is 0.1689460426568985\n",
      "epoch: 23 step: 627, loss is 0.4712361693382263\n",
      "epoch: 23 step: 628, loss is 0.11823432147502899\n",
      "epoch: 23 step: 629, loss is 0.062082502990961075\n",
      "epoch: 23 step: 630, loss is 0.16618908941745758\n",
      "epoch: 23 step: 631, loss is 0.18678177893161774\n",
      "epoch: 23 step: 632, loss is 0.296706885099411\n",
      "epoch: 23 step: 633, loss is 0.2359507530927658\n",
      "epoch: 23 step: 634, loss is 0.26327958703041077\n",
      "epoch: 23 step: 635, loss is 0.1494903862476349\n",
      "epoch: 23 step: 636, loss is 0.2845959961414337\n",
      "epoch: 23 step: 637, loss is 0.2591038942337036\n",
      "epoch: 23 step: 638, loss is 0.3488233685493469\n",
      "epoch: 23 step: 639, loss is 0.08507168292999268\n",
      "epoch: 23 step: 640, loss is 0.23333881795406342\n",
      "epoch: 23 step: 641, loss is 0.22263126075267792\n",
      "epoch: 23 step: 642, loss is 0.16966886818408966\n",
      "epoch: 23 step: 643, loss is 0.15422464907169342\n",
      "epoch: 23 step: 644, loss is 0.1806344985961914\n",
      "epoch: 23 step: 645, loss is 0.38530686497688293\n",
      "epoch: 23 step: 646, loss is 0.2587644159793854\n",
      "epoch: 23 step: 647, loss is 0.25745704770088196\n",
      "epoch: 23 step: 648, loss is 0.5084521770477295\n",
      "epoch: 23 step: 649, loss is 0.32937803864479065\n",
      "epoch: 23 step: 650, loss is 0.25483623147010803\n",
      "epoch: 23 step: 651, loss is 0.24536992609500885\n",
      "epoch: 23 step: 652, loss is 0.13349853456020355\n",
      "epoch: 23 step: 653, loss is 0.14704936742782593\n",
      "epoch: 23 step: 654, loss is 0.32499271631240845\n",
      "epoch: 23 step: 655, loss is 0.1311636120080948\n",
      "epoch: 23 step: 656, loss is 0.05943794175982475\n",
      "epoch: 23 step: 657, loss is 0.16125035285949707\n",
      "epoch: 23 step: 658, loss is 0.1831279695034027\n",
      "epoch: 23 step: 659, loss is 0.20832037925720215\n",
      "epoch: 23 step: 660, loss is 0.18847893178462982\n",
      "epoch: 23 step: 661, loss is 0.10346902906894684\n",
      "epoch: 23 step: 662, loss is 0.2950180470943451\n",
      "epoch: 23 step: 663, loss is 0.14460861682891846\n",
      "epoch: 23 step: 664, loss is 0.10859671235084534\n",
      "epoch: 23 step: 665, loss is 0.13550592958927155\n",
      "epoch: 23 step: 666, loss is 0.2907469570636749\n",
      "epoch: 23 step: 667, loss is 0.38212162256240845\n",
      "epoch: 23 step: 668, loss is 0.14190658926963806\n",
      "epoch: 23 step: 669, loss is 0.08026553690433502\n",
      "epoch: 23 step: 670, loss is 0.22394078969955444\n",
      "epoch: 23 step: 671, loss is 0.2769615054130554\n",
      "epoch: 23 step: 672, loss is 0.1942315548658371\n",
      "epoch: 23 step: 673, loss is 0.1885969340801239\n",
      "epoch: 23 step: 674, loss is 0.18072627484798431\n",
      "epoch: 23 step: 675, loss is 0.20493876934051514\n",
      "epoch: 23 step: 676, loss is 0.23669390380382538\n",
      "epoch: 23 step: 677, loss is 0.26372674107551575\n",
      "epoch: 23 step: 678, loss is 0.15615861117839813\n",
      "epoch: 23 step: 679, loss is 0.16026988625526428\n",
      "epoch: 23 step: 680, loss is 0.14509260654449463\n",
      "epoch: 23 step: 681, loss is 0.12782205641269684\n",
      "epoch: 23 step: 682, loss is 0.09276267141103745\n",
      "epoch: 23 step: 683, loss is 0.12009737640619278\n",
      "epoch: 23 step: 684, loss is 0.23978503048419952\n",
      "epoch: 23 step: 685, loss is 0.15200969576835632\n",
      "epoch: 23 step: 686, loss is 0.3119865357875824\n",
      "epoch: 23 step: 687, loss is 0.20754781365394592\n",
      "epoch: 23 step: 688, loss is 0.24699582159519196\n",
      "epoch: 23 step: 689, loss is 0.18096698820590973\n",
      "epoch: 23 step: 690, loss is 0.17677663266658783\n",
      "epoch: 23 step: 691, loss is 0.13645263016223907\n",
      "epoch: 23 step: 692, loss is 0.14405007660388947\n",
      "epoch: 23 step: 693, loss is 0.18308383226394653\n",
      "epoch: 23 step: 694, loss is 0.21045856177806854\n",
      "epoch: 23 step: 695, loss is 0.3310406804084778\n",
      "epoch: 23 step: 696, loss is 0.3128364384174347\n",
      "epoch: 23 step: 697, loss is 0.2927688658237457\n",
      "epoch: 23 step: 698, loss is 0.16907313466072083\n",
      "epoch: 23 step: 699, loss is 0.17050056159496307\n",
      "epoch: 23 step: 700, loss is 0.20634499192237854\n",
      "epoch: 23 step: 701, loss is 0.1468157321214676\n",
      "epoch: 23 step: 702, loss is 0.30974745750427246\n",
      "epoch: 23 step: 703, loss is 0.16268432140350342\n",
      "epoch: 23 step: 704, loss is 0.16418781876564026\n",
      "epoch: 23 step: 705, loss is 0.24555106461048126\n",
      "epoch: 23 step: 706, loss is 0.18676309287548065\n",
      "epoch: 23 step: 707, loss is 0.20485393702983856\n",
      "epoch: 23 step: 708, loss is 0.22598408162593842\n",
      "epoch: 23 step: 709, loss is 0.12535898387432098\n",
      "epoch: 23 step: 710, loss is 0.2265751212835312\n",
      "epoch: 23 step: 711, loss is 0.16645097732543945\n",
      "epoch: 23 step: 712, loss is 0.1616901010274887\n",
      "epoch: 23 step: 713, loss is 0.2693280577659607\n",
      "epoch: 23 step: 714, loss is 0.20827718079090118\n",
      "epoch: 23 step: 715, loss is 0.10629037022590637\n",
      "epoch: 23 step: 716, loss is 0.2605433762073517\n",
      "epoch: 23 step: 717, loss is 0.20367833971977234\n",
      "epoch: 23 step: 718, loss is 0.2554132044315338\n",
      "epoch: 23 step: 719, loss is 0.23341712355613708\n",
      "epoch: 23 step: 720, loss is 0.12552595138549805\n",
      "epoch: 23 step: 721, loss is 0.17535828053951263\n",
      "epoch: 23 step: 722, loss is 0.14128081500530243\n",
      "epoch: 23 step: 723, loss is 0.17272944748401642\n",
      "epoch: 23 step: 724, loss is 0.11631467193365097\n",
      "epoch: 23 step: 725, loss is 0.16484643518924713\n",
      "epoch: 23 step: 726, loss is 0.20899957418441772\n",
      "epoch: 23 step: 727, loss is 0.130499467253685\n",
      "epoch: 23 step: 728, loss is 0.20206508040428162\n",
      "epoch: 23 step: 729, loss is 0.08786017447710037\n",
      "epoch: 23 step: 730, loss is 0.10443167388439178\n",
      "epoch: 23 step: 731, loss is 0.1569848358631134\n",
      "epoch: 23 step: 732, loss is 0.15572099387645721\n",
      "epoch: 23 step: 733, loss is 0.12274640798568726\n",
      "epoch: 23 step: 734, loss is 0.23524188995361328\n",
      "epoch: 23 step: 735, loss is 0.29177454113960266\n",
      "epoch: 23 step: 736, loss is 0.19708535075187683\n",
      "epoch: 23 step: 737, loss is 0.07927830517292023\n",
      "epoch: 23 step: 738, loss is 0.2916170656681061\n",
      "epoch: 23 step: 739, loss is 0.40621039271354675\n",
      "epoch: 23 step: 740, loss is 0.2668953835964203\n",
      "epoch: 23 step: 741, loss is 0.09289292991161346\n",
      "epoch: 23 step: 742, loss is 0.1808486431837082\n",
      "epoch: 23 step: 743, loss is 0.1213223785161972\n",
      "epoch: 23 step: 744, loss is 0.20387393236160278\n",
      "epoch: 23 step: 745, loss is 0.09364817291498184\n",
      "epoch: 23 step: 746, loss is 0.1502857506275177\n",
      "epoch: 23 step: 747, loss is 0.15749773383140564\n",
      "epoch: 23 step: 748, loss is 0.18310227990150452\n",
      "epoch: 23 step: 749, loss is 0.2550676465034485\n",
      "epoch: 23 step: 750, loss is 0.2244768887758255\n",
      "epoch: 23 step: 751, loss is 0.25903967022895813\n",
      "epoch: 23 step: 752, loss is 0.06541825830936432\n",
      "epoch: 23 step: 753, loss is 0.1831163763999939\n",
      "epoch: 23 step: 754, loss is 0.2582513391971588\n",
      "epoch: 23 step: 755, loss is 0.14976616203784943\n",
      "epoch: 23 step: 756, loss is 0.2031872272491455\n",
      "epoch: 23 step: 757, loss is 0.14977844059467316\n",
      "epoch: 23 step: 758, loss is 0.36381596326828003\n",
      "epoch: 23 step: 759, loss is 0.21267014741897583\n",
      "epoch: 23 step: 760, loss is 0.23062776029109955\n",
      "epoch: 23 step: 761, loss is 0.20160099864006042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 762, loss is 0.15318842232227325\n",
      "epoch: 23 step: 763, loss is 0.4417768716812134\n",
      "epoch: 23 step: 764, loss is 0.20256806910037994\n",
      "epoch: 23 step: 765, loss is 0.34920650720596313\n",
      "epoch: 23 step: 766, loss is 0.3224632143974304\n",
      "epoch: 23 step: 767, loss is 0.37953755259513855\n",
      "epoch: 23 step: 768, loss is 0.33790409564971924\n",
      "epoch: 23 step: 769, loss is 0.36721864342689514\n",
      "epoch: 23 step: 770, loss is 0.24673347175121307\n",
      "epoch: 23 step: 771, loss is 0.06519965827465057\n",
      "epoch: 23 step: 772, loss is 0.14949238300323486\n",
      "epoch: 23 step: 773, loss is 0.15615318715572357\n",
      "epoch: 23 step: 774, loss is 0.15366540849208832\n",
      "epoch: 23 step: 775, loss is 0.14221060276031494\n",
      "epoch: 23 step: 776, loss is 0.11136063933372498\n",
      "epoch: 23 step: 777, loss is 0.15721122920513153\n",
      "epoch: 23 step: 778, loss is 0.18500402569770813\n",
      "epoch: 23 step: 779, loss is 0.13889165222644806\n",
      "epoch: 23 step: 780, loss is 0.11066749691963196\n",
      "epoch: 23 step: 781, loss is 0.15934044122695923\n",
      "epoch: 23 step: 782, loss is 0.23648782074451447\n",
      "epoch: 23 step: 783, loss is 0.2795649468898773\n",
      "epoch: 23 step: 784, loss is 0.204539954662323\n",
      "epoch: 23 step: 785, loss is 0.2612534165382385\n",
      "epoch: 23 step: 786, loss is 0.2916700839996338\n",
      "epoch: 23 step: 787, loss is 0.05728946998715401\n",
      "epoch: 23 step: 788, loss is 0.23423101007938385\n",
      "epoch: 23 step: 789, loss is 0.15932020545005798\n",
      "epoch: 23 step: 790, loss is 0.17190958559513092\n",
      "epoch: 23 step: 791, loss is 0.14525741338729858\n",
      "epoch: 23 step: 792, loss is 0.2213856428861618\n",
      "epoch: 23 step: 793, loss is 0.1252037137746811\n",
      "epoch: 23 step: 794, loss is 0.1428729146718979\n",
      "epoch: 23 step: 795, loss is 0.2580908536911011\n",
      "epoch: 23 step: 796, loss is 0.2077636569738388\n",
      "epoch: 23 step: 797, loss is 0.16037485003471375\n",
      "epoch: 23 step: 798, loss is 0.20837560296058655\n",
      "epoch: 23 step: 799, loss is 0.38446465134620667\n",
      "epoch: 23 step: 800, loss is 0.13448189198970795\n",
      "epoch: 23 step: 801, loss is 0.2118491232395172\n",
      "epoch: 23 step: 802, loss is 0.11118626594543457\n",
      "epoch: 23 step: 803, loss is 0.18145225942134857\n",
      "epoch: 23 step: 804, loss is 0.22916220128536224\n",
      "epoch: 23 step: 805, loss is 0.2158999741077423\n",
      "epoch: 23 step: 806, loss is 0.2695039212703705\n",
      "epoch: 23 step: 807, loss is 0.2755531370639801\n",
      "epoch: 23 step: 808, loss is 0.1760169267654419\n",
      "epoch: 23 step: 809, loss is 0.20363789796829224\n",
      "epoch: 23 step: 810, loss is 0.38730913400650024\n",
      "epoch: 23 step: 811, loss is 0.18346750736236572\n",
      "epoch: 23 step: 812, loss is 0.2297561764717102\n",
      "epoch: 23 step: 813, loss is 0.1601099669933319\n",
      "epoch: 23 step: 814, loss is 0.07748261839151382\n",
      "epoch: 23 step: 815, loss is 0.10305295884609222\n",
      "epoch: 23 step: 816, loss is 0.13891807198524475\n",
      "epoch: 23 step: 817, loss is 0.18489903211593628\n",
      "epoch: 23 step: 818, loss is 0.3291286528110504\n",
      "epoch: 23 step: 819, loss is 0.3334263861179352\n",
      "epoch: 23 step: 820, loss is 0.0798359215259552\n",
      "epoch: 23 step: 821, loss is 0.37446728348731995\n",
      "epoch: 23 step: 822, loss is 0.29444772005081177\n",
      "epoch: 23 step: 823, loss is 0.14651890099048615\n",
      "epoch: 23 step: 824, loss is 0.23856762051582336\n",
      "epoch: 23 step: 825, loss is 0.2660621702671051\n",
      "epoch: 23 step: 826, loss is 0.1070953831076622\n",
      "epoch: 23 step: 827, loss is 0.16359232366085052\n",
      "epoch: 23 step: 828, loss is 0.12767550349235535\n",
      "epoch: 23 step: 829, loss is 0.11042019724845886\n",
      "epoch: 23 step: 830, loss is 0.19767887890338898\n",
      "epoch: 23 step: 831, loss is 0.1438654512166977\n",
      "epoch: 23 step: 832, loss is 0.17822222411632538\n",
      "epoch: 23 step: 833, loss is 0.07983165234327316\n",
      "epoch: 23 step: 834, loss is 0.16334480047225952\n",
      "epoch: 23 step: 835, loss is 0.17742884159088135\n",
      "epoch: 23 step: 836, loss is 0.10318072885274887\n",
      "epoch: 23 step: 837, loss is 0.14351066946983337\n",
      "epoch: 23 step: 838, loss is 0.2507645785808563\n",
      "epoch: 23 step: 839, loss is 0.33283308148384094\n",
      "epoch: 23 step: 840, loss is 0.0918741300702095\n",
      "epoch: 23 step: 841, loss is 0.11413656920194626\n",
      "epoch: 23 step: 842, loss is 0.21903455257415771\n",
      "epoch: 23 step: 843, loss is 0.1826852709054947\n",
      "epoch: 23 step: 844, loss is 0.06939094513654709\n",
      "epoch: 23 step: 845, loss is 0.1270521730184555\n",
      "epoch: 23 step: 846, loss is 0.18583792448043823\n",
      "epoch: 23 step: 847, loss is 0.28308555483818054\n",
      "epoch: 23 step: 848, loss is 0.20003792643547058\n",
      "epoch: 23 step: 849, loss is 0.10281600058078766\n",
      "epoch: 23 step: 850, loss is 0.22195571660995483\n",
      "epoch: 23 step: 851, loss is 0.23206575214862823\n",
      "epoch: 23 step: 852, loss is 0.15348683297634125\n",
      "epoch: 23 step: 853, loss is 0.11968082189559937\n",
      "epoch: 23 step: 854, loss is 0.1442224085330963\n",
      "epoch: 23 step: 855, loss is 0.0755399763584137\n",
      "epoch: 23 step: 856, loss is 0.08090723305940628\n",
      "epoch: 23 step: 857, loss is 0.2255319207906723\n",
      "epoch: 23 step: 858, loss is 0.14869509637355804\n",
      "epoch: 23 step: 859, loss is 0.257963627576828\n",
      "epoch: 23 step: 860, loss is 0.3117808699607849\n",
      "epoch: 23 step: 861, loss is 0.21059969067573547\n",
      "epoch: 23 step: 862, loss is 0.1887105107307434\n",
      "epoch: 23 step: 863, loss is 0.3941408693790436\n",
      "epoch: 23 step: 864, loss is 0.12405494600534439\n",
      "epoch: 23 step: 865, loss is 0.23720483481884003\n",
      "epoch: 23 step: 866, loss is 0.07148617506027222\n",
      "epoch: 23 step: 867, loss is 0.11443113535642624\n",
      "epoch: 23 step: 868, loss is 0.23477520048618317\n",
      "epoch: 23 step: 869, loss is 0.10342147946357727\n",
      "epoch: 23 step: 870, loss is 0.21653757989406586\n",
      "epoch: 23 step: 871, loss is 0.17953678965568542\n",
      "epoch: 23 step: 872, loss is 0.22161081433296204\n",
      "epoch: 23 step: 873, loss is 0.23361419141292572\n",
      "epoch: 23 step: 874, loss is 0.15252694487571716\n",
      "epoch: 23 step: 875, loss is 0.08592486381530762\n",
      "epoch: 23 step: 876, loss is 0.14137302339076996\n",
      "epoch: 23 step: 877, loss is 0.20977021753787994\n",
      "epoch: 23 step: 878, loss is 0.22925347089767456\n",
      "epoch: 23 step: 879, loss is 0.11433832347393036\n",
      "epoch: 23 step: 880, loss is 0.08705956488847733\n",
      "epoch: 23 step: 881, loss is 0.19901664555072784\n",
      "epoch: 23 step: 882, loss is 0.24179881811141968\n",
      "epoch: 23 step: 883, loss is 0.531740128993988\n",
      "epoch: 23 step: 884, loss is 0.2550671696662903\n",
      "epoch: 23 step: 885, loss is 0.20104777812957764\n",
      "epoch: 23 step: 886, loss is 0.1656522899866104\n",
      "epoch: 23 step: 887, loss is 0.1665753722190857\n",
      "epoch: 23 step: 888, loss is 0.29448801279067993\n",
      "epoch: 23 step: 889, loss is 0.11381250619888306\n",
      "epoch: 23 step: 890, loss is 0.15073783695697784\n",
      "epoch: 23 step: 891, loss is 0.3557639718055725\n",
      "epoch: 23 step: 892, loss is 0.29258784651756287\n",
      "epoch: 23 step: 893, loss is 0.11376368999481201\n",
      "epoch: 23 step: 894, loss is 0.24674902856349945\n",
      "epoch: 23 step: 895, loss is 0.20759250223636627\n",
      "epoch: 23 step: 896, loss is 0.2289947271347046\n",
      "epoch: 23 step: 897, loss is 0.10105396062135696\n",
      "epoch: 23 step: 898, loss is 0.21480624377727509\n",
      "epoch: 23 step: 899, loss is 0.09081576764583588\n",
      "epoch: 23 step: 900, loss is 0.23732884228229523\n",
      "epoch: 23 step: 901, loss is 0.3210330903530121\n",
      "epoch: 23 step: 902, loss is 0.2876311242580414\n",
      "epoch: 23 step: 903, loss is 0.1543889343738556\n",
      "epoch: 23 step: 904, loss is 0.19075614213943481\n",
      "epoch: 23 step: 905, loss is 0.10988770425319672\n",
      "epoch: 23 step: 906, loss is 0.2683252990245819\n",
      "epoch: 23 step: 907, loss is 0.26097118854522705\n",
      "epoch: 23 step: 908, loss is 0.1946437805891037\n",
      "epoch: 23 step: 909, loss is 0.16922719776630402\n",
      "epoch: 23 step: 910, loss is 0.2342255562543869\n",
      "epoch: 23 step: 911, loss is 0.1445845365524292\n",
      "epoch: 23 step: 912, loss is 0.1457708477973938\n",
      "epoch: 23 step: 913, loss is 0.29546570777893066\n",
      "epoch: 23 step: 914, loss is 0.3097372055053711\n",
      "epoch: 23 step: 915, loss is 0.21524035930633545\n",
      "epoch: 23 step: 916, loss is 0.10920334607362747\n",
      "epoch: 23 step: 917, loss is 0.10390613973140717\n",
      "epoch: 23 step: 918, loss is 0.28466930985450745\n",
      "epoch: 23 step: 919, loss is 0.11562160402536392\n",
      "epoch: 23 step: 920, loss is 0.271933913230896\n",
      "epoch: 23 step: 921, loss is 0.23009411990642548\n",
      "epoch: 23 step: 922, loss is 0.13977228105068207\n",
      "epoch: 23 step: 923, loss is 0.17603455483913422\n",
      "epoch: 23 step: 924, loss is 0.09217030555009842\n",
      "epoch: 23 step: 925, loss is 0.34853318333625793\n",
      "epoch: 23 step: 926, loss is 0.09243274480104446\n",
      "epoch: 23 step: 927, loss is 0.16396097838878632\n",
      "epoch: 23 step: 928, loss is 0.17266462743282318\n",
      "epoch: 23 step: 929, loss is 0.22471730411052704\n",
      "epoch: 23 step: 930, loss is 0.17642413079738617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 931, loss is 0.1289234310388565\n",
      "epoch: 23 step: 932, loss is 0.128112331032753\n",
      "epoch: 23 step: 933, loss is 0.21765251457691193\n",
      "epoch: 23 step: 934, loss is 0.1465362161397934\n",
      "epoch: 23 step: 935, loss is 0.20562447607517242\n",
      "epoch: 23 step: 936, loss is 0.13202115893363953\n",
      "epoch: 23 step: 937, loss is 0.1986387073993683\n",
      "epoch: 24 step: 1, loss is 0.1313672512769699\n",
      "epoch: 24 step: 2, loss is 0.18838383257389069\n",
      "epoch: 24 step: 3, loss is 0.19317638874053955\n",
      "epoch: 24 step: 4, loss is 0.24046601355075836\n",
      "epoch: 24 step: 5, loss is 0.22318688035011292\n",
      "epoch: 24 step: 6, loss is 0.18371088802814484\n",
      "epoch: 24 step: 7, loss is 0.2847539186477661\n",
      "epoch: 24 step: 8, loss is 0.15444353222846985\n",
      "epoch: 24 step: 9, loss is 0.1400718092918396\n",
      "epoch: 24 step: 10, loss is 0.22032244503498077\n",
      "epoch: 24 step: 11, loss is 0.08797523379325867\n",
      "epoch: 24 step: 12, loss is 0.1474125236272812\n",
      "epoch: 24 step: 13, loss is 0.13613903522491455\n",
      "epoch: 24 step: 14, loss is 0.13375873863697052\n",
      "epoch: 24 step: 15, loss is 0.18444828689098358\n",
      "epoch: 24 step: 16, loss is 0.1624601036310196\n",
      "epoch: 24 step: 17, loss is 0.1427873820066452\n",
      "epoch: 24 step: 18, loss is 0.28076133131980896\n",
      "epoch: 24 step: 19, loss is 0.2376341074705124\n",
      "epoch: 24 step: 20, loss is 0.18651582300662994\n",
      "epoch: 24 step: 21, loss is 0.33819225430488586\n",
      "epoch: 24 step: 22, loss is 0.2870895564556122\n",
      "epoch: 24 step: 23, loss is 0.32850906252861023\n",
      "epoch: 24 step: 24, loss is 0.15343143045902252\n",
      "epoch: 24 step: 25, loss is 0.14636081457138062\n",
      "epoch: 24 step: 26, loss is 0.1638784110546112\n",
      "epoch: 24 step: 27, loss is 0.2203228920698166\n",
      "epoch: 24 step: 28, loss is 0.061204638332128525\n",
      "epoch: 24 step: 29, loss is 0.2630561590194702\n",
      "epoch: 24 step: 30, loss is 0.2690061032772064\n",
      "epoch: 24 step: 31, loss is 0.15441811084747314\n",
      "epoch: 24 step: 32, loss is 0.24567805230617523\n",
      "epoch: 24 step: 33, loss is 0.19332027435302734\n",
      "epoch: 24 step: 34, loss is 0.15621578693389893\n",
      "epoch: 24 step: 35, loss is 0.19224300980567932\n",
      "epoch: 24 step: 36, loss is 0.1822323203086853\n",
      "epoch: 24 step: 37, loss is 0.3482584059238434\n",
      "epoch: 24 step: 38, loss is 0.17383770644664764\n",
      "epoch: 24 step: 39, loss is 0.24794897437095642\n",
      "epoch: 24 step: 40, loss is 0.12797506153583527\n",
      "epoch: 24 step: 41, loss is 0.09903714805841446\n",
      "epoch: 24 step: 42, loss is 0.29732996225357056\n",
      "epoch: 24 step: 43, loss is 0.3231979012489319\n",
      "epoch: 24 step: 44, loss is 0.24518820643424988\n",
      "epoch: 24 step: 45, loss is 0.22124606370925903\n",
      "epoch: 24 step: 46, loss is 0.19540606439113617\n",
      "epoch: 24 step: 47, loss is 0.17004503309726715\n",
      "epoch: 24 step: 48, loss is 0.1293688267469406\n",
      "epoch: 24 step: 49, loss is 0.13788442313671112\n",
      "epoch: 24 step: 50, loss is 0.26681220531463623\n",
      "epoch: 24 step: 51, loss is 0.23608094453811646\n",
      "epoch: 24 step: 52, loss is 0.3178703188896179\n",
      "epoch: 24 step: 53, loss is 0.30708691477775574\n",
      "epoch: 24 step: 54, loss is 0.21239681541919708\n",
      "epoch: 24 step: 55, loss is 0.20319612324237823\n",
      "epoch: 24 step: 56, loss is 0.2734887897968292\n",
      "epoch: 24 step: 57, loss is 0.267483115196228\n",
      "epoch: 24 step: 58, loss is 0.08530396223068237\n",
      "epoch: 24 step: 59, loss is 0.10872098058462143\n",
      "epoch: 24 step: 60, loss is 0.11645080894231796\n",
      "epoch: 24 step: 61, loss is 0.18156622350215912\n",
      "epoch: 24 step: 62, loss is 0.1713869720697403\n",
      "epoch: 24 step: 63, loss is 0.26504185795783997\n",
      "epoch: 24 step: 64, loss is 0.06564094126224518\n",
      "epoch: 24 step: 65, loss is 0.17909692227840424\n",
      "epoch: 24 step: 66, loss is 0.22371312975883484\n",
      "epoch: 24 step: 67, loss is 0.23104079067707062\n",
      "epoch: 24 step: 68, loss is 0.16024410724639893\n",
      "epoch: 24 step: 69, loss is 0.16576984524726868\n",
      "epoch: 24 step: 70, loss is 0.0601959154009819\n",
      "epoch: 24 step: 71, loss is 0.07436078786849976\n",
      "epoch: 24 step: 72, loss is 0.1851658672094345\n",
      "epoch: 24 step: 73, loss is 0.10223650932312012\n",
      "epoch: 24 step: 74, loss is 0.15069085359573364\n",
      "epoch: 24 step: 75, loss is 0.19939391314983368\n",
      "epoch: 24 step: 76, loss is 0.233194038271904\n",
      "epoch: 24 step: 77, loss is 0.3703266382217407\n",
      "epoch: 24 step: 78, loss is 0.19072140753269196\n",
      "epoch: 24 step: 79, loss is 0.2883201539516449\n",
      "epoch: 24 step: 80, loss is 0.21175824105739594\n",
      "epoch: 24 step: 81, loss is 0.24204280972480774\n",
      "epoch: 24 step: 82, loss is 0.2519674301147461\n",
      "epoch: 24 step: 83, loss is 0.20412901043891907\n",
      "epoch: 24 step: 84, loss is 0.10906771570444107\n",
      "epoch: 24 step: 85, loss is 0.16395804286003113\n",
      "epoch: 24 step: 86, loss is 0.2134924829006195\n",
      "epoch: 24 step: 87, loss is 0.19052405655384064\n",
      "epoch: 24 step: 88, loss is 0.1838247925043106\n",
      "epoch: 24 step: 89, loss is 0.08234984427690506\n",
      "epoch: 24 step: 90, loss is 0.3408992886543274\n",
      "epoch: 24 step: 91, loss is 0.2212948054075241\n",
      "epoch: 24 step: 92, loss is 0.17062628269195557\n",
      "epoch: 24 step: 93, loss is 0.18659460544586182\n",
      "epoch: 24 step: 94, loss is 0.35195979475975037\n",
      "epoch: 24 step: 95, loss is 0.3274925947189331\n",
      "epoch: 24 step: 96, loss is 0.1111261174082756\n",
      "epoch: 24 step: 97, loss is 0.22980821132659912\n",
      "epoch: 24 step: 98, loss is 0.17483451962471008\n",
      "epoch: 24 step: 99, loss is 0.11740219593048096\n",
      "epoch: 24 step: 100, loss is 0.08860534429550171\n",
      "epoch: 24 step: 101, loss is 0.11255694925785065\n",
      "epoch: 24 step: 102, loss is 0.17271608114242554\n",
      "epoch: 24 step: 103, loss is 0.2441181093454361\n",
      "epoch: 24 step: 104, loss is 0.12179489433765411\n",
      "epoch: 24 step: 105, loss is 0.21850420534610748\n",
      "epoch: 24 step: 106, loss is 0.09347513318061829\n",
      "epoch: 24 step: 107, loss is 0.2618791460990906\n",
      "epoch: 24 step: 108, loss is 0.16217298805713654\n",
      "epoch: 24 step: 109, loss is 0.14735613763332367\n",
      "epoch: 24 step: 110, loss is 0.36392685770988464\n",
      "epoch: 24 step: 111, loss is 0.2054157555103302\n",
      "epoch: 24 step: 112, loss is 0.09211748093366623\n",
      "epoch: 24 step: 113, loss is 0.29234257340431213\n",
      "epoch: 24 step: 114, loss is 0.19534406065940857\n",
      "epoch: 24 step: 115, loss is 0.17980436980724335\n",
      "epoch: 24 step: 116, loss is 0.2476637214422226\n",
      "epoch: 24 step: 117, loss is 0.16573309898376465\n",
      "epoch: 24 step: 118, loss is 0.24542467296123505\n",
      "epoch: 24 step: 119, loss is 0.0895763635635376\n",
      "epoch: 24 step: 120, loss is 0.2389250248670578\n",
      "epoch: 24 step: 121, loss is 0.18497280776500702\n",
      "epoch: 24 step: 122, loss is 0.08415909111499786\n",
      "epoch: 24 step: 123, loss is 0.08656857162714005\n",
      "epoch: 24 step: 124, loss is 0.12390542775392532\n",
      "epoch: 24 step: 125, loss is 0.14650550484657288\n",
      "epoch: 24 step: 126, loss is 0.1530616283416748\n",
      "epoch: 24 step: 127, loss is 0.12129778414964676\n",
      "epoch: 24 step: 128, loss is 0.20614810287952423\n",
      "epoch: 24 step: 129, loss is 0.15829482674598694\n",
      "epoch: 24 step: 130, loss is 0.23409759998321533\n",
      "epoch: 24 step: 131, loss is 0.13831143081188202\n",
      "epoch: 24 step: 132, loss is 0.297639936208725\n",
      "epoch: 24 step: 133, loss is 0.13370747864246368\n",
      "epoch: 24 step: 134, loss is 0.16640333831310272\n",
      "epoch: 24 step: 135, loss is 0.18722304701805115\n",
      "epoch: 24 step: 136, loss is 0.13739101588726044\n",
      "epoch: 24 step: 137, loss is 0.2501443326473236\n",
      "epoch: 24 step: 138, loss is 0.2524290382862091\n",
      "epoch: 24 step: 139, loss is 0.2162618339061737\n",
      "epoch: 24 step: 140, loss is 0.2650119960308075\n",
      "epoch: 24 step: 141, loss is 0.24470549821853638\n",
      "epoch: 24 step: 142, loss is 0.17812271416187286\n",
      "epoch: 24 step: 143, loss is 0.1380263715982437\n",
      "epoch: 24 step: 144, loss is 0.18829716742038727\n",
      "epoch: 24 step: 145, loss is 0.09235992282629013\n",
      "epoch: 24 step: 146, loss is 0.16459201276302338\n",
      "epoch: 24 step: 147, loss is 0.08099693804979324\n",
      "epoch: 24 step: 148, loss is 0.2992658317089081\n",
      "epoch: 24 step: 149, loss is 0.13635610044002533\n",
      "epoch: 24 step: 150, loss is 0.20171093940734863\n",
      "epoch: 24 step: 151, loss is 0.21376213431358337\n",
      "epoch: 24 step: 152, loss is 0.22687086462974548\n",
      "epoch: 24 step: 153, loss is 0.15562647581100464\n",
      "epoch: 24 step: 154, loss is 0.2195683866739273\n",
      "epoch: 24 step: 155, loss is 0.10650835186243057\n",
      "epoch: 24 step: 156, loss is 0.2249164581298828\n",
      "epoch: 24 step: 157, loss is 0.15494495630264282\n",
      "epoch: 24 step: 158, loss is 0.18872785568237305\n",
      "epoch: 24 step: 159, loss is 0.21660341322422028\n",
      "epoch: 24 step: 160, loss is 0.2638152241706848\n",
      "epoch: 24 step: 161, loss is 0.0790971890091896\n",
      "epoch: 24 step: 162, loss is 0.3006298840045929\n",
      "epoch: 24 step: 163, loss is 0.24980232119560242\n",
      "epoch: 24 step: 164, loss is 0.2931627035140991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 165, loss is 0.17502941191196442\n",
      "epoch: 24 step: 166, loss is 0.1972533017396927\n",
      "epoch: 24 step: 167, loss is 0.1364096701145172\n",
      "epoch: 24 step: 168, loss is 0.33916065096855164\n",
      "epoch: 24 step: 169, loss is 0.13237296044826508\n",
      "epoch: 24 step: 170, loss is 0.2900203466415405\n",
      "epoch: 24 step: 171, loss is 0.189308762550354\n",
      "epoch: 24 step: 172, loss is 0.15428614616394043\n",
      "epoch: 24 step: 173, loss is 0.1436336487531662\n",
      "epoch: 24 step: 174, loss is 0.19258731603622437\n",
      "epoch: 24 step: 175, loss is 0.23798251152038574\n",
      "epoch: 24 step: 176, loss is 0.270682156085968\n",
      "epoch: 24 step: 177, loss is 0.1875843107700348\n",
      "epoch: 24 step: 178, loss is 0.3226527273654938\n",
      "epoch: 24 step: 179, loss is 0.10844636708498001\n",
      "epoch: 24 step: 180, loss is 0.14432844519615173\n",
      "epoch: 24 step: 181, loss is 0.2585684657096863\n",
      "epoch: 24 step: 182, loss is 0.10096193850040436\n",
      "epoch: 24 step: 183, loss is 0.10670047253370285\n",
      "epoch: 24 step: 184, loss is 0.35608845949172974\n",
      "epoch: 24 step: 185, loss is 0.13466063141822815\n",
      "epoch: 24 step: 186, loss is 0.11262567341327667\n",
      "epoch: 24 step: 187, loss is 0.17078259587287903\n",
      "epoch: 24 step: 188, loss is 0.08215727657079697\n",
      "epoch: 24 step: 189, loss is 0.15484866499900818\n",
      "epoch: 24 step: 190, loss is 0.15205548703670502\n",
      "epoch: 24 step: 191, loss is 0.13867902755737305\n",
      "epoch: 24 step: 192, loss is 0.0730489119887352\n",
      "epoch: 24 step: 193, loss is 0.1156630739569664\n",
      "epoch: 24 step: 194, loss is 0.3458869159221649\n",
      "epoch: 24 step: 195, loss is 0.08928164839744568\n",
      "epoch: 24 step: 196, loss is 0.21068714559078217\n",
      "epoch: 24 step: 197, loss is 0.1383209228515625\n",
      "epoch: 24 step: 198, loss is 0.3095007836818695\n",
      "epoch: 24 step: 199, loss is 0.10044185072183609\n",
      "epoch: 24 step: 200, loss is 0.12226855754852295\n",
      "epoch: 24 step: 201, loss is 0.12592850625514984\n",
      "epoch: 24 step: 202, loss is 0.29822978377342224\n",
      "epoch: 24 step: 203, loss is 0.10167897492647171\n",
      "epoch: 24 step: 204, loss is 0.3070327639579773\n",
      "epoch: 24 step: 205, loss is 0.2873043417930603\n",
      "epoch: 24 step: 206, loss is 0.12561899423599243\n",
      "epoch: 24 step: 207, loss is 0.20578962564468384\n",
      "epoch: 24 step: 208, loss is 0.38501766324043274\n",
      "epoch: 24 step: 209, loss is 0.09539584815502167\n",
      "epoch: 24 step: 210, loss is 0.1729218065738678\n",
      "epoch: 24 step: 211, loss is 0.19510211050510406\n",
      "epoch: 24 step: 212, loss is 0.1346329152584076\n",
      "epoch: 24 step: 213, loss is 0.17852725088596344\n",
      "epoch: 24 step: 214, loss is 0.2731720805168152\n",
      "epoch: 24 step: 215, loss is 0.19257718324661255\n",
      "epoch: 24 step: 216, loss is 0.238427996635437\n",
      "epoch: 24 step: 217, loss is 0.16574226319789886\n",
      "epoch: 24 step: 218, loss is 0.15634845197200775\n",
      "epoch: 24 step: 219, loss is 0.39563366770744324\n",
      "epoch: 24 step: 220, loss is 0.24058251082897186\n",
      "epoch: 24 step: 221, loss is 0.2505110204219818\n",
      "epoch: 24 step: 222, loss is 0.12462275475263596\n",
      "epoch: 24 step: 223, loss is 0.21620029211044312\n",
      "epoch: 24 step: 224, loss is 0.1259029507637024\n",
      "epoch: 24 step: 225, loss is 0.101073257625103\n",
      "epoch: 24 step: 226, loss is 0.2992382049560547\n",
      "epoch: 24 step: 227, loss is 0.15849004685878754\n",
      "epoch: 24 step: 228, loss is 0.15711380541324615\n",
      "epoch: 24 step: 229, loss is 0.20800626277923584\n",
      "epoch: 24 step: 230, loss is 0.1866491734981537\n",
      "epoch: 24 step: 231, loss is 0.09621487557888031\n",
      "epoch: 24 step: 232, loss is 0.21069039404392242\n",
      "epoch: 24 step: 233, loss is 0.12601710855960846\n",
      "epoch: 24 step: 234, loss is 0.21901370584964752\n",
      "epoch: 24 step: 235, loss is 0.16783136129379272\n",
      "epoch: 24 step: 236, loss is 0.2722727060317993\n",
      "epoch: 24 step: 237, loss is 0.06393130123615265\n",
      "epoch: 24 step: 238, loss is 0.1086595356464386\n",
      "epoch: 24 step: 239, loss is 0.18354074656963348\n",
      "epoch: 24 step: 240, loss is 0.24913276731967926\n",
      "epoch: 24 step: 241, loss is 0.24792924523353577\n",
      "epoch: 24 step: 242, loss is 0.11235667020082474\n",
      "epoch: 24 step: 243, loss is 0.14909832179546356\n",
      "epoch: 24 step: 244, loss is 0.17230819165706635\n",
      "epoch: 24 step: 245, loss is 0.16238799691200256\n",
      "epoch: 24 step: 246, loss is 0.13525044918060303\n",
      "epoch: 24 step: 247, loss is 0.23362532258033752\n",
      "epoch: 24 step: 248, loss is 0.14814366400241852\n",
      "epoch: 24 step: 249, loss is 0.19444283843040466\n",
      "epoch: 24 step: 250, loss is 0.10853052139282227\n",
      "epoch: 24 step: 251, loss is 0.2333643138408661\n",
      "epoch: 24 step: 252, loss is 0.16279730200767517\n",
      "epoch: 24 step: 253, loss is 0.17940402030944824\n",
      "epoch: 24 step: 254, loss is 0.2502872943878174\n",
      "epoch: 24 step: 255, loss is 0.13853465020656586\n",
      "epoch: 24 step: 256, loss is 0.2001383751630783\n",
      "epoch: 24 step: 257, loss is 0.2724824547767639\n",
      "epoch: 24 step: 258, loss is 0.1696239858865738\n",
      "epoch: 24 step: 259, loss is 0.22504328191280365\n",
      "epoch: 24 step: 260, loss is 0.25993433594703674\n",
      "epoch: 24 step: 261, loss is 0.2869274616241455\n",
      "epoch: 24 step: 262, loss is 0.09300290793180466\n",
      "epoch: 24 step: 263, loss is 0.23238541185855865\n",
      "epoch: 24 step: 264, loss is 0.2033289223909378\n",
      "epoch: 24 step: 265, loss is 0.16805493831634521\n",
      "epoch: 24 step: 266, loss is 0.11298917233943939\n",
      "epoch: 24 step: 267, loss is 0.08771498501300812\n",
      "epoch: 24 step: 268, loss is 0.1335659921169281\n",
      "epoch: 24 step: 269, loss is 0.18780161440372467\n",
      "epoch: 24 step: 270, loss is 0.23019972443580627\n",
      "epoch: 24 step: 271, loss is 0.20396526157855988\n",
      "epoch: 24 step: 272, loss is 0.1505250483751297\n",
      "epoch: 24 step: 273, loss is 0.269571453332901\n",
      "epoch: 24 step: 274, loss is 0.11739423871040344\n",
      "epoch: 24 step: 275, loss is 0.1515738070011139\n",
      "epoch: 24 step: 276, loss is 0.2429271638393402\n",
      "epoch: 24 step: 277, loss is 0.12668932974338531\n",
      "epoch: 24 step: 278, loss is 0.2686477303504944\n",
      "epoch: 24 step: 279, loss is 0.2432376742362976\n",
      "epoch: 24 step: 280, loss is 0.21700769662857056\n",
      "epoch: 24 step: 281, loss is 0.20617584884166718\n",
      "epoch: 24 step: 282, loss is 0.225996196269989\n",
      "epoch: 24 step: 283, loss is 0.21055427193641663\n",
      "epoch: 24 step: 284, loss is 0.20878243446350098\n",
      "epoch: 24 step: 285, loss is 0.17968563735485077\n",
      "epoch: 24 step: 286, loss is 0.17234297096729279\n",
      "epoch: 24 step: 287, loss is 0.19012568891048431\n",
      "epoch: 24 step: 288, loss is 0.22159536182880402\n",
      "epoch: 24 step: 289, loss is 0.25092199444770813\n",
      "epoch: 24 step: 290, loss is 0.23271414637565613\n",
      "epoch: 24 step: 291, loss is 0.15653586387634277\n",
      "epoch: 24 step: 292, loss is 0.26511213183403015\n",
      "epoch: 24 step: 293, loss is 0.35299766063690186\n",
      "epoch: 24 step: 294, loss is 0.16121216118335724\n",
      "epoch: 24 step: 295, loss is 0.26778948307037354\n",
      "epoch: 24 step: 296, loss is 0.08255108445882797\n",
      "epoch: 24 step: 297, loss is 0.2718302309513092\n",
      "epoch: 24 step: 298, loss is 0.1270662248134613\n",
      "epoch: 24 step: 299, loss is 0.1026730015873909\n",
      "epoch: 24 step: 300, loss is 0.3255329430103302\n",
      "epoch: 24 step: 301, loss is 0.11544492095708847\n",
      "epoch: 24 step: 302, loss is 0.1957976222038269\n",
      "epoch: 24 step: 303, loss is 0.20831777155399323\n",
      "epoch: 24 step: 304, loss is 0.33576488494873047\n",
      "epoch: 24 step: 305, loss is 0.29671263694763184\n",
      "epoch: 24 step: 306, loss is 0.24105077981948853\n",
      "epoch: 24 step: 307, loss is 0.19454383850097656\n",
      "epoch: 24 step: 308, loss is 0.1743568778038025\n",
      "epoch: 24 step: 309, loss is 0.2335415780544281\n",
      "epoch: 24 step: 310, loss is 0.24091683328151703\n",
      "epoch: 24 step: 311, loss is 0.2447827309370041\n",
      "epoch: 24 step: 312, loss is 0.1748807728290558\n",
      "epoch: 24 step: 313, loss is 0.10947293043136597\n",
      "epoch: 24 step: 314, loss is 0.1291169673204422\n",
      "epoch: 24 step: 315, loss is 0.1181485578417778\n",
      "epoch: 24 step: 316, loss is 0.1421816349029541\n",
      "epoch: 24 step: 317, loss is 0.17955966293811798\n",
      "epoch: 24 step: 318, loss is 0.17121970653533936\n",
      "epoch: 24 step: 319, loss is 0.17597350478172302\n",
      "epoch: 24 step: 320, loss is 0.24971017241477966\n",
      "epoch: 24 step: 321, loss is 0.38945823907852173\n",
      "epoch: 24 step: 322, loss is 0.15405014157295227\n",
      "epoch: 24 step: 323, loss is 0.2638779580593109\n",
      "epoch: 24 step: 324, loss is 0.1557879000902176\n",
      "epoch: 24 step: 325, loss is 0.10227880626916885\n",
      "epoch: 24 step: 326, loss is 0.13781625032424927\n",
      "epoch: 24 step: 327, loss is 0.14006268978118896\n",
      "epoch: 24 step: 328, loss is 0.3221307396888733\n",
      "epoch: 24 step: 329, loss is 0.1320355236530304\n",
      "epoch: 24 step: 330, loss is 0.11226177960634232\n",
      "epoch: 24 step: 331, loss is 0.15646852552890778\n",
      "epoch: 24 step: 332, loss is 0.1283247470855713\n",
      "epoch: 24 step: 333, loss is 0.25793007016181946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 334, loss is 0.09242790192365646\n",
      "epoch: 24 step: 335, loss is 0.1905534416437149\n",
      "epoch: 24 step: 336, loss is 0.3966340720653534\n",
      "epoch: 24 step: 337, loss is 0.16199176013469696\n",
      "epoch: 24 step: 338, loss is 0.12184365838766098\n",
      "epoch: 24 step: 339, loss is 0.17567181587219238\n",
      "epoch: 24 step: 340, loss is 0.24270270764827728\n",
      "epoch: 24 step: 341, loss is 0.15141598880290985\n",
      "epoch: 24 step: 342, loss is 0.20417284965515137\n",
      "epoch: 24 step: 343, loss is 0.12607069313526154\n",
      "epoch: 24 step: 344, loss is 0.15160956978797913\n",
      "epoch: 24 step: 345, loss is 0.28768864274024963\n",
      "epoch: 24 step: 346, loss is 0.13990578055381775\n",
      "epoch: 24 step: 347, loss is 0.13427023589611053\n",
      "epoch: 24 step: 348, loss is 0.30499324202537537\n",
      "epoch: 24 step: 349, loss is 0.08235745131969452\n",
      "epoch: 24 step: 350, loss is 0.2585431933403015\n",
      "epoch: 24 step: 351, loss is 0.25300824642181396\n",
      "epoch: 24 step: 352, loss is 0.11185821890830994\n",
      "epoch: 24 step: 353, loss is 0.07322019338607788\n",
      "epoch: 24 step: 354, loss is 0.10060396790504456\n",
      "epoch: 24 step: 355, loss is 0.1423877328634262\n",
      "epoch: 24 step: 356, loss is 0.18056538701057434\n",
      "epoch: 24 step: 357, loss is 0.1477385014295578\n",
      "epoch: 24 step: 358, loss is 0.31330567598342896\n",
      "epoch: 24 step: 359, loss is 0.16323356330394745\n",
      "epoch: 24 step: 360, loss is 0.23170147836208344\n",
      "epoch: 24 step: 361, loss is 0.12060469388961792\n",
      "epoch: 24 step: 362, loss is 0.1648378074169159\n",
      "epoch: 24 step: 363, loss is 0.2253492772579193\n",
      "epoch: 24 step: 364, loss is 0.17941118776798248\n",
      "epoch: 24 step: 365, loss is 0.1899733990430832\n",
      "epoch: 24 step: 366, loss is 0.19664715230464935\n",
      "epoch: 24 step: 367, loss is 0.20805922150611877\n",
      "epoch: 24 step: 368, loss is 0.23180755972862244\n",
      "epoch: 24 step: 369, loss is 0.38270384073257446\n",
      "epoch: 24 step: 370, loss is 0.19142358005046844\n",
      "epoch: 24 step: 371, loss is 0.16945499181747437\n",
      "epoch: 24 step: 372, loss is 0.14049439132213593\n",
      "epoch: 24 step: 373, loss is 0.2227877676486969\n",
      "epoch: 24 step: 374, loss is 0.23891261219978333\n",
      "epoch: 24 step: 375, loss is 0.1875002235174179\n",
      "epoch: 24 step: 376, loss is 0.14391851425170898\n",
      "epoch: 24 step: 377, loss is 0.09134212881326675\n",
      "epoch: 24 step: 378, loss is 0.40348896384239197\n",
      "epoch: 24 step: 379, loss is 0.15948347747325897\n",
      "epoch: 24 step: 380, loss is 0.31640690565109253\n",
      "epoch: 24 step: 381, loss is 0.1752980500459671\n",
      "epoch: 24 step: 382, loss is 0.06709485501050949\n",
      "epoch: 24 step: 383, loss is 0.14766626060009003\n",
      "epoch: 24 step: 384, loss is 0.14152389764785767\n",
      "epoch: 24 step: 385, loss is 0.12812654674053192\n",
      "epoch: 24 step: 386, loss is 0.3781471252441406\n",
      "epoch: 24 step: 387, loss is 0.33677807450294495\n",
      "epoch: 24 step: 388, loss is 0.39714452624320984\n",
      "epoch: 24 step: 389, loss is 0.13609281182289124\n",
      "epoch: 24 step: 390, loss is 0.11780469119548798\n",
      "epoch: 24 step: 391, loss is 0.2240135222673416\n",
      "epoch: 24 step: 392, loss is 0.21094413101673126\n",
      "epoch: 24 step: 393, loss is 0.31070610880851746\n",
      "epoch: 24 step: 394, loss is 0.1932491511106491\n",
      "epoch: 24 step: 395, loss is 0.20717880129814148\n",
      "epoch: 24 step: 396, loss is 0.19191999733448029\n",
      "epoch: 24 step: 397, loss is 0.19107910990715027\n",
      "epoch: 24 step: 398, loss is 0.16857318580150604\n",
      "epoch: 24 step: 399, loss is 0.1914861649274826\n",
      "epoch: 24 step: 400, loss is 0.18490968644618988\n",
      "epoch: 24 step: 401, loss is 0.11565617471933365\n",
      "epoch: 24 step: 402, loss is 0.2116568684577942\n",
      "epoch: 24 step: 403, loss is 0.16864342987537384\n",
      "epoch: 24 step: 404, loss is 0.20644329488277435\n",
      "epoch: 24 step: 405, loss is 0.34540611505508423\n",
      "epoch: 24 step: 406, loss is 0.09842886030673981\n",
      "epoch: 24 step: 407, loss is 0.21793963015079498\n",
      "epoch: 24 step: 408, loss is 0.23630626499652863\n",
      "epoch: 24 step: 409, loss is 0.12929853796958923\n",
      "epoch: 24 step: 410, loss is 0.3013406991958618\n",
      "epoch: 24 step: 411, loss is 0.18476872146129608\n",
      "epoch: 24 step: 412, loss is 0.26411867141723633\n",
      "epoch: 24 step: 413, loss is 0.18401169776916504\n",
      "epoch: 24 step: 414, loss is 0.16731446981430054\n",
      "epoch: 24 step: 415, loss is 0.16621509194374084\n",
      "epoch: 24 step: 416, loss is 0.20931372046470642\n",
      "epoch: 24 step: 417, loss is 0.20273754000663757\n",
      "epoch: 24 step: 418, loss is 0.1427496075630188\n",
      "epoch: 24 step: 419, loss is 0.22043001651763916\n",
      "epoch: 24 step: 420, loss is 0.18596383929252625\n",
      "epoch: 24 step: 421, loss is 0.26545247435569763\n",
      "epoch: 24 step: 422, loss is 0.293124258518219\n",
      "epoch: 24 step: 423, loss is 0.19359391927719116\n",
      "epoch: 24 step: 424, loss is 0.2430322766304016\n",
      "epoch: 24 step: 425, loss is 0.2331751435995102\n",
      "epoch: 24 step: 426, loss is 0.20181730389595032\n",
      "epoch: 24 step: 427, loss is 0.1407870054244995\n",
      "epoch: 24 step: 428, loss is 0.1850939691066742\n",
      "epoch: 24 step: 429, loss is 0.21881617605686188\n",
      "epoch: 24 step: 430, loss is 0.12322945147752762\n",
      "epoch: 24 step: 431, loss is 0.2762073874473572\n",
      "epoch: 24 step: 432, loss is 0.09046954661607742\n",
      "epoch: 24 step: 433, loss is 0.31697583198547363\n",
      "epoch: 24 step: 434, loss is 0.18094795942306519\n",
      "epoch: 24 step: 435, loss is 0.14579500257968903\n",
      "epoch: 24 step: 436, loss is 0.33730870485305786\n",
      "epoch: 24 step: 437, loss is 0.1410737782716751\n",
      "epoch: 24 step: 438, loss is 0.17944447696208954\n",
      "epoch: 24 step: 439, loss is 0.20115552842617035\n",
      "epoch: 24 step: 440, loss is 0.1292896568775177\n",
      "epoch: 24 step: 441, loss is 0.19689436256885529\n",
      "epoch: 24 step: 442, loss is 0.3389990031719208\n",
      "epoch: 24 step: 443, loss is 0.07429523766040802\n",
      "epoch: 24 step: 444, loss is 0.2307649701833725\n",
      "epoch: 24 step: 445, loss is 0.26705700159072876\n",
      "epoch: 24 step: 446, loss is 0.0886390283703804\n",
      "epoch: 24 step: 447, loss is 0.13148534297943115\n",
      "epoch: 24 step: 448, loss is 0.3350982367992401\n",
      "epoch: 24 step: 449, loss is 0.3197861909866333\n",
      "epoch: 24 step: 450, loss is 0.2897457480430603\n",
      "epoch: 24 step: 451, loss is 0.28427425026893616\n",
      "epoch: 24 step: 452, loss is 0.13539132475852966\n",
      "epoch: 24 step: 453, loss is 0.174376979470253\n",
      "epoch: 24 step: 454, loss is 0.08809178322553635\n",
      "epoch: 24 step: 455, loss is 0.08340312540531158\n",
      "epoch: 24 step: 456, loss is 0.2477603703737259\n",
      "epoch: 24 step: 457, loss is 0.37529197335243225\n",
      "epoch: 24 step: 458, loss is 0.205215185880661\n",
      "epoch: 24 step: 459, loss is 0.27239325642585754\n",
      "epoch: 24 step: 460, loss is 0.13207581639289856\n",
      "epoch: 24 step: 461, loss is 0.3326740860939026\n",
      "epoch: 24 step: 462, loss is 0.2735351324081421\n",
      "epoch: 24 step: 463, loss is 0.26522961258888245\n",
      "epoch: 24 step: 464, loss is 0.1383885145187378\n",
      "epoch: 24 step: 465, loss is 0.15187931060791016\n",
      "epoch: 24 step: 466, loss is 0.12948337197303772\n",
      "epoch: 24 step: 467, loss is 0.12736552953720093\n",
      "epoch: 24 step: 468, loss is 0.11618328839540482\n",
      "epoch: 24 step: 469, loss is 0.1790817230939865\n",
      "epoch: 24 step: 470, loss is 0.3394221365451813\n",
      "epoch: 24 step: 471, loss is 0.2308637797832489\n",
      "epoch: 24 step: 472, loss is 0.18098093569278717\n",
      "epoch: 24 step: 473, loss is 0.12787508964538574\n",
      "epoch: 24 step: 474, loss is 0.13472342491149902\n",
      "epoch: 24 step: 475, loss is 0.20581859350204468\n",
      "epoch: 24 step: 476, loss is 0.20688559114933014\n",
      "epoch: 24 step: 477, loss is 0.22727487981319427\n",
      "epoch: 24 step: 478, loss is 0.13974465429782867\n",
      "epoch: 24 step: 479, loss is 0.14447854459285736\n",
      "epoch: 24 step: 480, loss is 0.13313975930213928\n",
      "epoch: 24 step: 481, loss is 0.23734451830387115\n",
      "epoch: 24 step: 482, loss is 0.23384633660316467\n",
      "epoch: 24 step: 483, loss is 0.1917348951101303\n",
      "epoch: 24 step: 484, loss is 0.29782962799072266\n",
      "epoch: 24 step: 485, loss is 0.23937340080738068\n",
      "epoch: 24 step: 486, loss is 0.20184265077114105\n",
      "epoch: 24 step: 487, loss is 0.21276144683361053\n",
      "epoch: 24 step: 488, loss is 0.1560547798871994\n",
      "epoch: 24 step: 489, loss is 0.19156824052333832\n",
      "epoch: 24 step: 490, loss is 0.34865450859069824\n",
      "epoch: 24 step: 491, loss is 0.12601080536842346\n",
      "epoch: 24 step: 492, loss is 0.19149814546108246\n",
      "epoch: 24 step: 493, loss is 0.31049519777297974\n",
      "epoch: 24 step: 494, loss is 0.14947031438350677\n",
      "epoch: 24 step: 495, loss is 0.2772647440433502\n",
      "epoch: 24 step: 496, loss is 0.20173059403896332\n",
      "epoch: 24 step: 497, loss is 0.2512071132659912\n",
      "epoch: 24 step: 498, loss is 0.18544206023216248\n",
      "epoch: 24 step: 499, loss is 0.2469269037246704\n",
      "epoch: 24 step: 500, loss is 0.11241617798805237\n",
      "epoch: 24 step: 501, loss is 0.1676262617111206\n",
      "epoch: 24 step: 502, loss is 0.1427024006843567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 503, loss is 0.1387791484594345\n",
      "epoch: 24 step: 504, loss is 0.15651828050613403\n",
      "epoch: 24 step: 505, loss is 0.15798906981945038\n",
      "epoch: 24 step: 506, loss is 0.2837618291378021\n",
      "epoch: 24 step: 507, loss is 0.1460440307855606\n",
      "epoch: 24 step: 508, loss is 0.20433540642261505\n",
      "epoch: 24 step: 509, loss is 0.1448899358510971\n",
      "epoch: 24 step: 510, loss is 0.24992495775222778\n",
      "epoch: 24 step: 511, loss is 0.19814133644104004\n",
      "epoch: 24 step: 512, loss is 0.10230317711830139\n",
      "epoch: 24 step: 513, loss is 0.26519888639450073\n",
      "epoch: 24 step: 514, loss is 0.15190334618091583\n",
      "epoch: 24 step: 515, loss is 0.11540216207504272\n",
      "epoch: 24 step: 516, loss is 0.18883857131004333\n",
      "epoch: 24 step: 517, loss is 0.2518520951271057\n",
      "epoch: 24 step: 518, loss is 0.2756664454936981\n",
      "epoch: 24 step: 519, loss is 0.07251369953155518\n",
      "epoch: 24 step: 520, loss is 0.2096424102783203\n",
      "epoch: 24 step: 521, loss is 0.13360027968883514\n",
      "epoch: 24 step: 522, loss is 0.16379888355731964\n",
      "epoch: 24 step: 523, loss is 0.23561830818653107\n",
      "epoch: 24 step: 524, loss is 0.20463065803050995\n",
      "epoch: 24 step: 525, loss is 0.37901613116264343\n",
      "epoch: 24 step: 526, loss is 0.2722773551940918\n",
      "epoch: 24 step: 527, loss is 0.22356802225112915\n",
      "epoch: 24 step: 528, loss is 0.16149847209453583\n",
      "epoch: 24 step: 529, loss is 0.16647861897945404\n",
      "epoch: 24 step: 530, loss is 0.3141672909259796\n",
      "epoch: 24 step: 531, loss is 0.3535771667957306\n",
      "epoch: 24 step: 532, loss is 0.13075308501720428\n",
      "epoch: 24 step: 533, loss is 0.16903582215309143\n",
      "epoch: 24 step: 534, loss is 0.09154846519231796\n",
      "epoch: 24 step: 535, loss is 0.19855757057666779\n",
      "epoch: 24 step: 536, loss is 0.13729453086853027\n",
      "epoch: 24 step: 537, loss is 0.1336173415184021\n",
      "epoch: 24 step: 538, loss is 0.3014860153198242\n",
      "epoch: 24 step: 539, loss is 0.1071075052022934\n",
      "epoch: 24 step: 540, loss is 0.2717515826225281\n",
      "epoch: 24 step: 541, loss is 0.3254697322845459\n",
      "epoch: 24 step: 542, loss is 0.09029389172792435\n",
      "epoch: 24 step: 543, loss is 0.2224467694759369\n",
      "epoch: 24 step: 544, loss is 0.1890418827533722\n",
      "epoch: 24 step: 545, loss is 0.1852015107870102\n",
      "epoch: 24 step: 546, loss is 0.3103094696998596\n",
      "epoch: 24 step: 547, loss is 0.10389728099107742\n",
      "epoch: 24 step: 548, loss is 0.3101615905761719\n",
      "epoch: 24 step: 549, loss is 0.17121797800064087\n",
      "epoch: 24 step: 550, loss is 0.2169145792722702\n",
      "epoch: 24 step: 551, loss is 0.22732210159301758\n",
      "epoch: 24 step: 552, loss is 0.29249265789985657\n",
      "epoch: 24 step: 553, loss is 0.19468282163143158\n",
      "epoch: 24 step: 554, loss is 0.22192615270614624\n",
      "epoch: 24 step: 555, loss is 0.15988415479660034\n",
      "epoch: 24 step: 556, loss is 0.08682095259428024\n",
      "epoch: 24 step: 557, loss is 0.18836469948291779\n",
      "epoch: 24 step: 558, loss is 0.19211387634277344\n",
      "epoch: 24 step: 559, loss is 0.16591033339500427\n",
      "epoch: 24 step: 560, loss is 0.1333789825439453\n",
      "epoch: 24 step: 561, loss is 0.25785717368125916\n",
      "epoch: 24 step: 562, loss is 0.3737567961215973\n",
      "epoch: 24 step: 563, loss is 0.2354116588830948\n",
      "epoch: 24 step: 564, loss is 0.2739426791667938\n",
      "epoch: 24 step: 565, loss is 0.11829089373350143\n",
      "epoch: 24 step: 566, loss is 0.15916819870471954\n",
      "epoch: 24 step: 567, loss is 0.18566101789474487\n",
      "epoch: 24 step: 568, loss is 0.2634986639022827\n",
      "epoch: 24 step: 569, loss is 0.22008772194385529\n",
      "epoch: 24 step: 570, loss is 0.13394896686077118\n",
      "epoch: 24 step: 571, loss is 0.17756113409996033\n",
      "epoch: 24 step: 572, loss is 0.14937880635261536\n",
      "epoch: 24 step: 573, loss is 0.08506765961647034\n",
      "epoch: 24 step: 574, loss is 0.22142133116722107\n",
      "epoch: 24 step: 575, loss is 0.1759975701570511\n",
      "epoch: 24 step: 576, loss is 0.2662118375301361\n",
      "epoch: 24 step: 577, loss is 0.12284361571073532\n",
      "epoch: 24 step: 578, loss is 0.3197650611400604\n",
      "epoch: 24 step: 579, loss is 0.07350270450115204\n",
      "epoch: 24 step: 580, loss is 0.13299600780010223\n",
      "epoch: 24 step: 581, loss is 0.13834165036678314\n",
      "epoch: 24 step: 582, loss is 0.1572951376438141\n",
      "epoch: 24 step: 583, loss is 0.2449154555797577\n",
      "epoch: 24 step: 584, loss is 0.12201953679323196\n",
      "epoch: 24 step: 585, loss is 0.31166768074035645\n",
      "epoch: 24 step: 586, loss is 0.12877751886844635\n",
      "epoch: 24 step: 587, loss is 0.2296295166015625\n",
      "epoch: 24 step: 588, loss is 0.26449957489967346\n",
      "epoch: 24 step: 589, loss is 0.298472136259079\n",
      "epoch: 24 step: 590, loss is 0.1584520787000656\n",
      "epoch: 24 step: 591, loss is 0.3303067088127136\n",
      "epoch: 24 step: 592, loss is 0.2508751153945923\n",
      "epoch: 24 step: 593, loss is 0.20858122408390045\n",
      "epoch: 24 step: 594, loss is 0.04883928969502449\n",
      "epoch: 24 step: 595, loss is 0.1361934393644333\n",
      "epoch: 24 step: 596, loss is 0.20146025717258453\n",
      "epoch: 24 step: 597, loss is 0.1296129822731018\n",
      "epoch: 24 step: 598, loss is 0.17225833237171173\n",
      "epoch: 24 step: 599, loss is 0.12375982105731964\n",
      "epoch: 24 step: 600, loss is 0.23172065615653992\n",
      "epoch: 24 step: 601, loss is 0.3668554425239563\n",
      "epoch: 24 step: 602, loss is 0.18251918256282806\n",
      "epoch: 24 step: 603, loss is 0.2861623167991638\n",
      "epoch: 24 step: 604, loss is 0.13668236136436462\n",
      "epoch: 24 step: 605, loss is 0.2603592872619629\n",
      "epoch: 24 step: 606, loss is 0.20571617782115936\n",
      "epoch: 24 step: 607, loss is 0.15094545483589172\n",
      "epoch: 24 step: 608, loss is 0.19570206105709076\n",
      "epoch: 24 step: 609, loss is 0.22364389896392822\n",
      "epoch: 24 step: 610, loss is 0.2270871102809906\n",
      "epoch: 24 step: 611, loss is 0.1382204294204712\n",
      "epoch: 24 step: 612, loss is 0.19852130115032196\n",
      "epoch: 24 step: 613, loss is 0.26315152645111084\n",
      "epoch: 24 step: 614, loss is 0.21144787967205048\n",
      "epoch: 24 step: 615, loss is 0.23379546403884888\n",
      "epoch: 24 step: 616, loss is 0.24346259236335754\n",
      "epoch: 24 step: 617, loss is 0.16350707411766052\n",
      "epoch: 24 step: 618, loss is 0.11664973944425583\n",
      "epoch: 24 step: 619, loss is 0.17311380803585052\n",
      "epoch: 24 step: 620, loss is 0.18291430175304413\n",
      "epoch: 24 step: 621, loss is 0.16915863752365112\n",
      "epoch: 24 step: 622, loss is 0.14308282732963562\n",
      "epoch: 24 step: 623, loss is 0.276543527841568\n",
      "epoch: 24 step: 624, loss is 0.10539359599351883\n",
      "epoch: 24 step: 625, loss is 0.10532604157924652\n",
      "epoch: 24 step: 626, loss is 0.15416556596755981\n",
      "epoch: 24 step: 627, loss is 0.2743990123271942\n",
      "epoch: 24 step: 628, loss is 0.1169079914689064\n",
      "epoch: 24 step: 629, loss is 0.2195359468460083\n",
      "epoch: 24 step: 630, loss is 0.17935366928577423\n",
      "epoch: 24 step: 631, loss is 0.0748831108212471\n",
      "epoch: 24 step: 632, loss is 0.22563791275024414\n",
      "epoch: 24 step: 633, loss is 0.1581624150276184\n",
      "epoch: 24 step: 634, loss is 0.22960080206394196\n",
      "epoch: 24 step: 635, loss is 0.18245302140712738\n",
      "epoch: 24 step: 636, loss is 0.2125186026096344\n",
      "epoch: 24 step: 637, loss is 0.21515406668186188\n",
      "epoch: 24 step: 638, loss is 0.27266958355903625\n",
      "epoch: 24 step: 639, loss is 0.17905683815479279\n",
      "epoch: 24 step: 640, loss is 0.1015142947435379\n",
      "epoch: 24 step: 641, loss is 0.16542528569698334\n",
      "epoch: 24 step: 642, loss is 0.07900633662939072\n",
      "epoch: 24 step: 643, loss is 0.2654213309288025\n",
      "epoch: 24 step: 644, loss is 0.0980469211935997\n",
      "epoch: 24 step: 645, loss is 0.16109342873096466\n",
      "epoch: 24 step: 646, loss is 0.23425976932048798\n",
      "epoch: 24 step: 647, loss is 0.23730066418647766\n",
      "epoch: 24 step: 648, loss is 0.20087073743343353\n",
      "epoch: 24 step: 649, loss is 0.2536735534667969\n",
      "epoch: 24 step: 650, loss is 0.3898124396800995\n",
      "epoch: 24 step: 651, loss is 0.2385612428188324\n",
      "epoch: 24 step: 652, loss is 0.17046691477298737\n",
      "epoch: 24 step: 653, loss is 0.1174454540014267\n",
      "epoch: 24 step: 654, loss is 0.2715705931186676\n",
      "epoch: 24 step: 655, loss is 0.21514195203781128\n",
      "epoch: 24 step: 656, loss is 0.20564082264900208\n",
      "epoch: 24 step: 657, loss is 0.18730077147483826\n",
      "epoch: 24 step: 658, loss is 0.10369110107421875\n",
      "epoch: 24 step: 659, loss is 0.1158597320318222\n",
      "epoch: 24 step: 660, loss is 0.07752526551485062\n",
      "epoch: 24 step: 661, loss is 0.1348665952682495\n",
      "epoch: 24 step: 662, loss is 0.2616956830024719\n",
      "epoch: 24 step: 663, loss is 0.20539255440235138\n",
      "epoch: 24 step: 664, loss is 0.19948230683803558\n",
      "epoch: 24 step: 665, loss is 0.07921012490987778\n",
      "epoch: 24 step: 666, loss is 0.2945992946624756\n",
      "epoch: 24 step: 667, loss is 0.22789950668811798\n",
      "epoch: 24 step: 668, loss is 0.153903067111969\n",
      "epoch: 24 step: 669, loss is 0.0859149619936943\n",
      "epoch: 24 step: 670, loss is 0.08634921163320541\n",
      "epoch: 24 step: 671, loss is 0.2873840034008026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 672, loss is 0.1705729365348816\n",
      "epoch: 24 step: 673, loss is 0.05531397461891174\n",
      "epoch: 24 step: 674, loss is 0.09337200969457626\n",
      "epoch: 24 step: 675, loss is 0.28651177883148193\n",
      "epoch: 24 step: 676, loss is 0.26001712679862976\n",
      "epoch: 24 step: 677, loss is 0.24584615230560303\n",
      "epoch: 24 step: 678, loss is 0.21853497624397278\n",
      "epoch: 24 step: 679, loss is 0.1822478026151657\n",
      "epoch: 24 step: 680, loss is 0.2173747718334198\n",
      "epoch: 24 step: 681, loss is 0.1572403907775879\n",
      "epoch: 24 step: 682, loss is 0.26830437779426575\n",
      "epoch: 24 step: 683, loss is 0.2554384469985962\n",
      "epoch: 24 step: 684, loss is 0.26205331087112427\n",
      "epoch: 24 step: 685, loss is 0.19606913626194\n",
      "epoch: 24 step: 686, loss is 0.1753617376089096\n",
      "epoch: 24 step: 687, loss is 0.14735300838947296\n",
      "epoch: 24 step: 688, loss is 0.1748056560754776\n",
      "epoch: 24 step: 689, loss is 0.2187766134738922\n",
      "epoch: 24 step: 690, loss is 0.2524181008338928\n",
      "epoch: 24 step: 691, loss is 0.3052734434604645\n",
      "epoch: 24 step: 692, loss is 0.2465888410806656\n",
      "epoch: 24 step: 693, loss is 0.3238637447357178\n",
      "epoch: 24 step: 694, loss is 0.2450741082429886\n",
      "epoch: 24 step: 695, loss is 0.12171679735183716\n",
      "epoch: 24 step: 696, loss is 0.28381800651550293\n",
      "epoch: 24 step: 697, loss is 0.0922810360789299\n",
      "epoch: 24 step: 698, loss is 0.1314024031162262\n",
      "epoch: 24 step: 699, loss is 0.22424322366714478\n",
      "epoch: 24 step: 700, loss is 0.22162353992462158\n",
      "epoch: 24 step: 701, loss is 0.18224672973155975\n",
      "epoch: 24 step: 702, loss is 0.30433768033981323\n",
      "epoch: 24 step: 703, loss is 0.2650562822818756\n",
      "epoch: 24 step: 704, loss is 0.2511012554168701\n",
      "epoch: 24 step: 705, loss is 0.15514890849590302\n",
      "epoch: 24 step: 706, loss is 0.22786201536655426\n",
      "epoch: 24 step: 707, loss is 0.1323692500591278\n",
      "epoch: 24 step: 708, loss is 0.23820671439170837\n",
      "epoch: 24 step: 709, loss is 0.22116486728191376\n",
      "epoch: 24 step: 710, loss is 0.12600672245025635\n",
      "epoch: 24 step: 711, loss is 0.1778634786605835\n",
      "epoch: 24 step: 712, loss is 0.21358704566955566\n",
      "epoch: 24 step: 713, loss is 0.18858788907527924\n",
      "epoch: 24 step: 714, loss is 0.09059129655361176\n",
      "epoch: 24 step: 715, loss is 0.20202144980430603\n",
      "epoch: 24 step: 716, loss is 0.2593083381652832\n",
      "epoch: 24 step: 717, loss is 0.1200152188539505\n",
      "epoch: 24 step: 718, loss is 0.20270605385303497\n",
      "epoch: 24 step: 719, loss is 0.21799814701080322\n",
      "epoch: 24 step: 720, loss is 0.1726801097393036\n",
      "epoch: 24 step: 721, loss is 0.24154388904571533\n",
      "epoch: 24 step: 722, loss is 0.17903654277324677\n",
      "epoch: 24 step: 723, loss is 0.18479610979557037\n",
      "epoch: 24 step: 724, loss is 0.21991987526416779\n",
      "epoch: 24 step: 725, loss is 0.15391120314598083\n",
      "epoch: 24 step: 726, loss is 0.2594403624534607\n",
      "epoch: 24 step: 727, loss is 0.26070690155029297\n",
      "epoch: 24 step: 728, loss is 0.1573272943496704\n",
      "epoch: 24 step: 729, loss is 0.16598455607891083\n",
      "epoch: 24 step: 730, loss is 0.13345378637313843\n",
      "epoch: 24 step: 731, loss is 0.12677854299545288\n",
      "epoch: 24 step: 732, loss is 0.1912158876657486\n",
      "epoch: 24 step: 733, loss is 0.1843131184577942\n",
      "epoch: 24 step: 734, loss is 0.15190096199512482\n",
      "epoch: 24 step: 735, loss is 0.21435672044754028\n",
      "epoch: 24 step: 736, loss is 0.129866823554039\n",
      "epoch: 24 step: 737, loss is 0.1189185082912445\n",
      "epoch: 24 step: 738, loss is 0.2923928201198578\n",
      "epoch: 24 step: 739, loss is 0.18760071694850922\n",
      "epoch: 24 step: 740, loss is 0.14880895614624023\n",
      "epoch: 24 step: 741, loss is 0.2121378630399704\n",
      "epoch: 24 step: 742, loss is 0.16949178278446198\n",
      "epoch: 24 step: 743, loss is 0.3073161244392395\n",
      "epoch: 24 step: 744, loss is 0.1563328057527542\n",
      "epoch: 24 step: 745, loss is 0.09035530686378479\n",
      "epoch: 24 step: 746, loss is 0.37162715196609497\n",
      "epoch: 24 step: 747, loss is 0.20161661505699158\n",
      "epoch: 24 step: 748, loss is 0.1527649164199829\n",
      "epoch: 24 step: 749, loss is 0.20647364854812622\n",
      "epoch: 24 step: 750, loss is 0.18593385815620422\n",
      "epoch: 24 step: 751, loss is 0.3749810755252838\n",
      "epoch: 24 step: 752, loss is 0.1262223869562149\n",
      "epoch: 24 step: 753, loss is 0.0872807502746582\n",
      "epoch: 24 step: 754, loss is 0.11570549011230469\n",
      "epoch: 24 step: 755, loss is 0.30240610241889954\n",
      "epoch: 24 step: 756, loss is 0.17689718306064606\n",
      "epoch: 24 step: 757, loss is 0.23872675001621246\n",
      "epoch: 24 step: 758, loss is 0.15057368576526642\n",
      "epoch: 24 step: 759, loss is 0.05489857867360115\n",
      "epoch: 24 step: 760, loss is 0.11872511357069016\n",
      "epoch: 24 step: 761, loss is 0.2168239951133728\n",
      "epoch: 24 step: 762, loss is 0.17499490082263947\n",
      "epoch: 24 step: 763, loss is 0.14413730800151825\n",
      "epoch: 24 step: 764, loss is 0.1358969658613205\n",
      "epoch: 24 step: 765, loss is 0.1918732076883316\n",
      "epoch: 24 step: 766, loss is 0.11850413680076599\n",
      "epoch: 24 step: 767, loss is 0.24211202561855316\n",
      "epoch: 24 step: 768, loss is 0.23519305884838104\n",
      "epoch: 24 step: 769, loss is 0.13067489862442017\n",
      "epoch: 24 step: 770, loss is 0.14651978015899658\n",
      "epoch: 24 step: 771, loss is 0.2282974272966385\n",
      "epoch: 24 step: 772, loss is 0.1659659445285797\n",
      "epoch: 24 step: 773, loss is 0.10435615479946136\n",
      "epoch: 24 step: 774, loss is 0.21710439026355743\n",
      "epoch: 24 step: 775, loss is 0.21993355453014374\n",
      "epoch: 24 step: 776, loss is 0.29314571619033813\n",
      "epoch: 24 step: 777, loss is 0.25907301902770996\n",
      "epoch: 24 step: 778, loss is 0.16719630360603333\n",
      "epoch: 24 step: 779, loss is 0.12810994684696198\n",
      "epoch: 24 step: 780, loss is 0.18107394874095917\n",
      "epoch: 24 step: 781, loss is 0.16695144772529602\n",
      "epoch: 24 step: 782, loss is 0.15013840794563293\n",
      "epoch: 24 step: 783, loss is 0.21641840040683746\n",
      "epoch: 24 step: 784, loss is 0.16396212577819824\n",
      "epoch: 24 step: 785, loss is 0.12721839547157288\n",
      "epoch: 24 step: 786, loss is 0.18597979843616486\n",
      "epoch: 24 step: 787, loss is 0.19372837245464325\n",
      "epoch: 24 step: 788, loss is 0.10176710039377213\n",
      "epoch: 24 step: 789, loss is 0.21882310509681702\n",
      "epoch: 24 step: 790, loss is 0.2415221780538559\n",
      "epoch: 24 step: 791, loss is 0.12255432456731796\n",
      "epoch: 24 step: 792, loss is 0.216414675116539\n",
      "epoch: 24 step: 793, loss is 0.12401954084634781\n",
      "epoch: 24 step: 794, loss is 0.24788427352905273\n",
      "epoch: 24 step: 795, loss is 0.2440015822649002\n",
      "epoch: 24 step: 796, loss is 0.14353850483894348\n",
      "epoch: 24 step: 797, loss is 0.29113879799842834\n",
      "epoch: 24 step: 798, loss is 0.23228928446769714\n",
      "epoch: 24 step: 799, loss is 0.12915152311325073\n",
      "epoch: 24 step: 800, loss is 0.2666183114051819\n",
      "epoch: 24 step: 801, loss is 0.20627784729003906\n",
      "epoch: 24 step: 802, loss is 0.12742960453033447\n",
      "epoch: 24 step: 803, loss is 0.1691358983516693\n",
      "epoch: 24 step: 804, loss is 0.2720501124858856\n",
      "epoch: 24 step: 805, loss is 0.1207527443766594\n",
      "epoch: 24 step: 806, loss is 0.11195900291204453\n",
      "epoch: 24 step: 807, loss is 0.21492251753807068\n",
      "epoch: 24 step: 808, loss is 0.14547312259674072\n",
      "epoch: 24 step: 809, loss is 0.24574482440948486\n",
      "epoch: 24 step: 810, loss is 0.21062718331813812\n",
      "epoch: 24 step: 811, loss is 0.14703211188316345\n",
      "epoch: 24 step: 812, loss is 0.361652135848999\n",
      "epoch: 24 step: 813, loss is 0.2465156465768814\n",
      "epoch: 24 step: 814, loss is 0.1168280765414238\n",
      "epoch: 24 step: 815, loss is 0.273651123046875\n",
      "epoch: 24 step: 816, loss is 0.1609235554933548\n",
      "epoch: 24 step: 817, loss is 0.05820450559258461\n",
      "epoch: 24 step: 818, loss is 0.2018616497516632\n",
      "epoch: 24 step: 819, loss is 0.1644715964794159\n",
      "epoch: 24 step: 820, loss is 0.13951154053211212\n",
      "epoch: 24 step: 821, loss is 0.22128833830356598\n",
      "epoch: 24 step: 822, loss is 0.2150363177061081\n",
      "epoch: 24 step: 823, loss is 0.2067842036485672\n",
      "epoch: 24 step: 824, loss is 0.27563899755477905\n",
      "epoch: 24 step: 825, loss is 0.19730456173419952\n",
      "epoch: 24 step: 826, loss is 0.33071789145469666\n",
      "epoch: 24 step: 827, loss is 0.19891571998596191\n",
      "epoch: 24 step: 828, loss is 0.2260647714138031\n",
      "epoch: 24 step: 829, loss is 0.21247106790542603\n",
      "epoch: 24 step: 830, loss is 0.182536780834198\n",
      "epoch: 24 step: 831, loss is 0.1889868527650833\n",
      "epoch: 24 step: 832, loss is 0.15034128725528717\n",
      "epoch: 24 step: 833, loss is 0.27178841829299927\n",
      "epoch: 24 step: 834, loss is 0.23022766411304474\n",
      "epoch: 24 step: 835, loss is 0.18502330780029297\n",
      "epoch: 24 step: 836, loss is 0.19742926955223083\n",
      "epoch: 24 step: 837, loss is 0.209378182888031\n",
      "epoch: 24 step: 838, loss is 0.17164649069309235\n",
      "epoch: 24 step: 839, loss is 0.2885122001171112\n",
      "epoch: 24 step: 840, loss is 0.16361768543720245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 841, loss is 0.2217133790254593\n",
      "epoch: 24 step: 842, loss is 0.16253431141376495\n",
      "epoch: 24 step: 843, loss is 0.2779846787452698\n",
      "epoch: 24 step: 844, loss is 0.10298371315002441\n",
      "epoch: 24 step: 845, loss is 0.3361699879169464\n",
      "epoch: 24 step: 846, loss is 0.08083748072385788\n",
      "epoch: 24 step: 847, loss is 0.14768843352794647\n",
      "epoch: 24 step: 848, loss is 0.25639453530311584\n",
      "epoch: 24 step: 849, loss is 0.15060777962207794\n",
      "epoch: 24 step: 850, loss is 0.23002572357654572\n",
      "epoch: 24 step: 851, loss is 0.12490157783031464\n",
      "epoch: 24 step: 852, loss is 0.13322077691555023\n",
      "epoch: 24 step: 853, loss is 0.24901174008846283\n",
      "epoch: 24 step: 854, loss is 0.13430295884609222\n",
      "epoch: 24 step: 855, loss is 0.2302769422531128\n",
      "epoch: 24 step: 856, loss is 0.2630464732646942\n",
      "epoch: 24 step: 857, loss is 0.20344597101211548\n",
      "epoch: 24 step: 858, loss is 0.1435364931821823\n",
      "epoch: 24 step: 859, loss is 0.11074836552143097\n",
      "epoch: 24 step: 860, loss is 0.13916637003421783\n",
      "epoch: 24 step: 861, loss is 0.33541423082351685\n",
      "epoch: 24 step: 862, loss is 0.17485184967517853\n",
      "epoch: 24 step: 863, loss is 0.33087313175201416\n",
      "epoch: 24 step: 864, loss is 0.12919116020202637\n",
      "epoch: 24 step: 865, loss is 0.2410891354084015\n",
      "epoch: 24 step: 866, loss is 0.2144232988357544\n",
      "epoch: 24 step: 867, loss is 0.18851014971733093\n",
      "epoch: 24 step: 868, loss is 0.2006622701883316\n",
      "epoch: 24 step: 869, loss is 0.12233465164899826\n",
      "epoch: 24 step: 870, loss is 0.2280549257993698\n",
      "epoch: 24 step: 871, loss is 0.32043880224227905\n",
      "epoch: 24 step: 872, loss is 0.18087613582611084\n",
      "epoch: 24 step: 873, loss is 0.3684944808483124\n",
      "epoch: 24 step: 874, loss is 0.3852415084838867\n",
      "epoch: 24 step: 875, loss is 0.10991109162569046\n",
      "epoch: 24 step: 876, loss is 0.23317597806453705\n",
      "epoch: 24 step: 877, loss is 0.12263970077037811\n",
      "epoch: 24 step: 878, loss is 0.12860137224197388\n",
      "epoch: 24 step: 879, loss is 0.2765299081802368\n",
      "epoch: 24 step: 880, loss is 0.07756837457418442\n",
      "epoch: 24 step: 881, loss is 0.4057898223400116\n",
      "epoch: 24 step: 882, loss is 0.31611794233322144\n",
      "epoch: 24 step: 883, loss is 0.2378845065832138\n",
      "epoch: 24 step: 884, loss is 0.09689277410507202\n",
      "epoch: 24 step: 885, loss is 0.1537134051322937\n",
      "epoch: 24 step: 886, loss is 0.12160130590200424\n",
      "epoch: 24 step: 887, loss is 0.05543873831629753\n",
      "epoch: 24 step: 888, loss is 0.1938025951385498\n",
      "epoch: 24 step: 889, loss is 0.21742892265319824\n",
      "epoch: 24 step: 890, loss is 0.28263357281684875\n",
      "epoch: 24 step: 891, loss is 0.3273674547672272\n",
      "epoch: 24 step: 892, loss is 0.1746552288532257\n",
      "epoch: 24 step: 893, loss is 0.12566505372524261\n",
      "epoch: 24 step: 894, loss is 0.1520383208990097\n",
      "epoch: 24 step: 895, loss is 0.24345307052135468\n",
      "epoch: 24 step: 896, loss is 0.1699986308813095\n",
      "epoch: 24 step: 897, loss is 0.11181151121854782\n",
      "epoch: 24 step: 898, loss is 0.4521210789680481\n",
      "epoch: 24 step: 899, loss is 0.18166333436965942\n",
      "epoch: 24 step: 900, loss is 0.20565752685070038\n",
      "epoch: 24 step: 901, loss is 0.20691624283790588\n",
      "epoch: 24 step: 902, loss is 0.23883119225502014\n",
      "epoch: 24 step: 903, loss is 0.15099488198757172\n",
      "epoch: 24 step: 904, loss is 0.1872347742319107\n",
      "epoch: 24 step: 905, loss is 0.3727351427078247\n",
      "epoch: 24 step: 906, loss is 0.17320069670677185\n",
      "epoch: 24 step: 907, loss is 0.06217871606349945\n",
      "epoch: 24 step: 908, loss is 0.14551301300525665\n",
      "epoch: 24 step: 909, loss is 0.06459714472293854\n",
      "epoch: 24 step: 910, loss is 0.15683680772781372\n",
      "epoch: 24 step: 911, loss is 0.22954873740673065\n",
      "epoch: 24 step: 912, loss is 0.1621396839618683\n",
      "epoch: 24 step: 913, loss is 0.13540755212306976\n",
      "epoch: 24 step: 914, loss is 0.21754059195518494\n",
      "epoch: 24 step: 915, loss is 0.12299606949090958\n",
      "epoch: 24 step: 916, loss is 0.2279232293367386\n",
      "epoch: 24 step: 917, loss is 0.2737453579902649\n",
      "epoch: 24 step: 918, loss is 0.22281359136104584\n",
      "epoch: 24 step: 919, loss is 0.17758403718471527\n",
      "epoch: 24 step: 920, loss is 0.1758212298154831\n",
      "epoch: 24 step: 921, loss is 0.16752345860004425\n",
      "epoch: 24 step: 922, loss is 0.19253335893154144\n",
      "epoch: 24 step: 923, loss is 0.11093272268772125\n",
      "epoch: 24 step: 924, loss is 0.11746058613061905\n",
      "epoch: 24 step: 925, loss is 0.12032337486743927\n",
      "epoch: 24 step: 926, loss is 0.12956826388835907\n",
      "epoch: 24 step: 927, loss is 0.29109159111976624\n",
      "epoch: 24 step: 928, loss is 0.06735168397426605\n",
      "epoch: 24 step: 929, loss is 0.256122350692749\n",
      "epoch: 24 step: 930, loss is 0.2532937228679657\n",
      "epoch: 24 step: 931, loss is 0.13868452608585358\n",
      "epoch: 24 step: 932, loss is 0.23792000114917755\n",
      "epoch: 24 step: 933, loss is 0.23286749422550201\n",
      "epoch: 24 step: 934, loss is 0.08982512354850769\n",
      "epoch: 24 step: 935, loss is 0.20710161328315735\n",
      "epoch: 24 step: 936, loss is 0.23351876437664032\n",
      "epoch: 24 step: 937, loss is 0.12891785800457\n",
      "epoch: 25 step: 1, loss is 0.1543506532907486\n",
      "epoch: 25 step: 2, loss is 0.13393475115299225\n",
      "epoch: 25 step: 3, loss is 0.11297012120485306\n",
      "epoch: 25 step: 4, loss is 0.2095605731010437\n",
      "epoch: 25 step: 5, loss is 0.16758327186107635\n",
      "epoch: 25 step: 6, loss is 0.21466989815235138\n",
      "epoch: 25 step: 7, loss is 0.1754574477672577\n",
      "epoch: 25 step: 8, loss is 0.09550085663795471\n",
      "epoch: 25 step: 9, loss is 0.09056341648101807\n",
      "epoch: 25 step: 10, loss is 0.2634738087654114\n",
      "epoch: 25 step: 11, loss is 0.2041841596364975\n",
      "epoch: 25 step: 12, loss is 0.0707770362496376\n",
      "epoch: 25 step: 13, loss is 0.21694663166999817\n",
      "epoch: 25 step: 14, loss is 0.2423507273197174\n",
      "epoch: 25 step: 15, loss is 0.19098994135856628\n",
      "epoch: 25 step: 16, loss is 0.11149806529283524\n",
      "epoch: 25 step: 17, loss is 0.13219723105430603\n",
      "epoch: 25 step: 18, loss is 0.2860775887966156\n",
      "epoch: 25 step: 19, loss is 0.2690788209438324\n",
      "epoch: 25 step: 20, loss is 0.21419930458068848\n",
      "epoch: 25 step: 21, loss is 0.09476684033870697\n",
      "epoch: 25 step: 22, loss is 0.24745865166187286\n",
      "epoch: 25 step: 23, loss is 0.2843981087207794\n",
      "epoch: 25 step: 24, loss is 0.12729234993457794\n",
      "epoch: 25 step: 25, loss is 0.274799108505249\n",
      "epoch: 25 step: 26, loss is 0.15162639319896698\n",
      "epoch: 25 step: 27, loss is 0.07978708297014236\n",
      "epoch: 25 step: 28, loss is 0.21766729652881622\n",
      "epoch: 25 step: 29, loss is 0.1169990748167038\n",
      "epoch: 25 step: 30, loss is 0.22849100828170776\n",
      "epoch: 25 step: 31, loss is 0.11089891940355301\n",
      "epoch: 25 step: 32, loss is 0.06884735822677612\n",
      "epoch: 25 step: 33, loss is 0.1330161839723587\n",
      "epoch: 25 step: 34, loss is 0.2826476991176605\n",
      "epoch: 25 step: 35, loss is 0.20597884058952332\n",
      "epoch: 25 step: 36, loss is 0.19202174246311188\n",
      "epoch: 25 step: 37, loss is 0.3051711916923523\n",
      "epoch: 25 step: 38, loss is 0.1768878549337387\n",
      "epoch: 25 step: 39, loss is 0.20701339840888977\n",
      "epoch: 25 step: 40, loss is 0.1367512196302414\n",
      "epoch: 25 step: 41, loss is 0.22187654674053192\n",
      "epoch: 25 step: 42, loss is 0.21377502381801605\n",
      "epoch: 25 step: 43, loss is 0.2532910704612732\n",
      "epoch: 25 step: 44, loss is 0.08269111812114716\n",
      "epoch: 25 step: 45, loss is 0.2774556875228882\n",
      "epoch: 25 step: 46, loss is 0.14887122809886932\n",
      "epoch: 25 step: 47, loss is 0.2152104377746582\n",
      "epoch: 25 step: 48, loss is 0.11351797729730606\n",
      "epoch: 25 step: 49, loss is 0.18015217781066895\n",
      "epoch: 25 step: 50, loss is 0.16918416321277618\n",
      "epoch: 25 step: 51, loss is 0.23390193283557892\n",
      "epoch: 25 step: 52, loss is 0.2870306968688965\n",
      "epoch: 25 step: 53, loss is 0.09683609753847122\n",
      "epoch: 25 step: 54, loss is 0.1261119842529297\n",
      "epoch: 25 step: 55, loss is 0.0988992303609848\n",
      "epoch: 25 step: 56, loss is 0.21866270899772644\n",
      "epoch: 25 step: 57, loss is 0.15064583718776703\n",
      "epoch: 25 step: 58, loss is 0.20509091019630432\n",
      "epoch: 25 step: 59, loss is 0.16693483293056488\n",
      "epoch: 25 step: 60, loss is 0.3124733567237854\n",
      "epoch: 25 step: 61, loss is 0.33677053451538086\n",
      "epoch: 25 step: 62, loss is 0.12648355960845947\n",
      "epoch: 25 step: 63, loss is 0.2923668622970581\n",
      "epoch: 25 step: 64, loss is 0.20076270401477814\n",
      "epoch: 25 step: 65, loss is 0.28087344765663147\n",
      "epoch: 25 step: 66, loss is 0.33061420917510986\n",
      "epoch: 25 step: 67, loss is 0.21449409425258636\n",
      "epoch: 25 step: 68, loss is 0.26006537675857544\n",
      "epoch: 25 step: 69, loss is 0.1390092670917511\n",
      "epoch: 25 step: 70, loss is 0.2707845866680145\n",
      "epoch: 25 step: 71, loss is 0.1189669519662857\n",
      "epoch: 25 step: 72, loss is 0.12377595156431198\n",
      "epoch: 25 step: 73, loss is 0.22629354894161224\n",
      "epoch: 25 step: 74, loss is 0.15937626361846924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 75, loss is 0.15913020074367523\n",
      "epoch: 25 step: 76, loss is 0.15652996301651\n",
      "epoch: 25 step: 77, loss is 0.027178244665265083\n",
      "epoch: 25 step: 78, loss is 0.17729966342449188\n",
      "epoch: 25 step: 79, loss is 0.2329685091972351\n",
      "epoch: 25 step: 80, loss is 0.10994891077280045\n",
      "epoch: 25 step: 81, loss is 0.308835506439209\n",
      "epoch: 25 step: 82, loss is 0.327875018119812\n",
      "epoch: 25 step: 83, loss is 0.2899133265018463\n",
      "epoch: 25 step: 84, loss is 0.08980308473110199\n",
      "epoch: 25 step: 85, loss is 0.17168381810188293\n",
      "epoch: 25 step: 86, loss is 0.18813925981521606\n",
      "epoch: 25 step: 87, loss is 0.28264203667640686\n",
      "epoch: 25 step: 88, loss is 0.11588034778833389\n",
      "epoch: 25 step: 89, loss is 0.17426814138889313\n",
      "epoch: 25 step: 90, loss is 0.18990503251552582\n",
      "epoch: 25 step: 91, loss is 0.07740569114685059\n",
      "epoch: 25 step: 92, loss is 0.12520664930343628\n",
      "epoch: 25 step: 93, loss is 0.3592694401741028\n",
      "epoch: 25 step: 94, loss is 0.23873727023601532\n",
      "epoch: 25 step: 95, loss is 0.2128201127052307\n",
      "epoch: 25 step: 96, loss is 0.14363452792167664\n",
      "epoch: 25 step: 97, loss is 0.2812653183937073\n",
      "epoch: 25 step: 98, loss is 0.15094099938869476\n",
      "epoch: 25 step: 99, loss is 0.14905774593353271\n",
      "epoch: 25 step: 100, loss is 0.18957120180130005\n",
      "epoch: 25 step: 101, loss is 0.22707007825374603\n",
      "epoch: 25 step: 102, loss is 0.21012137830257416\n",
      "epoch: 25 step: 103, loss is 0.1113223284482956\n",
      "epoch: 25 step: 104, loss is 0.11359500139951706\n",
      "epoch: 25 step: 105, loss is 0.30977126955986023\n",
      "epoch: 25 step: 106, loss is 0.4630852937698364\n",
      "epoch: 25 step: 107, loss is 0.12167485058307648\n",
      "epoch: 25 step: 108, loss is 0.10060818493366241\n",
      "epoch: 25 step: 109, loss is 0.10237421840429306\n",
      "epoch: 25 step: 110, loss is 0.13961462676525116\n",
      "epoch: 25 step: 111, loss is 0.11926421523094177\n",
      "epoch: 25 step: 112, loss is 0.062441304326057434\n",
      "epoch: 25 step: 113, loss is 0.09939437359571457\n",
      "epoch: 25 step: 114, loss is 0.17967094480991364\n",
      "epoch: 25 step: 115, loss is 0.06448286771774292\n",
      "epoch: 25 step: 116, loss is 0.20985136926174164\n",
      "epoch: 25 step: 117, loss is 0.2443987876176834\n",
      "epoch: 25 step: 118, loss is 0.11852291226387024\n",
      "epoch: 25 step: 119, loss is 0.09773683547973633\n",
      "epoch: 25 step: 120, loss is 0.12500323355197906\n",
      "epoch: 25 step: 121, loss is 0.09963902086019516\n",
      "epoch: 25 step: 122, loss is 0.2344076931476593\n",
      "epoch: 25 step: 123, loss is 0.37167465686798096\n",
      "epoch: 25 step: 124, loss is 0.11684200167655945\n",
      "epoch: 25 step: 125, loss is 0.20410697162151337\n",
      "epoch: 25 step: 126, loss is 0.13528704643249512\n",
      "epoch: 25 step: 127, loss is 0.15406298637390137\n",
      "epoch: 25 step: 128, loss is 0.1927952915430069\n",
      "epoch: 25 step: 129, loss is 0.21054525673389435\n",
      "epoch: 25 step: 130, loss is 0.17553065717220306\n",
      "epoch: 25 step: 131, loss is 0.13075657188892365\n",
      "epoch: 25 step: 132, loss is 0.13842664659023285\n",
      "epoch: 25 step: 133, loss is 0.1949756145477295\n",
      "epoch: 25 step: 134, loss is 0.1788381040096283\n",
      "epoch: 25 step: 135, loss is 0.2356911450624466\n",
      "epoch: 25 step: 136, loss is 0.14935189485549927\n",
      "epoch: 25 step: 137, loss is 0.13131119310855865\n",
      "epoch: 25 step: 138, loss is 0.20687900483608246\n",
      "epoch: 25 step: 139, loss is 0.20885592699050903\n",
      "epoch: 25 step: 140, loss is 0.14614588022232056\n",
      "epoch: 25 step: 141, loss is 0.25998520851135254\n",
      "epoch: 25 step: 142, loss is 0.3269149959087372\n",
      "epoch: 25 step: 143, loss is 0.18255086243152618\n",
      "epoch: 25 step: 144, loss is 0.19152896106243134\n",
      "epoch: 25 step: 145, loss is 0.16083066165447235\n",
      "epoch: 25 step: 146, loss is 0.14153195917606354\n",
      "epoch: 25 step: 147, loss is 0.25063830614089966\n",
      "epoch: 25 step: 148, loss is 0.3379073143005371\n",
      "epoch: 25 step: 149, loss is 0.29070761799812317\n",
      "epoch: 25 step: 150, loss is 0.09656679630279541\n",
      "epoch: 25 step: 151, loss is 0.3101048171520233\n",
      "epoch: 25 step: 152, loss is 0.12865932285785675\n",
      "epoch: 25 step: 153, loss is 0.2854619026184082\n",
      "epoch: 25 step: 154, loss is 0.1318400353193283\n",
      "epoch: 25 step: 155, loss is 0.160443514585495\n",
      "epoch: 25 step: 156, loss is 0.11663265526294708\n",
      "epoch: 25 step: 157, loss is 0.1571541130542755\n",
      "epoch: 25 step: 158, loss is 0.2230755239725113\n",
      "epoch: 25 step: 159, loss is 0.23712074756622314\n",
      "epoch: 25 step: 160, loss is 0.1433928906917572\n",
      "epoch: 25 step: 161, loss is 0.3344578444957733\n",
      "epoch: 25 step: 162, loss is 0.13420724868774414\n",
      "epoch: 25 step: 163, loss is 0.1517423838376999\n",
      "epoch: 25 step: 164, loss is 0.14933499693870544\n",
      "epoch: 25 step: 165, loss is 0.3729710876941681\n",
      "epoch: 25 step: 166, loss is 0.24656467139720917\n",
      "epoch: 25 step: 167, loss is 0.14919701218605042\n",
      "epoch: 25 step: 168, loss is 0.1647440493106842\n",
      "epoch: 25 step: 169, loss is 0.19870957732200623\n",
      "epoch: 25 step: 170, loss is 0.2196507304906845\n",
      "epoch: 25 step: 171, loss is 0.1712343990802765\n",
      "epoch: 25 step: 172, loss is 0.18258890509605408\n",
      "epoch: 25 step: 173, loss is 0.13994330167770386\n",
      "epoch: 25 step: 174, loss is 0.15469343960285187\n",
      "epoch: 25 step: 175, loss is 0.23736825585365295\n",
      "epoch: 25 step: 176, loss is 0.174419105052948\n",
      "epoch: 25 step: 177, loss is 0.11013246327638626\n",
      "epoch: 25 step: 178, loss is 0.1920824646949768\n",
      "epoch: 25 step: 179, loss is 0.3143150508403778\n",
      "epoch: 25 step: 180, loss is 0.10215789079666138\n",
      "epoch: 25 step: 181, loss is 0.10467812418937683\n",
      "epoch: 25 step: 182, loss is 0.3982316553592682\n",
      "epoch: 25 step: 183, loss is 0.03988020867109299\n",
      "epoch: 25 step: 184, loss is 0.20135557651519775\n",
      "epoch: 25 step: 185, loss is 0.20810578763484955\n",
      "epoch: 25 step: 186, loss is 0.07595033198595047\n",
      "epoch: 25 step: 187, loss is 0.2285194993019104\n",
      "epoch: 25 step: 188, loss is 0.197156623005867\n",
      "epoch: 25 step: 189, loss is 0.16839304566383362\n",
      "epoch: 25 step: 190, loss is 0.09852523356676102\n",
      "epoch: 25 step: 191, loss is 0.12550285458564758\n",
      "epoch: 25 step: 192, loss is 0.1313430368900299\n",
      "epoch: 25 step: 193, loss is 0.280901700258255\n",
      "epoch: 25 step: 194, loss is 0.20320916175842285\n",
      "epoch: 25 step: 195, loss is 0.14138053357601166\n",
      "epoch: 25 step: 196, loss is 0.2863539159297943\n",
      "epoch: 25 step: 197, loss is 0.2340618222951889\n",
      "epoch: 25 step: 198, loss is 0.10216358304023743\n",
      "epoch: 25 step: 199, loss is 0.15773023664951324\n",
      "epoch: 25 step: 200, loss is 0.1988954395055771\n",
      "epoch: 25 step: 201, loss is 0.2713923752307892\n",
      "epoch: 25 step: 202, loss is 0.17786598205566406\n",
      "epoch: 25 step: 203, loss is 0.2002786248922348\n",
      "epoch: 25 step: 204, loss is 0.17626534402370453\n",
      "epoch: 25 step: 205, loss is 0.12288882583379745\n",
      "epoch: 25 step: 206, loss is 0.20844022929668427\n",
      "epoch: 25 step: 207, loss is 0.12441512197256088\n",
      "epoch: 25 step: 208, loss is 0.20338118076324463\n",
      "epoch: 25 step: 209, loss is 0.17621757090091705\n",
      "epoch: 25 step: 210, loss is 0.15567873418331146\n",
      "epoch: 25 step: 211, loss is 0.18144646286964417\n",
      "epoch: 25 step: 212, loss is 0.21439288556575775\n",
      "epoch: 25 step: 213, loss is 0.14262785017490387\n",
      "epoch: 25 step: 214, loss is 0.09952503442764282\n",
      "epoch: 25 step: 215, loss is 0.3019118010997772\n",
      "epoch: 25 step: 216, loss is 0.23447299003601074\n",
      "epoch: 25 step: 217, loss is 0.41420960426330566\n",
      "epoch: 25 step: 218, loss is 0.20741473138332367\n",
      "epoch: 25 step: 219, loss is 0.17106954753398895\n",
      "epoch: 25 step: 220, loss is 0.16485300660133362\n",
      "epoch: 25 step: 221, loss is 0.19931699335575104\n",
      "epoch: 25 step: 222, loss is 0.1903972625732422\n",
      "epoch: 25 step: 223, loss is 0.1613513082265854\n",
      "epoch: 25 step: 224, loss is 0.05291774496436119\n",
      "epoch: 25 step: 225, loss is 0.0957459956407547\n",
      "epoch: 25 step: 226, loss is 0.2752102315425873\n",
      "epoch: 25 step: 227, loss is 0.07729479670524597\n",
      "epoch: 25 step: 228, loss is 0.18443775177001953\n",
      "epoch: 25 step: 229, loss is 0.1520308554172516\n",
      "epoch: 25 step: 230, loss is 0.14633555710315704\n",
      "epoch: 25 step: 231, loss is 0.21270553767681122\n",
      "epoch: 25 step: 232, loss is 0.1463870406150818\n",
      "epoch: 25 step: 233, loss is 0.19629912078380585\n",
      "epoch: 25 step: 234, loss is 0.281669020652771\n",
      "epoch: 25 step: 235, loss is 0.3246493339538574\n",
      "epoch: 25 step: 236, loss is 0.27271032333374023\n",
      "epoch: 25 step: 237, loss is 0.139070063829422\n",
      "epoch: 25 step: 238, loss is 0.11408524215221405\n",
      "epoch: 25 step: 239, loss is 0.05565658584237099\n",
      "epoch: 25 step: 240, loss is 0.11939665675163269\n",
      "epoch: 25 step: 241, loss is 0.12121166288852692\n",
      "epoch: 25 step: 242, loss is 0.1632867157459259\n",
      "epoch: 25 step: 243, loss is 0.10035616904497147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 244, loss is 0.2765815556049347\n",
      "epoch: 25 step: 245, loss is 0.198255717754364\n",
      "epoch: 25 step: 246, loss is 0.12533053755760193\n",
      "epoch: 25 step: 247, loss is 0.33112001419067383\n",
      "epoch: 25 step: 248, loss is 0.0698968693614006\n",
      "epoch: 25 step: 249, loss is 0.3253015875816345\n",
      "epoch: 25 step: 250, loss is 0.15597231686115265\n",
      "epoch: 25 step: 251, loss is 0.13189974427223206\n",
      "epoch: 25 step: 252, loss is 0.11938020586967468\n",
      "epoch: 25 step: 253, loss is 0.2066422700881958\n",
      "epoch: 25 step: 254, loss is 0.1734878569841385\n",
      "epoch: 25 step: 255, loss is 0.09637133032083511\n",
      "epoch: 25 step: 256, loss is 0.25589829683303833\n",
      "epoch: 25 step: 257, loss is 0.18888726830482483\n",
      "epoch: 25 step: 258, loss is 0.19407998025417328\n",
      "epoch: 25 step: 259, loss is 0.16197066009044647\n",
      "epoch: 25 step: 260, loss is 0.20670050382614136\n",
      "epoch: 25 step: 261, loss is 0.15964609384536743\n",
      "epoch: 25 step: 262, loss is 0.2544020414352417\n",
      "epoch: 25 step: 263, loss is 0.111028291285038\n",
      "epoch: 25 step: 264, loss is 0.43819302320480347\n",
      "epoch: 25 step: 265, loss is 0.13849890232086182\n",
      "epoch: 25 step: 266, loss is 0.18021392822265625\n",
      "epoch: 25 step: 267, loss is 0.30356085300445557\n",
      "epoch: 25 step: 268, loss is 0.09585839509963989\n",
      "epoch: 25 step: 269, loss is 0.13099436461925507\n",
      "epoch: 25 step: 270, loss is 0.11264447122812271\n",
      "epoch: 25 step: 271, loss is 0.15850719809532166\n",
      "epoch: 25 step: 272, loss is 0.19223977625370026\n",
      "epoch: 25 step: 273, loss is 0.3292849659919739\n",
      "epoch: 25 step: 274, loss is 0.23536883294582367\n",
      "epoch: 25 step: 275, loss is 0.28914862871170044\n",
      "epoch: 25 step: 276, loss is 0.19654153287410736\n",
      "epoch: 25 step: 277, loss is 0.303288072347641\n",
      "epoch: 25 step: 278, loss is 0.13575150072574615\n",
      "epoch: 25 step: 279, loss is 0.3276912569999695\n",
      "epoch: 25 step: 280, loss is 0.21405167877674103\n",
      "epoch: 25 step: 281, loss is 0.2831207811832428\n",
      "epoch: 25 step: 282, loss is 0.10133244097232819\n",
      "epoch: 25 step: 283, loss is 0.23431360721588135\n",
      "epoch: 25 step: 284, loss is 0.11302853375673294\n",
      "epoch: 25 step: 285, loss is 0.32055333256721497\n",
      "epoch: 25 step: 286, loss is 0.11250171810388565\n",
      "epoch: 25 step: 287, loss is 0.21768900752067566\n",
      "epoch: 25 step: 288, loss is 0.3296544849872589\n",
      "epoch: 25 step: 289, loss is 0.21621470153331757\n",
      "epoch: 25 step: 290, loss is 0.22631573677062988\n",
      "epoch: 25 step: 291, loss is 0.36065319180488586\n",
      "epoch: 25 step: 292, loss is 0.08227770775556564\n",
      "epoch: 25 step: 293, loss is 0.06806673109531403\n",
      "epoch: 25 step: 294, loss is 0.14725415408611298\n",
      "epoch: 25 step: 295, loss is 0.13494297862052917\n",
      "epoch: 25 step: 296, loss is 0.15298369526863098\n",
      "epoch: 25 step: 297, loss is 0.12643557786941528\n",
      "epoch: 25 step: 298, loss is 0.23773565888404846\n",
      "epoch: 25 step: 299, loss is 0.1379040628671646\n",
      "epoch: 25 step: 300, loss is 0.22569797933101654\n",
      "epoch: 25 step: 301, loss is 0.09730654209852219\n",
      "epoch: 25 step: 302, loss is 0.19854168593883514\n",
      "epoch: 25 step: 303, loss is 0.222281813621521\n",
      "epoch: 25 step: 304, loss is 0.15391553938388824\n",
      "epoch: 25 step: 305, loss is 0.2718668580055237\n",
      "epoch: 25 step: 306, loss is 0.248378187417984\n",
      "epoch: 25 step: 307, loss is 0.1398795247077942\n",
      "epoch: 25 step: 308, loss is 0.3473585844039917\n",
      "epoch: 25 step: 309, loss is 0.11950604617595673\n",
      "epoch: 25 step: 310, loss is 0.20240437984466553\n",
      "epoch: 25 step: 311, loss is 0.354789674282074\n",
      "epoch: 25 step: 312, loss is 0.17884084582328796\n",
      "epoch: 25 step: 313, loss is 0.2065122127532959\n",
      "epoch: 25 step: 314, loss is 0.1662774235010147\n",
      "epoch: 25 step: 315, loss is 0.2470325380563736\n",
      "epoch: 25 step: 316, loss is 0.18164411187171936\n",
      "epoch: 25 step: 317, loss is 0.13047020137310028\n",
      "epoch: 25 step: 318, loss is 0.30899590253829956\n",
      "epoch: 25 step: 319, loss is 0.19467772543430328\n",
      "epoch: 25 step: 320, loss is 0.22461268305778503\n",
      "epoch: 25 step: 321, loss is 0.2510019838809967\n",
      "epoch: 25 step: 322, loss is 0.1418987214565277\n",
      "epoch: 25 step: 323, loss is 0.19435180723667145\n",
      "epoch: 25 step: 324, loss is 0.1139640063047409\n",
      "epoch: 25 step: 325, loss is 0.20706389844417572\n",
      "epoch: 25 step: 326, loss is 0.17247487604618073\n",
      "epoch: 25 step: 327, loss is 0.13562671840190887\n",
      "epoch: 25 step: 328, loss is 0.23124824464321136\n",
      "epoch: 25 step: 329, loss is 0.11871187388896942\n",
      "epoch: 25 step: 330, loss is 0.16443082690238953\n",
      "epoch: 25 step: 331, loss is 0.1622878909111023\n",
      "epoch: 25 step: 332, loss is 0.26781076192855835\n",
      "epoch: 25 step: 333, loss is 0.14310939610004425\n",
      "epoch: 25 step: 334, loss is 0.10669019818305969\n",
      "epoch: 25 step: 335, loss is 0.12894849479198456\n",
      "epoch: 25 step: 336, loss is 0.3202947974205017\n",
      "epoch: 25 step: 337, loss is 0.36548081040382385\n",
      "epoch: 25 step: 338, loss is 0.3408983051776886\n",
      "epoch: 25 step: 339, loss is 0.1207602396607399\n",
      "epoch: 25 step: 340, loss is 0.18881933391094208\n",
      "epoch: 25 step: 341, loss is 0.13657192885875702\n",
      "epoch: 25 step: 342, loss is 0.1642596423625946\n",
      "epoch: 25 step: 343, loss is 0.21627648174762726\n",
      "epoch: 25 step: 344, loss is 0.1945391297340393\n",
      "epoch: 25 step: 345, loss is 0.24034593999385834\n",
      "epoch: 25 step: 346, loss is 0.12423385679721832\n",
      "epoch: 25 step: 347, loss is 0.12704624235630035\n",
      "epoch: 25 step: 348, loss is 0.14903989434242249\n",
      "epoch: 25 step: 349, loss is 0.17419905960559845\n",
      "epoch: 25 step: 350, loss is 0.14646999537944794\n",
      "epoch: 25 step: 351, loss is 0.24760612845420837\n",
      "epoch: 25 step: 352, loss is 0.17460329830646515\n",
      "epoch: 25 step: 353, loss is 0.17272815108299255\n",
      "epoch: 25 step: 354, loss is 0.2784070670604706\n",
      "epoch: 25 step: 355, loss is 0.2786458730697632\n",
      "epoch: 25 step: 356, loss is 0.15308541059494019\n",
      "epoch: 25 step: 357, loss is 0.3166617751121521\n",
      "epoch: 25 step: 358, loss is 0.21724534034729004\n",
      "epoch: 25 step: 359, loss is 0.08696024119853973\n",
      "epoch: 25 step: 360, loss is 0.1468782275915146\n",
      "epoch: 25 step: 361, loss is 0.2672739326953888\n",
      "epoch: 25 step: 362, loss is 0.20126529037952423\n",
      "epoch: 25 step: 363, loss is 0.2290516495704651\n",
      "epoch: 25 step: 364, loss is 0.14706973731517792\n",
      "epoch: 25 step: 365, loss is 0.14328691363334656\n",
      "epoch: 25 step: 366, loss is 0.16560739278793335\n",
      "epoch: 25 step: 367, loss is 0.12988823652267456\n",
      "epoch: 25 step: 368, loss is 0.20574639737606049\n",
      "epoch: 25 step: 369, loss is 0.23022660613059998\n",
      "epoch: 25 step: 370, loss is 0.14692263305187225\n",
      "epoch: 25 step: 371, loss is 0.15544307231903076\n",
      "epoch: 25 step: 372, loss is 0.16610504686832428\n",
      "epoch: 25 step: 373, loss is 0.07662539184093475\n",
      "epoch: 25 step: 374, loss is 0.12218817323446274\n",
      "epoch: 25 step: 375, loss is 0.31322619318962097\n",
      "epoch: 25 step: 376, loss is 0.13825391232967377\n",
      "epoch: 25 step: 377, loss is 0.3861946761608124\n",
      "epoch: 25 step: 378, loss is 0.13944917917251587\n",
      "epoch: 25 step: 379, loss is 0.06990796327590942\n",
      "epoch: 25 step: 380, loss is 0.19899359345436096\n",
      "epoch: 25 step: 381, loss is 0.07595842331647873\n",
      "epoch: 25 step: 382, loss is 0.22033481299877167\n",
      "epoch: 25 step: 383, loss is 0.07042247802019119\n",
      "epoch: 25 step: 384, loss is 0.18530714511871338\n",
      "epoch: 25 step: 385, loss is 0.18548819422721863\n",
      "epoch: 25 step: 386, loss is 0.21713577210903168\n",
      "epoch: 25 step: 387, loss is 0.12967343628406525\n",
      "epoch: 25 step: 388, loss is 0.19496825337409973\n",
      "epoch: 25 step: 389, loss is 0.20648643374443054\n",
      "epoch: 25 step: 390, loss is 0.3349934220314026\n",
      "epoch: 25 step: 391, loss is 0.19453741610050201\n",
      "epoch: 25 step: 392, loss is 0.2793932259082794\n",
      "epoch: 25 step: 393, loss is 0.16491751372814178\n",
      "epoch: 25 step: 394, loss is 0.2803858816623688\n",
      "epoch: 25 step: 395, loss is 0.08489497005939484\n",
      "epoch: 25 step: 396, loss is 0.10808119177818298\n",
      "epoch: 25 step: 397, loss is 0.17319586873054504\n",
      "epoch: 25 step: 398, loss is 0.21983812749385834\n",
      "epoch: 25 step: 399, loss is 0.20112867653369904\n",
      "epoch: 25 step: 400, loss is 0.11092624068260193\n",
      "epoch: 25 step: 401, loss is 0.17761537432670593\n",
      "epoch: 25 step: 402, loss is 0.09310489892959595\n",
      "epoch: 25 step: 403, loss is 0.24130788445472717\n",
      "epoch: 25 step: 404, loss is 0.06895539909601212\n",
      "epoch: 25 step: 405, loss is 0.31456539034843445\n",
      "epoch: 25 step: 406, loss is 0.21208471059799194\n",
      "epoch: 25 step: 407, loss is 0.49696993827819824\n",
      "epoch: 25 step: 408, loss is 0.2426910698413849\n",
      "epoch: 25 step: 409, loss is 0.20829351246356964\n",
      "epoch: 25 step: 410, loss is 0.16643427312374115\n",
      "epoch: 25 step: 411, loss is 0.29243895411491394\n",
      "epoch: 25 step: 412, loss is 0.12442212551832199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 413, loss is 0.13375411927700043\n",
      "epoch: 25 step: 414, loss is 0.1779402494430542\n",
      "epoch: 25 step: 415, loss is 0.23072712123394012\n",
      "epoch: 25 step: 416, loss is 0.15422898530960083\n",
      "epoch: 25 step: 417, loss is 0.161322221159935\n",
      "epoch: 25 step: 418, loss is 0.2565828263759613\n",
      "epoch: 25 step: 419, loss is 0.2336786687374115\n",
      "epoch: 25 step: 420, loss is 0.36814984679222107\n",
      "epoch: 25 step: 421, loss is 0.21042858064174652\n",
      "epoch: 25 step: 422, loss is 0.23252610862255096\n",
      "epoch: 25 step: 423, loss is 0.12190530449151993\n",
      "epoch: 25 step: 424, loss is 0.19306029379367828\n",
      "epoch: 25 step: 425, loss is 0.21387915313243866\n",
      "epoch: 25 step: 426, loss is 0.2143055498600006\n",
      "epoch: 25 step: 427, loss is 0.20102062821388245\n",
      "epoch: 25 step: 428, loss is 0.1248200461268425\n",
      "epoch: 25 step: 429, loss is 0.06426946818828583\n",
      "epoch: 25 step: 430, loss is 0.09609098732471466\n",
      "epoch: 25 step: 431, loss is 0.1819898635149002\n",
      "epoch: 25 step: 432, loss is 0.18467985093593597\n",
      "epoch: 25 step: 433, loss is 0.200076162815094\n",
      "epoch: 25 step: 434, loss is 0.14441043138504028\n",
      "epoch: 25 step: 435, loss is 0.10771273076534271\n",
      "epoch: 25 step: 436, loss is 0.24976111948490143\n",
      "epoch: 25 step: 437, loss is 0.1902061551809311\n",
      "epoch: 25 step: 438, loss is 0.26142099499702454\n",
      "epoch: 25 step: 439, loss is 0.11889389157295227\n",
      "epoch: 25 step: 440, loss is 0.1919981688261032\n",
      "epoch: 25 step: 441, loss is 0.18062268197536469\n",
      "epoch: 25 step: 442, loss is 0.16640836000442505\n",
      "epoch: 25 step: 443, loss is 0.11525283753871918\n",
      "epoch: 25 step: 444, loss is 0.23238171637058258\n",
      "epoch: 25 step: 445, loss is 0.10730046778917313\n",
      "epoch: 25 step: 446, loss is 0.17966055870056152\n",
      "epoch: 25 step: 447, loss is 0.15281903743743896\n",
      "epoch: 25 step: 448, loss is 0.21543055772781372\n",
      "epoch: 25 step: 449, loss is 0.11169011890888214\n",
      "epoch: 25 step: 450, loss is 0.1749788075685501\n",
      "epoch: 25 step: 451, loss is 0.17649036645889282\n",
      "epoch: 25 step: 452, loss is 0.1449330449104309\n",
      "epoch: 25 step: 453, loss is 0.1698969006538391\n",
      "epoch: 25 step: 454, loss is 0.16635726392269135\n",
      "epoch: 25 step: 455, loss is 0.10770554840564728\n",
      "epoch: 25 step: 456, loss is 0.2570061683654785\n",
      "epoch: 25 step: 457, loss is 0.053732797503471375\n",
      "epoch: 25 step: 458, loss is 0.1038404032588005\n",
      "epoch: 25 step: 459, loss is 0.14530950784683228\n",
      "epoch: 25 step: 460, loss is 0.17825046181678772\n",
      "epoch: 25 step: 461, loss is 0.20834468305110931\n",
      "epoch: 25 step: 462, loss is 0.15062853693962097\n",
      "epoch: 25 step: 463, loss is 0.17844198644161224\n",
      "epoch: 25 step: 464, loss is 0.1263335943222046\n",
      "epoch: 25 step: 465, loss is 0.05293925479054451\n",
      "epoch: 25 step: 466, loss is 0.25760242342948914\n",
      "epoch: 25 step: 467, loss is 0.15893463790416718\n",
      "epoch: 25 step: 468, loss is 0.1126176044344902\n",
      "epoch: 25 step: 469, loss is 0.29540887475013733\n",
      "epoch: 25 step: 470, loss is 0.31294187903404236\n",
      "epoch: 25 step: 471, loss is 0.22408682107925415\n",
      "epoch: 25 step: 472, loss is 0.12458347529172897\n",
      "epoch: 25 step: 473, loss is 0.17661523818969727\n",
      "epoch: 25 step: 474, loss is 0.15216055512428284\n",
      "epoch: 25 step: 475, loss is 0.06055241450667381\n",
      "epoch: 25 step: 476, loss is 0.2549137771129608\n",
      "epoch: 25 step: 477, loss is 0.10529502481222153\n",
      "epoch: 25 step: 478, loss is 0.2605191767215729\n",
      "epoch: 25 step: 479, loss is 0.1312774419784546\n",
      "epoch: 25 step: 480, loss is 0.21751615405082703\n",
      "epoch: 25 step: 481, loss is 0.1948811262845993\n",
      "epoch: 25 step: 482, loss is 0.2176736295223236\n",
      "epoch: 25 step: 483, loss is 0.07863354682922363\n",
      "epoch: 25 step: 484, loss is 0.21669261157512665\n",
      "epoch: 25 step: 485, loss is 0.1500619351863861\n",
      "epoch: 25 step: 486, loss is 0.22167713940143585\n",
      "epoch: 25 step: 487, loss is 0.109577976167202\n",
      "epoch: 25 step: 488, loss is 0.10952040553092957\n",
      "epoch: 25 step: 489, loss is 0.24900677800178528\n",
      "epoch: 25 step: 490, loss is 0.12019163370132446\n",
      "epoch: 25 step: 491, loss is 0.16726091504096985\n",
      "epoch: 25 step: 492, loss is 0.18138381838798523\n",
      "epoch: 25 step: 493, loss is 0.2469022274017334\n",
      "epoch: 25 step: 494, loss is 0.08888120204210281\n",
      "epoch: 25 step: 495, loss is 0.17014829814434052\n",
      "epoch: 25 step: 496, loss is 0.21085116267204285\n",
      "epoch: 25 step: 497, loss is 0.16829638183116913\n",
      "epoch: 25 step: 498, loss is 0.05478696525096893\n",
      "epoch: 25 step: 499, loss is 0.2192002534866333\n",
      "epoch: 25 step: 500, loss is 0.14654240012168884\n",
      "epoch: 25 step: 501, loss is 0.17586539685726166\n",
      "epoch: 25 step: 502, loss is 0.1561581790447235\n",
      "epoch: 25 step: 503, loss is 0.12319396436214447\n",
      "epoch: 25 step: 504, loss is 0.32985448837280273\n",
      "epoch: 25 step: 505, loss is 0.18860161304473877\n",
      "epoch: 25 step: 506, loss is 0.11342543363571167\n",
      "epoch: 25 step: 507, loss is 0.22483327984809875\n",
      "epoch: 25 step: 508, loss is 0.12006771564483643\n",
      "epoch: 25 step: 509, loss is 0.14915764331817627\n",
      "epoch: 25 step: 510, loss is 0.1822979748249054\n",
      "epoch: 25 step: 511, loss is 0.05181928351521492\n",
      "epoch: 25 step: 512, loss is 0.3414868116378784\n",
      "epoch: 25 step: 513, loss is 0.23346367478370667\n",
      "epoch: 25 step: 514, loss is 0.30405548214912415\n",
      "epoch: 25 step: 515, loss is 0.09593512862920761\n",
      "epoch: 25 step: 516, loss is 0.13673990964889526\n",
      "epoch: 25 step: 517, loss is 0.16595646739006042\n",
      "epoch: 25 step: 518, loss is 0.21928806602954865\n",
      "epoch: 25 step: 519, loss is 0.3784119486808777\n",
      "epoch: 25 step: 520, loss is 0.189864382147789\n",
      "epoch: 25 step: 521, loss is 0.08970722556114197\n",
      "epoch: 25 step: 522, loss is 0.22997939586639404\n",
      "epoch: 25 step: 523, loss is 0.3877696096897125\n",
      "epoch: 25 step: 524, loss is 0.16496460139751434\n",
      "epoch: 25 step: 525, loss is 0.14774200320243835\n",
      "epoch: 25 step: 526, loss is 0.14381055533885956\n",
      "epoch: 25 step: 527, loss is 0.19424863159656525\n",
      "epoch: 25 step: 528, loss is 0.2008940875530243\n",
      "epoch: 25 step: 529, loss is 0.1596193164587021\n",
      "epoch: 25 step: 530, loss is 0.17809610068798065\n",
      "epoch: 25 step: 531, loss is 0.13718879222869873\n",
      "epoch: 25 step: 532, loss is 0.11188055574893951\n",
      "epoch: 25 step: 533, loss is 0.20541983842849731\n",
      "epoch: 25 step: 534, loss is 0.32961899042129517\n",
      "epoch: 25 step: 535, loss is 0.12774640321731567\n",
      "epoch: 25 step: 536, loss is 0.16181348264217377\n",
      "epoch: 25 step: 537, loss is 0.2884089946746826\n",
      "epoch: 25 step: 538, loss is 0.12032951414585114\n",
      "epoch: 25 step: 539, loss is 0.3472050726413727\n",
      "epoch: 25 step: 540, loss is 0.14165273308753967\n",
      "epoch: 25 step: 541, loss is 0.12186961621046066\n",
      "epoch: 25 step: 542, loss is 0.2528708875179291\n",
      "epoch: 25 step: 543, loss is 0.15076905488967896\n",
      "epoch: 25 step: 544, loss is 0.12279161810874939\n",
      "epoch: 25 step: 545, loss is 0.14689482748508453\n",
      "epoch: 25 step: 546, loss is 0.13427074253559113\n",
      "epoch: 25 step: 547, loss is 0.20034904778003693\n",
      "epoch: 25 step: 548, loss is 0.13868600130081177\n",
      "epoch: 25 step: 549, loss is 0.2956494987010956\n",
      "epoch: 25 step: 550, loss is 0.12201566994190216\n",
      "epoch: 25 step: 551, loss is 0.24742764234542847\n",
      "epoch: 25 step: 552, loss is 0.15568332374095917\n",
      "epoch: 25 step: 553, loss is 0.28024762868881226\n",
      "epoch: 25 step: 554, loss is 0.1363411694765091\n",
      "epoch: 25 step: 555, loss is 0.3223968744277954\n",
      "epoch: 25 step: 556, loss is 0.0729190856218338\n",
      "epoch: 25 step: 557, loss is 0.25353187322616577\n",
      "epoch: 25 step: 558, loss is 0.11134931445121765\n",
      "epoch: 25 step: 559, loss is 0.1413944959640503\n",
      "epoch: 25 step: 560, loss is 0.25091904401779175\n",
      "epoch: 25 step: 561, loss is 0.1671283096075058\n",
      "epoch: 25 step: 562, loss is 0.2509455978870392\n",
      "epoch: 25 step: 563, loss is 0.33236804604530334\n",
      "epoch: 25 step: 564, loss is 0.3718419373035431\n",
      "epoch: 25 step: 565, loss is 0.1463751196861267\n",
      "epoch: 25 step: 566, loss is 0.30103641748428345\n",
      "epoch: 25 step: 567, loss is 0.28563445806503296\n",
      "epoch: 25 step: 568, loss is 0.28208187222480774\n",
      "epoch: 25 step: 569, loss is 0.1661057323217392\n",
      "epoch: 25 step: 570, loss is 0.14436142146587372\n",
      "epoch: 25 step: 571, loss is 0.17521315813064575\n",
      "epoch: 25 step: 572, loss is 0.3870235085487366\n",
      "epoch: 25 step: 573, loss is 0.17974676191806793\n",
      "epoch: 25 step: 574, loss is 0.25148868560791016\n",
      "epoch: 25 step: 575, loss is 0.21498827636241913\n",
      "epoch: 25 step: 576, loss is 0.11677851527929306\n",
      "epoch: 25 step: 577, loss is 0.18148599565029144\n",
      "epoch: 25 step: 578, loss is 0.15240715444087982\n",
      "epoch: 25 step: 579, loss is 0.24240082502365112\n",
      "epoch: 25 step: 580, loss is 0.10344621539115906\n",
      "epoch: 25 step: 581, loss is 0.11208489537239075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 582, loss is 0.1981125771999359\n",
      "epoch: 25 step: 583, loss is 0.11545029282569885\n",
      "epoch: 25 step: 584, loss is 0.16986598074436188\n",
      "epoch: 25 step: 585, loss is 0.19154444336891174\n",
      "epoch: 25 step: 586, loss is 0.11791512370109558\n",
      "epoch: 25 step: 587, loss is 0.17200051248073578\n",
      "epoch: 25 step: 588, loss is 0.24318069219589233\n",
      "epoch: 25 step: 589, loss is 0.09801448881626129\n",
      "epoch: 25 step: 590, loss is 0.11021221429109573\n",
      "epoch: 25 step: 591, loss is 0.21972794830799103\n",
      "epoch: 25 step: 592, loss is 0.10625947266817093\n",
      "epoch: 25 step: 593, loss is 0.1984557956457138\n",
      "epoch: 25 step: 594, loss is 0.12185803055763245\n",
      "epoch: 25 step: 595, loss is 0.08801349252462387\n",
      "epoch: 25 step: 596, loss is 0.2985738515853882\n",
      "epoch: 25 step: 597, loss is 0.2085297405719757\n",
      "epoch: 25 step: 598, loss is 0.1354038119316101\n",
      "epoch: 25 step: 599, loss is 0.18856650590896606\n",
      "epoch: 25 step: 600, loss is 0.3290725648403168\n",
      "epoch: 25 step: 601, loss is 0.2886592447757721\n",
      "epoch: 25 step: 602, loss is 0.14571082592010498\n",
      "epoch: 25 step: 603, loss is 0.21078723669052124\n",
      "epoch: 25 step: 604, loss is 0.15278737246990204\n",
      "epoch: 25 step: 605, loss is 0.2202174961566925\n",
      "epoch: 25 step: 606, loss is 0.11544708907604218\n",
      "epoch: 25 step: 607, loss is 0.24277934432029724\n",
      "epoch: 25 step: 608, loss is 0.1619817316532135\n",
      "epoch: 25 step: 609, loss is 0.19067241251468658\n",
      "epoch: 25 step: 610, loss is 0.1487509310245514\n",
      "epoch: 25 step: 611, loss is 0.1787731796503067\n",
      "epoch: 25 step: 612, loss is 0.18345358967781067\n",
      "epoch: 25 step: 613, loss is 0.23384004831314087\n",
      "epoch: 25 step: 614, loss is 0.10506384074687958\n",
      "epoch: 25 step: 615, loss is 0.22336497902870178\n",
      "epoch: 25 step: 616, loss is 0.14233484864234924\n",
      "epoch: 25 step: 617, loss is 0.1265122890472412\n",
      "epoch: 25 step: 618, loss is 0.24907784163951874\n",
      "epoch: 25 step: 619, loss is 0.17357392609119415\n",
      "epoch: 25 step: 620, loss is 0.13443920016288757\n",
      "epoch: 25 step: 621, loss is 0.35515862703323364\n",
      "epoch: 25 step: 622, loss is 0.2243259996175766\n",
      "epoch: 25 step: 623, loss is 0.25188955664634705\n",
      "epoch: 25 step: 624, loss is 0.22296541929244995\n",
      "epoch: 25 step: 625, loss is 0.12867200374603271\n",
      "epoch: 25 step: 626, loss is 0.203934445977211\n",
      "epoch: 25 step: 627, loss is 0.15577886998653412\n",
      "epoch: 25 step: 628, loss is 0.13768109679222107\n",
      "epoch: 25 step: 629, loss is 0.1627732366323471\n",
      "epoch: 25 step: 630, loss is 0.1541338413953781\n",
      "epoch: 25 step: 631, loss is 0.24890495836734772\n",
      "epoch: 25 step: 632, loss is 0.16020850837230682\n",
      "epoch: 25 step: 633, loss is 0.12052031606435776\n",
      "epoch: 25 step: 634, loss is 0.09825614094734192\n",
      "epoch: 25 step: 635, loss is 0.29648807644844055\n",
      "epoch: 25 step: 636, loss is 0.09226630628108978\n",
      "epoch: 25 step: 637, loss is 0.18596191704273224\n",
      "epoch: 25 step: 638, loss is 0.18512940406799316\n",
      "epoch: 25 step: 639, loss is 0.1902817338705063\n",
      "epoch: 25 step: 640, loss is 0.20243771374225616\n",
      "epoch: 25 step: 641, loss is 0.2554172873497009\n",
      "epoch: 25 step: 642, loss is 0.26691582798957825\n",
      "epoch: 25 step: 643, loss is 0.22430677711963654\n",
      "epoch: 25 step: 644, loss is 0.10596273839473724\n",
      "epoch: 25 step: 645, loss is 0.0985664576292038\n",
      "epoch: 25 step: 646, loss is 0.08728610724210739\n",
      "epoch: 25 step: 647, loss is 0.17230816185474396\n",
      "epoch: 25 step: 648, loss is 0.15140686929225922\n",
      "epoch: 25 step: 649, loss is 0.21774302423000336\n",
      "epoch: 25 step: 650, loss is 0.33371269702911377\n",
      "epoch: 25 step: 651, loss is 0.22316217422485352\n",
      "epoch: 25 step: 652, loss is 0.2142547070980072\n",
      "epoch: 25 step: 653, loss is 0.134538471698761\n",
      "epoch: 25 step: 654, loss is 0.22773976624011993\n",
      "epoch: 25 step: 655, loss is 0.31291937828063965\n",
      "epoch: 25 step: 656, loss is 0.2685714066028595\n",
      "epoch: 25 step: 657, loss is 0.06306237727403641\n",
      "epoch: 25 step: 658, loss is 0.09320087730884552\n",
      "epoch: 25 step: 659, loss is 0.33167698979377747\n",
      "epoch: 25 step: 660, loss is 0.16473668813705444\n",
      "epoch: 25 step: 661, loss is 0.2691729664802551\n",
      "epoch: 25 step: 662, loss is 0.23681539297103882\n",
      "epoch: 25 step: 663, loss is 0.21699011325836182\n",
      "epoch: 25 step: 664, loss is 0.166603684425354\n",
      "epoch: 25 step: 665, loss is 0.31530043482780457\n",
      "epoch: 25 step: 666, loss is 0.3378846049308777\n",
      "epoch: 25 step: 667, loss is 0.3262498676776886\n",
      "epoch: 25 step: 668, loss is 0.1067274883389473\n",
      "epoch: 25 step: 669, loss is 0.08440975099802017\n",
      "epoch: 25 step: 670, loss is 0.4269397556781769\n",
      "epoch: 25 step: 671, loss is 0.2569422423839569\n",
      "epoch: 25 step: 672, loss is 0.20888148248195648\n",
      "epoch: 25 step: 673, loss is 0.1867530196905136\n",
      "epoch: 25 step: 674, loss is 0.30371931195259094\n",
      "epoch: 25 step: 675, loss is 0.36383211612701416\n",
      "epoch: 25 step: 676, loss is 0.1614716798067093\n",
      "epoch: 25 step: 677, loss is 0.14757661521434784\n",
      "epoch: 25 step: 678, loss is 0.2095586359500885\n",
      "epoch: 25 step: 679, loss is 0.25518831610679626\n",
      "epoch: 25 step: 680, loss is 0.13212861120700836\n",
      "epoch: 25 step: 681, loss is 0.17148104310035706\n",
      "epoch: 25 step: 682, loss is 0.30185362696647644\n",
      "epoch: 25 step: 683, loss is 0.10601966083049774\n",
      "epoch: 25 step: 684, loss is 0.3384975790977478\n",
      "epoch: 25 step: 685, loss is 0.17599238455295563\n",
      "epoch: 25 step: 686, loss is 0.12469841539859772\n",
      "epoch: 25 step: 687, loss is 0.09792547672986984\n",
      "epoch: 25 step: 688, loss is 0.12008114159107208\n",
      "epoch: 25 step: 689, loss is 0.2204982042312622\n",
      "epoch: 25 step: 690, loss is 0.1688389629125595\n",
      "epoch: 25 step: 691, loss is 0.08480575680732727\n",
      "epoch: 25 step: 692, loss is 0.30004411935806274\n",
      "epoch: 25 step: 693, loss is 0.18773344159126282\n",
      "epoch: 25 step: 694, loss is 0.13144952058792114\n",
      "epoch: 25 step: 695, loss is 0.18226206302642822\n",
      "epoch: 25 step: 696, loss is 0.29839035868644714\n",
      "epoch: 25 step: 697, loss is 0.11155207455158234\n",
      "epoch: 25 step: 698, loss is 0.13126513361930847\n",
      "epoch: 25 step: 699, loss is 0.17867302894592285\n",
      "epoch: 25 step: 700, loss is 0.3573906421661377\n",
      "epoch: 25 step: 701, loss is 0.1080508679151535\n",
      "epoch: 25 step: 702, loss is 0.31202608346939087\n",
      "epoch: 25 step: 703, loss is 0.2898620367050171\n",
      "epoch: 25 step: 704, loss is 0.11806295067071915\n",
      "epoch: 25 step: 705, loss is 0.23899677395820618\n",
      "epoch: 25 step: 706, loss is 0.2334105670452118\n",
      "epoch: 25 step: 707, loss is 0.2028540074825287\n",
      "epoch: 25 step: 708, loss is 0.20003032684326172\n",
      "epoch: 25 step: 709, loss is 0.20690777897834778\n",
      "epoch: 25 step: 710, loss is 0.17155343294143677\n",
      "epoch: 25 step: 711, loss is 0.21001802384853363\n",
      "epoch: 25 step: 712, loss is 0.2592780292034149\n",
      "epoch: 25 step: 713, loss is 0.12074951827526093\n",
      "epoch: 25 step: 714, loss is 0.3026847541332245\n",
      "epoch: 25 step: 715, loss is 0.15142680704593658\n",
      "epoch: 25 step: 716, loss is 0.26576873660087585\n",
      "epoch: 25 step: 717, loss is 0.4823925197124481\n",
      "epoch: 25 step: 718, loss is 0.16062235832214355\n",
      "epoch: 25 step: 719, loss is 0.2022625058889389\n",
      "epoch: 25 step: 720, loss is 0.1656804084777832\n",
      "epoch: 25 step: 721, loss is 0.06704051792621613\n",
      "epoch: 25 step: 722, loss is 0.16986025869846344\n",
      "epoch: 25 step: 723, loss is 0.24605007469654083\n",
      "epoch: 25 step: 724, loss is 0.3116513788700104\n",
      "epoch: 25 step: 725, loss is 0.19389455020427704\n",
      "epoch: 25 step: 726, loss is 0.2797509431838989\n",
      "epoch: 25 step: 727, loss is 0.18271569907665253\n",
      "epoch: 25 step: 728, loss is 0.11027289927005768\n",
      "epoch: 25 step: 729, loss is 0.1444230079650879\n",
      "epoch: 25 step: 730, loss is 0.12629449367523193\n",
      "epoch: 25 step: 731, loss is 0.13099311292171478\n",
      "epoch: 25 step: 732, loss is 0.15564517676830292\n",
      "epoch: 25 step: 733, loss is 0.17823199927806854\n",
      "epoch: 25 step: 734, loss is 0.0884077176451683\n",
      "epoch: 25 step: 735, loss is 0.25822147727012634\n",
      "epoch: 25 step: 736, loss is 0.11849533021450043\n",
      "epoch: 25 step: 737, loss is 0.1209978386759758\n",
      "epoch: 25 step: 738, loss is 0.16932383179664612\n",
      "epoch: 25 step: 739, loss is 0.25539034605026245\n",
      "epoch: 25 step: 740, loss is 0.2296455204486847\n",
      "epoch: 25 step: 741, loss is 0.235052227973938\n",
      "epoch: 25 step: 742, loss is 0.16399511694908142\n",
      "epoch: 25 step: 743, loss is 0.3208257257938385\n",
      "epoch: 25 step: 744, loss is 0.14285697042942047\n",
      "epoch: 25 step: 745, loss is 0.2764956057071686\n",
      "epoch: 25 step: 746, loss is 0.32082104682922363\n",
      "epoch: 25 step: 747, loss is 0.1425355225801468\n",
      "epoch: 25 step: 748, loss is 0.24851864576339722\n",
      "epoch: 25 step: 749, loss is 0.2095232456922531\n",
      "epoch: 25 step: 750, loss is 0.13790002465248108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 751, loss is 0.15132024884223938\n",
      "epoch: 25 step: 752, loss is 0.15940967202186584\n",
      "epoch: 25 step: 753, loss is 0.24246621131896973\n",
      "epoch: 25 step: 754, loss is 0.12743927538394928\n",
      "epoch: 25 step: 755, loss is 0.1259322166442871\n",
      "epoch: 25 step: 756, loss is 0.2575836479663849\n",
      "epoch: 25 step: 757, loss is 0.15971507132053375\n",
      "epoch: 25 step: 758, loss is 0.2777426242828369\n",
      "epoch: 25 step: 759, loss is 0.22889451682567596\n",
      "epoch: 25 step: 760, loss is 0.145362988114357\n",
      "epoch: 25 step: 761, loss is 0.2692185640335083\n",
      "epoch: 25 step: 762, loss is 0.25732117891311646\n",
      "epoch: 25 step: 763, loss is 0.16804690659046173\n",
      "epoch: 25 step: 764, loss is 0.221744567155838\n",
      "epoch: 25 step: 765, loss is 0.21393510699272156\n",
      "epoch: 25 step: 766, loss is 0.2333424836397171\n",
      "epoch: 25 step: 767, loss is 0.08428815007209778\n",
      "epoch: 25 step: 768, loss is 0.2967396676540375\n",
      "epoch: 25 step: 769, loss is 0.2809414565563202\n",
      "epoch: 25 step: 770, loss is 0.23214146494865417\n",
      "epoch: 25 step: 771, loss is 0.31567662954330444\n",
      "epoch: 25 step: 772, loss is 0.2516813576221466\n",
      "epoch: 25 step: 773, loss is 0.22856931388378143\n",
      "epoch: 25 step: 774, loss is 0.2629500925540924\n",
      "epoch: 25 step: 775, loss is 0.06298669427633286\n",
      "epoch: 25 step: 776, loss is 0.10256224870681763\n",
      "epoch: 25 step: 777, loss is 0.1449214369058609\n",
      "epoch: 25 step: 778, loss is 0.13387227058410645\n",
      "epoch: 25 step: 779, loss is 0.30249032378196716\n",
      "epoch: 25 step: 780, loss is 0.20657549798488617\n",
      "epoch: 25 step: 781, loss is 0.22187736630439758\n",
      "epoch: 25 step: 782, loss is 0.21920567750930786\n",
      "epoch: 25 step: 783, loss is 0.14459025859832764\n",
      "epoch: 25 step: 784, loss is 0.18875917792320251\n",
      "epoch: 25 step: 785, loss is 0.22221647202968597\n",
      "epoch: 25 step: 786, loss is 0.1359015256166458\n",
      "epoch: 25 step: 787, loss is 0.11557052284479141\n",
      "epoch: 25 step: 788, loss is 0.1558080017566681\n",
      "epoch: 25 step: 789, loss is 0.16538187861442566\n",
      "epoch: 25 step: 790, loss is 0.25462791323661804\n",
      "epoch: 25 step: 791, loss is 0.20142534375190735\n",
      "epoch: 25 step: 792, loss is 0.226218581199646\n",
      "epoch: 25 step: 793, loss is 0.1024365946650505\n",
      "epoch: 25 step: 794, loss is 0.17410846054553986\n",
      "epoch: 25 step: 795, loss is 0.10571058094501495\n",
      "epoch: 25 step: 796, loss is 0.29748284816741943\n",
      "epoch: 25 step: 797, loss is 0.16471558809280396\n",
      "epoch: 25 step: 798, loss is 0.10018245875835419\n",
      "epoch: 25 step: 799, loss is 0.20176340639591217\n",
      "epoch: 25 step: 800, loss is 0.21882925927639008\n",
      "epoch: 25 step: 801, loss is 0.30211153626441956\n",
      "epoch: 25 step: 802, loss is 0.19524729251861572\n",
      "epoch: 25 step: 803, loss is 0.20059868693351746\n",
      "epoch: 25 step: 804, loss is 0.1930098980665207\n",
      "epoch: 25 step: 805, loss is 0.1460624486207962\n",
      "epoch: 25 step: 806, loss is 0.23766353726387024\n",
      "epoch: 25 step: 807, loss is 0.2617846429347992\n",
      "epoch: 25 step: 808, loss is 0.35366564989089966\n",
      "epoch: 25 step: 809, loss is 0.1336105912923813\n",
      "epoch: 25 step: 810, loss is 0.09082875400781631\n",
      "epoch: 25 step: 811, loss is 0.17023558914661407\n",
      "epoch: 25 step: 812, loss is 0.2848657965660095\n",
      "epoch: 25 step: 813, loss is 0.2065296769142151\n",
      "epoch: 25 step: 814, loss is 0.2546986937522888\n",
      "epoch: 25 step: 815, loss is 0.24711458384990692\n",
      "epoch: 25 step: 816, loss is 0.13758042454719543\n",
      "epoch: 25 step: 817, loss is 0.22438721358776093\n",
      "epoch: 25 step: 818, loss is 0.09918315708637238\n",
      "epoch: 25 step: 819, loss is 0.10762252658605576\n",
      "epoch: 25 step: 820, loss is 0.12792415916919708\n",
      "epoch: 25 step: 821, loss is 0.15200234949588776\n",
      "epoch: 25 step: 822, loss is 0.07721646875143051\n",
      "epoch: 25 step: 823, loss is 0.17174434661865234\n",
      "epoch: 25 step: 824, loss is 0.15759366750717163\n",
      "epoch: 25 step: 825, loss is 0.2275327891111374\n",
      "epoch: 25 step: 826, loss is 0.2737027108669281\n",
      "epoch: 25 step: 827, loss is 0.2241664081811905\n",
      "epoch: 25 step: 828, loss is 0.22315597534179688\n",
      "epoch: 25 step: 829, loss is 0.1046726256608963\n",
      "epoch: 25 step: 830, loss is 0.27006688714027405\n",
      "epoch: 25 step: 831, loss is 0.11727425456047058\n",
      "epoch: 25 step: 832, loss is 0.2460622936487198\n",
      "epoch: 25 step: 833, loss is 0.2505638301372528\n",
      "epoch: 25 step: 834, loss is 0.11321651935577393\n",
      "epoch: 25 step: 835, loss is 0.19116201996803284\n",
      "epoch: 25 step: 836, loss is 0.1036495789885521\n",
      "epoch: 25 step: 837, loss is 0.09277556091547012\n",
      "epoch: 25 step: 838, loss is 0.1369147002696991\n",
      "epoch: 25 step: 839, loss is 0.1526530683040619\n",
      "epoch: 25 step: 840, loss is 0.24792294204235077\n",
      "epoch: 25 step: 841, loss is 0.21596486866474152\n",
      "epoch: 25 step: 842, loss is 0.2871834635734558\n",
      "epoch: 25 step: 843, loss is 0.13834932446479797\n",
      "epoch: 25 step: 844, loss is 0.12942185997962952\n",
      "epoch: 25 step: 845, loss is 0.0848613828420639\n",
      "epoch: 25 step: 846, loss is 0.20351232588291168\n",
      "epoch: 25 step: 847, loss is 0.3655136525630951\n",
      "epoch: 25 step: 848, loss is 0.12383387237787247\n",
      "epoch: 25 step: 849, loss is 0.1337021440267563\n",
      "epoch: 25 step: 850, loss is 0.10218958556652069\n",
      "epoch: 25 step: 851, loss is 0.19359362125396729\n",
      "epoch: 25 step: 852, loss is 0.2381623089313507\n",
      "epoch: 25 step: 853, loss is 0.09452734887599945\n",
      "epoch: 25 step: 854, loss is 0.24552589654922485\n",
      "epoch: 25 step: 855, loss is 0.14693371951580048\n",
      "epoch: 25 step: 856, loss is 0.35296058654785156\n",
      "epoch: 25 step: 857, loss is 0.12942330539226532\n",
      "epoch: 25 step: 858, loss is 0.24427767097949982\n",
      "epoch: 25 step: 859, loss is 0.1569141298532486\n",
      "epoch: 25 step: 860, loss is 0.18782976269721985\n",
      "epoch: 25 step: 861, loss is 0.19579866528511047\n",
      "epoch: 25 step: 862, loss is 0.1470889449119568\n",
      "epoch: 25 step: 863, loss is 0.21166305243968964\n",
      "epoch: 25 step: 864, loss is 0.1574462503194809\n",
      "epoch: 25 step: 865, loss is 0.17363852262496948\n",
      "epoch: 25 step: 866, loss is 0.2751772999763489\n",
      "epoch: 25 step: 867, loss is 0.07809707522392273\n",
      "epoch: 25 step: 868, loss is 0.2154211848974228\n",
      "epoch: 25 step: 869, loss is 0.18335267901420593\n",
      "epoch: 25 step: 870, loss is 0.1104130819439888\n",
      "epoch: 25 step: 871, loss is 0.3401855528354645\n",
      "epoch: 25 step: 872, loss is 0.14993451535701752\n",
      "epoch: 25 step: 873, loss is 0.26505449414253235\n",
      "epoch: 25 step: 874, loss is 0.22435446083545685\n",
      "epoch: 25 step: 875, loss is 0.1771489828824997\n",
      "epoch: 25 step: 876, loss is 0.10764230787754059\n",
      "epoch: 25 step: 877, loss is 0.256836861371994\n",
      "epoch: 25 step: 878, loss is 0.1856124848127365\n",
      "epoch: 25 step: 879, loss is 0.17962278425693512\n",
      "epoch: 25 step: 880, loss is 0.3205699920654297\n",
      "epoch: 25 step: 881, loss is 0.18460601568222046\n",
      "epoch: 25 step: 882, loss is 0.1585724651813507\n",
      "epoch: 25 step: 883, loss is 0.14904287457466125\n",
      "epoch: 25 step: 884, loss is 0.2580786347389221\n",
      "epoch: 25 step: 885, loss is 0.25947797298431396\n",
      "epoch: 25 step: 886, loss is 0.11525678634643555\n",
      "epoch: 25 step: 887, loss is 0.1254834681749344\n",
      "epoch: 25 step: 888, loss is 0.30897167325019836\n",
      "epoch: 25 step: 889, loss is 0.21010556817054749\n",
      "epoch: 25 step: 890, loss is 0.1940905749797821\n",
      "epoch: 25 step: 891, loss is 0.31471744179725647\n",
      "epoch: 25 step: 892, loss is 0.34163904190063477\n",
      "epoch: 25 step: 893, loss is 0.21439166367053986\n",
      "epoch: 25 step: 894, loss is 0.21997159719467163\n",
      "epoch: 25 step: 895, loss is 0.18118520081043243\n",
      "epoch: 25 step: 896, loss is 0.09800895303487778\n",
      "epoch: 25 step: 897, loss is 0.3040392994880676\n",
      "epoch: 25 step: 898, loss is 0.2527168393135071\n",
      "epoch: 25 step: 899, loss is 0.15262803435325623\n",
      "epoch: 25 step: 900, loss is 0.15149138867855072\n",
      "epoch: 25 step: 901, loss is 0.23319965600967407\n",
      "epoch: 25 step: 902, loss is 0.0921584814786911\n",
      "epoch: 25 step: 903, loss is 0.19452154636383057\n",
      "epoch: 25 step: 904, loss is 0.14084255695343018\n",
      "epoch: 25 step: 905, loss is 0.1893240213394165\n",
      "epoch: 25 step: 906, loss is 0.2872140109539032\n",
      "epoch: 25 step: 907, loss is 0.17458634078502655\n",
      "epoch: 25 step: 908, loss is 0.16611462831497192\n",
      "epoch: 25 step: 909, loss is 0.35795703530311584\n",
      "epoch: 25 step: 910, loss is 0.16122344136238098\n",
      "epoch: 25 step: 911, loss is 0.0987306609749794\n",
      "epoch: 25 step: 912, loss is 0.14413994550704956\n",
      "epoch: 25 step: 913, loss is 0.08245369791984558\n",
      "epoch: 25 step: 914, loss is 0.20443421602249146\n",
      "epoch: 25 step: 915, loss is 0.12456279247999191\n",
      "epoch: 25 step: 916, loss is 0.2638051509857178\n",
      "epoch: 25 step: 917, loss is 0.2360648810863495\n",
      "epoch: 25 step: 918, loss is 0.24198833107948303\n",
      "epoch: 25 step: 919, loss is 0.2971511781215668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 920, loss is 0.13674701750278473\n",
      "epoch: 25 step: 921, loss is 0.08911062777042389\n",
      "epoch: 25 step: 922, loss is 0.21937327086925507\n",
      "epoch: 25 step: 923, loss is 0.19759400188922882\n",
      "epoch: 25 step: 924, loss is 0.18180732429027557\n",
      "epoch: 25 step: 925, loss is 0.13800261914730072\n",
      "epoch: 25 step: 926, loss is 0.148002028465271\n",
      "epoch: 25 step: 927, loss is 0.24207068979740143\n",
      "epoch: 25 step: 928, loss is 0.21938765048980713\n",
      "epoch: 25 step: 929, loss is 0.11655918508768082\n",
      "epoch: 25 step: 930, loss is 0.181284561753273\n",
      "epoch: 25 step: 931, loss is 0.0635111853480339\n",
      "epoch: 25 step: 932, loss is 0.15831156075000763\n",
      "epoch: 25 step: 933, loss is 0.15626958012580872\n",
      "epoch: 25 step: 934, loss is 0.12646254897117615\n",
      "epoch: 25 step: 935, loss is 0.1967221051454544\n",
      "epoch: 25 step: 936, loss is 0.18095459043979645\n",
      "epoch: 25 step: 937, loss is 0.11199978739023209\n",
      "epoch: 26 step: 1, loss is 0.16327273845672607\n",
      "epoch: 26 step: 2, loss is 0.25314122438430786\n",
      "epoch: 26 step: 3, loss is 0.1733289211988449\n",
      "epoch: 26 step: 4, loss is 0.09252171963453293\n",
      "epoch: 26 step: 5, loss is 0.19770580530166626\n",
      "epoch: 26 step: 6, loss is 0.3918192386627197\n",
      "epoch: 26 step: 7, loss is 0.23212984204292297\n",
      "epoch: 26 step: 8, loss is 0.28127092123031616\n",
      "epoch: 26 step: 9, loss is 0.16608202457427979\n",
      "epoch: 26 step: 10, loss is 0.13545091450214386\n",
      "epoch: 26 step: 11, loss is 0.13189712166786194\n",
      "epoch: 26 step: 12, loss is 0.18940281867980957\n",
      "epoch: 26 step: 13, loss is 0.13780613243579865\n",
      "epoch: 26 step: 14, loss is 0.07854017615318298\n",
      "epoch: 26 step: 15, loss is 0.0711275264620781\n",
      "epoch: 26 step: 16, loss is 0.20339488983154297\n",
      "epoch: 26 step: 17, loss is 0.225972980260849\n",
      "epoch: 26 step: 18, loss is 0.12400346994400024\n",
      "epoch: 26 step: 19, loss is 0.31593701243400574\n",
      "epoch: 26 step: 20, loss is 0.11703181266784668\n",
      "epoch: 26 step: 21, loss is 0.2003597617149353\n",
      "epoch: 26 step: 22, loss is 0.10743584483861923\n",
      "epoch: 26 step: 23, loss is 0.18024642765522003\n",
      "epoch: 26 step: 24, loss is 0.12506118416786194\n",
      "epoch: 26 step: 25, loss is 0.13382157683372498\n",
      "epoch: 26 step: 26, loss is 0.08660086989402771\n",
      "epoch: 26 step: 27, loss is 0.14354734122753143\n",
      "epoch: 26 step: 28, loss is 0.10961338877677917\n",
      "epoch: 26 step: 29, loss is 0.08002032339572906\n",
      "epoch: 26 step: 30, loss is 0.17965979874134064\n",
      "epoch: 26 step: 31, loss is 0.10781829059123993\n",
      "epoch: 26 step: 32, loss is 0.48317578434944153\n",
      "epoch: 26 step: 33, loss is 0.3697358965873718\n",
      "epoch: 26 step: 34, loss is 0.19513246417045593\n",
      "epoch: 26 step: 35, loss is 0.21085698902606964\n",
      "epoch: 26 step: 36, loss is 0.2143300622701645\n",
      "epoch: 26 step: 37, loss is 0.25526663661003113\n",
      "epoch: 26 step: 38, loss is 0.13963063061237335\n",
      "epoch: 26 step: 39, loss is 0.1249280571937561\n",
      "epoch: 26 step: 40, loss is 0.20517076551914215\n",
      "epoch: 26 step: 41, loss is 0.2040024697780609\n",
      "epoch: 26 step: 42, loss is 0.17158719897270203\n",
      "epoch: 26 step: 43, loss is 0.1819515824317932\n",
      "epoch: 26 step: 44, loss is 0.12257073819637299\n",
      "epoch: 26 step: 45, loss is 0.09887506812810898\n",
      "epoch: 26 step: 46, loss is 0.09822635352611542\n",
      "epoch: 26 step: 47, loss is 0.05627010390162468\n",
      "epoch: 26 step: 48, loss is 0.1655442714691162\n",
      "epoch: 26 step: 49, loss is 0.26360857486724854\n",
      "epoch: 26 step: 50, loss is 0.2476871758699417\n",
      "epoch: 26 step: 51, loss is 0.19504687190055847\n",
      "epoch: 26 step: 52, loss is 0.16757522523403168\n",
      "epoch: 26 step: 53, loss is 0.12010820209980011\n",
      "epoch: 26 step: 54, loss is 0.19575904309749603\n",
      "epoch: 26 step: 55, loss is 0.23219911754131317\n",
      "epoch: 26 step: 56, loss is 0.16049790382385254\n",
      "epoch: 26 step: 57, loss is 0.3049001395702362\n",
      "epoch: 26 step: 58, loss is 0.1716303527355194\n",
      "epoch: 26 step: 59, loss is 0.25595295429229736\n",
      "epoch: 26 step: 60, loss is 0.13912759721279144\n",
      "epoch: 26 step: 61, loss is 0.22286522388458252\n",
      "epoch: 26 step: 62, loss is 0.13951724767684937\n",
      "epoch: 26 step: 63, loss is 0.12935349345207214\n",
      "epoch: 26 step: 64, loss is 0.1615874022245407\n",
      "epoch: 26 step: 65, loss is 0.22029441595077515\n",
      "epoch: 26 step: 66, loss is 0.2104816734790802\n",
      "epoch: 26 step: 67, loss is 0.10582432895898819\n",
      "epoch: 26 step: 68, loss is 0.13172391057014465\n",
      "epoch: 26 step: 69, loss is 0.2510453462600708\n",
      "epoch: 26 step: 70, loss is 0.2629300653934479\n",
      "epoch: 26 step: 71, loss is 0.2506009340286255\n",
      "epoch: 26 step: 72, loss is 0.14460055530071259\n",
      "epoch: 26 step: 73, loss is 0.23694108426570892\n",
      "epoch: 26 step: 74, loss is 0.18610015511512756\n",
      "epoch: 26 step: 75, loss is 0.11977773904800415\n",
      "epoch: 26 step: 76, loss is 0.19783616065979004\n",
      "epoch: 26 step: 77, loss is 0.37434643507003784\n",
      "epoch: 26 step: 78, loss is 0.20095983147621155\n",
      "epoch: 26 step: 79, loss is 0.36186084151268005\n",
      "epoch: 26 step: 80, loss is 0.24571658670902252\n",
      "epoch: 26 step: 81, loss is 0.2620329260826111\n",
      "epoch: 26 step: 82, loss is 0.12789250910282135\n",
      "epoch: 26 step: 83, loss is 0.266465961933136\n",
      "epoch: 26 step: 84, loss is 0.08483319729566574\n",
      "epoch: 26 step: 85, loss is 0.10652869194746017\n",
      "epoch: 26 step: 86, loss is 0.1544751077890396\n",
      "epoch: 26 step: 87, loss is 0.2526129186153412\n",
      "epoch: 26 step: 88, loss is 0.15979516506195068\n",
      "epoch: 26 step: 89, loss is 0.27843788266181946\n",
      "epoch: 26 step: 90, loss is 0.26403331756591797\n",
      "epoch: 26 step: 91, loss is 0.23552590608596802\n",
      "epoch: 26 step: 92, loss is 0.08794961124658585\n",
      "epoch: 26 step: 93, loss is 0.07114732265472412\n",
      "epoch: 26 step: 94, loss is 0.10668628662824631\n",
      "epoch: 26 step: 95, loss is 0.1118396520614624\n",
      "epoch: 26 step: 96, loss is 0.12677320837974548\n",
      "epoch: 26 step: 97, loss is 0.2001454383134842\n",
      "epoch: 26 step: 98, loss is 0.11737057566642761\n",
      "epoch: 26 step: 99, loss is 0.13035105168819427\n",
      "epoch: 26 step: 100, loss is 0.1122111827135086\n",
      "epoch: 26 step: 101, loss is 0.09383658319711685\n",
      "epoch: 26 step: 102, loss is 0.09239531308412552\n",
      "epoch: 26 step: 103, loss is 0.2541257441043854\n",
      "epoch: 26 step: 104, loss is 0.1654309779405594\n",
      "epoch: 26 step: 105, loss is 0.2680889666080475\n",
      "epoch: 26 step: 106, loss is 0.14524060487747192\n",
      "epoch: 26 step: 107, loss is 0.21500298380851746\n",
      "epoch: 26 step: 108, loss is 0.2769991457462311\n",
      "epoch: 26 step: 109, loss is 0.12303526699542999\n",
      "epoch: 26 step: 110, loss is 0.08536072820425034\n",
      "epoch: 26 step: 111, loss is 0.24098335206508636\n",
      "epoch: 26 step: 112, loss is 0.2238176316022873\n",
      "epoch: 26 step: 113, loss is 0.1260175257921219\n",
      "epoch: 26 step: 114, loss is 0.25124210119247437\n",
      "epoch: 26 step: 115, loss is 0.25401753187179565\n",
      "epoch: 26 step: 116, loss is 0.1731852889060974\n",
      "epoch: 26 step: 117, loss is 0.12584005296230316\n",
      "epoch: 26 step: 118, loss is 0.14043733477592468\n",
      "epoch: 26 step: 119, loss is 0.13514459133148193\n",
      "epoch: 26 step: 120, loss is 0.15398971736431122\n",
      "epoch: 26 step: 121, loss is 0.2532956600189209\n",
      "epoch: 26 step: 122, loss is 0.21310368180274963\n",
      "epoch: 26 step: 123, loss is 0.33690759539604187\n",
      "epoch: 26 step: 124, loss is 0.14424431324005127\n",
      "epoch: 26 step: 125, loss is 0.1862591952085495\n",
      "epoch: 26 step: 126, loss is 0.254167765378952\n",
      "epoch: 26 step: 127, loss is 0.15201307833194733\n",
      "epoch: 26 step: 128, loss is 0.16773125529289246\n",
      "epoch: 26 step: 129, loss is 0.19928261637687683\n",
      "epoch: 26 step: 130, loss is 0.03829485550522804\n",
      "epoch: 26 step: 131, loss is 0.18953557312488556\n",
      "epoch: 26 step: 132, loss is 0.14297346770763397\n",
      "epoch: 26 step: 133, loss is 0.27513810992240906\n",
      "epoch: 26 step: 134, loss is 0.22485141456127167\n",
      "epoch: 26 step: 135, loss is 0.17709188163280487\n",
      "epoch: 26 step: 136, loss is 0.19237592816352844\n",
      "epoch: 26 step: 137, loss is 0.26344209909439087\n",
      "epoch: 26 step: 138, loss is 0.21372687816619873\n",
      "epoch: 26 step: 139, loss is 0.1620904803276062\n",
      "epoch: 26 step: 140, loss is 0.10788554698228836\n",
      "epoch: 26 step: 141, loss is 0.13630017638206482\n",
      "epoch: 26 step: 142, loss is 0.17166882753372192\n",
      "epoch: 26 step: 143, loss is 0.3097953200340271\n",
      "epoch: 26 step: 144, loss is 0.1712837815284729\n",
      "epoch: 26 step: 145, loss is 0.1678323894739151\n",
      "epoch: 26 step: 146, loss is 0.1702900379896164\n",
      "epoch: 26 step: 147, loss is 0.1625272035598755\n",
      "epoch: 26 step: 148, loss is 0.2582850754261017\n",
      "epoch: 26 step: 149, loss is 0.0917014554142952\n",
      "epoch: 26 step: 150, loss is 0.2083597481250763\n",
      "epoch: 26 step: 151, loss is 0.14932364225387573\n",
      "epoch: 26 step: 152, loss is 0.2558805048465729\n",
      "epoch: 26 step: 153, loss is 0.20520645380020142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 154, loss is 0.12360892444849014\n",
      "epoch: 26 step: 155, loss is 0.11372866481542587\n",
      "epoch: 26 step: 156, loss is 0.31357064843177795\n",
      "epoch: 26 step: 157, loss is 0.16107134521007538\n",
      "epoch: 26 step: 158, loss is 0.20936870574951172\n",
      "epoch: 26 step: 159, loss is 0.08354812860488892\n",
      "epoch: 26 step: 160, loss is 0.2759802043437958\n",
      "epoch: 26 step: 161, loss is 0.19653530418872833\n",
      "epoch: 26 step: 162, loss is 0.23957353830337524\n",
      "epoch: 26 step: 163, loss is 0.2021232396364212\n",
      "epoch: 26 step: 164, loss is 0.21760469675064087\n",
      "epoch: 26 step: 165, loss is 0.15868720412254333\n",
      "epoch: 26 step: 166, loss is 0.11008314043283463\n",
      "epoch: 26 step: 167, loss is 0.1725776493549347\n",
      "epoch: 26 step: 168, loss is 0.19584523141384125\n",
      "epoch: 26 step: 169, loss is 0.16672900319099426\n",
      "epoch: 26 step: 170, loss is 0.1455078274011612\n",
      "epoch: 26 step: 171, loss is 0.24718187749385834\n",
      "epoch: 26 step: 172, loss is 0.09481336921453476\n",
      "epoch: 26 step: 173, loss is 0.20457828044891357\n",
      "epoch: 26 step: 174, loss is 0.1431952267885208\n",
      "epoch: 26 step: 175, loss is 0.20557495951652527\n",
      "epoch: 26 step: 176, loss is 0.09361919015645981\n",
      "epoch: 26 step: 177, loss is 0.1340651512145996\n",
      "epoch: 26 step: 178, loss is 0.37931928038597107\n",
      "epoch: 26 step: 179, loss is 0.11666491627693176\n",
      "epoch: 26 step: 180, loss is 0.08647601306438446\n",
      "epoch: 26 step: 181, loss is 0.20713336765766144\n",
      "epoch: 26 step: 182, loss is 0.19461575150489807\n",
      "epoch: 26 step: 183, loss is 0.4523497521877289\n",
      "epoch: 26 step: 184, loss is 0.12150949239730835\n",
      "epoch: 26 step: 185, loss is 0.22473272681236267\n",
      "epoch: 26 step: 186, loss is 0.2559387683868408\n",
      "epoch: 26 step: 187, loss is 0.1763276308774948\n",
      "epoch: 26 step: 188, loss is 0.30157309770584106\n",
      "epoch: 26 step: 189, loss is 0.09812851250171661\n",
      "epoch: 26 step: 190, loss is 0.19936519861221313\n",
      "epoch: 26 step: 191, loss is 0.1286071091890335\n",
      "epoch: 26 step: 192, loss is 0.3256682753562927\n",
      "epoch: 26 step: 193, loss is 0.19244608283042908\n",
      "epoch: 26 step: 194, loss is 0.08991217613220215\n",
      "epoch: 26 step: 195, loss is 0.1606721431016922\n",
      "epoch: 26 step: 196, loss is 0.10487212240695953\n",
      "epoch: 26 step: 197, loss is 0.20381376147270203\n",
      "epoch: 26 step: 198, loss is 0.3113285005092621\n",
      "epoch: 26 step: 199, loss is 0.3530893921852112\n",
      "epoch: 26 step: 200, loss is 0.10437816381454468\n",
      "epoch: 26 step: 201, loss is 0.14932572841644287\n",
      "epoch: 26 step: 202, loss is 0.11410611122846603\n",
      "epoch: 26 step: 203, loss is 0.098388671875\n",
      "epoch: 26 step: 204, loss is 0.08247143030166626\n",
      "epoch: 26 step: 205, loss is 0.11075759679079056\n",
      "epoch: 26 step: 206, loss is 0.3719640374183655\n",
      "epoch: 26 step: 207, loss is 0.1448703408241272\n",
      "epoch: 26 step: 208, loss is 0.07875194400548935\n",
      "epoch: 26 step: 209, loss is 0.3343176245689392\n",
      "epoch: 26 step: 210, loss is 0.2358287125825882\n",
      "epoch: 26 step: 211, loss is 0.17802833020687103\n",
      "epoch: 26 step: 212, loss is 0.1871033012866974\n",
      "epoch: 26 step: 213, loss is 0.19761471450328827\n",
      "epoch: 26 step: 214, loss is 0.3084335923194885\n",
      "epoch: 26 step: 215, loss is 0.13460373878479004\n",
      "epoch: 26 step: 216, loss is 0.22110526263713837\n",
      "epoch: 26 step: 217, loss is 0.19895599782466888\n",
      "epoch: 26 step: 218, loss is 0.10482332855463028\n",
      "epoch: 26 step: 219, loss is 0.21324817836284637\n",
      "epoch: 26 step: 220, loss is 0.1006200835108757\n",
      "epoch: 26 step: 221, loss is 0.22477447986602783\n",
      "epoch: 26 step: 222, loss is 0.125555619597435\n",
      "epoch: 26 step: 223, loss is 0.2281418889760971\n",
      "epoch: 26 step: 224, loss is 0.17480018734931946\n",
      "epoch: 26 step: 225, loss is 0.13761214911937714\n",
      "epoch: 26 step: 226, loss is 0.08445437997579575\n",
      "epoch: 26 step: 227, loss is 0.16880066692829132\n",
      "epoch: 26 step: 228, loss is 0.2319551259279251\n",
      "epoch: 26 step: 229, loss is 0.2878018915653229\n",
      "epoch: 26 step: 230, loss is 0.3343619108200073\n",
      "epoch: 26 step: 231, loss is 0.17201414704322815\n",
      "epoch: 26 step: 232, loss is 0.1903315633535385\n",
      "epoch: 26 step: 233, loss is 0.1173391118645668\n",
      "epoch: 26 step: 234, loss is 0.1579473316669464\n",
      "epoch: 26 step: 235, loss is 0.3003919720649719\n",
      "epoch: 26 step: 236, loss is 0.313455730676651\n",
      "epoch: 26 step: 237, loss is 0.31086254119873047\n",
      "epoch: 26 step: 238, loss is 0.22154830396175385\n",
      "epoch: 26 step: 239, loss is 0.03266153484582901\n",
      "epoch: 26 step: 240, loss is 0.20616260170936584\n",
      "epoch: 26 step: 241, loss is 0.20829525589942932\n",
      "epoch: 26 step: 242, loss is 0.21085645258426666\n",
      "epoch: 26 step: 243, loss is 0.14434289932250977\n",
      "epoch: 26 step: 244, loss is 0.18640455603599548\n",
      "epoch: 26 step: 245, loss is 0.14752672612667084\n",
      "epoch: 26 step: 246, loss is 0.1251526176929474\n",
      "epoch: 26 step: 247, loss is 0.23349839448928833\n",
      "epoch: 26 step: 248, loss is 0.20399536192417145\n",
      "epoch: 26 step: 249, loss is 0.22835391759872437\n",
      "epoch: 26 step: 250, loss is 0.09996785968542099\n",
      "epoch: 26 step: 251, loss is 0.10351504385471344\n",
      "epoch: 26 step: 252, loss is 0.17059609293937683\n",
      "epoch: 26 step: 253, loss is 0.3189002573490143\n",
      "epoch: 26 step: 254, loss is 0.1571425199508667\n",
      "epoch: 26 step: 255, loss is 0.23001542687416077\n",
      "epoch: 26 step: 256, loss is 0.1322777271270752\n",
      "epoch: 26 step: 257, loss is 0.15920855104923248\n",
      "epoch: 26 step: 258, loss is 0.25090649724006653\n",
      "epoch: 26 step: 259, loss is 0.29114794731140137\n",
      "epoch: 26 step: 260, loss is 0.2108154594898224\n",
      "epoch: 26 step: 261, loss is 0.17949169874191284\n",
      "epoch: 26 step: 262, loss is 0.17050884664058685\n",
      "epoch: 26 step: 263, loss is 0.18835434317588806\n",
      "epoch: 26 step: 264, loss is 0.17802923917770386\n",
      "epoch: 26 step: 265, loss is 0.25793302059173584\n",
      "epoch: 26 step: 266, loss is 0.2700407803058624\n",
      "epoch: 26 step: 267, loss is 0.09641005098819733\n",
      "epoch: 26 step: 268, loss is 0.3149939179420471\n",
      "epoch: 26 step: 269, loss is 0.1820015013217926\n",
      "epoch: 26 step: 270, loss is 0.2073909491300583\n",
      "epoch: 26 step: 271, loss is 0.06593620777130127\n",
      "epoch: 26 step: 272, loss is 0.2336609661579132\n",
      "epoch: 26 step: 273, loss is 0.11998886615037918\n",
      "epoch: 26 step: 274, loss is 0.3280036151409149\n",
      "epoch: 26 step: 275, loss is 0.17459364235401154\n",
      "epoch: 26 step: 276, loss is 0.15424518287181854\n",
      "epoch: 26 step: 277, loss is 0.37801969051361084\n",
      "epoch: 26 step: 278, loss is 0.10866876691579819\n",
      "epoch: 26 step: 279, loss is 0.30701035261154175\n",
      "epoch: 26 step: 280, loss is 0.23256459832191467\n",
      "epoch: 26 step: 281, loss is 0.2766824960708618\n",
      "epoch: 26 step: 282, loss is 0.4903911054134369\n",
      "epoch: 26 step: 283, loss is 0.1896767020225525\n",
      "epoch: 26 step: 284, loss is 0.21403762698173523\n",
      "epoch: 26 step: 285, loss is 0.13644032180309296\n",
      "epoch: 26 step: 286, loss is 0.31051182746887207\n",
      "epoch: 26 step: 287, loss is 0.198780819773674\n",
      "epoch: 26 step: 288, loss is 0.14469486474990845\n",
      "epoch: 26 step: 289, loss is 0.16638486087322235\n",
      "epoch: 26 step: 290, loss is 0.21155303716659546\n",
      "epoch: 26 step: 291, loss is 0.249350443482399\n",
      "epoch: 26 step: 292, loss is 0.36388736963272095\n",
      "epoch: 26 step: 293, loss is 0.23571652173995972\n",
      "epoch: 26 step: 294, loss is 0.2027711719274521\n",
      "epoch: 26 step: 295, loss is 0.1529182642698288\n",
      "epoch: 26 step: 296, loss is 0.16150137782096863\n",
      "epoch: 26 step: 297, loss is 0.062298741191625595\n",
      "epoch: 26 step: 298, loss is 0.19973714649677277\n",
      "epoch: 26 step: 299, loss is 0.17603574693202972\n",
      "epoch: 26 step: 300, loss is 0.12259986251592636\n",
      "epoch: 26 step: 301, loss is 0.21434079110622406\n",
      "epoch: 26 step: 302, loss is 0.08224797993898392\n",
      "epoch: 26 step: 303, loss is 0.13767416775226593\n",
      "epoch: 26 step: 304, loss is 0.2025519609451294\n",
      "epoch: 26 step: 305, loss is 0.09689649194478989\n",
      "epoch: 26 step: 306, loss is 0.07384965568780899\n",
      "epoch: 26 step: 307, loss is 0.09765518456697464\n",
      "epoch: 26 step: 308, loss is 0.2830825448036194\n",
      "epoch: 26 step: 309, loss is 0.07478690147399902\n",
      "epoch: 26 step: 310, loss is 0.09120962023735046\n",
      "epoch: 26 step: 311, loss is 0.25072577595710754\n",
      "epoch: 26 step: 312, loss is 0.24018430709838867\n",
      "epoch: 26 step: 313, loss is 0.24094918370246887\n",
      "epoch: 26 step: 314, loss is 0.2615610957145691\n",
      "epoch: 26 step: 315, loss is 0.10973276942968369\n",
      "epoch: 26 step: 316, loss is 0.2923496961593628\n",
      "epoch: 26 step: 317, loss is 0.11405621469020844\n",
      "epoch: 26 step: 318, loss is 0.2760307788848877\n",
      "epoch: 26 step: 319, loss is 0.28383344411849976\n",
      "epoch: 26 step: 320, loss is 0.1682814210653305\n",
      "epoch: 26 step: 321, loss is 0.08404582738876343\n",
      "epoch: 26 step: 322, loss is 0.2566055357456207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 323, loss is 0.1013830155134201\n",
      "epoch: 26 step: 324, loss is 0.23263485729694366\n",
      "epoch: 26 step: 325, loss is 0.15710823237895966\n",
      "epoch: 26 step: 326, loss is 0.28868380188941956\n",
      "epoch: 26 step: 327, loss is 0.10251224786043167\n",
      "epoch: 26 step: 328, loss is 0.258931964635849\n",
      "epoch: 26 step: 329, loss is 0.14155790209770203\n",
      "epoch: 26 step: 330, loss is 0.22120818495750427\n",
      "epoch: 26 step: 331, loss is 0.13951125741004944\n",
      "epoch: 26 step: 332, loss is 0.18537649512290955\n",
      "epoch: 26 step: 333, loss is 0.15875208377838135\n",
      "epoch: 26 step: 334, loss is 0.07182350009679794\n",
      "epoch: 26 step: 335, loss is 0.26696330308914185\n",
      "epoch: 26 step: 336, loss is 0.16336333751678467\n",
      "epoch: 26 step: 337, loss is 0.17828349769115448\n",
      "epoch: 26 step: 338, loss is 0.12490570545196533\n",
      "epoch: 26 step: 339, loss is 0.10373694449663162\n",
      "epoch: 26 step: 340, loss is 0.2137106955051422\n",
      "epoch: 26 step: 341, loss is 0.24413347244262695\n",
      "epoch: 26 step: 342, loss is 0.1842721849679947\n",
      "epoch: 26 step: 343, loss is 0.20385679602622986\n",
      "epoch: 26 step: 344, loss is 0.11280778050422668\n",
      "epoch: 26 step: 345, loss is 0.22254304587841034\n",
      "epoch: 26 step: 346, loss is 0.1856662929058075\n",
      "epoch: 26 step: 347, loss is 0.14620079100131989\n",
      "epoch: 26 step: 348, loss is 0.07728708535432816\n",
      "epoch: 26 step: 349, loss is 0.13180504739284515\n",
      "epoch: 26 step: 350, loss is 0.12959299981594086\n",
      "epoch: 26 step: 351, loss is 0.11603598296642303\n",
      "epoch: 26 step: 352, loss is 0.2472991943359375\n",
      "epoch: 26 step: 353, loss is 0.17503046989440918\n",
      "epoch: 26 step: 354, loss is 0.10589811950922012\n",
      "epoch: 26 step: 355, loss is 0.15995727479457855\n",
      "epoch: 26 step: 356, loss is 0.30184611678123474\n",
      "epoch: 26 step: 357, loss is 0.15394484996795654\n",
      "epoch: 26 step: 358, loss is 0.15545763075351715\n",
      "epoch: 26 step: 359, loss is 0.24478182196617126\n",
      "epoch: 26 step: 360, loss is 0.15445423126220703\n",
      "epoch: 26 step: 361, loss is 0.0808444693684578\n",
      "epoch: 26 step: 362, loss is 0.08251255750656128\n",
      "epoch: 26 step: 363, loss is 0.20077699422836304\n",
      "epoch: 26 step: 364, loss is 0.15648943185806274\n",
      "epoch: 26 step: 365, loss is 0.20586885511875153\n",
      "epoch: 26 step: 366, loss is 0.12870562076568604\n",
      "epoch: 26 step: 367, loss is 0.3846261501312256\n",
      "epoch: 26 step: 368, loss is 0.15852853655815125\n",
      "epoch: 26 step: 369, loss is 0.11829223483800888\n",
      "epoch: 26 step: 370, loss is 0.2713712751865387\n",
      "epoch: 26 step: 371, loss is 0.16736112534999847\n",
      "epoch: 26 step: 372, loss is 0.15054205060005188\n",
      "epoch: 26 step: 373, loss is 0.10917242616415024\n",
      "epoch: 26 step: 374, loss is 0.1485351026058197\n",
      "epoch: 26 step: 375, loss is 0.18851825594902039\n",
      "epoch: 26 step: 376, loss is 0.12621474266052246\n",
      "epoch: 26 step: 377, loss is 0.19571639597415924\n",
      "epoch: 26 step: 378, loss is 0.1025966927409172\n",
      "epoch: 26 step: 379, loss is 0.10029161721467972\n",
      "epoch: 26 step: 380, loss is 0.2548896074295044\n",
      "epoch: 26 step: 381, loss is 0.1400648057460785\n",
      "epoch: 26 step: 382, loss is 0.16853070259094238\n",
      "epoch: 26 step: 383, loss is 0.2436528205871582\n",
      "epoch: 26 step: 384, loss is 0.16979312896728516\n",
      "epoch: 26 step: 385, loss is 0.14661400020122528\n",
      "epoch: 26 step: 386, loss is 0.2570435106754303\n",
      "epoch: 26 step: 387, loss is 0.2532033324241638\n",
      "epoch: 26 step: 388, loss is 0.17817825078964233\n",
      "epoch: 26 step: 389, loss is 0.21144038438796997\n",
      "epoch: 26 step: 390, loss is 0.12005967646837234\n",
      "epoch: 26 step: 391, loss is 0.17586955428123474\n",
      "epoch: 26 step: 392, loss is 0.22018735110759735\n",
      "epoch: 26 step: 393, loss is 0.20077921450138092\n",
      "epoch: 26 step: 394, loss is 0.20739179849624634\n",
      "epoch: 26 step: 395, loss is 0.3699025511741638\n",
      "epoch: 26 step: 396, loss is 0.1310613602399826\n",
      "epoch: 26 step: 397, loss is 0.1713690161705017\n",
      "epoch: 26 step: 398, loss is 0.11091979593038559\n",
      "epoch: 26 step: 399, loss is 0.12335601449012756\n",
      "epoch: 26 step: 400, loss is 0.1585032045841217\n",
      "epoch: 26 step: 401, loss is 0.20609118044376373\n",
      "epoch: 26 step: 402, loss is 0.0855245441198349\n",
      "epoch: 26 step: 403, loss is 0.18792606890201569\n",
      "epoch: 26 step: 404, loss is 0.24761685729026794\n",
      "epoch: 26 step: 405, loss is 0.11686491966247559\n",
      "epoch: 26 step: 406, loss is 0.13491326570510864\n",
      "epoch: 26 step: 407, loss is 0.046379026025533676\n",
      "epoch: 26 step: 408, loss is 0.2962656021118164\n",
      "epoch: 26 step: 409, loss is 0.13287904858589172\n",
      "epoch: 26 step: 410, loss is 0.1462704837322235\n",
      "epoch: 26 step: 411, loss is 0.17447441816329956\n",
      "epoch: 26 step: 412, loss is 0.22294734418392181\n",
      "epoch: 26 step: 413, loss is 0.08577393740415573\n",
      "epoch: 26 step: 414, loss is 0.15800394117832184\n",
      "epoch: 26 step: 415, loss is 0.13466499745845795\n",
      "epoch: 26 step: 416, loss is 0.32632339000701904\n",
      "epoch: 26 step: 417, loss is 0.15128999948501587\n",
      "epoch: 26 step: 418, loss is 0.1207386702299118\n",
      "epoch: 26 step: 419, loss is 0.1600607931613922\n",
      "epoch: 26 step: 420, loss is 0.14363622665405273\n",
      "epoch: 26 step: 421, loss is 0.21909508109092712\n",
      "epoch: 26 step: 422, loss is 0.2393074333667755\n",
      "epoch: 26 step: 423, loss is 0.3220146596431732\n",
      "epoch: 26 step: 424, loss is 0.28036338090896606\n",
      "epoch: 26 step: 425, loss is 0.26197537779808044\n",
      "epoch: 26 step: 426, loss is 0.2286645770072937\n",
      "epoch: 26 step: 427, loss is 0.05065729841589928\n",
      "epoch: 26 step: 428, loss is 0.11630570888519287\n",
      "epoch: 26 step: 429, loss is 0.1382286101579666\n",
      "epoch: 26 step: 430, loss is 0.19277095794677734\n",
      "epoch: 26 step: 431, loss is 0.27540409564971924\n",
      "epoch: 26 step: 432, loss is 0.2667604386806488\n",
      "epoch: 26 step: 433, loss is 0.09992669522762299\n",
      "epoch: 26 step: 434, loss is 0.08057556301355362\n",
      "epoch: 26 step: 435, loss is 0.16519272327423096\n",
      "epoch: 26 step: 436, loss is 0.07042129337787628\n",
      "epoch: 26 step: 437, loss is 0.23902741074562073\n",
      "epoch: 26 step: 438, loss is 0.09244700521230698\n",
      "epoch: 26 step: 439, loss is 0.08028474450111389\n",
      "epoch: 26 step: 440, loss is 0.0876002386212349\n",
      "epoch: 26 step: 441, loss is 0.26129692792892456\n",
      "epoch: 26 step: 442, loss is 0.37476930022239685\n",
      "epoch: 26 step: 443, loss is 0.1429518163204193\n",
      "epoch: 26 step: 444, loss is 0.34677088260650635\n",
      "epoch: 26 step: 445, loss is 0.15366853773593903\n",
      "epoch: 26 step: 446, loss is 0.14817653596401215\n",
      "epoch: 26 step: 447, loss is 0.24909350275993347\n",
      "epoch: 26 step: 448, loss is 0.07206389307975769\n",
      "epoch: 26 step: 449, loss is 0.07936035841703415\n",
      "epoch: 26 step: 450, loss is 0.12249255925416946\n",
      "epoch: 26 step: 451, loss is 0.278120219707489\n",
      "epoch: 26 step: 452, loss is 0.1734035760164261\n",
      "epoch: 26 step: 453, loss is 0.1843605935573578\n",
      "epoch: 26 step: 454, loss is 0.18494993448257446\n",
      "epoch: 26 step: 455, loss is 0.10659819096326828\n",
      "epoch: 26 step: 456, loss is 0.14496465027332306\n",
      "epoch: 26 step: 457, loss is 0.24541394412517548\n",
      "epoch: 26 step: 458, loss is 0.17692147195339203\n",
      "epoch: 26 step: 459, loss is 0.17277315258979797\n",
      "epoch: 26 step: 460, loss is 0.31649050116539\n",
      "epoch: 26 step: 461, loss is 0.16257940232753754\n",
      "epoch: 26 step: 462, loss is 0.2097851037979126\n",
      "epoch: 26 step: 463, loss is 0.41277408599853516\n",
      "epoch: 26 step: 464, loss is 0.19026713073253632\n",
      "epoch: 26 step: 465, loss is 0.1807604283094406\n",
      "epoch: 26 step: 466, loss is 0.11088920384645462\n",
      "epoch: 26 step: 467, loss is 0.19101271033287048\n",
      "epoch: 26 step: 468, loss is 0.19498373568058014\n",
      "epoch: 26 step: 469, loss is 0.15651880204677582\n",
      "epoch: 26 step: 470, loss is 0.086703360080719\n",
      "epoch: 26 step: 471, loss is 0.07907762378454208\n",
      "epoch: 26 step: 472, loss is 0.2528400123119354\n",
      "epoch: 26 step: 473, loss is 0.1702149659395218\n",
      "epoch: 26 step: 474, loss is 0.2606518268585205\n",
      "epoch: 26 step: 475, loss is 0.17027612030506134\n",
      "epoch: 26 step: 476, loss is 0.08044058084487915\n",
      "epoch: 26 step: 477, loss is 0.11026953160762787\n",
      "epoch: 26 step: 478, loss is 0.1024889126420021\n",
      "epoch: 26 step: 479, loss is 0.18742945790290833\n",
      "epoch: 26 step: 480, loss is 0.14341144263744354\n",
      "epoch: 26 step: 481, loss is 0.12561237812042236\n",
      "epoch: 26 step: 482, loss is 0.21956972777843475\n",
      "epoch: 26 step: 483, loss is 0.2850215435028076\n",
      "epoch: 26 step: 484, loss is 0.07566523551940918\n",
      "epoch: 26 step: 485, loss is 0.08723073452711105\n",
      "epoch: 26 step: 486, loss is 0.2544471025466919\n",
      "epoch: 26 step: 487, loss is 0.1324128806591034\n",
      "epoch: 26 step: 488, loss is 0.06607147306203842\n",
      "epoch: 26 step: 489, loss is 0.11809251457452774\n",
      "epoch: 26 step: 490, loss is 0.3460880219936371\n",
      "epoch: 26 step: 491, loss is 0.25723788142204285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 492, loss is 0.13641862571239471\n",
      "epoch: 26 step: 493, loss is 0.13471385836601257\n",
      "epoch: 26 step: 494, loss is 0.2456073760986328\n",
      "epoch: 26 step: 495, loss is 0.09279249608516693\n",
      "epoch: 26 step: 496, loss is 0.163707435131073\n",
      "epoch: 26 step: 497, loss is 0.30815064907073975\n",
      "epoch: 26 step: 498, loss is 0.15647578239440918\n",
      "epoch: 26 step: 499, loss is 0.15563656389713287\n",
      "epoch: 26 step: 500, loss is 0.07701588422060013\n",
      "epoch: 26 step: 501, loss is 0.11778362095355988\n",
      "epoch: 26 step: 502, loss is 0.11566286534070969\n",
      "epoch: 26 step: 503, loss is 0.11922671645879745\n",
      "epoch: 26 step: 504, loss is 0.13815239071846008\n",
      "epoch: 26 step: 505, loss is 0.2307332456111908\n",
      "epoch: 26 step: 506, loss is 0.2849161624908447\n",
      "epoch: 26 step: 507, loss is 0.17256757616996765\n",
      "epoch: 26 step: 508, loss is 0.2006353735923767\n",
      "epoch: 26 step: 509, loss is 0.2284853160381317\n",
      "epoch: 26 step: 510, loss is 0.18904490768909454\n",
      "epoch: 26 step: 511, loss is 0.15092867612838745\n",
      "epoch: 26 step: 512, loss is 0.17538394033908844\n",
      "epoch: 26 step: 513, loss is 0.1491604447364807\n",
      "epoch: 26 step: 514, loss is 0.10887642204761505\n",
      "epoch: 26 step: 515, loss is 0.0863894373178482\n",
      "epoch: 26 step: 516, loss is 0.27382808923721313\n",
      "epoch: 26 step: 517, loss is 0.126722052693367\n",
      "epoch: 26 step: 518, loss is 0.2939012348651886\n",
      "epoch: 26 step: 519, loss is 0.1445549875497818\n",
      "epoch: 26 step: 520, loss is 0.13306234776973724\n",
      "epoch: 26 step: 521, loss is 0.13369663059711456\n",
      "epoch: 26 step: 522, loss is 0.13562971353530884\n",
      "epoch: 26 step: 523, loss is 0.22851386666297913\n",
      "epoch: 26 step: 524, loss is 0.07372135668992996\n",
      "epoch: 26 step: 525, loss is 0.11233682930469513\n",
      "epoch: 26 step: 526, loss is 0.15974394977092743\n",
      "epoch: 26 step: 527, loss is 0.3648775517940521\n",
      "epoch: 26 step: 528, loss is 0.10747339576482773\n",
      "epoch: 26 step: 529, loss is 0.1668771654367447\n",
      "epoch: 26 step: 530, loss is 0.1951933056116104\n",
      "epoch: 26 step: 531, loss is 0.22312261164188385\n",
      "epoch: 26 step: 532, loss is 0.15957599878311157\n",
      "epoch: 26 step: 533, loss is 0.15220043063163757\n",
      "epoch: 26 step: 534, loss is 0.2236659824848175\n",
      "epoch: 26 step: 535, loss is 0.1992158442735672\n",
      "epoch: 26 step: 536, loss is 0.07659285515546799\n",
      "epoch: 26 step: 537, loss is 0.31651321053504944\n",
      "epoch: 26 step: 538, loss is 0.23334649205207825\n",
      "epoch: 26 step: 539, loss is 0.19052959978580475\n",
      "epoch: 26 step: 540, loss is 0.16203436255455017\n",
      "epoch: 26 step: 541, loss is 0.2147791087627411\n",
      "epoch: 26 step: 542, loss is 0.10423290729522705\n",
      "epoch: 26 step: 543, loss is 0.14186394214630127\n",
      "epoch: 26 step: 544, loss is 0.18031325936317444\n",
      "epoch: 26 step: 545, loss is 0.2707746922969818\n",
      "epoch: 26 step: 546, loss is 0.13902230560779572\n",
      "epoch: 26 step: 547, loss is 0.21002624928951263\n",
      "epoch: 26 step: 548, loss is 0.09224202483892441\n",
      "epoch: 26 step: 549, loss is 0.15138362348079681\n",
      "epoch: 26 step: 550, loss is 0.186636820435524\n",
      "epoch: 26 step: 551, loss is 0.18302404880523682\n",
      "epoch: 26 step: 552, loss is 0.18253286182880402\n",
      "epoch: 26 step: 553, loss is 0.4523960053920746\n",
      "epoch: 26 step: 554, loss is 0.10194075852632523\n",
      "epoch: 26 step: 555, loss is 0.3435114920139313\n",
      "epoch: 26 step: 556, loss is 0.19602037966251373\n",
      "epoch: 26 step: 557, loss is 0.20365777611732483\n",
      "epoch: 26 step: 558, loss is 0.26565322279930115\n",
      "epoch: 26 step: 559, loss is 0.24670693278312683\n",
      "epoch: 26 step: 560, loss is 0.1425260305404663\n",
      "epoch: 26 step: 561, loss is 0.24983540177345276\n",
      "epoch: 26 step: 562, loss is 0.18509800732135773\n",
      "epoch: 26 step: 563, loss is 0.20355549454689026\n",
      "epoch: 26 step: 564, loss is 0.2047482430934906\n",
      "epoch: 26 step: 565, loss is 0.15072154998779297\n",
      "epoch: 26 step: 566, loss is 0.12305768579244614\n",
      "epoch: 26 step: 567, loss is 0.19079405069351196\n",
      "epoch: 26 step: 568, loss is 0.15559829771518707\n",
      "epoch: 26 step: 569, loss is 0.21408212184906006\n",
      "epoch: 26 step: 570, loss is 0.09532596170902252\n",
      "epoch: 26 step: 571, loss is 0.13907137513160706\n",
      "epoch: 26 step: 572, loss is 0.28447577357292175\n",
      "epoch: 26 step: 573, loss is 0.2544460594654083\n",
      "epoch: 26 step: 574, loss is 0.08641316741704941\n",
      "epoch: 26 step: 575, loss is 0.28520163893699646\n",
      "epoch: 26 step: 576, loss is 0.176813542842865\n",
      "epoch: 26 step: 577, loss is 0.21459375321865082\n",
      "epoch: 26 step: 578, loss is 0.2790394723415375\n",
      "epoch: 26 step: 579, loss is 0.33518797159194946\n",
      "epoch: 26 step: 580, loss is 0.10339520126581192\n",
      "epoch: 26 step: 581, loss is 0.4009256064891815\n",
      "epoch: 26 step: 582, loss is 0.17187540233135223\n",
      "epoch: 26 step: 583, loss is 0.1056203842163086\n",
      "epoch: 26 step: 584, loss is 0.17389221489429474\n",
      "epoch: 26 step: 585, loss is 0.14416146278381348\n",
      "epoch: 26 step: 586, loss is 0.1058776006102562\n",
      "epoch: 26 step: 587, loss is 0.22334390878677368\n",
      "epoch: 26 step: 588, loss is 0.23381662368774414\n",
      "epoch: 26 step: 589, loss is 0.1437867432832718\n",
      "epoch: 26 step: 590, loss is 0.17805759608745575\n",
      "epoch: 26 step: 591, loss is 0.17106635868549347\n",
      "epoch: 26 step: 592, loss is 0.12363317608833313\n",
      "epoch: 26 step: 593, loss is 0.1520736813545227\n",
      "epoch: 26 step: 594, loss is 0.20569945871829987\n",
      "epoch: 26 step: 595, loss is 0.231917604804039\n",
      "epoch: 26 step: 596, loss is 0.21175895631313324\n",
      "epoch: 26 step: 597, loss is 0.12906283140182495\n",
      "epoch: 26 step: 598, loss is 0.16998498141765594\n",
      "epoch: 26 step: 599, loss is 0.2713879942893982\n",
      "epoch: 26 step: 600, loss is 0.2289053499698639\n",
      "epoch: 26 step: 601, loss is 0.08730068057775497\n",
      "epoch: 26 step: 602, loss is 0.17513251304626465\n",
      "epoch: 26 step: 603, loss is 0.23714029788970947\n",
      "epoch: 26 step: 604, loss is 0.13932472467422485\n",
      "epoch: 26 step: 605, loss is 0.1950836330652237\n",
      "epoch: 26 step: 606, loss is 0.22448976337909698\n",
      "epoch: 26 step: 607, loss is 0.11494685709476471\n",
      "epoch: 26 step: 608, loss is 0.14547249674797058\n",
      "epoch: 26 step: 609, loss is 0.14477305114269257\n",
      "epoch: 26 step: 610, loss is 0.30022311210632324\n",
      "epoch: 26 step: 611, loss is 0.17520561814308167\n",
      "epoch: 26 step: 612, loss is 0.341553658246994\n",
      "epoch: 26 step: 613, loss is 0.12010383605957031\n",
      "epoch: 26 step: 614, loss is 0.2350979596376419\n",
      "epoch: 26 step: 615, loss is 0.18720155954360962\n",
      "epoch: 26 step: 616, loss is 0.23221378028392792\n",
      "epoch: 26 step: 617, loss is 0.15210644900798798\n",
      "epoch: 26 step: 618, loss is 0.15341167151927948\n",
      "epoch: 26 step: 619, loss is 0.25294849276542664\n",
      "epoch: 26 step: 620, loss is 0.14543211460113525\n",
      "epoch: 26 step: 621, loss is 0.14172500371932983\n",
      "epoch: 26 step: 622, loss is 0.10933953523635864\n",
      "epoch: 26 step: 623, loss is 0.17940843105316162\n",
      "epoch: 26 step: 624, loss is 0.22063012421131134\n",
      "epoch: 26 step: 625, loss is 0.18240170180797577\n",
      "epoch: 26 step: 626, loss is 0.4120334982872009\n",
      "epoch: 26 step: 627, loss is 0.2824239134788513\n",
      "epoch: 26 step: 628, loss is 0.20229245722293854\n",
      "epoch: 26 step: 629, loss is 0.25359246134757996\n",
      "epoch: 26 step: 630, loss is 0.1498921513557434\n",
      "epoch: 26 step: 631, loss is 0.21628440916538239\n",
      "epoch: 26 step: 632, loss is 0.15110710263252258\n",
      "epoch: 26 step: 633, loss is 0.3723497688770294\n",
      "epoch: 26 step: 634, loss is 0.10078714042901993\n",
      "epoch: 26 step: 635, loss is 0.2034626603126526\n",
      "epoch: 26 step: 636, loss is 0.132832869887352\n",
      "epoch: 26 step: 637, loss is 0.19529536366462708\n",
      "epoch: 26 step: 638, loss is 0.14672787487506866\n",
      "epoch: 26 step: 639, loss is 0.15092627704143524\n",
      "epoch: 26 step: 640, loss is 0.134583979845047\n",
      "epoch: 26 step: 641, loss is 0.1026293933391571\n",
      "epoch: 26 step: 642, loss is 0.19912973046302795\n",
      "epoch: 26 step: 643, loss is 0.19326208531856537\n",
      "epoch: 26 step: 644, loss is 0.13131509721279144\n",
      "epoch: 26 step: 645, loss is 0.17995619773864746\n",
      "epoch: 26 step: 646, loss is 0.2270410805940628\n",
      "epoch: 26 step: 647, loss is 0.12812915444374084\n",
      "epoch: 26 step: 648, loss is 0.21050171554088593\n",
      "epoch: 26 step: 649, loss is 0.33115658164024353\n",
      "epoch: 26 step: 650, loss is 0.1728234440088272\n",
      "epoch: 26 step: 651, loss is 0.07370363920927048\n",
      "epoch: 26 step: 652, loss is 0.1344539374113083\n",
      "epoch: 26 step: 653, loss is 0.13890080153942108\n",
      "epoch: 26 step: 654, loss is 0.21961869299411774\n",
      "epoch: 26 step: 655, loss is 0.23784470558166504\n",
      "epoch: 26 step: 656, loss is 0.2320711463689804\n",
      "epoch: 26 step: 657, loss is 0.1661931276321411\n",
      "epoch: 26 step: 658, loss is 0.1415948122739792\n",
      "epoch: 26 step: 659, loss is 0.13401177525520325\n",
      "epoch: 26 step: 660, loss is 0.15837904810905457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 661, loss is 0.16470757126808167\n",
      "epoch: 26 step: 662, loss is 0.1438511162996292\n",
      "epoch: 26 step: 663, loss is 0.20173689723014832\n",
      "epoch: 26 step: 664, loss is 0.2298266440629959\n",
      "epoch: 26 step: 665, loss is 0.1314515918493271\n",
      "epoch: 26 step: 666, loss is 0.16431047022342682\n",
      "epoch: 26 step: 667, loss is 0.13318733870983124\n",
      "epoch: 26 step: 668, loss is 0.14786551892757416\n",
      "epoch: 26 step: 669, loss is 0.2895081341266632\n",
      "epoch: 26 step: 670, loss is 0.2189784049987793\n",
      "epoch: 26 step: 671, loss is 0.16558952629566193\n",
      "epoch: 26 step: 672, loss is 0.12494902312755585\n",
      "epoch: 26 step: 673, loss is 0.1340022087097168\n",
      "epoch: 26 step: 674, loss is 0.1645946353673935\n",
      "epoch: 26 step: 675, loss is 0.24344882369041443\n",
      "epoch: 26 step: 676, loss is 0.3643760085105896\n",
      "epoch: 26 step: 677, loss is 0.3218739628791809\n",
      "epoch: 26 step: 678, loss is 0.2226567268371582\n",
      "epoch: 26 step: 679, loss is 0.21459078788757324\n",
      "epoch: 26 step: 680, loss is 0.20199212431907654\n",
      "epoch: 26 step: 681, loss is 0.24405018985271454\n",
      "epoch: 26 step: 682, loss is 0.17370525002479553\n",
      "epoch: 26 step: 683, loss is 0.2679438889026642\n",
      "epoch: 26 step: 684, loss is 0.15105463564395905\n",
      "epoch: 26 step: 685, loss is 0.34247270226478577\n",
      "epoch: 26 step: 686, loss is 0.16108345985412598\n",
      "epoch: 26 step: 687, loss is 0.09002887457609177\n",
      "epoch: 26 step: 688, loss is 0.08153036236763\n",
      "epoch: 26 step: 689, loss is 0.13679981231689453\n",
      "epoch: 26 step: 690, loss is 0.0985792800784111\n",
      "epoch: 26 step: 691, loss is 0.12347841262817383\n",
      "epoch: 26 step: 692, loss is 0.12499400228261948\n",
      "epoch: 26 step: 693, loss is 0.21271970868110657\n",
      "epoch: 26 step: 694, loss is 0.08445949852466583\n",
      "epoch: 26 step: 695, loss is 0.14418740570545197\n",
      "epoch: 26 step: 696, loss is 0.13681797683238983\n",
      "epoch: 26 step: 697, loss is 0.2505815029144287\n",
      "epoch: 26 step: 698, loss is 0.25692811608314514\n",
      "epoch: 26 step: 699, loss is 0.2620052695274353\n",
      "epoch: 26 step: 700, loss is 0.27194759249687195\n",
      "epoch: 26 step: 701, loss is 0.1560678333044052\n",
      "epoch: 26 step: 702, loss is 0.24828553199768066\n",
      "epoch: 26 step: 703, loss is 0.07624873518943787\n",
      "epoch: 26 step: 704, loss is 0.16230545938014984\n",
      "epoch: 26 step: 705, loss is 0.13540250062942505\n",
      "epoch: 26 step: 706, loss is 0.10386531054973602\n",
      "epoch: 26 step: 707, loss is 0.13796840608119965\n",
      "epoch: 26 step: 708, loss is 0.12342552840709686\n",
      "epoch: 26 step: 709, loss is 0.3104279339313507\n",
      "epoch: 26 step: 710, loss is 0.0823172926902771\n",
      "epoch: 26 step: 711, loss is 0.22194510698318481\n",
      "epoch: 26 step: 712, loss is 0.1784336268901825\n",
      "epoch: 26 step: 713, loss is 0.2339893877506256\n",
      "epoch: 26 step: 714, loss is 0.06901146471500397\n",
      "epoch: 26 step: 715, loss is 0.2170989066362381\n",
      "epoch: 26 step: 716, loss is 0.2474505454301834\n",
      "epoch: 26 step: 717, loss is 0.10621752589941025\n",
      "epoch: 26 step: 718, loss is 0.12157312035560608\n",
      "epoch: 26 step: 719, loss is 0.08761373907327652\n",
      "epoch: 26 step: 720, loss is 0.13873030245304108\n",
      "epoch: 26 step: 721, loss is 0.14168649911880493\n",
      "epoch: 26 step: 722, loss is 0.10908472537994385\n",
      "epoch: 26 step: 723, loss is 0.17755325138568878\n",
      "epoch: 26 step: 724, loss is 0.17158138751983643\n",
      "epoch: 26 step: 725, loss is 0.08721679449081421\n",
      "epoch: 26 step: 726, loss is 0.16778704524040222\n",
      "epoch: 26 step: 727, loss is 0.17936326563358307\n",
      "epoch: 26 step: 728, loss is 0.327595055103302\n",
      "epoch: 26 step: 729, loss is 0.17467667162418365\n",
      "epoch: 26 step: 730, loss is 0.17189887166023254\n",
      "epoch: 26 step: 731, loss is 0.24536029994487762\n",
      "epoch: 26 step: 732, loss is 0.266836941242218\n",
      "epoch: 26 step: 733, loss is 0.06457715481519699\n",
      "epoch: 26 step: 734, loss is 0.1102706789970398\n",
      "epoch: 26 step: 735, loss is 0.19870442152023315\n",
      "epoch: 26 step: 736, loss is 0.23962409794330597\n",
      "epoch: 26 step: 737, loss is 0.26973384618759155\n",
      "epoch: 26 step: 738, loss is 0.25334280729293823\n",
      "epoch: 26 step: 739, loss is 0.20887985825538635\n",
      "epoch: 26 step: 740, loss is 0.09985555708408356\n",
      "epoch: 26 step: 741, loss is 0.13405542075634003\n",
      "epoch: 26 step: 742, loss is 0.2128274291753769\n",
      "epoch: 26 step: 743, loss is 0.14292502403259277\n",
      "epoch: 26 step: 744, loss is 0.19971056282520294\n",
      "epoch: 26 step: 745, loss is 0.38175132870674133\n",
      "epoch: 26 step: 746, loss is 0.291329026222229\n",
      "epoch: 26 step: 747, loss is 0.16448524594306946\n",
      "epoch: 26 step: 748, loss is 0.1906333565711975\n",
      "epoch: 26 step: 749, loss is 0.16866235435009003\n",
      "epoch: 26 step: 750, loss is 0.1340099424123764\n",
      "epoch: 26 step: 751, loss is 0.10924205183982849\n",
      "epoch: 26 step: 752, loss is 0.1799902766942978\n",
      "epoch: 26 step: 753, loss is 0.2012685388326645\n",
      "epoch: 26 step: 754, loss is 0.19046489894390106\n",
      "epoch: 26 step: 755, loss is 0.08582046627998352\n",
      "epoch: 26 step: 756, loss is 0.2552031874656677\n",
      "epoch: 26 step: 757, loss is 0.15890821814537048\n",
      "epoch: 26 step: 758, loss is 0.18586283922195435\n",
      "epoch: 26 step: 759, loss is 0.14262092113494873\n",
      "epoch: 26 step: 760, loss is 0.1620478332042694\n",
      "epoch: 26 step: 761, loss is 0.10943648219108582\n",
      "epoch: 26 step: 762, loss is 0.10796225070953369\n",
      "epoch: 26 step: 763, loss is 0.23919473588466644\n",
      "epoch: 26 step: 764, loss is 0.11487197130918503\n",
      "epoch: 26 step: 765, loss is 0.22282879054546356\n",
      "epoch: 26 step: 766, loss is 0.17211955785751343\n",
      "epoch: 26 step: 767, loss is 0.21589124202728271\n",
      "epoch: 26 step: 768, loss is 0.10971513390541077\n",
      "epoch: 26 step: 769, loss is 0.26831674575805664\n",
      "epoch: 26 step: 770, loss is 0.18138054013252258\n",
      "epoch: 26 step: 771, loss is 0.14073412120342255\n",
      "epoch: 26 step: 772, loss is 0.12890227138996124\n",
      "epoch: 26 step: 773, loss is 0.1040143221616745\n",
      "epoch: 26 step: 774, loss is 0.4199567139148712\n",
      "epoch: 26 step: 775, loss is 0.12521368265151978\n",
      "epoch: 26 step: 776, loss is 0.11025670915842056\n",
      "epoch: 26 step: 777, loss is 0.16495981812477112\n",
      "epoch: 26 step: 778, loss is 0.1531028002500534\n",
      "epoch: 26 step: 779, loss is 0.2747223377227783\n",
      "epoch: 26 step: 780, loss is 0.25146913528442383\n",
      "epoch: 26 step: 781, loss is 0.14828410744667053\n",
      "epoch: 26 step: 782, loss is 0.08810865879058838\n",
      "epoch: 26 step: 783, loss is 0.15973395109176636\n",
      "epoch: 26 step: 784, loss is 0.3433082699775696\n",
      "epoch: 26 step: 785, loss is 0.20847167074680328\n",
      "epoch: 26 step: 786, loss is 0.20441679656505585\n",
      "epoch: 26 step: 787, loss is 0.2353219985961914\n",
      "epoch: 26 step: 788, loss is 0.3535363972187042\n",
      "epoch: 26 step: 789, loss is 0.2301740199327469\n",
      "epoch: 26 step: 790, loss is 0.25435230135917664\n",
      "epoch: 26 step: 791, loss is 0.1483200639486313\n",
      "epoch: 26 step: 792, loss is 0.12932543456554413\n",
      "epoch: 26 step: 793, loss is 0.32217302918434143\n",
      "epoch: 26 step: 794, loss is 0.13556386530399323\n",
      "epoch: 26 step: 795, loss is 0.18048402667045593\n",
      "epoch: 26 step: 796, loss is 0.1491916924715042\n",
      "epoch: 26 step: 797, loss is 0.17175395786762238\n",
      "epoch: 26 step: 798, loss is 0.09705976396799088\n",
      "epoch: 26 step: 799, loss is 0.17466498911380768\n",
      "epoch: 26 step: 800, loss is 0.2057494968175888\n",
      "epoch: 26 step: 801, loss is 0.35677942633628845\n",
      "epoch: 26 step: 802, loss is 0.2346181571483612\n",
      "epoch: 26 step: 803, loss is 0.17507782578468323\n",
      "epoch: 26 step: 804, loss is 0.3399580717086792\n",
      "epoch: 26 step: 805, loss is 0.26335614919662476\n",
      "epoch: 26 step: 806, loss is 0.16513162851333618\n",
      "epoch: 26 step: 807, loss is 0.25525858998298645\n",
      "epoch: 26 step: 808, loss is 0.19213047623634338\n",
      "epoch: 26 step: 809, loss is 0.24483376741409302\n",
      "epoch: 26 step: 810, loss is 0.19138795137405396\n",
      "epoch: 26 step: 811, loss is 0.2724490463733673\n",
      "epoch: 26 step: 812, loss is 0.13731709122657776\n",
      "epoch: 26 step: 813, loss is 0.17940771579742432\n",
      "epoch: 26 step: 814, loss is 0.4237535297870636\n",
      "epoch: 26 step: 815, loss is 0.1536274403333664\n",
      "epoch: 26 step: 816, loss is 0.2368309050798416\n",
      "epoch: 26 step: 817, loss is 0.11374128609895706\n",
      "epoch: 26 step: 818, loss is 0.21487264335155487\n",
      "epoch: 26 step: 819, loss is 0.20695383846759796\n",
      "epoch: 26 step: 820, loss is 0.21091262996196747\n",
      "epoch: 26 step: 821, loss is 0.2312447428703308\n",
      "epoch: 26 step: 822, loss is 0.18828624486923218\n",
      "epoch: 26 step: 823, loss is 0.15338116884231567\n",
      "epoch: 26 step: 824, loss is 0.1320369839668274\n",
      "epoch: 26 step: 825, loss is 0.312849760055542\n",
      "epoch: 26 step: 826, loss is 0.19888265430927277\n",
      "epoch: 26 step: 827, loss is 0.14757470786571503\n",
      "epoch: 26 step: 828, loss is 0.1687757968902588\n",
      "epoch: 26 step: 829, loss is 0.2096138745546341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 830, loss is 0.33248671889305115\n",
      "epoch: 26 step: 831, loss is 0.1354508101940155\n",
      "epoch: 26 step: 832, loss is 0.17726227641105652\n",
      "epoch: 26 step: 833, loss is 0.24740895628929138\n",
      "epoch: 26 step: 834, loss is 0.22185967862606049\n",
      "epoch: 26 step: 835, loss is 0.0984756201505661\n",
      "epoch: 26 step: 836, loss is 0.2830509543418884\n",
      "epoch: 26 step: 837, loss is 0.1557149738073349\n",
      "epoch: 26 step: 838, loss is 0.25938230752944946\n",
      "epoch: 26 step: 839, loss is 0.16156966984272003\n",
      "epoch: 26 step: 840, loss is 0.2535150945186615\n",
      "epoch: 26 step: 841, loss is 0.2499767392873764\n",
      "epoch: 26 step: 842, loss is 0.37000736594200134\n",
      "epoch: 26 step: 843, loss is 0.13818985223770142\n",
      "epoch: 26 step: 844, loss is 0.1617455780506134\n",
      "epoch: 26 step: 845, loss is 0.23126251995563507\n",
      "epoch: 26 step: 846, loss is 0.2019934356212616\n",
      "epoch: 26 step: 847, loss is 0.35529541969299316\n",
      "epoch: 26 step: 848, loss is 0.11973277479410172\n",
      "epoch: 26 step: 849, loss is 0.10151301324367523\n",
      "epoch: 26 step: 850, loss is 0.11895723640918732\n",
      "epoch: 26 step: 851, loss is 0.07815738022327423\n",
      "epoch: 26 step: 852, loss is 0.16649332642555237\n",
      "epoch: 26 step: 853, loss is 0.1884424388408661\n",
      "epoch: 26 step: 854, loss is 0.17973677814006805\n",
      "epoch: 26 step: 855, loss is 0.1215386614203453\n",
      "epoch: 26 step: 856, loss is 0.16770435869693756\n",
      "epoch: 26 step: 857, loss is 0.29309332370758057\n",
      "epoch: 26 step: 858, loss is 0.13510733842849731\n",
      "epoch: 26 step: 859, loss is 0.37000441551208496\n",
      "epoch: 26 step: 860, loss is 0.1686413735151291\n",
      "epoch: 26 step: 861, loss is 0.150440976023674\n",
      "epoch: 26 step: 862, loss is 0.169363871216774\n",
      "epoch: 26 step: 863, loss is 0.19339190423488617\n",
      "epoch: 26 step: 864, loss is 0.28737661242485046\n",
      "epoch: 26 step: 865, loss is 0.5397585034370422\n",
      "epoch: 26 step: 866, loss is 0.15735124051570892\n",
      "epoch: 26 step: 867, loss is 0.31835058331489563\n",
      "epoch: 26 step: 868, loss is 0.3398001194000244\n",
      "epoch: 26 step: 869, loss is 0.18003015220165253\n",
      "epoch: 26 step: 870, loss is 0.20662961900234222\n",
      "epoch: 26 step: 871, loss is 0.19638964533805847\n",
      "epoch: 26 step: 872, loss is 0.2493709772825241\n",
      "epoch: 26 step: 873, loss is 0.1284860223531723\n",
      "epoch: 26 step: 874, loss is 0.3915942311286926\n",
      "epoch: 26 step: 875, loss is 0.09327728301286697\n",
      "epoch: 26 step: 876, loss is 0.31211405992507935\n",
      "epoch: 26 step: 877, loss is 0.16392622888088226\n",
      "epoch: 26 step: 878, loss is 0.1795806586742401\n",
      "epoch: 26 step: 879, loss is 0.08536409586668015\n",
      "epoch: 26 step: 880, loss is 0.18697494268417358\n",
      "epoch: 26 step: 881, loss is 0.06447729468345642\n",
      "epoch: 26 step: 882, loss is 0.23816250264644623\n",
      "epoch: 26 step: 883, loss is 0.2229832261800766\n",
      "epoch: 26 step: 884, loss is 0.19639134407043457\n",
      "epoch: 26 step: 885, loss is 0.08575963228940964\n",
      "epoch: 26 step: 886, loss is 0.13934655487537384\n",
      "epoch: 26 step: 887, loss is 0.149615079164505\n",
      "epoch: 26 step: 888, loss is 0.2134849727153778\n",
      "epoch: 26 step: 889, loss is 0.15163575112819672\n",
      "epoch: 26 step: 890, loss is 0.15814945101737976\n",
      "epoch: 26 step: 891, loss is 0.12968729436397552\n",
      "epoch: 26 step: 892, loss is 0.16697564721107483\n",
      "epoch: 26 step: 893, loss is 0.1258264034986496\n",
      "epoch: 26 step: 894, loss is 0.17157314717769623\n",
      "epoch: 26 step: 895, loss is 0.15405918657779694\n",
      "epoch: 26 step: 896, loss is 0.2297111302614212\n",
      "epoch: 26 step: 897, loss is 0.16930201649665833\n",
      "epoch: 26 step: 898, loss is 0.14833658933639526\n",
      "epoch: 26 step: 899, loss is 0.04401275888085365\n",
      "epoch: 26 step: 900, loss is 0.15326039493083954\n",
      "epoch: 26 step: 901, loss is 0.16139447689056396\n",
      "epoch: 26 step: 902, loss is 0.1640557050704956\n",
      "epoch: 26 step: 903, loss is 0.2897063195705414\n",
      "epoch: 26 step: 904, loss is 0.43093448877334595\n",
      "epoch: 26 step: 905, loss is 0.19857773184776306\n",
      "epoch: 26 step: 906, loss is 0.30040958523750305\n",
      "epoch: 26 step: 907, loss is 0.3184639513492584\n",
      "epoch: 26 step: 908, loss is 0.1170991063117981\n",
      "epoch: 26 step: 909, loss is 0.1375480741262436\n",
      "epoch: 26 step: 910, loss is 0.21949607133865356\n",
      "epoch: 26 step: 911, loss is 0.2360912263393402\n",
      "epoch: 26 step: 912, loss is 0.15709415078163147\n",
      "epoch: 26 step: 913, loss is 0.09454015642404556\n",
      "epoch: 26 step: 914, loss is 0.2709836959838867\n",
      "epoch: 26 step: 915, loss is 0.1795225739479065\n",
      "epoch: 26 step: 916, loss is 0.30952078104019165\n",
      "epoch: 26 step: 917, loss is 0.1657262146472931\n",
      "epoch: 26 step: 918, loss is 0.08879715204238892\n",
      "epoch: 26 step: 919, loss is 0.18308648467063904\n",
      "epoch: 26 step: 920, loss is 0.10844571143388748\n",
      "epoch: 26 step: 921, loss is 0.3629551827907562\n",
      "epoch: 26 step: 922, loss is 0.10195574909448624\n",
      "epoch: 26 step: 923, loss is 0.20378980040550232\n",
      "epoch: 26 step: 924, loss is 0.11761843413114548\n",
      "epoch: 26 step: 925, loss is 0.07098712772130966\n",
      "epoch: 26 step: 926, loss is 0.18495431542396545\n",
      "epoch: 26 step: 927, loss is 0.16536304354667664\n",
      "epoch: 26 step: 928, loss is 0.2904427647590637\n",
      "epoch: 26 step: 929, loss is 0.1955385059118271\n",
      "epoch: 26 step: 930, loss is 0.12274400889873505\n",
      "epoch: 26 step: 931, loss is 0.2394075244665146\n",
      "epoch: 26 step: 932, loss is 0.14666084945201874\n",
      "epoch: 26 step: 933, loss is 0.13510046899318695\n",
      "epoch: 26 step: 934, loss is 0.19237691164016724\n",
      "epoch: 26 step: 935, loss is 0.16837681829929352\n",
      "epoch: 26 step: 936, loss is 0.160897359251976\n",
      "epoch: 26 step: 937, loss is 0.28573077917099\n",
      "epoch: 27 step: 1, loss is 0.18950042128562927\n",
      "epoch: 27 step: 2, loss is 0.2702884376049042\n",
      "epoch: 27 step: 3, loss is 0.09736324101686478\n",
      "epoch: 27 step: 4, loss is 0.03974466398358345\n",
      "epoch: 27 step: 5, loss is 0.0938628762960434\n",
      "epoch: 27 step: 6, loss is 0.3090246915817261\n",
      "epoch: 27 step: 7, loss is 0.27836233377456665\n",
      "epoch: 27 step: 8, loss is 0.15402968227863312\n",
      "epoch: 27 step: 9, loss is 0.11352230608463287\n",
      "epoch: 27 step: 10, loss is 0.2185971736907959\n",
      "epoch: 27 step: 11, loss is 0.07333103567361832\n",
      "epoch: 27 step: 12, loss is 0.19283241033554077\n",
      "epoch: 27 step: 13, loss is 0.1362558752298355\n",
      "epoch: 27 step: 14, loss is 0.21294686198234558\n",
      "epoch: 27 step: 15, loss is 0.07602196931838989\n",
      "epoch: 27 step: 16, loss is 0.2066524624824524\n",
      "epoch: 27 step: 17, loss is 0.20333220064640045\n",
      "epoch: 27 step: 18, loss is 0.14560452103614807\n",
      "epoch: 27 step: 19, loss is 0.2485031634569168\n",
      "epoch: 27 step: 20, loss is 0.18991301953792572\n",
      "epoch: 27 step: 21, loss is 0.20177848637104034\n",
      "epoch: 27 step: 22, loss is 0.15244977176189423\n",
      "epoch: 27 step: 23, loss is 0.27605757117271423\n",
      "epoch: 27 step: 24, loss is 0.30856695771217346\n",
      "epoch: 27 step: 25, loss is 0.2759922444820404\n",
      "epoch: 27 step: 26, loss is 0.34865909814834595\n",
      "epoch: 27 step: 27, loss is 0.38469362258911133\n",
      "epoch: 27 step: 28, loss is 0.15133610367774963\n",
      "epoch: 27 step: 29, loss is 0.1476321816444397\n",
      "epoch: 27 step: 30, loss is 0.13463690876960754\n",
      "epoch: 27 step: 31, loss is 0.1477302759885788\n",
      "epoch: 27 step: 32, loss is 0.10039772093296051\n",
      "epoch: 27 step: 33, loss is 0.1053134873509407\n",
      "epoch: 27 step: 34, loss is 0.12061221897602081\n",
      "epoch: 27 step: 35, loss is 0.306312620639801\n",
      "epoch: 27 step: 36, loss is 0.2177080363035202\n",
      "epoch: 27 step: 37, loss is 0.13534684479236603\n",
      "epoch: 27 step: 38, loss is 0.08404158800840378\n",
      "epoch: 27 step: 39, loss is 0.08687888830900192\n",
      "epoch: 27 step: 40, loss is 0.13512004911899567\n",
      "epoch: 27 step: 41, loss is 0.09578701853752136\n",
      "epoch: 27 step: 42, loss is 0.1657010167837143\n",
      "epoch: 27 step: 43, loss is 0.19647276401519775\n",
      "epoch: 27 step: 44, loss is 0.12972059845924377\n",
      "epoch: 27 step: 45, loss is 0.20064805448055267\n",
      "epoch: 27 step: 46, loss is 0.15910860896110535\n",
      "epoch: 27 step: 47, loss is 0.15226660668849945\n",
      "epoch: 27 step: 48, loss is 0.1206139624118805\n",
      "epoch: 27 step: 49, loss is 0.1379821002483368\n",
      "epoch: 27 step: 50, loss is 0.18763715028762817\n",
      "epoch: 27 step: 51, loss is 0.1948532909154892\n",
      "epoch: 27 step: 52, loss is 0.1902361810207367\n",
      "epoch: 27 step: 53, loss is 0.17395992577075958\n",
      "epoch: 27 step: 54, loss is 0.250840425491333\n",
      "epoch: 27 step: 55, loss is 0.20659425854682922\n",
      "epoch: 27 step: 56, loss is 0.11938892304897308\n",
      "epoch: 27 step: 57, loss is 0.17230691015720367\n",
      "epoch: 27 step: 58, loss is 0.20426495373249054\n",
      "epoch: 27 step: 59, loss is 0.1742599904537201\n",
      "epoch: 27 step: 60, loss is 0.13689179718494415\n",
      "epoch: 27 step: 61, loss is 0.16366565227508545\n",
      "epoch: 27 step: 62, loss is 0.30114051699638367\n",
      "epoch: 27 step: 63, loss is 0.062112826853990555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 64, loss is 0.09025715291500092\n",
      "epoch: 27 step: 65, loss is 0.23450878262519836\n",
      "epoch: 27 step: 66, loss is 0.09151476621627808\n",
      "epoch: 27 step: 67, loss is 0.1796572357416153\n",
      "epoch: 27 step: 68, loss is 0.36610013246536255\n",
      "epoch: 27 step: 69, loss is 0.11305199563503265\n",
      "epoch: 27 step: 70, loss is 0.2345924824476242\n",
      "epoch: 27 step: 71, loss is 0.11291895806789398\n",
      "epoch: 27 step: 72, loss is 0.23169195652008057\n",
      "epoch: 27 step: 73, loss is 0.3940156102180481\n",
      "epoch: 27 step: 74, loss is 0.30375832319259644\n",
      "epoch: 27 step: 75, loss is 0.21480704843997955\n",
      "epoch: 27 step: 76, loss is 0.18090879917144775\n",
      "epoch: 27 step: 77, loss is 0.12202414125204086\n",
      "epoch: 27 step: 78, loss is 0.11415062844753265\n",
      "epoch: 27 step: 79, loss is 0.27741217613220215\n",
      "epoch: 27 step: 80, loss is 0.18794982135295868\n",
      "epoch: 27 step: 81, loss is 0.12543892860412598\n",
      "epoch: 27 step: 82, loss is 0.16688337922096252\n",
      "epoch: 27 step: 83, loss is 0.11507517844438553\n",
      "epoch: 27 step: 84, loss is 0.34589070081710815\n",
      "epoch: 27 step: 85, loss is 0.16926129162311554\n",
      "epoch: 27 step: 86, loss is 0.21988093852996826\n",
      "epoch: 27 step: 87, loss is 0.16513080894947052\n",
      "epoch: 27 step: 88, loss is 0.1830577850341797\n",
      "epoch: 27 step: 89, loss is 0.22633162140846252\n",
      "epoch: 27 step: 90, loss is 0.10127580165863037\n",
      "epoch: 27 step: 91, loss is 0.09993305057287216\n",
      "epoch: 27 step: 92, loss is 0.2041250318288803\n",
      "epoch: 27 step: 93, loss is 0.32915401458740234\n",
      "epoch: 27 step: 94, loss is 0.13413268327713013\n",
      "epoch: 27 step: 95, loss is 0.2684950530529022\n",
      "epoch: 27 step: 96, loss is 0.07803209125995636\n",
      "epoch: 27 step: 97, loss is 0.18245400488376617\n",
      "epoch: 27 step: 98, loss is 0.3529300391674042\n",
      "epoch: 27 step: 99, loss is 0.13845980167388916\n",
      "epoch: 27 step: 100, loss is 0.2525542378425598\n",
      "epoch: 27 step: 101, loss is 0.1816711276769638\n",
      "epoch: 27 step: 102, loss is 0.21486611664295197\n",
      "epoch: 27 step: 103, loss is 0.17108768224716187\n",
      "epoch: 27 step: 104, loss is 0.2099331170320511\n",
      "epoch: 27 step: 105, loss is 0.05719559267163277\n",
      "epoch: 27 step: 106, loss is 0.09641265124082565\n",
      "epoch: 27 step: 107, loss is 0.08339483290910721\n",
      "epoch: 27 step: 108, loss is 0.06889740377664566\n",
      "epoch: 27 step: 109, loss is 0.19689567387104034\n",
      "epoch: 27 step: 110, loss is 0.22684116661548615\n",
      "epoch: 27 step: 111, loss is 0.2271318882703781\n",
      "epoch: 27 step: 112, loss is 0.3510225713253021\n",
      "epoch: 27 step: 113, loss is 0.13256776332855225\n",
      "epoch: 27 step: 114, loss is 0.15089350938796997\n",
      "epoch: 27 step: 115, loss is 0.15509700775146484\n",
      "epoch: 27 step: 116, loss is 0.16758224368095398\n",
      "epoch: 27 step: 117, loss is 0.30446821451187134\n",
      "epoch: 27 step: 118, loss is 0.14737069606781006\n",
      "epoch: 27 step: 119, loss is 0.2554187774658203\n",
      "epoch: 27 step: 120, loss is 0.0525970533490181\n",
      "epoch: 27 step: 121, loss is 0.0800083577632904\n",
      "epoch: 27 step: 122, loss is 0.2633925974369049\n",
      "epoch: 27 step: 123, loss is 0.39041921496391296\n",
      "epoch: 27 step: 124, loss is 0.13948747515678406\n",
      "epoch: 27 step: 125, loss is 0.11807514727115631\n",
      "epoch: 27 step: 126, loss is 0.16719798743724823\n",
      "epoch: 27 step: 127, loss is 0.16969478130340576\n",
      "epoch: 27 step: 128, loss is 0.23289616405963898\n",
      "epoch: 27 step: 129, loss is 0.14770880341529846\n",
      "epoch: 27 step: 130, loss is 0.27514976263046265\n",
      "epoch: 27 step: 131, loss is 0.20136874914169312\n",
      "epoch: 27 step: 132, loss is 0.1994805634021759\n",
      "epoch: 27 step: 133, loss is 0.1185847818851471\n",
      "epoch: 27 step: 134, loss is 0.3392395079135895\n",
      "epoch: 27 step: 135, loss is 0.16147610545158386\n",
      "epoch: 27 step: 136, loss is 0.15870949625968933\n",
      "epoch: 27 step: 137, loss is 0.1470649689435959\n",
      "epoch: 27 step: 138, loss is 0.17837272584438324\n",
      "epoch: 27 step: 139, loss is 0.14127328991889954\n",
      "epoch: 27 step: 140, loss is 0.06287065893411636\n",
      "epoch: 27 step: 141, loss is 0.2059856653213501\n",
      "epoch: 27 step: 142, loss is 0.12074000388383865\n",
      "epoch: 27 step: 143, loss is 0.18443094193935394\n",
      "epoch: 27 step: 144, loss is 0.18118998408317566\n",
      "epoch: 27 step: 145, loss is 0.08847589790821075\n",
      "epoch: 27 step: 146, loss is 0.10019249469041824\n",
      "epoch: 27 step: 147, loss is 0.14397376775741577\n",
      "epoch: 27 step: 148, loss is 0.12514187395572662\n",
      "epoch: 27 step: 149, loss is 0.14262905716896057\n",
      "epoch: 27 step: 150, loss is 0.18735213577747345\n",
      "epoch: 27 step: 151, loss is 0.08319175243377686\n",
      "epoch: 27 step: 152, loss is 0.10829365253448486\n",
      "epoch: 27 step: 153, loss is 0.23533371090888977\n",
      "epoch: 27 step: 154, loss is 0.13310891389846802\n",
      "epoch: 27 step: 155, loss is 0.25100985169410706\n",
      "epoch: 27 step: 156, loss is 0.18204303085803986\n",
      "epoch: 27 step: 157, loss is 0.39247700572013855\n",
      "epoch: 27 step: 158, loss is 0.31162896752357483\n",
      "epoch: 27 step: 159, loss is 0.10305623710155487\n",
      "epoch: 27 step: 160, loss is 0.22253434360027313\n",
      "epoch: 27 step: 161, loss is 0.04959256574511528\n",
      "epoch: 27 step: 162, loss is 0.22489048540592194\n",
      "epoch: 27 step: 163, loss is 0.1656244546175003\n",
      "epoch: 27 step: 164, loss is 0.22085769474506378\n",
      "epoch: 27 step: 165, loss is 0.25492289662361145\n",
      "epoch: 27 step: 166, loss is 0.05563114210963249\n",
      "epoch: 27 step: 167, loss is 0.15452614426612854\n",
      "epoch: 27 step: 168, loss is 0.1566825956106186\n",
      "epoch: 27 step: 169, loss is 0.10181023925542831\n",
      "epoch: 27 step: 170, loss is 0.2906172275543213\n",
      "epoch: 27 step: 171, loss is 0.4658017158508301\n",
      "epoch: 27 step: 172, loss is 0.40238478779792786\n",
      "epoch: 27 step: 173, loss is 0.2460755705833435\n",
      "epoch: 27 step: 174, loss is 0.15932050347328186\n",
      "epoch: 27 step: 175, loss is 0.14551414549350739\n",
      "epoch: 27 step: 176, loss is 0.1159561276435852\n",
      "epoch: 27 step: 177, loss is 0.12080386281013489\n",
      "epoch: 27 step: 178, loss is 0.2234046459197998\n",
      "epoch: 27 step: 179, loss is 0.2352130115032196\n",
      "epoch: 27 step: 180, loss is 0.1556101143360138\n",
      "epoch: 27 step: 181, loss is 0.1500951498746872\n",
      "epoch: 27 step: 182, loss is 0.1863667368888855\n",
      "epoch: 27 step: 183, loss is 0.16689953207969666\n",
      "epoch: 27 step: 184, loss is 0.14721636474132538\n",
      "epoch: 27 step: 185, loss is 0.18632672727108002\n",
      "epoch: 27 step: 186, loss is 0.06808796525001526\n",
      "epoch: 27 step: 187, loss is 0.1192115843296051\n",
      "epoch: 27 step: 188, loss is 0.18986867368221283\n",
      "epoch: 27 step: 189, loss is 0.20094043016433716\n",
      "epoch: 27 step: 190, loss is 0.19637596607208252\n",
      "epoch: 27 step: 191, loss is 0.20309388637542725\n",
      "epoch: 27 step: 192, loss is 0.16707836091518402\n",
      "epoch: 27 step: 193, loss is 0.16783316433429718\n",
      "epoch: 27 step: 194, loss is 0.1488557606935501\n",
      "epoch: 27 step: 195, loss is 0.2735885977745056\n",
      "epoch: 27 step: 196, loss is 0.16841048002243042\n",
      "epoch: 27 step: 197, loss is 0.11257711797952652\n",
      "epoch: 27 step: 198, loss is 0.09128585457801819\n",
      "epoch: 27 step: 199, loss is 0.23540058732032776\n",
      "epoch: 27 step: 200, loss is 0.1172538548707962\n",
      "epoch: 27 step: 201, loss is 0.14371518790721893\n",
      "epoch: 27 step: 202, loss is 0.11994720995426178\n",
      "epoch: 27 step: 203, loss is 0.1757962554693222\n",
      "epoch: 27 step: 204, loss is 0.2219049334526062\n",
      "epoch: 27 step: 205, loss is 0.1514580249786377\n",
      "epoch: 27 step: 206, loss is 0.31839093565940857\n",
      "epoch: 27 step: 207, loss is 0.2100357562303543\n",
      "epoch: 27 step: 208, loss is 0.14167523384094238\n",
      "epoch: 27 step: 209, loss is 0.15600571036338806\n",
      "epoch: 27 step: 210, loss is 0.19736313819885254\n",
      "epoch: 27 step: 211, loss is 0.18273389339447021\n",
      "epoch: 27 step: 212, loss is 0.10995444655418396\n",
      "epoch: 27 step: 213, loss is 0.15514665842056274\n",
      "epoch: 27 step: 214, loss is 0.23254628479480743\n",
      "epoch: 27 step: 215, loss is 0.22041894495487213\n",
      "epoch: 27 step: 216, loss is 0.10073675960302353\n",
      "epoch: 27 step: 217, loss is 0.14771752059459686\n",
      "epoch: 27 step: 218, loss is 0.31006330251693726\n",
      "epoch: 27 step: 219, loss is 0.08567441999912262\n",
      "epoch: 27 step: 220, loss is 0.11692792177200317\n",
      "epoch: 27 step: 221, loss is 0.08106063306331635\n",
      "epoch: 27 step: 222, loss is 0.21479757130146027\n",
      "epoch: 27 step: 223, loss is 0.29645779728889465\n",
      "epoch: 27 step: 224, loss is 0.25918808579444885\n",
      "epoch: 27 step: 225, loss is 0.2015039473772049\n",
      "epoch: 27 step: 226, loss is 0.25455328822135925\n",
      "epoch: 27 step: 227, loss is 0.11641351878643036\n",
      "epoch: 27 step: 228, loss is 0.20150257647037506\n",
      "epoch: 27 step: 229, loss is 0.12542961537837982\n",
      "epoch: 27 step: 230, loss is 0.14817672967910767\n",
      "epoch: 27 step: 231, loss is 0.26556840538978577\n",
      "epoch: 27 step: 232, loss is 0.1363341063261032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 233, loss is 0.22107252478599548\n",
      "epoch: 27 step: 234, loss is 0.20380741357803345\n",
      "epoch: 27 step: 235, loss is 0.2244311273097992\n",
      "epoch: 27 step: 236, loss is 0.07562510669231415\n",
      "epoch: 27 step: 237, loss is 0.18939810991287231\n",
      "epoch: 27 step: 238, loss is 0.19326676428318024\n",
      "epoch: 27 step: 239, loss is 0.2912245988845825\n",
      "epoch: 27 step: 240, loss is 0.19503095746040344\n",
      "epoch: 27 step: 241, loss is 0.19068515300750732\n",
      "epoch: 27 step: 242, loss is 0.23599804937839508\n",
      "epoch: 27 step: 243, loss is 0.25105607509613037\n",
      "epoch: 27 step: 244, loss is 0.1505831778049469\n",
      "epoch: 27 step: 245, loss is 0.33866459131240845\n",
      "epoch: 27 step: 246, loss is 0.08314745128154755\n",
      "epoch: 27 step: 247, loss is 0.15475033223628998\n",
      "epoch: 27 step: 248, loss is 0.18628467619419098\n",
      "epoch: 27 step: 249, loss is 0.16030126810073853\n",
      "epoch: 27 step: 250, loss is 0.16034629940986633\n",
      "epoch: 27 step: 251, loss is 0.23422543704509735\n",
      "epoch: 27 step: 252, loss is 0.2467866688966751\n",
      "epoch: 27 step: 253, loss is 0.04767666757106781\n",
      "epoch: 27 step: 254, loss is 0.13787344098091125\n",
      "epoch: 27 step: 255, loss is 0.06263583153486252\n",
      "epoch: 27 step: 256, loss is 0.14280161261558533\n",
      "epoch: 27 step: 257, loss is 0.2779296934604645\n",
      "epoch: 27 step: 258, loss is 0.09950703382492065\n",
      "epoch: 27 step: 259, loss is 0.2722601294517517\n",
      "epoch: 27 step: 260, loss is 0.17784468829631805\n",
      "epoch: 27 step: 261, loss is 0.16527259349822998\n",
      "epoch: 27 step: 262, loss is 0.15497560799121857\n",
      "epoch: 27 step: 263, loss is 0.2456367462873459\n",
      "epoch: 27 step: 264, loss is 0.1941281408071518\n",
      "epoch: 27 step: 265, loss is 0.20643438398838043\n",
      "epoch: 27 step: 266, loss is 0.3228379786014557\n",
      "epoch: 27 step: 267, loss is 0.2802048325538635\n",
      "epoch: 27 step: 268, loss is 0.3345888555049896\n",
      "epoch: 27 step: 269, loss is 0.18773241341114044\n",
      "epoch: 27 step: 270, loss is 0.30087757110595703\n",
      "epoch: 27 step: 271, loss is 0.36162081360816956\n",
      "epoch: 27 step: 272, loss is 0.13714587688446045\n",
      "epoch: 27 step: 273, loss is 0.23435203731060028\n",
      "epoch: 27 step: 274, loss is 0.28028497099876404\n",
      "epoch: 27 step: 275, loss is 0.22070316970348358\n",
      "epoch: 27 step: 276, loss is 0.21304543316364288\n",
      "epoch: 27 step: 277, loss is 0.2480926811695099\n",
      "epoch: 27 step: 278, loss is 0.20422913134098053\n",
      "epoch: 27 step: 279, loss is 0.11291646212339401\n",
      "epoch: 27 step: 280, loss is 0.1946641504764557\n",
      "epoch: 27 step: 281, loss is 0.22231663763523102\n",
      "epoch: 27 step: 282, loss is 0.10508403927087784\n",
      "epoch: 27 step: 283, loss is 0.1200709268450737\n",
      "epoch: 27 step: 284, loss is 0.09068910032510757\n",
      "epoch: 27 step: 285, loss is 0.20583008229732513\n",
      "epoch: 27 step: 286, loss is 0.21388699114322662\n",
      "epoch: 27 step: 287, loss is 0.24004094302654266\n",
      "epoch: 27 step: 288, loss is 0.12217505276203156\n",
      "epoch: 27 step: 289, loss is 0.1844981610774994\n",
      "epoch: 27 step: 290, loss is 0.1527581512928009\n",
      "epoch: 27 step: 291, loss is 0.1780865639448166\n",
      "epoch: 27 step: 292, loss is 0.12016472965478897\n",
      "epoch: 27 step: 293, loss is 0.12166275829076767\n",
      "epoch: 27 step: 294, loss is 0.05702156201004982\n",
      "epoch: 27 step: 295, loss is 0.08056721091270447\n",
      "epoch: 27 step: 296, loss is 0.24505680799484253\n",
      "epoch: 27 step: 297, loss is 0.17567531764507294\n",
      "epoch: 27 step: 298, loss is 0.20668061077594757\n",
      "epoch: 27 step: 299, loss is 0.11644221842288971\n",
      "epoch: 27 step: 300, loss is 0.27467069029808044\n",
      "epoch: 27 step: 301, loss is 0.19155842065811157\n",
      "epoch: 27 step: 302, loss is 0.19731014966964722\n",
      "epoch: 27 step: 303, loss is 0.22019289433956146\n",
      "epoch: 27 step: 304, loss is 0.12778308987617493\n",
      "epoch: 27 step: 305, loss is 0.14055253565311432\n",
      "epoch: 27 step: 306, loss is 0.2229681760072708\n",
      "epoch: 27 step: 307, loss is 0.3104371726512909\n",
      "epoch: 27 step: 308, loss is 0.27787530422210693\n",
      "epoch: 27 step: 309, loss is 0.2821260392665863\n",
      "epoch: 27 step: 310, loss is 0.19544817507266998\n",
      "epoch: 27 step: 311, loss is 0.2542397975921631\n",
      "epoch: 27 step: 312, loss is 0.2879215478897095\n",
      "epoch: 27 step: 313, loss is 0.11290094256401062\n",
      "epoch: 27 step: 314, loss is 0.1320476531982422\n",
      "epoch: 27 step: 315, loss is 0.3110782504081726\n",
      "epoch: 27 step: 316, loss is 0.06296021491289139\n",
      "epoch: 27 step: 317, loss is 0.145940899848938\n",
      "epoch: 27 step: 318, loss is 0.25883811712265015\n",
      "epoch: 27 step: 319, loss is 0.14434850215911865\n",
      "epoch: 27 step: 320, loss is 0.11990285664796829\n",
      "epoch: 27 step: 321, loss is 0.15511377155780792\n",
      "epoch: 27 step: 322, loss is 0.45887038111686707\n",
      "epoch: 27 step: 323, loss is 0.30356472730636597\n",
      "epoch: 27 step: 324, loss is 0.40602412819862366\n",
      "epoch: 27 step: 325, loss is 0.22552116215229034\n",
      "epoch: 27 step: 326, loss is 0.15827979147434235\n",
      "epoch: 27 step: 327, loss is 0.12623292207717896\n",
      "epoch: 27 step: 328, loss is 0.17387521266937256\n",
      "epoch: 27 step: 329, loss is 0.18372119963169098\n",
      "epoch: 27 step: 330, loss is 0.11801354587078094\n",
      "epoch: 27 step: 331, loss is 0.2796576917171478\n",
      "epoch: 27 step: 332, loss is 0.17598378658294678\n",
      "epoch: 27 step: 333, loss is 0.12761344015598297\n",
      "epoch: 27 step: 334, loss is 0.18459650874137878\n",
      "epoch: 27 step: 335, loss is 0.19653281569480896\n",
      "epoch: 27 step: 336, loss is 0.12049788981676102\n",
      "epoch: 27 step: 337, loss is 0.11946150660514832\n",
      "epoch: 27 step: 338, loss is 0.24433064460754395\n",
      "epoch: 27 step: 339, loss is 0.28134435415267944\n",
      "epoch: 27 step: 340, loss is 0.08606782555580139\n",
      "epoch: 27 step: 341, loss is 0.1201404258608818\n",
      "epoch: 27 step: 342, loss is 0.13653673231601715\n",
      "epoch: 27 step: 343, loss is 0.0914502814412117\n",
      "epoch: 27 step: 344, loss is 0.25175634026527405\n",
      "epoch: 27 step: 345, loss is 0.3577370345592499\n",
      "epoch: 27 step: 346, loss is 0.23066434264183044\n",
      "epoch: 27 step: 347, loss is 0.2132837027311325\n",
      "epoch: 27 step: 348, loss is 0.13743014633655548\n",
      "epoch: 27 step: 349, loss is 0.2840994894504547\n",
      "epoch: 27 step: 350, loss is 0.13876378536224365\n",
      "epoch: 27 step: 351, loss is 0.11404937505722046\n",
      "epoch: 27 step: 352, loss is 0.13035793602466583\n",
      "epoch: 27 step: 353, loss is 0.15249955654144287\n",
      "epoch: 27 step: 354, loss is 0.1539187878370285\n",
      "epoch: 27 step: 355, loss is 0.20075646042823792\n",
      "epoch: 27 step: 356, loss is 0.23964671790599823\n",
      "epoch: 27 step: 357, loss is 0.21094687283039093\n",
      "epoch: 27 step: 358, loss is 0.12748387455940247\n",
      "epoch: 27 step: 359, loss is 0.1194937601685524\n",
      "epoch: 27 step: 360, loss is 0.20085734128952026\n",
      "epoch: 27 step: 361, loss is 0.07534772902727127\n",
      "epoch: 27 step: 362, loss is 0.10407121479511261\n",
      "epoch: 27 step: 363, loss is 0.14296770095825195\n",
      "epoch: 27 step: 364, loss is 0.15114657580852509\n",
      "epoch: 27 step: 365, loss is 0.08171520382165909\n",
      "epoch: 27 step: 366, loss is 0.09677142649888992\n",
      "epoch: 27 step: 367, loss is 0.20100580155849457\n",
      "epoch: 27 step: 368, loss is 0.1623685359954834\n",
      "epoch: 27 step: 369, loss is 0.17506517469882965\n",
      "epoch: 27 step: 370, loss is 0.2602224349975586\n",
      "epoch: 27 step: 371, loss is 0.09360232949256897\n",
      "epoch: 27 step: 372, loss is 0.19244635105133057\n",
      "epoch: 27 step: 373, loss is 0.2373022884130478\n",
      "epoch: 27 step: 374, loss is 0.27976706624031067\n",
      "epoch: 27 step: 375, loss is 0.3477887511253357\n",
      "epoch: 27 step: 376, loss is 0.11121036857366562\n",
      "epoch: 27 step: 377, loss is 0.18729914724826813\n",
      "epoch: 27 step: 378, loss is 0.23336775600910187\n",
      "epoch: 27 step: 379, loss is 0.31468307971954346\n",
      "epoch: 27 step: 380, loss is 0.18547755479812622\n",
      "epoch: 27 step: 381, loss is 0.1240839883685112\n",
      "epoch: 27 step: 382, loss is 0.19569392502307892\n",
      "epoch: 27 step: 383, loss is 0.1567741483449936\n",
      "epoch: 27 step: 384, loss is 0.1843332201242447\n",
      "epoch: 27 step: 385, loss is 0.12069478631019592\n",
      "epoch: 27 step: 386, loss is 0.29353228211402893\n",
      "epoch: 27 step: 387, loss is 0.12780481576919556\n",
      "epoch: 27 step: 388, loss is 0.2446727305650711\n",
      "epoch: 27 step: 389, loss is 0.10719887167215347\n",
      "epoch: 27 step: 390, loss is 0.25376027822494507\n",
      "epoch: 27 step: 391, loss is 0.16962893307209015\n",
      "epoch: 27 step: 392, loss is 0.22253580391407013\n",
      "epoch: 27 step: 393, loss is 0.31931960582733154\n",
      "epoch: 27 step: 394, loss is 0.13556808233261108\n",
      "epoch: 27 step: 395, loss is 0.10676080733537674\n",
      "epoch: 27 step: 396, loss is 0.18562951683998108\n",
      "epoch: 27 step: 397, loss is 0.20022356510162354\n",
      "epoch: 27 step: 398, loss is 0.2110344022512436\n",
      "epoch: 27 step: 399, loss is 0.13424496352672577\n",
      "epoch: 27 step: 400, loss is 0.17933276295661926\n",
      "epoch: 27 step: 401, loss is 0.19710248708724976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 402, loss is 0.10795161873102188\n",
      "epoch: 27 step: 403, loss is 0.11472199112176895\n",
      "epoch: 27 step: 404, loss is 0.1453007310628891\n",
      "epoch: 27 step: 405, loss is 0.1899668127298355\n",
      "epoch: 27 step: 406, loss is 0.13820220530033112\n",
      "epoch: 27 step: 407, loss is 0.24265286326408386\n",
      "epoch: 27 step: 408, loss is 0.36530736088752747\n",
      "epoch: 27 step: 409, loss is 0.16375361382961273\n",
      "epoch: 27 step: 410, loss is 0.24159230291843414\n",
      "epoch: 27 step: 411, loss is 0.23807230591773987\n",
      "epoch: 27 step: 412, loss is 0.18934543430805206\n",
      "epoch: 27 step: 413, loss is 0.2149229347705841\n",
      "epoch: 27 step: 414, loss is 0.19925031065940857\n",
      "epoch: 27 step: 415, loss is 0.240997776389122\n",
      "epoch: 27 step: 416, loss is 0.1394856721162796\n",
      "epoch: 27 step: 417, loss is 0.22337794303894043\n",
      "epoch: 27 step: 418, loss is 0.17345987260341644\n",
      "epoch: 27 step: 419, loss is 0.23708267509937286\n",
      "epoch: 27 step: 420, loss is 0.17193211615085602\n",
      "epoch: 27 step: 421, loss is 0.13034945726394653\n",
      "epoch: 27 step: 422, loss is 0.2531999945640564\n",
      "epoch: 27 step: 423, loss is 0.11725243180990219\n",
      "epoch: 27 step: 424, loss is 0.11458448320627213\n",
      "epoch: 27 step: 425, loss is 0.28481414914131165\n",
      "epoch: 27 step: 426, loss is 0.22485172748565674\n",
      "epoch: 27 step: 427, loss is 0.19511762261390686\n",
      "epoch: 27 step: 428, loss is 0.32557469606399536\n",
      "epoch: 27 step: 429, loss is 0.2582440972328186\n",
      "epoch: 27 step: 430, loss is 0.06302569061517715\n",
      "epoch: 27 step: 431, loss is 0.1273445188999176\n",
      "epoch: 27 step: 432, loss is 0.2031489908695221\n",
      "epoch: 27 step: 433, loss is 0.3174116611480713\n",
      "epoch: 27 step: 434, loss is 0.2591119408607483\n",
      "epoch: 27 step: 435, loss is 0.12158429622650146\n",
      "epoch: 27 step: 436, loss is 0.09766793251037598\n",
      "epoch: 27 step: 437, loss is 0.23239067196846008\n",
      "epoch: 27 step: 438, loss is 0.24943894147872925\n",
      "epoch: 27 step: 439, loss is 0.17380660772323608\n",
      "epoch: 27 step: 440, loss is 0.12832139432430267\n",
      "epoch: 27 step: 441, loss is 0.15029355883598328\n",
      "epoch: 27 step: 442, loss is 0.37710386514663696\n",
      "epoch: 27 step: 443, loss is 0.2224227339029312\n",
      "epoch: 27 step: 444, loss is 0.11933135986328125\n",
      "epoch: 27 step: 445, loss is 0.2521849572658539\n",
      "epoch: 27 step: 446, loss is 0.12554270029067993\n",
      "epoch: 27 step: 447, loss is 0.20931774377822876\n",
      "epoch: 27 step: 448, loss is 0.15168192982673645\n",
      "epoch: 27 step: 449, loss is 0.1807611733675003\n",
      "epoch: 27 step: 450, loss is 0.1466093212366104\n",
      "epoch: 27 step: 451, loss is 0.09615685045719147\n",
      "epoch: 27 step: 452, loss is 0.14045269787311554\n",
      "epoch: 27 step: 453, loss is 0.16517888009548187\n",
      "epoch: 27 step: 454, loss is 0.2638145089149475\n",
      "epoch: 27 step: 455, loss is 0.23282721638679504\n",
      "epoch: 27 step: 456, loss is 0.1012180894613266\n",
      "epoch: 27 step: 457, loss is 0.20591208338737488\n",
      "epoch: 27 step: 458, loss is 0.2630957067012787\n",
      "epoch: 27 step: 459, loss is 0.24327059090137482\n",
      "epoch: 27 step: 460, loss is 0.307312548160553\n",
      "epoch: 27 step: 461, loss is 0.26449742913246155\n",
      "epoch: 27 step: 462, loss is 0.4847266376018524\n",
      "epoch: 27 step: 463, loss is 0.19560731947422028\n",
      "epoch: 27 step: 464, loss is 0.1295057088136673\n",
      "epoch: 27 step: 465, loss is 0.1578826904296875\n",
      "epoch: 27 step: 466, loss is 0.21681725978851318\n",
      "epoch: 27 step: 467, loss is 0.15235203504562378\n",
      "epoch: 27 step: 468, loss is 0.19469276070594788\n",
      "epoch: 27 step: 469, loss is 0.31843626499176025\n",
      "epoch: 27 step: 470, loss is 0.11655169725418091\n",
      "epoch: 27 step: 471, loss is 0.2563800513744354\n",
      "epoch: 27 step: 472, loss is 0.14862336218357086\n",
      "epoch: 27 step: 473, loss is 0.1763802319765091\n",
      "epoch: 27 step: 474, loss is 0.17968466877937317\n",
      "epoch: 27 step: 475, loss is 0.2682388126850128\n",
      "epoch: 27 step: 476, loss is 0.14128214120864868\n",
      "epoch: 27 step: 477, loss is 0.29084035754203796\n",
      "epoch: 27 step: 478, loss is 0.15278145670890808\n",
      "epoch: 27 step: 479, loss is 0.06609952449798584\n",
      "epoch: 27 step: 480, loss is 0.2701849937438965\n",
      "epoch: 27 step: 481, loss is 0.17639324069023132\n",
      "epoch: 27 step: 482, loss is 0.19361045956611633\n",
      "epoch: 27 step: 483, loss is 0.15963703393936157\n",
      "epoch: 27 step: 484, loss is 0.2765079736709595\n",
      "epoch: 27 step: 485, loss is 0.1276252269744873\n",
      "epoch: 27 step: 486, loss is 0.29252123832702637\n",
      "epoch: 27 step: 487, loss is 0.12712396681308746\n",
      "epoch: 27 step: 488, loss is 0.11508330702781677\n",
      "epoch: 27 step: 489, loss is 0.24948269128799438\n",
      "epoch: 27 step: 490, loss is 0.36545929312705994\n",
      "epoch: 27 step: 491, loss is 0.07473167032003403\n",
      "epoch: 27 step: 492, loss is 0.24532438814640045\n",
      "epoch: 27 step: 493, loss is 0.1485387086868286\n",
      "epoch: 27 step: 494, loss is 0.1075063943862915\n",
      "epoch: 27 step: 495, loss is 0.1209360808134079\n",
      "epoch: 27 step: 496, loss is 0.1852036714553833\n",
      "epoch: 27 step: 497, loss is 0.10656795650720596\n",
      "epoch: 27 step: 498, loss is 0.13405492901802063\n",
      "epoch: 27 step: 499, loss is 0.11370452493429184\n",
      "epoch: 27 step: 500, loss is 0.15984098613262177\n",
      "epoch: 27 step: 501, loss is 0.15355508029460907\n",
      "epoch: 27 step: 502, loss is 0.12627428770065308\n",
      "epoch: 27 step: 503, loss is 0.14176586270332336\n",
      "epoch: 27 step: 504, loss is 0.11770986020565033\n",
      "epoch: 27 step: 505, loss is 0.1412142515182495\n",
      "epoch: 27 step: 506, loss is 0.15247869491577148\n",
      "epoch: 27 step: 507, loss is 0.1710188388824463\n",
      "epoch: 27 step: 508, loss is 0.29210326075553894\n",
      "epoch: 27 step: 509, loss is 0.20573170483112335\n",
      "epoch: 27 step: 510, loss is 0.1483447402715683\n",
      "epoch: 27 step: 511, loss is 0.3805098533630371\n",
      "epoch: 27 step: 512, loss is 0.19159825146198273\n",
      "epoch: 27 step: 513, loss is 0.20026589930057526\n",
      "epoch: 27 step: 514, loss is 0.264128714799881\n",
      "epoch: 27 step: 515, loss is 0.27937325835227966\n",
      "epoch: 27 step: 516, loss is 0.21331225335597992\n",
      "epoch: 27 step: 517, loss is 0.26372843980789185\n",
      "epoch: 27 step: 518, loss is 0.1954919993877411\n",
      "epoch: 27 step: 519, loss is 0.14926758408546448\n",
      "epoch: 27 step: 520, loss is 0.2661733031272888\n",
      "epoch: 27 step: 521, loss is 0.27317777276039124\n",
      "epoch: 27 step: 522, loss is 0.15345066785812378\n",
      "epoch: 27 step: 523, loss is 0.1767706274986267\n",
      "epoch: 27 step: 524, loss is 0.1688462346792221\n",
      "epoch: 27 step: 525, loss is 0.19828185439109802\n",
      "epoch: 27 step: 526, loss is 0.13138598203659058\n",
      "epoch: 27 step: 527, loss is 0.3761918246746063\n",
      "epoch: 27 step: 528, loss is 0.1458882838487625\n",
      "epoch: 27 step: 529, loss is 0.26655325293540955\n",
      "epoch: 27 step: 530, loss is 0.11166199296712875\n",
      "epoch: 27 step: 531, loss is 0.27645808458328247\n",
      "epoch: 27 step: 532, loss is 0.10091521590948105\n",
      "epoch: 27 step: 533, loss is 0.1891973316669464\n",
      "epoch: 27 step: 534, loss is 0.19702695310115814\n",
      "epoch: 27 step: 535, loss is 0.22052790224552155\n",
      "epoch: 27 step: 536, loss is 0.17713916301727295\n",
      "epoch: 27 step: 537, loss is 0.10114099830389023\n",
      "epoch: 27 step: 538, loss is 0.15362617373466492\n",
      "epoch: 27 step: 539, loss is 0.2496418058872223\n",
      "epoch: 27 step: 540, loss is 0.1655714362859726\n",
      "epoch: 27 step: 541, loss is 0.12234446406364441\n",
      "epoch: 27 step: 542, loss is 0.17598575353622437\n",
      "epoch: 27 step: 543, loss is 0.08571533858776093\n",
      "epoch: 27 step: 544, loss is 0.26561224460601807\n",
      "epoch: 27 step: 545, loss is 0.2422083616256714\n",
      "epoch: 27 step: 546, loss is 0.12319136410951614\n",
      "epoch: 27 step: 547, loss is 0.31673839688301086\n",
      "epoch: 27 step: 548, loss is 0.19438384473323822\n",
      "epoch: 27 step: 549, loss is 0.1714397668838501\n",
      "epoch: 27 step: 550, loss is 0.1697518676519394\n",
      "epoch: 27 step: 551, loss is 0.15919406712055206\n",
      "epoch: 27 step: 552, loss is 0.257148414850235\n",
      "epoch: 27 step: 553, loss is 0.1625702679157257\n",
      "epoch: 27 step: 554, loss is 0.20289842784404755\n",
      "epoch: 27 step: 555, loss is 0.12270084023475647\n",
      "epoch: 27 step: 556, loss is 0.25787094235420227\n",
      "epoch: 27 step: 557, loss is 0.26234763860702515\n",
      "epoch: 27 step: 558, loss is 0.24800577759742737\n",
      "epoch: 27 step: 559, loss is 0.24249011278152466\n",
      "epoch: 27 step: 560, loss is 0.21504178643226624\n",
      "epoch: 27 step: 561, loss is 0.18483151495456696\n",
      "epoch: 27 step: 562, loss is 0.24914368987083435\n",
      "epoch: 27 step: 563, loss is 0.07111804187297821\n",
      "epoch: 27 step: 564, loss is 0.2303062081336975\n",
      "epoch: 27 step: 565, loss is 0.2749018967151642\n",
      "epoch: 27 step: 566, loss is 0.10939645022153854\n",
      "epoch: 27 step: 567, loss is 0.1882079690694809\n",
      "epoch: 27 step: 568, loss is 0.08618156611919403\n",
      "epoch: 27 step: 569, loss is 0.10417145490646362\n",
      "epoch: 27 step: 570, loss is 0.17904753983020782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 571, loss is 0.18135298788547516\n",
      "epoch: 27 step: 572, loss is 0.13126695156097412\n",
      "epoch: 27 step: 573, loss is 0.3237254321575165\n",
      "epoch: 27 step: 574, loss is 0.1972169429063797\n",
      "epoch: 27 step: 575, loss is 0.12415763735771179\n",
      "epoch: 27 step: 576, loss is 0.10790478438138962\n",
      "epoch: 27 step: 577, loss is 0.34152182936668396\n",
      "epoch: 27 step: 578, loss is 0.10520881414413452\n",
      "epoch: 27 step: 579, loss is 0.10974220931529999\n",
      "epoch: 27 step: 580, loss is 0.07425446063280106\n",
      "epoch: 27 step: 581, loss is 0.30332404375076294\n",
      "epoch: 27 step: 582, loss is 0.15095126628875732\n",
      "epoch: 27 step: 583, loss is 0.21418237686157227\n",
      "epoch: 27 step: 584, loss is 0.1757401078939438\n",
      "epoch: 27 step: 585, loss is 0.15028375387191772\n",
      "epoch: 27 step: 586, loss is 0.08261612802743912\n",
      "epoch: 27 step: 587, loss is 0.23003628849983215\n",
      "epoch: 27 step: 588, loss is 0.08322475850582123\n",
      "epoch: 27 step: 589, loss is 0.10059870034456253\n",
      "epoch: 27 step: 590, loss is 0.24453207850456238\n",
      "epoch: 27 step: 591, loss is 0.1998075395822525\n",
      "epoch: 27 step: 592, loss is 0.12258068472146988\n",
      "epoch: 27 step: 593, loss is 0.1790066808462143\n",
      "epoch: 27 step: 594, loss is 0.2986316978931427\n",
      "epoch: 27 step: 595, loss is 0.23283016681671143\n",
      "epoch: 27 step: 596, loss is 0.27726617455482483\n",
      "epoch: 27 step: 597, loss is 0.23613619804382324\n",
      "epoch: 27 step: 598, loss is 0.17562824487686157\n",
      "epoch: 27 step: 599, loss is 0.07795580476522446\n",
      "epoch: 27 step: 600, loss is 0.22791078686714172\n",
      "epoch: 27 step: 601, loss is 0.266923189163208\n",
      "epoch: 27 step: 602, loss is 0.1743588149547577\n",
      "epoch: 27 step: 603, loss is 0.154185488820076\n",
      "epoch: 27 step: 604, loss is 0.11429120600223541\n",
      "epoch: 27 step: 605, loss is 0.17322826385498047\n",
      "epoch: 27 step: 606, loss is 0.1284063756465912\n",
      "epoch: 27 step: 607, loss is 0.23814666271209717\n",
      "epoch: 27 step: 608, loss is 0.15758052468299866\n",
      "epoch: 27 step: 609, loss is 0.13468484580516815\n",
      "epoch: 27 step: 610, loss is 0.19300080835819244\n",
      "epoch: 27 step: 611, loss is 0.24001291394233704\n",
      "epoch: 27 step: 612, loss is 0.20917940139770508\n",
      "epoch: 27 step: 613, loss is 0.15030337870121002\n",
      "epoch: 27 step: 614, loss is 0.31264638900756836\n",
      "epoch: 27 step: 615, loss is 0.26571834087371826\n",
      "epoch: 27 step: 616, loss is 0.331231951713562\n",
      "epoch: 27 step: 617, loss is 0.28199300169944763\n",
      "epoch: 27 step: 618, loss is 0.1980246603488922\n",
      "epoch: 27 step: 619, loss is 0.24626824259757996\n",
      "epoch: 27 step: 620, loss is 0.175483837723732\n",
      "epoch: 27 step: 621, loss is 0.15223118662834167\n",
      "epoch: 27 step: 622, loss is 0.21027562022209167\n",
      "epoch: 27 step: 623, loss is 0.17174148559570312\n",
      "epoch: 27 step: 624, loss is 0.19916856288909912\n",
      "epoch: 27 step: 625, loss is 0.13211049139499664\n",
      "epoch: 27 step: 626, loss is 0.14731179177761078\n",
      "epoch: 27 step: 627, loss is 0.4369299113750458\n",
      "epoch: 27 step: 628, loss is 0.2061043083667755\n",
      "epoch: 27 step: 629, loss is 0.1670241355895996\n",
      "epoch: 27 step: 630, loss is 0.06752006709575653\n",
      "epoch: 27 step: 631, loss is 0.12896426022052765\n",
      "epoch: 27 step: 632, loss is 0.15056852996349335\n",
      "epoch: 27 step: 633, loss is 0.182527557015419\n",
      "epoch: 27 step: 634, loss is 0.11747518181800842\n",
      "epoch: 27 step: 635, loss is 0.23114363849163055\n",
      "epoch: 27 step: 636, loss is 0.10744436085224152\n",
      "epoch: 27 step: 637, loss is 0.29400017857551575\n",
      "epoch: 27 step: 638, loss is 0.18246638774871826\n",
      "epoch: 27 step: 639, loss is 0.1375848948955536\n",
      "epoch: 27 step: 640, loss is 0.18950775265693665\n",
      "epoch: 27 step: 641, loss is 0.12034384906291962\n",
      "epoch: 27 step: 642, loss is 0.4987320899963379\n",
      "epoch: 27 step: 643, loss is 0.1001814603805542\n",
      "epoch: 27 step: 644, loss is 0.24615783989429474\n",
      "epoch: 27 step: 645, loss is 0.15950410068035126\n",
      "epoch: 27 step: 646, loss is 0.16397807002067566\n",
      "epoch: 27 step: 647, loss is 0.054787203669548035\n",
      "epoch: 27 step: 648, loss is 0.15886959433555603\n",
      "epoch: 27 step: 649, loss is 0.11904755234718323\n",
      "epoch: 27 step: 650, loss is 0.3104133605957031\n",
      "epoch: 27 step: 651, loss is 0.3600621819496155\n",
      "epoch: 27 step: 652, loss is 0.18641327321529388\n",
      "epoch: 27 step: 653, loss is 0.15351314842700958\n",
      "epoch: 27 step: 654, loss is 0.23868881165981293\n",
      "epoch: 27 step: 655, loss is 0.1638800948858261\n",
      "epoch: 27 step: 656, loss is 0.1428215652704239\n",
      "epoch: 27 step: 657, loss is 0.2772384285926819\n",
      "epoch: 27 step: 658, loss is 0.16630956530570984\n",
      "epoch: 27 step: 659, loss is 0.25118178129196167\n",
      "epoch: 27 step: 660, loss is 0.15768086910247803\n",
      "epoch: 27 step: 661, loss is 0.244197815656662\n",
      "epoch: 27 step: 662, loss is 0.3616221249103546\n",
      "epoch: 27 step: 663, loss is 0.17196515202522278\n",
      "epoch: 27 step: 664, loss is 0.2084932178258896\n",
      "epoch: 27 step: 665, loss is 0.16038215160369873\n",
      "epoch: 27 step: 666, loss is 0.07762356102466583\n",
      "epoch: 27 step: 667, loss is 0.3008732497692108\n",
      "epoch: 27 step: 668, loss is 0.31135109066963196\n",
      "epoch: 27 step: 669, loss is 0.10417374223470688\n",
      "epoch: 27 step: 670, loss is 0.14529770612716675\n",
      "epoch: 27 step: 671, loss is 0.15351992845535278\n",
      "epoch: 27 step: 672, loss is 0.20334412157535553\n",
      "epoch: 27 step: 673, loss is 0.14480674266815186\n",
      "epoch: 27 step: 674, loss is 0.21307258307933807\n",
      "epoch: 27 step: 675, loss is 0.31054651737213135\n",
      "epoch: 27 step: 676, loss is 0.14835487306118011\n",
      "epoch: 27 step: 677, loss is 0.11205758154392242\n",
      "epoch: 27 step: 678, loss is 0.052232805639505386\n",
      "epoch: 27 step: 679, loss is 0.12623187899589539\n",
      "epoch: 27 step: 680, loss is 0.1653941422700882\n",
      "epoch: 27 step: 681, loss is 0.13273310661315918\n",
      "epoch: 27 step: 682, loss is 0.23728160560131073\n",
      "epoch: 27 step: 683, loss is 0.2236499786376953\n",
      "epoch: 27 step: 684, loss is 0.17727555334568024\n",
      "epoch: 27 step: 685, loss is 0.24169033765792847\n",
      "epoch: 27 step: 686, loss is 0.20056048035621643\n",
      "epoch: 27 step: 687, loss is 0.1058168038725853\n",
      "epoch: 27 step: 688, loss is 0.20743905007839203\n",
      "epoch: 27 step: 689, loss is 0.18795572221279144\n",
      "epoch: 27 step: 690, loss is 0.2109954059123993\n",
      "epoch: 27 step: 691, loss is 0.144847109913826\n",
      "epoch: 27 step: 692, loss is 0.18731334805488586\n",
      "epoch: 27 step: 693, loss is 0.27919042110443115\n",
      "epoch: 27 step: 694, loss is 0.16513796150684357\n",
      "epoch: 27 step: 695, loss is 0.12117971479892731\n",
      "epoch: 27 step: 696, loss is 0.286441832780838\n",
      "epoch: 27 step: 697, loss is 0.18262925744056702\n",
      "epoch: 27 step: 698, loss is 0.15845881402492523\n",
      "epoch: 27 step: 699, loss is 0.17702504992485046\n",
      "epoch: 27 step: 700, loss is 0.24124331772327423\n",
      "epoch: 27 step: 701, loss is 0.20302720367908478\n",
      "epoch: 27 step: 702, loss is 0.16919778287410736\n",
      "epoch: 27 step: 703, loss is 0.33177700638771057\n",
      "epoch: 27 step: 704, loss is 0.20805624127388\n",
      "epoch: 27 step: 705, loss is 0.2432066798210144\n",
      "epoch: 27 step: 706, loss is 0.1707332730293274\n",
      "epoch: 27 step: 707, loss is 0.24701352417469025\n",
      "epoch: 27 step: 708, loss is 0.19540035724639893\n",
      "epoch: 27 step: 709, loss is 0.1431422084569931\n",
      "epoch: 27 step: 710, loss is 0.24533802270889282\n",
      "epoch: 27 step: 711, loss is 0.2127503603696823\n",
      "epoch: 27 step: 712, loss is 0.4176437258720398\n",
      "epoch: 27 step: 713, loss is 0.11675664782524109\n",
      "epoch: 27 step: 714, loss is 0.18417158722877502\n",
      "epoch: 27 step: 715, loss is 0.1904347687959671\n",
      "epoch: 27 step: 716, loss is 0.15223616361618042\n",
      "epoch: 27 step: 717, loss is 0.30690544843673706\n",
      "epoch: 27 step: 718, loss is 0.05921851098537445\n",
      "epoch: 27 step: 719, loss is 0.37943294644355774\n",
      "epoch: 27 step: 720, loss is 0.21050158143043518\n",
      "epoch: 27 step: 721, loss is 0.14497151970863342\n",
      "epoch: 27 step: 722, loss is 0.2033061683177948\n",
      "epoch: 27 step: 723, loss is 0.16348277032375336\n",
      "epoch: 27 step: 724, loss is 0.25329384207725525\n",
      "epoch: 27 step: 725, loss is 0.2205270677804947\n",
      "epoch: 27 step: 726, loss is 0.244388148188591\n",
      "epoch: 27 step: 727, loss is 0.16737374663352966\n",
      "epoch: 27 step: 728, loss is 0.25808727741241455\n",
      "epoch: 27 step: 729, loss is 0.10789468884468079\n",
      "epoch: 27 step: 730, loss is 0.22132492065429688\n",
      "epoch: 27 step: 731, loss is 0.24116262793540955\n",
      "epoch: 27 step: 732, loss is 0.21754774451255798\n",
      "epoch: 27 step: 733, loss is 0.11217556893825531\n",
      "epoch: 27 step: 734, loss is 0.1495118886232376\n",
      "epoch: 27 step: 735, loss is 0.13518904149532318\n",
      "epoch: 27 step: 736, loss is 0.23409713804721832\n",
      "epoch: 27 step: 737, loss is 0.16496281325817108\n",
      "epoch: 27 step: 738, loss is 0.16032078862190247\n",
      "epoch: 27 step: 739, loss is 0.33211857080459595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 740, loss is 0.2121714949607849\n",
      "epoch: 27 step: 741, loss is 0.3167750835418701\n",
      "epoch: 27 step: 742, loss is 0.27453872561454773\n",
      "epoch: 27 step: 743, loss is 0.2685103416442871\n",
      "epoch: 27 step: 744, loss is 0.15041998028755188\n",
      "epoch: 27 step: 745, loss is 0.1347280740737915\n",
      "epoch: 27 step: 746, loss is 0.18870367109775543\n",
      "epoch: 27 step: 747, loss is 0.11557646840810776\n",
      "epoch: 27 step: 748, loss is 0.16082443296909332\n",
      "epoch: 27 step: 749, loss is 0.07518145442008972\n",
      "epoch: 27 step: 750, loss is 0.13345561921596527\n",
      "epoch: 27 step: 751, loss is 0.16106995940208435\n",
      "epoch: 27 step: 752, loss is 0.19964049756526947\n",
      "epoch: 27 step: 753, loss is 0.27608522772789\n",
      "epoch: 27 step: 754, loss is 0.12945720553398132\n",
      "epoch: 27 step: 755, loss is 0.11839038133621216\n",
      "epoch: 27 step: 756, loss is 0.13207469880580902\n",
      "epoch: 27 step: 757, loss is 0.21433661878108978\n",
      "epoch: 27 step: 758, loss is 0.06858721375465393\n",
      "epoch: 27 step: 759, loss is 0.22100883722305298\n",
      "epoch: 27 step: 760, loss is 0.09736171364784241\n",
      "epoch: 27 step: 761, loss is 0.11034135520458221\n",
      "epoch: 27 step: 762, loss is 0.3015686571598053\n",
      "epoch: 27 step: 763, loss is 0.2831871509552002\n",
      "epoch: 27 step: 764, loss is 0.14447742700576782\n",
      "epoch: 27 step: 765, loss is 0.3695901334285736\n",
      "epoch: 27 step: 766, loss is 0.1561913639307022\n",
      "epoch: 27 step: 767, loss is 0.09438641369342804\n",
      "epoch: 27 step: 768, loss is 0.11854862421751022\n",
      "epoch: 27 step: 769, loss is 0.2130345106124878\n",
      "epoch: 27 step: 770, loss is 0.1747296154499054\n",
      "epoch: 27 step: 771, loss is 0.3969377875328064\n",
      "epoch: 27 step: 772, loss is 0.06496480852365494\n",
      "epoch: 27 step: 773, loss is 0.21473419666290283\n",
      "epoch: 27 step: 774, loss is 0.046863555908203125\n",
      "epoch: 27 step: 775, loss is 0.15714552998542786\n",
      "epoch: 27 step: 776, loss is 0.16160231828689575\n",
      "epoch: 27 step: 777, loss is 0.07940993458032608\n",
      "epoch: 27 step: 778, loss is 0.2683670222759247\n",
      "epoch: 27 step: 779, loss is 0.26408451795578003\n",
      "epoch: 27 step: 780, loss is 0.2675687074661255\n",
      "epoch: 27 step: 781, loss is 0.14547964930534363\n",
      "epoch: 27 step: 782, loss is 0.22326433658599854\n",
      "epoch: 27 step: 783, loss is 0.24104706943035126\n",
      "epoch: 27 step: 784, loss is 0.17777298390865326\n",
      "epoch: 27 step: 785, loss is 0.09480685740709305\n",
      "epoch: 27 step: 786, loss is 0.12123135477304459\n",
      "epoch: 27 step: 787, loss is 0.3538612425327301\n",
      "epoch: 27 step: 788, loss is 0.1167244166135788\n",
      "epoch: 27 step: 789, loss is 0.1572607457637787\n",
      "epoch: 27 step: 790, loss is 0.14125297963619232\n",
      "epoch: 27 step: 791, loss is 0.14023084938526154\n",
      "epoch: 27 step: 792, loss is 0.09306465089321136\n",
      "epoch: 27 step: 793, loss is 0.27747172117233276\n",
      "epoch: 27 step: 794, loss is 0.15861059725284576\n",
      "epoch: 27 step: 795, loss is 0.1224718764424324\n",
      "epoch: 27 step: 796, loss is 0.129537433385849\n",
      "epoch: 27 step: 797, loss is 0.16162970662117004\n",
      "epoch: 27 step: 798, loss is 0.1406777799129486\n",
      "epoch: 27 step: 799, loss is 0.2438511997461319\n",
      "epoch: 27 step: 800, loss is 0.17874756455421448\n",
      "epoch: 27 step: 801, loss is 0.10497608035802841\n",
      "epoch: 27 step: 802, loss is 0.15573541820049286\n",
      "epoch: 27 step: 803, loss is 0.15278513729572296\n",
      "epoch: 27 step: 804, loss is 0.18692393600940704\n",
      "epoch: 27 step: 805, loss is 0.07039499282836914\n",
      "epoch: 27 step: 806, loss is 0.22799912095069885\n",
      "epoch: 27 step: 807, loss is 0.2114683836698532\n",
      "epoch: 27 step: 808, loss is 0.11461005359888077\n",
      "epoch: 27 step: 809, loss is 0.17005851864814758\n",
      "epoch: 27 step: 810, loss is 0.1568581908941269\n",
      "epoch: 27 step: 811, loss is 0.17400141060352325\n",
      "epoch: 27 step: 812, loss is 0.23898695409297943\n",
      "epoch: 27 step: 813, loss is 0.2117471545934677\n",
      "epoch: 27 step: 814, loss is 0.17019589245319366\n",
      "epoch: 27 step: 815, loss is 0.1329662948846817\n",
      "epoch: 27 step: 816, loss is 0.23947428166866302\n",
      "epoch: 27 step: 817, loss is 0.13325439393520355\n",
      "epoch: 27 step: 818, loss is 0.2016187161207199\n",
      "epoch: 27 step: 819, loss is 0.05637761205434799\n",
      "epoch: 27 step: 820, loss is 0.13542892038822174\n",
      "epoch: 27 step: 821, loss is 0.12731897830963135\n",
      "epoch: 27 step: 822, loss is 0.1437905728816986\n",
      "epoch: 27 step: 823, loss is 0.15216845273971558\n",
      "epoch: 27 step: 824, loss is 0.23705461621284485\n",
      "epoch: 27 step: 825, loss is 0.13484728336334229\n",
      "epoch: 27 step: 826, loss is 0.06895802915096283\n",
      "epoch: 27 step: 827, loss is 0.22509197890758514\n",
      "epoch: 27 step: 828, loss is 0.19716764986515045\n",
      "epoch: 27 step: 829, loss is 0.28095653653144836\n",
      "epoch: 27 step: 830, loss is 0.24855943024158478\n",
      "epoch: 27 step: 831, loss is 0.21655823290348053\n",
      "epoch: 27 step: 832, loss is 0.18964779376983643\n",
      "epoch: 27 step: 833, loss is 0.10887441784143448\n",
      "epoch: 27 step: 834, loss is 0.17211833596229553\n",
      "epoch: 27 step: 835, loss is 0.06352903693914413\n",
      "epoch: 27 step: 836, loss is 0.24019527435302734\n",
      "epoch: 27 step: 837, loss is 0.15566635131835938\n",
      "epoch: 27 step: 838, loss is 0.1939462572336197\n",
      "epoch: 27 step: 839, loss is 0.17699654400348663\n",
      "epoch: 27 step: 840, loss is 0.3623848855495453\n",
      "epoch: 27 step: 841, loss is 0.14035561680793762\n",
      "epoch: 27 step: 842, loss is 0.20616289973258972\n",
      "epoch: 27 step: 843, loss is 0.37138113379478455\n",
      "epoch: 27 step: 844, loss is 0.3593110740184784\n",
      "epoch: 27 step: 845, loss is 0.11916592717170715\n",
      "epoch: 27 step: 846, loss is 0.21543113887310028\n",
      "epoch: 27 step: 847, loss is 0.18198420107364655\n",
      "epoch: 27 step: 848, loss is 0.07850020378828049\n",
      "epoch: 27 step: 849, loss is 0.15059886872768402\n",
      "epoch: 27 step: 850, loss is 0.05351215973496437\n",
      "epoch: 27 step: 851, loss is 0.18720217049121857\n",
      "epoch: 27 step: 852, loss is 0.18162818253040314\n",
      "epoch: 27 step: 853, loss is 0.15072429180145264\n",
      "epoch: 27 step: 854, loss is 0.19311368465423584\n",
      "epoch: 27 step: 855, loss is 0.20020316541194916\n",
      "epoch: 27 step: 856, loss is 0.18436090648174286\n",
      "epoch: 27 step: 857, loss is 0.09118304401636124\n",
      "epoch: 27 step: 858, loss is 0.2056480497121811\n",
      "epoch: 27 step: 859, loss is 0.15388429164886475\n",
      "epoch: 27 step: 860, loss is 0.42677101492881775\n",
      "epoch: 27 step: 861, loss is 0.2071899026632309\n",
      "epoch: 27 step: 862, loss is 0.24890531599521637\n",
      "epoch: 27 step: 863, loss is 0.18815289437770844\n",
      "epoch: 27 step: 864, loss is 0.29592612385749817\n",
      "epoch: 27 step: 865, loss is 0.24647416174411774\n",
      "epoch: 27 step: 866, loss is 0.23945710062980652\n",
      "epoch: 27 step: 867, loss is 0.25098860263824463\n",
      "epoch: 27 step: 868, loss is 0.2641291618347168\n",
      "epoch: 27 step: 869, loss is 0.13594266772270203\n",
      "epoch: 27 step: 870, loss is 0.2856931984424591\n",
      "epoch: 27 step: 871, loss is 0.1276213377714157\n",
      "epoch: 27 step: 872, loss is 0.151589497923851\n",
      "epoch: 27 step: 873, loss is 0.18087470531463623\n",
      "epoch: 27 step: 874, loss is 0.23941537737846375\n",
      "epoch: 27 step: 875, loss is 0.11425605416297913\n",
      "epoch: 27 step: 876, loss is 0.15884967148303986\n",
      "epoch: 27 step: 877, loss is 0.12068711221218109\n",
      "epoch: 27 step: 878, loss is 0.17346327006816864\n",
      "epoch: 27 step: 879, loss is 0.14634159207344055\n",
      "epoch: 27 step: 880, loss is 0.11084579676389694\n",
      "epoch: 27 step: 881, loss is 0.21791984140872955\n",
      "epoch: 27 step: 882, loss is 0.19428333640098572\n",
      "epoch: 27 step: 883, loss is 0.14024470746517181\n",
      "epoch: 27 step: 884, loss is 0.10513622313737869\n",
      "epoch: 27 step: 885, loss is 0.2662607729434967\n",
      "epoch: 27 step: 886, loss is 0.18497471511363983\n",
      "epoch: 27 step: 887, loss is 0.16832327842712402\n",
      "epoch: 27 step: 888, loss is 0.25318846106529236\n",
      "epoch: 27 step: 889, loss is 0.07873935997486115\n",
      "epoch: 27 step: 890, loss is 0.12027956545352936\n",
      "epoch: 27 step: 891, loss is 0.2397363781929016\n",
      "epoch: 27 step: 892, loss is 0.15917128324508667\n",
      "epoch: 27 step: 893, loss is 0.3356883227825165\n",
      "epoch: 27 step: 894, loss is 0.17149855196475983\n",
      "epoch: 27 step: 895, loss is 0.18319754302501678\n",
      "epoch: 27 step: 896, loss is 0.1146722361445427\n",
      "epoch: 27 step: 897, loss is 0.15711595118045807\n",
      "epoch: 27 step: 898, loss is 0.19645291566848755\n",
      "epoch: 27 step: 899, loss is 0.21526743471622467\n",
      "epoch: 27 step: 900, loss is 0.21004773676395416\n",
      "epoch: 27 step: 901, loss is 0.31770455837249756\n",
      "epoch: 27 step: 902, loss is 0.15740838646888733\n",
      "epoch: 27 step: 903, loss is 0.05514140799641609\n",
      "epoch: 27 step: 904, loss is 0.12023553252220154\n",
      "epoch: 27 step: 905, loss is 0.1455889344215393\n",
      "epoch: 27 step: 906, loss is 0.14584635198116302\n",
      "epoch: 27 step: 907, loss is 0.14282603561878204\n",
      "epoch: 27 step: 908, loss is 0.1707574427127838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 909, loss is 0.2107521891593933\n",
      "epoch: 27 step: 910, loss is 0.12504833936691284\n",
      "epoch: 27 step: 911, loss is 0.46866950392723083\n",
      "epoch: 27 step: 912, loss is 0.2411549836397171\n",
      "epoch: 27 step: 913, loss is 0.2611743211746216\n",
      "epoch: 27 step: 914, loss is 0.16132885217666626\n",
      "epoch: 27 step: 915, loss is 0.319461464881897\n",
      "epoch: 27 step: 916, loss is 0.1025390475988388\n",
      "epoch: 27 step: 917, loss is 0.1618848592042923\n",
      "epoch: 27 step: 918, loss is 0.1246882900595665\n",
      "epoch: 27 step: 919, loss is 0.278004914522171\n",
      "epoch: 27 step: 920, loss is 0.1585734635591507\n",
      "epoch: 27 step: 921, loss is 0.25715193152427673\n",
      "epoch: 27 step: 922, loss is 0.22151999175548553\n",
      "epoch: 27 step: 923, loss is 0.24889187514781952\n",
      "epoch: 27 step: 924, loss is 0.09745312482118607\n",
      "epoch: 27 step: 925, loss is 0.1353176385164261\n",
      "epoch: 27 step: 926, loss is 0.16296537220478058\n",
      "epoch: 27 step: 927, loss is 0.1453414261341095\n",
      "epoch: 27 step: 928, loss is 0.14858682453632355\n",
      "epoch: 27 step: 929, loss is 0.24505913257598877\n",
      "epoch: 27 step: 930, loss is 0.1370605081319809\n",
      "epoch: 27 step: 931, loss is 0.21378794312477112\n",
      "epoch: 27 step: 932, loss is 0.3056662678718567\n",
      "epoch: 27 step: 933, loss is 0.26788103580474854\n",
      "epoch: 27 step: 934, loss is 0.24291421473026276\n",
      "epoch: 27 step: 935, loss is 0.32892999053001404\n",
      "epoch: 27 step: 936, loss is 0.2557487189769745\n",
      "epoch: 27 step: 937, loss is 0.31029802560806274\n",
      "epoch: 28 step: 1, loss is 0.12569278478622437\n",
      "epoch: 28 step: 2, loss is 0.22516927123069763\n",
      "epoch: 28 step: 3, loss is 0.05306694656610489\n",
      "epoch: 28 step: 4, loss is 0.1357710212469101\n",
      "epoch: 28 step: 5, loss is 0.18581275641918182\n",
      "epoch: 28 step: 6, loss is 0.24476347863674164\n",
      "epoch: 28 step: 7, loss is 0.18508005142211914\n",
      "epoch: 28 step: 8, loss is 0.14608804881572723\n",
      "epoch: 28 step: 9, loss is 0.12517930567264557\n",
      "epoch: 28 step: 10, loss is 0.12137431651353836\n",
      "epoch: 28 step: 11, loss is 0.19190514087677002\n",
      "epoch: 28 step: 12, loss is 0.19335149228572845\n",
      "epoch: 28 step: 13, loss is 0.22812195122241974\n",
      "epoch: 28 step: 14, loss is 0.2410019189119339\n",
      "epoch: 28 step: 15, loss is 0.1511330008506775\n",
      "epoch: 28 step: 16, loss is 0.16876360774040222\n",
      "epoch: 28 step: 17, loss is 0.24223710596561432\n",
      "epoch: 28 step: 18, loss is 0.16098864376544952\n",
      "epoch: 28 step: 19, loss is 0.16045604646205902\n",
      "epoch: 28 step: 20, loss is 0.13990551233291626\n",
      "epoch: 28 step: 21, loss is 0.13692791759967804\n",
      "epoch: 28 step: 22, loss is 0.17922842502593994\n",
      "epoch: 28 step: 23, loss is 0.2606044113636017\n",
      "epoch: 28 step: 24, loss is 0.21082986891269684\n",
      "epoch: 28 step: 25, loss is 0.08003631234169006\n",
      "epoch: 28 step: 26, loss is 0.1536918431520462\n",
      "epoch: 28 step: 27, loss is 0.15554013848304749\n",
      "epoch: 28 step: 28, loss is 0.17943525314331055\n",
      "epoch: 28 step: 29, loss is 0.20966066420078278\n",
      "epoch: 28 step: 30, loss is 0.12650536000728607\n",
      "epoch: 28 step: 31, loss is 0.16052496433258057\n",
      "epoch: 28 step: 32, loss is 0.08816222101449966\n",
      "epoch: 28 step: 33, loss is 0.2211325615644455\n",
      "epoch: 28 step: 34, loss is 0.08131744712591171\n",
      "epoch: 28 step: 35, loss is 0.2251771241426468\n",
      "epoch: 28 step: 36, loss is 0.22025981545448303\n",
      "epoch: 28 step: 37, loss is 0.13559076189994812\n",
      "epoch: 28 step: 38, loss is 0.15555009245872498\n",
      "epoch: 28 step: 39, loss is 0.2328469306230545\n",
      "epoch: 28 step: 40, loss is 0.26495102047920227\n",
      "epoch: 28 step: 41, loss is 0.12960578501224518\n",
      "epoch: 28 step: 42, loss is 0.17373885214328766\n",
      "epoch: 28 step: 43, loss is 0.1263546645641327\n",
      "epoch: 28 step: 44, loss is 0.26098406314849854\n",
      "epoch: 28 step: 45, loss is 0.31213241815567017\n",
      "epoch: 28 step: 46, loss is 0.25200363993644714\n",
      "epoch: 28 step: 47, loss is 0.14873673021793365\n",
      "epoch: 28 step: 48, loss is 0.14545083045959473\n",
      "epoch: 28 step: 49, loss is 0.1276555359363556\n",
      "epoch: 28 step: 50, loss is 0.11469660699367523\n",
      "epoch: 28 step: 51, loss is 0.22363777458667755\n",
      "epoch: 28 step: 52, loss is 0.09604894369840622\n",
      "epoch: 28 step: 53, loss is 0.1909635365009308\n",
      "epoch: 28 step: 54, loss is 0.27087509632110596\n",
      "epoch: 28 step: 55, loss is 0.1379227340221405\n",
      "epoch: 28 step: 56, loss is 0.3425578773021698\n",
      "epoch: 28 step: 57, loss is 0.1973867565393448\n",
      "epoch: 28 step: 58, loss is 0.2686885893344879\n",
      "epoch: 28 step: 59, loss is 0.2582761347293854\n",
      "epoch: 28 step: 60, loss is 0.1842820793390274\n",
      "epoch: 28 step: 61, loss is 0.15255196392536163\n",
      "epoch: 28 step: 62, loss is 0.2190595418214798\n",
      "epoch: 28 step: 63, loss is 0.12748293578624725\n",
      "epoch: 28 step: 64, loss is 0.2767978012561798\n",
      "epoch: 28 step: 65, loss is 0.19382253289222717\n",
      "epoch: 28 step: 66, loss is 0.09698381274938583\n",
      "epoch: 28 step: 67, loss is 0.11686452478170395\n",
      "epoch: 28 step: 68, loss is 0.33283406496047974\n",
      "epoch: 28 step: 69, loss is 0.22702766954898834\n",
      "epoch: 28 step: 70, loss is 0.20202891528606415\n",
      "epoch: 28 step: 71, loss is 0.16516263782978058\n",
      "epoch: 28 step: 72, loss is 0.19382518529891968\n",
      "epoch: 28 step: 73, loss is 0.2089906930923462\n",
      "epoch: 28 step: 74, loss is 0.15951046347618103\n",
      "epoch: 28 step: 75, loss is 0.1756506860256195\n",
      "epoch: 28 step: 76, loss is 0.2300247699022293\n",
      "epoch: 28 step: 77, loss is 0.17679816484451294\n",
      "epoch: 28 step: 78, loss is 0.15474051237106323\n",
      "epoch: 28 step: 79, loss is 0.2954237163066864\n",
      "epoch: 28 step: 80, loss is 0.1725972592830658\n",
      "epoch: 28 step: 81, loss is 0.11353688687086105\n",
      "epoch: 28 step: 82, loss is 0.06858404725790024\n",
      "epoch: 28 step: 83, loss is 0.1766486018896103\n",
      "epoch: 28 step: 84, loss is 0.18744707107543945\n",
      "epoch: 28 step: 85, loss is 0.13963617384433746\n",
      "epoch: 28 step: 86, loss is 0.2264857143163681\n",
      "epoch: 28 step: 87, loss is 0.1415058821439743\n",
      "epoch: 28 step: 88, loss is 0.32393208146095276\n",
      "epoch: 28 step: 89, loss is 0.20732401311397552\n",
      "epoch: 28 step: 90, loss is 0.14229722321033478\n",
      "epoch: 28 step: 91, loss is 0.2465776950120926\n",
      "epoch: 28 step: 92, loss is 0.24970482289791107\n",
      "epoch: 28 step: 93, loss is 0.1790069192647934\n",
      "epoch: 28 step: 94, loss is 0.40656551718711853\n",
      "epoch: 28 step: 95, loss is 0.20222648978233337\n",
      "epoch: 28 step: 96, loss is 0.13400740921497345\n",
      "epoch: 28 step: 97, loss is 0.19892250001430511\n",
      "epoch: 28 step: 98, loss is 0.12044435739517212\n",
      "epoch: 28 step: 99, loss is 0.06858759373426437\n",
      "epoch: 28 step: 100, loss is 0.1114746704697609\n",
      "epoch: 28 step: 101, loss is 0.1436392068862915\n",
      "epoch: 28 step: 102, loss is 0.0842445120215416\n",
      "epoch: 28 step: 103, loss is 0.17149829864501953\n",
      "epoch: 28 step: 104, loss is 0.2842372953891754\n",
      "epoch: 28 step: 105, loss is 0.10353919863700867\n",
      "epoch: 28 step: 106, loss is 0.1525322049856186\n",
      "epoch: 28 step: 107, loss is 0.1263095587491989\n",
      "epoch: 28 step: 108, loss is 0.09798550605773926\n",
      "epoch: 28 step: 109, loss is 0.2287721037864685\n",
      "epoch: 28 step: 110, loss is 0.1671488732099533\n",
      "epoch: 28 step: 111, loss is 0.10752016305923462\n",
      "epoch: 28 step: 112, loss is 0.16920612752437592\n",
      "epoch: 28 step: 113, loss is 0.1543753743171692\n",
      "epoch: 28 step: 114, loss is 0.20391029119491577\n",
      "epoch: 28 step: 115, loss is 0.22134754061698914\n",
      "epoch: 28 step: 116, loss is 0.19959671795368195\n",
      "epoch: 28 step: 117, loss is 0.09542452543973923\n",
      "epoch: 28 step: 118, loss is 0.1585426777601242\n",
      "epoch: 28 step: 119, loss is 0.05958591774106026\n",
      "epoch: 28 step: 120, loss is 0.2673380374908447\n",
      "epoch: 28 step: 121, loss is 0.232103630900383\n",
      "epoch: 28 step: 122, loss is 0.1561071276664734\n",
      "epoch: 28 step: 123, loss is 0.1519782841205597\n",
      "epoch: 28 step: 124, loss is 0.24506133794784546\n",
      "epoch: 28 step: 125, loss is 0.10724596679210663\n",
      "epoch: 28 step: 126, loss is 0.062006935477256775\n",
      "epoch: 28 step: 127, loss is 0.18087156116962433\n",
      "epoch: 28 step: 128, loss is 0.2511325478553772\n",
      "epoch: 28 step: 129, loss is 0.12773792445659637\n",
      "epoch: 28 step: 130, loss is 0.07549015432596207\n",
      "epoch: 28 step: 131, loss is 0.09764977544546127\n",
      "epoch: 28 step: 132, loss is 0.12217520922422409\n",
      "epoch: 28 step: 133, loss is 0.1704813688993454\n",
      "epoch: 28 step: 134, loss is 0.13887059688568115\n",
      "epoch: 28 step: 135, loss is 0.09721140563488007\n",
      "epoch: 28 step: 136, loss is 0.15711934864521027\n",
      "epoch: 28 step: 137, loss is 0.18723991513252258\n",
      "epoch: 28 step: 138, loss is 0.14709550142288208\n",
      "epoch: 28 step: 139, loss is 0.18247482180595398\n",
      "epoch: 28 step: 140, loss is 0.1586371660232544\n",
      "epoch: 28 step: 141, loss is 0.09495189785957336\n",
      "epoch: 28 step: 142, loss is 0.06022536754608154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 143, loss is 0.14640003442764282\n",
      "epoch: 28 step: 144, loss is 0.09523318707942963\n",
      "epoch: 28 step: 145, loss is 0.09225983917713165\n",
      "epoch: 28 step: 146, loss is 0.15059776604175568\n",
      "epoch: 28 step: 147, loss is 0.24711473286151886\n",
      "epoch: 28 step: 148, loss is 0.13338418304920197\n",
      "epoch: 28 step: 149, loss is 0.159481018781662\n",
      "epoch: 28 step: 150, loss is 0.0683114230632782\n",
      "epoch: 28 step: 151, loss is 0.1047094538807869\n",
      "epoch: 28 step: 152, loss is 0.13153016567230225\n",
      "epoch: 28 step: 153, loss is 0.2278723567724228\n",
      "epoch: 28 step: 154, loss is 0.2210874706506729\n",
      "epoch: 28 step: 155, loss is 0.1137186735868454\n",
      "epoch: 28 step: 156, loss is 0.1300169825553894\n",
      "epoch: 28 step: 157, loss is 0.21520543098449707\n",
      "epoch: 28 step: 158, loss is 0.1025785282254219\n",
      "epoch: 28 step: 159, loss is 0.3412646949291229\n",
      "epoch: 28 step: 160, loss is 0.13865095376968384\n",
      "epoch: 28 step: 161, loss is 0.23781123757362366\n",
      "epoch: 28 step: 162, loss is 0.08715994656085968\n",
      "epoch: 28 step: 163, loss is 0.26009318232536316\n",
      "epoch: 28 step: 164, loss is 0.13988733291625977\n",
      "epoch: 28 step: 165, loss is 0.25681108236312866\n",
      "epoch: 28 step: 166, loss is 0.2425282746553421\n",
      "epoch: 28 step: 167, loss is 0.16578249633312225\n",
      "epoch: 28 step: 168, loss is 0.3356724679470062\n",
      "epoch: 28 step: 169, loss is 0.21831348538398743\n",
      "epoch: 28 step: 170, loss is 0.16261664032936096\n",
      "epoch: 28 step: 171, loss is 0.1269104778766632\n",
      "epoch: 28 step: 172, loss is 0.14286048710346222\n",
      "epoch: 28 step: 173, loss is 0.2934395670890808\n",
      "epoch: 28 step: 174, loss is 0.1718122363090515\n",
      "epoch: 28 step: 175, loss is 0.11306149512529373\n",
      "epoch: 28 step: 176, loss is 0.1750326007604599\n",
      "epoch: 28 step: 177, loss is 0.17951788008213043\n",
      "epoch: 28 step: 178, loss is 0.19297893345355988\n",
      "epoch: 28 step: 179, loss is 0.24701738357543945\n",
      "epoch: 28 step: 180, loss is 0.34673866629600525\n",
      "epoch: 28 step: 181, loss is 0.2311503291130066\n",
      "epoch: 28 step: 182, loss is 0.2112337201833725\n",
      "epoch: 28 step: 183, loss is 0.09185072034597397\n",
      "epoch: 28 step: 184, loss is 0.25712862610816956\n",
      "epoch: 28 step: 185, loss is 0.1151529923081398\n",
      "epoch: 28 step: 186, loss is 0.16691625118255615\n",
      "epoch: 28 step: 187, loss is 0.133628249168396\n",
      "epoch: 28 step: 188, loss is 0.22611771523952484\n",
      "epoch: 28 step: 189, loss is 0.27780279517173767\n",
      "epoch: 28 step: 190, loss is 0.22351066768169403\n",
      "epoch: 28 step: 191, loss is 0.19962622225284576\n",
      "epoch: 28 step: 192, loss is 0.15817290544509888\n",
      "epoch: 28 step: 193, loss is 0.12720641493797302\n",
      "epoch: 28 step: 194, loss is 0.13329297304153442\n",
      "epoch: 28 step: 195, loss is 0.2638791799545288\n",
      "epoch: 28 step: 196, loss is 0.08098310977220535\n",
      "epoch: 28 step: 197, loss is 0.2220914512872696\n",
      "epoch: 28 step: 198, loss is 0.2205166220664978\n",
      "epoch: 28 step: 199, loss is 0.13300460577011108\n",
      "epoch: 28 step: 200, loss is 0.18253245949745178\n",
      "epoch: 28 step: 201, loss is 0.15960940718650818\n",
      "epoch: 28 step: 202, loss is 0.3500540852546692\n",
      "epoch: 28 step: 203, loss is 0.11518082022666931\n",
      "epoch: 28 step: 204, loss is 0.18652991950511932\n",
      "epoch: 28 step: 205, loss is 0.08508366346359253\n",
      "epoch: 28 step: 206, loss is 0.17303237318992615\n",
      "epoch: 28 step: 207, loss is 0.16888146102428436\n",
      "epoch: 28 step: 208, loss is 0.14493241906166077\n",
      "epoch: 28 step: 209, loss is 0.3449326157569885\n",
      "epoch: 28 step: 210, loss is 0.16010087728500366\n",
      "epoch: 28 step: 211, loss is 0.19949838519096375\n",
      "epoch: 28 step: 212, loss is 0.2660246789455414\n",
      "epoch: 28 step: 213, loss is 0.19806824624538422\n",
      "epoch: 28 step: 214, loss is 0.10396093875169754\n",
      "epoch: 28 step: 215, loss is 0.1664070338010788\n",
      "epoch: 28 step: 216, loss is 0.03454965353012085\n",
      "epoch: 28 step: 217, loss is 0.18669752776622772\n",
      "epoch: 28 step: 218, loss is 0.17304910719394684\n",
      "epoch: 28 step: 219, loss is 0.30560895800590515\n",
      "epoch: 28 step: 220, loss is 0.13736706972122192\n",
      "epoch: 28 step: 221, loss is 0.1360752433538437\n",
      "epoch: 28 step: 222, loss is 0.2713812589645386\n",
      "epoch: 28 step: 223, loss is 0.09901062399148941\n",
      "epoch: 28 step: 224, loss is 0.29978761076927185\n",
      "epoch: 28 step: 225, loss is 0.11385458707809448\n",
      "epoch: 28 step: 226, loss is 0.2076081931591034\n",
      "epoch: 28 step: 227, loss is 0.1843363493680954\n",
      "epoch: 28 step: 228, loss is 0.19514651596546173\n",
      "epoch: 28 step: 229, loss is 0.208248570561409\n",
      "epoch: 28 step: 230, loss is 0.34856972098350525\n",
      "epoch: 28 step: 231, loss is 0.18986119329929352\n",
      "epoch: 28 step: 232, loss is 0.27453720569610596\n",
      "epoch: 28 step: 233, loss is 0.29856598377227783\n",
      "epoch: 28 step: 234, loss is 0.29817789793014526\n",
      "epoch: 28 step: 235, loss is 0.20975390076637268\n",
      "epoch: 28 step: 236, loss is 0.13809257745742798\n",
      "epoch: 28 step: 237, loss is 0.2526938021183014\n",
      "epoch: 28 step: 238, loss is 0.13348636031150818\n",
      "epoch: 28 step: 239, loss is 0.40313252806663513\n",
      "epoch: 28 step: 240, loss is 0.1768796443939209\n",
      "epoch: 28 step: 241, loss is 0.15749377012252808\n",
      "epoch: 28 step: 242, loss is 0.21326282620429993\n",
      "epoch: 28 step: 243, loss is 0.33471184968948364\n",
      "epoch: 28 step: 244, loss is 0.3006695508956909\n",
      "epoch: 28 step: 245, loss is 0.16306035220623016\n",
      "epoch: 28 step: 246, loss is 0.14221011102199554\n",
      "epoch: 28 step: 247, loss is 0.3813515305519104\n",
      "epoch: 28 step: 248, loss is 0.2888900935649872\n",
      "epoch: 28 step: 249, loss is 0.21419693529605865\n",
      "epoch: 28 step: 250, loss is 0.27038607001304626\n",
      "epoch: 28 step: 251, loss is 0.1318628340959549\n",
      "epoch: 28 step: 252, loss is 0.151776522397995\n",
      "epoch: 28 step: 253, loss is 0.1905403435230255\n",
      "epoch: 28 step: 254, loss is 0.10376609861850739\n",
      "epoch: 28 step: 255, loss is 0.2710370719432831\n",
      "epoch: 28 step: 256, loss is 0.2170906960964203\n",
      "epoch: 28 step: 257, loss is 0.1374790370464325\n",
      "epoch: 28 step: 258, loss is 0.20533892512321472\n",
      "epoch: 28 step: 259, loss is 0.22352972626686096\n",
      "epoch: 28 step: 260, loss is 0.21901951730251312\n",
      "epoch: 28 step: 261, loss is 0.18446849286556244\n",
      "epoch: 28 step: 262, loss is 0.36181774735450745\n",
      "epoch: 28 step: 263, loss is 0.27350443601608276\n",
      "epoch: 28 step: 264, loss is 0.08390699326992035\n",
      "epoch: 28 step: 265, loss is 0.11193685978651047\n",
      "epoch: 28 step: 266, loss is 0.2572561502456665\n",
      "epoch: 28 step: 267, loss is 0.10288568586111069\n",
      "epoch: 28 step: 268, loss is 0.1756848394870758\n",
      "epoch: 28 step: 269, loss is 0.14490914344787598\n",
      "epoch: 28 step: 270, loss is 0.41385412216186523\n",
      "epoch: 28 step: 271, loss is 0.36243072152137756\n",
      "epoch: 28 step: 272, loss is 0.17774643003940582\n",
      "epoch: 28 step: 273, loss is 0.153750479221344\n",
      "epoch: 28 step: 274, loss is 0.1733517199754715\n",
      "epoch: 28 step: 275, loss is 0.14155888557434082\n",
      "epoch: 28 step: 276, loss is 0.1852516233921051\n",
      "epoch: 28 step: 277, loss is 0.13038435578346252\n",
      "epoch: 28 step: 278, loss is 0.1507033258676529\n",
      "epoch: 28 step: 279, loss is 0.20401735603809357\n",
      "epoch: 28 step: 280, loss is 0.22789819538593292\n",
      "epoch: 28 step: 281, loss is 0.11982598155736923\n",
      "epoch: 28 step: 282, loss is 0.11015342175960541\n",
      "epoch: 28 step: 283, loss is 0.0992521420121193\n",
      "epoch: 28 step: 284, loss is 0.2918265163898468\n",
      "epoch: 28 step: 285, loss is 0.22970089316368103\n",
      "epoch: 28 step: 286, loss is 0.09107374399900436\n",
      "epoch: 28 step: 287, loss is 0.18809835612773895\n",
      "epoch: 28 step: 288, loss is 0.15907594561576843\n",
      "epoch: 28 step: 289, loss is 0.2327413558959961\n",
      "epoch: 28 step: 290, loss is 0.09042399376630783\n",
      "epoch: 28 step: 291, loss is 0.22714070975780487\n",
      "epoch: 28 step: 292, loss is 0.1897851675748825\n",
      "epoch: 28 step: 293, loss is 0.2562382221221924\n",
      "epoch: 28 step: 294, loss is 0.1755724400281906\n",
      "epoch: 28 step: 295, loss is 0.2706906497478485\n",
      "epoch: 28 step: 296, loss is 0.17425912618637085\n",
      "epoch: 28 step: 297, loss is 0.11748465150594711\n",
      "epoch: 28 step: 298, loss is 0.22921836376190186\n",
      "epoch: 28 step: 299, loss is 0.1534430980682373\n",
      "epoch: 28 step: 300, loss is 0.13440269231796265\n",
      "epoch: 28 step: 301, loss is 0.14850972592830658\n",
      "epoch: 28 step: 302, loss is 0.16980358958244324\n",
      "epoch: 28 step: 303, loss is 0.20055188238620758\n",
      "epoch: 28 step: 304, loss is 0.13932755589485168\n",
      "epoch: 28 step: 305, loss is 0.10927996784448624\n",
      "epoch: 28 step: 306, loss is 0.2506561577320099\n",
      "epoch: 28 step: 307, loss is 0.1157527044415474\n",
      "epoch: 28 step: 308, loss is 0.1898767203092575\n",
      "epoch: 28 step: 309, loss is 0.09914319962263107\n",
      "epoch: 28 step: 310, loss is 0.16521215438842773\n",
      "epoch: 28 step: 311, loss is 0.26437073945999146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 312, loss is 0.11288460344076157\n",
      "epoch: 28 step: 313, loss is 0.06635037809610367\n",
      "epoch: 28 step: 314, loss is 0.14285467565059662\n",
      "epoch: 28 step: 315, loss is 0.24491508305072784\n",
      "epoch: 28 step: 316, loss is 0.11405351758003235\n",
      "epoch: 28 step: 317, loss is 0.10958349704742432\n",
      "epoch: 28 step: 318, loss is 0.13862620294094086\n",
      "epoch: 28 step: 319, loss is 0.046120185405015945\n",
      "epoch: 28 step: 320, loss is 0.22027122974395752\n",
      "epoch: 28 step: 321, loss is 0.2621104419231415\n",
      "epoch: 28 step: 322, loss is 0.17811881005764008\n",
      "epoch: 28 step: 323, loss is 0.11989861726760864\n",
      "epoch: 28 step: 324, loss is 0.21241717040538788\n",
      "epoch: 28 step: 325, loss is 0.2159273475408554\n",
      "epoch: 28 step: 326, loss is 0.2031749188899994\n",
      "epoch: 28 step: 327, loss is 0.08621186017990112\n",
      "epoch: 28 step: 328, loss is 0.13382543623447418\n",
      "epoch: 28 step: 329, loss is 0.29636529088020325\n",
      "epoch: 28 step: 330, loss is 0.2286611646413803\n",
      "epoch: 28 step: 331, loss is 0.16787688434123993\n",
      "epoch: 28 step: 332, loss is 0.08512093871831894\n",
      "epoch: 28 step: 333, loss is 0.19694779813289642\n",
      "epoch: 28 step: 334, loss is 0.2759590744972229\n",
      "epoch: 28 step: 335, loss is 0.28072023391723633\n",
      "epoch: 28 step: 336, loss is 0.17703860998153687\n",
      "epoch: 28 step: 337, loss is 0.3166968524456024\n",
      "epoch: 28 step: 338, loss is 0.2887507379055023\n",
      "epoch: 28 step: 339, loss is 0.12050811201334\n",
      "epoch: 28 step: 340, loss is 0.07066583633422852\n",
      "epoch: 28 step: 341, loss is 0.1363358497619629\n",
      "epoch: 28 step: 342, loss is 0.20976996421813965\n",
      "epoch: 28 step: 343, loss is 0.10205859690904617\n",
      "epoch: 28 step: 344, loss is 0.09070621430873871\n",
      "epoch: 28 step: 345, loss is 0.24817514419555664\n",
      "epoch: 28 step: 346, loss is 0.1504056453704834\n",
      "epoch: 28 step: 347, loss is 0.16055376827716827\n",
      "epoch: 28 step: 348, loss is 0.11486726254224777\n",
      "epoch: 28 step: 349, loss is 0.12432104349136353\n",
      "epoch: 28 step: 350, loss is 0.15548989176750183\n",
      "epoch: 28 step: 351, loss is 0.19667978584766388\n",
      "epoch: 28 step: 352, loss is 0.2811170220375061\n",
      "epoch: 28 step: 353, loss is 0.1435120701789856\n",
      "epoch: 28 step: 354, loss is 0.22175565361976624\n",
      "epoch: 28 step: 355, loss is 0.14378494024276733\n",
      "epoch: 28 step: 356, loss is 0.507754921913147\n",
      "epoch: 28 step: 357, loss is 0.2692824602127075\n",
      "epoch: 28 step: 358, loss is 0.09671660512685776\n",
      "epoch: 28 step: 359, loss is 0.12138517946004868\n",
      "epoch: 28 step: 360, loss is 0.10973681509494781\n",
      "epoch: 28 step: 361, loss is 0.17918901145458221\n",
      "epoch: 28 step: 362, loss is 0.14716531336307526\n",
      "epoch: 28 step: 363, loss is 0.1888725757598877\n",
      "epoch: 28 step: 364, loss is 0.2946971356868744\n",
      "epoch: 28 step: 365, loss is 0.25422602891921997\n",
      "epoch: 28 step: 366, loss is 0.1345277726650238\n",
      "epoch: 28 step: 367, loss is 0.19636815786361694\n",
      "epoch: 28 step: 368, loss is 0.1287946105003357\n",
      "epoch: 28 step: 369, loss is 0.1564200222492218\n",
      "epoch: 28 step: 370, loss is 0.23643599450588226\n",
      "epoch: 28 step: 371, loss is 0.10245894640684128\n",
      "epoch: 28 step: 372, loss is 0.3405682444572449\n",
      "epoch: 28 step: 373, loss is 0.1416979283094406\n",
      "epoch: 28 step: 374, loss is 0.15039336681365967\n",
      "epoch: 28 step: 375, loss is 0.06040063872933388\n",
      "epoch: 28 step: 376, loss is 0.12177380919456482\n",
      "epoch: 28 step: 377, loss is 0.14513768255710602\n",
      "epoch: 28 step: 378, loss is 0.25758469104766846\n",
      "epoch: 28 step: 379, loss is 0.12504185736179352\n",
      "epoch: 28 step: 380, loss is 0.2040642499923706\n",
      "epoch: 28 step: 381, loss is 0.17686396837234497\n",
      "epoch: 28 step: 382, loss is 0.31280168890953064\n",
      "epoch: 28 step: 383, loss is 0.36709821224212646\n",
      "epoch: 28 step: 384, loss is 0.14230811595916748\n",
      "epoch: 28 step: 385, loss is 0.13618895411491394\n",
      "epoch: 28 step: 386, loss is 0.19828622043132782\n",
      "epoch: 28 step: 387, loss is 0.17388832569122314\n",
      "epoch: 28 step: 388, loss is 0.15897302329540253\n",
      "epoch: 28 step: 389, loss is 0.13293655216693878\n",
      "epoch: 28 step: 390, loss is 0.16749940812587738\n",
      "epoch: 28 step: 391, loss is 0.2024146467447281\n",
      "epoch: 28 step: 392, loss is 0.2233029454946518\n",
      "epoch: 28 step: 393, loss is 0.20932850241661072\n",
      "epoch: 28 step: 394, loss is 0.16022764146327972\n",
      "epoch: 28 step: 395, loss is 0.17001983523368835\n",
      "epoch: 28 step: 396, loss is 0.1393488347530365\n",
      "epoch: 28 step: 397, loss is 0.1361987292766571\n",
      "epoch: 28 step: 398, loss is 0.10892508924007416\n",
      "epoch: 28 step: 399, loss is 0.12758702039718628\n",
      "epoch: 28 step: 400, loss is 0.1499221920967102\n",
      "epoch: 28 step: 401, loss is 0.20374540984630585\n",
      "epoch: 28 step: 402, loss is 0.1509283035993576\n",
      "epoch: 28 step: 403, loss is 0.26679205894470215\n",
      "epoch: 28 step: 404, loss is 0.15857619047164917\n",
      "epoch: 28 step: 405, loss is 0.2731071710586548\n",
      "epoch: 28 step: 406, loss is 0.1636407971382141\n",
      "epoch: 28 step: 407, loss is 0.12565867602825165\n",
      "epoch: 28 step: 408, loss is 0.17022278904914856\n",
      "epoch: 28 step: 409, loss is 0.14106114208698273\n",
      "epoch: 28 step: 410, loss is 0.09703954309225082\n",
      "epoch: 28 step: 411, loss is 0.15212814509868622\n",
      "epoch: 28 step: 412, loss is 0.2735746502876282\n",
      "epoch: 28 step: 413, loss is 0.12380047142505646\n",
      "epoch: 28 step: 414, loss is 0.29077672958374023\n",
      "epoch: 28 step: 415, loss is 0.21762490272521973\n",
      "epoch: 28 step: 416, loss is 0.08800217509269714\n",
      "epoch: 28 step: 417, loss is 0.08611297607421875\n",
      "epoch: 28 step: 418, loss is 0.3367611765861511\n",
      "epoch: 28 step: 419, loss is 0.2627929449081421\n",
      "epoch: 28 step: 420, loss is 0.17746497690677643\n",
      "epoch: 28 step: 421, loss is 0.2053307741880417\n",
      "epoch: 28 step: 422, loss is 0.10945635288953781\n",
      "epoch: 28 step: 423, loss is 0.29809486865997314\n",
      "epoch: 28 step: 424, loss is 0.20321494340896606\n",
      "epoch: 28 step: 425, loss is 0.12750229239463806\n",
      "epoch: 28 step: 426, loss is 0.18584468960762024\n",
      "epoch: 28 step: 427, loss is 0.2090114951133728\n",
      "epoch: 28 step: 428, loss is 0.19159726798534393\n",
      "epoch: 28 step: 429, loss is 0.17893250286579132\n",
      "epoch: 28 step: 430, loss is 0.22048859298229218\n",
      "epoch: 28 step: 431, loss is 0.14540542662143707\n",
      "epoch: 28 step: 432, loss is 0.13004983961582184\n",
      "epoch: 28 step: 433, loss is 0.10088874399662018\n",
      "epoch: 28 step: 434, loss is 0.3534565269947052\n",
      "epoch: 28 step: 435, loss is 0.2005893737077713\n",
      "epoch: 28 step: 436, loss is 0.11937365680932999\n",
      "epoch: 28 step: 437, loss is 0.3087523281574249\n",
      "epoch: 28 step: 438, loss is 0.27301695942878723\n",
      "epoch: 28 step: 439, loss is 0.20136071741580963\n",
      "epoch: 28 step: 440, loss is 0.0869947299361229\n",
      "epoch: 28 step: 441, loss is 0.23759090900421143\n",
      "epoch: 28 step: 442, loss is 0.20692558586597443\n",
      "epoch: 28 step: 443, loss is 0.10004478693008423\n",
      "epoch: 28 step: 444, loss is 0.11423351615667343\n",
      "epoch: 28 step: 445, loss is 0.11961562931537628\n",
      "epoch: 28 step: 446, loss is 0.44801607728004456\n",
      "epoch: 28 step: 447, loss is 0.14637552201747894\n",
      "epoch: 28 step: 448, loss is 0.171015664935112\n",
      "epoch: 28 step: 449, loss is 0.19151027500629425\n",
      "epoch: 28 step: 450, loss is 0.09578610211610794\n",
      "epoch: 28 step: 451, loss is 0.12478333711624146\n",
      "epoch: 28 step: 452, loss is 0.10521966964006424\n",
      "epoch: 28 step: 453, loss is 0.2403528243303299\n",
      "epoch: 28 step: 454, loss is 0.21470907330513\n",
      "epoch: 28 step: 455, loss is 0.1620645523071289\n",
      "epoch: 28 step: 456, loss is 0.16126085817813873\n",
      "epoch: 28 step: 457, loss is 0.17802783846855164\n",
      "epoch: 28 step: 458, loss is 0.28486818075180054\n",
      "epoch: 28 step: 459, loss is 0.316133975982666\n",
      "epoch: 28 step: 460, loss is 0.12447129189968109\n",
      "epoch: 28 step: 461, loss is 0.16948720812797546\n",
      "epoch: 28 step: 462, loss is 0.24516426026821136\n",
      "epoch: 28 step: 463, loss is 0.16425812244415283\n",
      "epoch: 28 step: 464, loss is 0.19761408865451813\n",
      "epoch: 28 step: 465, loss is 0.10807449370622635\n",
      "epoch: 28 step: 466, loss is 0.1508575826883316\n",
      "epoch: 28 step: 467, loss is 0.15369939804077148\n",
      "epoch: 28 step: 468, loss is 0.09856667369604111\n",
      "epoch: 28 step: 469, loss is 0.3341591954231262\n",
      "epoch: 28 step: 470, loss is 0.096570685505867\n",
      "epoch: 28 step: 471, loss is 0.07982221245765686\n",
      "epoch: 28 step: 472, loss is 0.2095811814069748\n",
      "epoch: 28 step: 473, loss is 0.15348473191261292\n",
      "epoch: 28 step: 474, loss is 0.1308162659406662\n",
      "epoch: 28 step: 475, loss is 0.2202291339635849\n",
      "epoch: 28 step: 476, loss is 0.10595501214265823\n",
      "epoch: 28 step: 477, loss is 0.09715765714645386\n",
      "epoch: 28 step: 478, loss is 0.31585294008255005\n",
      "epoch: 28 step: 479, loss is 0.16294917464256287\n",
      "epoch: 28 step: 480, loss is 0.21372370421886444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 481, loss is 0.11758127808570862\n",
      "epoch: 28 step: 482, loss is 0.16507472097873688\n",
      "epoch: 28 step: 483, loss is 0.09638077765703201\n",
      "epoch: 28 step: 484, loss is 0.335614413022995\n",
      "epoch: 28 step: 485, loss is 0.17903056740760803\n",
      "epoch: 28 step: 486, loss is 0.2013406753540039\n",
      "epoch: 28 step: 487, loss is 0.16967712342739105\n",
      "epoch: 28 step: 488, loss is 0.06712248176336288\n",
      "epoch: 28 step: 489, loss is 0.23546981811523438\n",
      "epoch: 28 step: 490, loss is 0.21289263665676117\n",
      "epoch: 28 step: 491, loss is 0.2282019704580307\n",
      "epoch: 28 step: 492, loss is 0.09763404726982117\n",
      "epoch: 28 step: 493, loss is 0.255596786737442\n",
      "epoch: 28 step: 494, loss is 0.23095932602882385\n",
      "epoch: 28 step: 495, loss is 0.18592332303524017\n",
      "epoch: 28 step: 496, loss is 0.27962934970855713\n",
      "epoch: 28 step: 497, loss is 0.19402118027210236\n",
      "epoch: 28 step: 498, loss is 0.10034261643886566\n",
      "epoch: 28 step: 499, loss is 0.182461217045784\n",
      "epoch: 28 step: 500, loss is 0.34270453453063965\n",
      "epoch: 28 step: 501, loss is 0.13646478950977325\n",
      "epoch: 28 step: 502, loss is 0.13315030932426453\n",
      "epoch: 28 step: 503, loss is 0.2990693747997284\n",
      "epoch: 28 step: 504, loss is 0.19887587428092957\n",
      "epoch: 28 step: 505, loss is 0.2721255421638489\n",
      "epoch: 28 step: 506, loss is 0.16161781549453735\n",
      "epoch: 28 step: 507, loss is 0.14542005956172943\n",
      "epoch: 28 step: 508, loss is 0.19447053968906403\n",
      "epoch: 28 step: 509, loss is 0.11560425907373428\n",
      "epoch: 28 step: 510, loss is 0.2073589265346527\n",
      "epoch: 28 step: 511, loss is 0.18476127088069916\n",
      "epoch: 28 step: 512, loss is 0.18358281254768372\n",
      "epoch: 28 step: 513, loss is 0.19038555026054382\n",
      "epoch: 28 step: 514, loss is 0.14210349321365356\n",
      "epoch: 28 step: 515, loss is 0.1887064129114151\n",
      "epoch: 28 step: 516, loss is 0.13548117876052856\n",
      "epoch: 28 step: 517, loss is 0.19227324426174164\n",
      "epoch: 28 step: 518, loss is 0.15783269703388214\n",
      "epoch: 28 step: 519, loss is 0.1602638065814972\n",
      "epoch: 28 step: 520, loss is 0.2135220766067505\n",
      "epoch: 28 step: 521, loss is 0.1947224736213684\n",
      "epoch: 28 step: 522, loss is 0.1676264852285385\n",
      "epoch: 28 step: 523, loss is 0.26380595564842224\n",
      "epoch: 28 step: 524, loss is 0.38915082812309265\n",
      "epoch: 28 step: 525, loss is 0.12112391740083694\n",
      "epoch: 28 step: 526, loss is 0.20341669023036957\n",
      "epoch: 28 step: 527, loss is 0.047952838242053986\n",
      "epoch: 28 step: 528, loss is 0.28472423553466797\n",
      "epoch: 28 step: 529, loss is 0.07431255280971527\n",
      "epoch: 28 step: 530, loss is 0.15859322249889374\n",
      "epoch: 28 step: 531, loss is 0.14853152632713318\n",
      "epoch: 28 step: 532, loss is 0.11066853255033493\n",
      "epoch: 28 step: 533, loss is 0.1414758563041687\n",
      "epoch: 28 step: 534, loss is 0.12136196345090866\n",
      "epoch: 28 step: 535, loss is 0.0810621902346611\n",
      "epoch: 28 step: 536, loss is 0.12095856666564941\n",
      "epoch: 28 step: 537, loss is 0.2127152979373932\n",
      "epoch: 28 step: 538, loss is 0.18082812428474426\n",
      "epoch: 28 step: 539, loss is 0.14074166119098663\n",
      "epoch: 28 step: 540, loss is 0.27670589089393616\n",
      "epoch: 28 step: 541, loss is 0.10181381553411484\n",
      "epoch: 28 step: 542, loss is 0.12548241019248962\n",
      "epoch: 28 step: 543, loss is 0.14311741292476654\n",
      "epoch: 28 step: 544, loss is 0.17334042489528656\n",
      "epoch: 28 step: 545, loss is 0.23459790647029877\n",
      "epoch: 28 step: 546, loss is 0.16801680624485016\n",
      "epoch: 28 step: 547, loss is 0.13602405786514282\n",
      "epoch: 28 step: 548, loss is 0.27221041917800903\n",
      "epoch: 28 step: 549, loss is 0.16711266338825226\n",
      "epoch: 28 step: 550, loss is 0.13791081309318542\n",
      "epoch: 28 step: 551, loss is 0.20034688711166382\n",
      "epoch: 28 step: 552, loss is 0.2481158822774887\n",
      "epoch: 28 step: 553, loss is 0.30364641547203064\n",
      "epoch: 28 step: 554, loss is 0.408751517534256\n",
      "epoch: 28 step: 555, loss is 0.11775503307580948\n",
      "epoch: 28 step: 556, loss is 0.26683297753334045\n",
      "epoch: 28 step: 557, loss is 0.15926648676395416\n",
      "epoch: 28 step: 558, loss is 0.3561488091945648\n",
      "epoch: 28 step: 559, loss is 0.10527440160512924\n",
      "epoch: 28 step: 560, loss is 0.11246146261692047\n",
      "epoch: 28 step: 561, loss is 0.3230239152908325\n",
      "epoch: 28 step: 562, loss is 0.10990192741155624\n",
      "epoch: 28 step: 563, loss is 0.21124058961868286\n",
      "epoch: 28 step: 564, loss is 0.10728077590465546\n",
      "epoch: 28 step: 565, loss is 0.10800599306821823\n",
      "epoch: 28 step: 566, loss is 0.20692244172096252\n",
      "epoch: 28 step: 567, loss is 0.2284507006406784\n",
      "epoch: 28 step: 568, loss is 0.15222708880901337\n",
      "epoch: 28 step: 569, loss is 0.2631378769874573\n",
      "epoch: 28 step: 570, loss is 0.12806469202041626\n",
      "epoch: 28 step: 571, loss is 0.18905964493751526\n",
      "epoch: 28 step: 572, loss is 0.087089404463768\n",
      "epoch: 28 step: 573, loss is 0.22332099080085754\n",
      "epoch: 28 step: 574, loss is 0.2454328089952469\n",
      "epoch: 28 step: 575, loss is 0.10611685365438461\n",
      "epoch: 28 step: 576, loss is 0.18718557059764862\n",
      "epoch: 28 step: 577, loss is 0.1528337299823761\n",
      "epoch: 28 step: 578, loss is 0.2054697424173355\n",
      "epoch: 28 step: 579, loss is 0.21171678602695465\n",
      "epoch: 28 step: 580, loss is 0.17706066370010376\n",
      "epoch: 28 step: 581, loss is 0.21914134919643402\n",
      "epoch: 28 step: 582, loss is 0.15665145218372345\n",
      "epoch: 28 step: 583, loss is 0.07133936882019043\n",
      "epoch: 28 step: 584, loss is 0.2086721509695053\n",
      "epoch: 28 step: 585, loss is 0.12685970962047577\n",
      "epoch: 28 step: 586, loss is 0.2565056383609772\n",
      "epoch: 28 step: 587, loss is 0.21510326862335205\n",
      "epoch: 28 step: 588, loss is 0.2508769929409027\n",
      "epoch: 28 step: 589, loss is 0.1710594743490219\n",
      "epoch: 28 step: 590, loss is 0.21429912745952606\n",
      "epoch: 28 step: 591, loss is 0.21477486193180084\n",
      "epoch: 28 step: 592, loss is 0.1319560557603836\n",
      "epoch: 28 step: 593, loss is 0.23891334235668182\n",
      "epoch: 28 step: 594, loss is 0.05890822783112526\n",
      "epoch: 28 step: 595, loss is 0.13026246428489685\n",
      "epoch: 28 step: 596, loss is 0.06898736953735352\n",
      "epoch: 28 step: 597, loss is 0.3134588301181793\n",
      "epoch: 28 step: 598, loss is 0.11558102071285248\n",
      "epoch: 28 step: 599, loss is 0.21997404098510742\n",
      "epoch: 28 step: 600, loss is 0.3023606538772583\n",
      "epoch: 28 step: 601, loss is 0.2604278028011322\n",
      "epoch: 28 step: 602, loss is 0.18784983456134796\n",
      "epoch: 28 step: 603, loss is 0.28769025206565857\n",
      "epoch: 28 step: 604, loss is 0.17527571320533752\n",
      "epoch: 28 step: 605, loss is 0.18119800090789795\n",
      "epoch: 28 step: 606, loss is 0.110758937895298\n",
      "epoch: 28 step: 607, loss is 0.3125047981739044\n",
      "epoch: 28 step: 608, loss is 0.20802074670791626\n",
      "epoch: 28 step: 609, loss is 0.15064285695552826\n",
      "epoch: 28 step: 610, loss is 0.08580281585454941\n",
      "epoch: 28 step: 611, loss is 0.10442376881837845\n",
      "epoch: 28 step: 612, loss is 0.27038681507110596\n",
      "epoch: 28 step: 613, loss is 0.14912228286266327\n",
      "epoch: 28 step: 614, loss is 0.10408073663711548\n",
      "epoch: 28 step: 615, loss is 0.3654920756816864\n",
      "epoch: 28 step: 616, loss is 0.34747761487960815\n",
      "epoch: 28 step: 617, loss is 0.12437256425619125\n",
      "epoch: 28 step: 618, loss is 0.10788046568632126\n",
      "epoch: 28 step: 619, loss is 0.09204839915037155\n",
      "epoch: 28 step: 620, loss is 0.12421979755163193\n",
      "epoch: 28 step: 621, loss is 0.09328169375658035\n",
      "epoch: 28 step: 622, loss is 0.29287734627723694\n",
      "epoch: 28 step: 623, loss is 0.20294564962387085\n",
      "epoch: 28 step: 624, loss is 0.2382102757692337\n",
      "epoch: 28 step: 625, loss is 0.0621270127594471\n",
      "epoch: 28 step: 626, loss is 0.2347092181444168\n",
      "epoch: 28 step: 627, loss is 0.13417468965053558\n",
      "epoch: 28 step: 628, loss is 0.09439300000667572\n",
      "epoch: 28 step: 629, loss is 0.16450366377830505\n",
      "epoch: 28 step: 630, loss is 0.1268942952156067\n",
      "epoch: 28 step: 631, loss is 0.313245952129364\n",
      "epoch: 28 step: 632, loss is 0.18649761378765106\n",
      "epoch: 28 step: 633, loss is 0.1729385405778885\n",
      "epoch: 28 step: 634, loss is 0.21087414026260376\n",
      "epoch: 28 step: 635, loss is 0.11897078156471252\n",
      "epoch: 28 step: 636, loss is 0.39290815591812134\n",
      "epoch: 28 step: 637, loss is 0.22483587265014648\n",
      "epoch: 28 step: 638, loss is 0.2467571347951889\n",
      "epoch: 28 step: 639, loss is 0.34192395210266113\n",
      "epoch: 28 step: 640, loss is 0.3781184256076813\n",
      "epoch: 28 step: 641, loss is 0.20608071982860565\n",
      "epoch: 28 step: 642, loss is 0.37643545866012573\n",
      "epoch: 28 step: 643, loss is 0.2506743371486664\n",
      "epoch: 28 step: 644, loss is 0.2962082028388977\n",
      "epoch: 28 step: 645, loss is 0.21544446051120758\n",
      "epoch: 28 step: 646, loss is 0.26797303557395935\n",
      "epoch: 28 step: 647, loss is 0.16111794114112854\n",
      "epoch: 28 step: 648, loss is 0.29128560423851013\n",
      "epoch: 28 step: 649, loss is 0.23915405571460724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 650, loss is 0.1469852775335312\n",
      "epoch: 28 step: 651, loss is 0.17719583213329315\n",
      "epoch: 28 step: 652, loss is 0.1266920417547226\n",
      "epoch: 28 step: 653, loss is 0.1100926622748375\n",
      "epoch: 28 step: 654, loss is 0.15792270004749298\n",
      "epoch: 28 step: 655, loss is 0.12534844875335693\n",
      "epoch: 28 step: 656, loss is 0.1862863302230835\n",
      "epoch: 28 step: 657, loss is 0.1579664647579193\n",
      "epoch: 28 step: 658, loss is 0.1851082295179367\n",
      "epoch: 28 step: 659, loss is 0.05502502620220184\n",
      "epoch: 28 step: 660, loss is 0.17607514560222626\n",
      "epoch: 28 step: 661, loss is 0.06006304919719696\n",
      "epoch: 28 step: 662, loss is 0.1853414922952652\n",
      "epoch: 28 step: 663, loss is 0.08145393431186676\n",
      "epoch: 28 step: 664, loss is 0.26277339458465576\n",
      "epoch: 28 step: 665, loss is 0.17541906237602234\n",
      "epoch: 28 step: 666, loss is 0.2176206260919571\n",
      "epoch: 28 step: 667, loss is 0.17480884492397308\n",
      "epoch: 28 step: 668, loss is 0.14514566957950592\n",
      "epoch: 28 step: 669, loss is 0.15049861371517181\n",
      "epoch: 28 step: 670, loss is 0.05733514577150345\n",
      "epoch: 28 step: 671, loss is 0.24374671280384064\n",
      "epoch: 28 step: 672, loss is 0.3216358423233032\n",
      "epoch: 28 step: 673, loss is 0.10621173679828644\n",
      "epoch: 28 step: 674, loss is 0.19450244307518005\n",
      "epoch: 28 step: 675, loss is 0.09228766709566116\n",
      "epoch: 28 step: 676, loss is 0.09905050694942474\n",
      "epoch: 28 step: 677, loss is 0.27533847093582153\n",
      "epoch: 28 step: 678, loss is 0.2023036628961563\n",
      "epoch: 28 step: 679, loss is 0.075377456843853\n",
      "epoch: 28 step: 680, loss is 0.22295452654361725\n",
      "epoch: 28 step: 681, loss is 0.1315932422876358\n",
      "epoch: 28 step: 682, loss is 0.20917990803718567\n",
      "epoch: 28 step: 683, loss is 0.24815542995929718\n",
      "epoch: 28 step: 684, loss is 0.18457980453968048\n",
      "epoch: 28 step: 685, loss is 0.11163590848445892\n",
      "epoch: 28 step: 686, loss is 0.08853723853826523\n",
      "epoch: 28 step: 687, loss is 0.05664816498756409\n",
      "epoch: 28 step: 688, loss is 0.11413761973381042\n",
      "epoch: 28 step: 689, loss is 0.16582942008972168\n",
      "epoch: 28 step: 690, loss is 0.09185275435447693\n",
      "epoch: 28 step: 691, loss is 0.3350093960762024\n",
      "epoch: 28 step: 692, loss is 0.18449999392032623\n",
      "epoch: 28 step: 693, loss is 0.19681969285011292\n",
      "epoch: 28 step: 694, loss is 0.11874345690011978\n",
      "epoch: 28 step: 695, loss is 0.2645901143550873\n",
      "epoch: 28 step: 696, loss is 0.19379553198814392\n",
      "epoch: 28 step: 697, loss is 0.1101139560341835\n",
      "epoch: 28 step: 698, loss is 0.20243147015571594\n",
      "epoch: 28 step: 699, loss is 0.1180616170167923\n",
      "epoch: 28 step: 700, loss is 0.16528531908988953\n",
      "epoch: 28 step: 701, loss is 0.26114004850387573\n",
      "epoch: 28 step: 702, loss is 0.0833403542637825\n",
      "epoch: 28 step: 703, loss is 0.09409978240728378\n",
      "epoch: 28 step: 704, loss is 0.15070246160030365\n",
      "epoch: 28 step: 705, loss is 0.13140688836574554\n",
      "epoch: 28 step: 706, loss is 0.09743068367242813\n",
      "epoch: 28 step: 707, loss is 0.09561706334352493\n",
      "epoch: 28 step: 708, loss is 0.11413376033306122\n",
      "epoch: 28 step: 709, loss is 0.07958877086639404\n",
      "epoch: 28 step: 710, loss is 0.1454366147518158\n",
      "epoch: 28 step: 711, loss is 0.07390670478343964\n",
      "epoch: 28 step: 712, loss is 0.3058420419692993\n",
      "epoch: 28 step: 713, loss is 0.3136376142501831\n",
      "epoch: 28 step: 714, loss is 0.18364748358726501\n",
      "epoch: 28 step: 715, loss is 0.19601422548294067\n",
      "epoch: 28 step: 716, loss is 0.1494712382555008\n",
      "epoch: 28 step: 717, loss is 0.16974453628063202\n",
      "epoch: 28 step: 718, loss is 0.19028465449810028\n",
      "epoch: 28 step: 719, loss is 0.1602228283882141\n",
      "epoch: 28 step: 720, loss is 0.18097522854804993\n",
      "epoch: 28 step: 721, loss is 0.13565990328788757\n",
      "epoch: 28 step: 722, loss is 0.18588094413280487\n",
      "epoch: 28 step: 723, loss is 0.1373799592256546\n",
      "epoch: 28 step: 724, loss is 0.152248352766037\n",
      "epoch: 28 step: 725, loss is 0.11430653929710388\n",
      "epoch: 28 step: 726, loss is 0.08871307969093323\n",
      "epoch: 28 step: 727, loss is 0.12696672976016998\n",
      "epoch: 28 step: 728, loss is 0.23656600713729858\n",
      "epoch: 28 step: 729, loss is 0.1327049434185028\n",
      "epoch: 28 step: 730, loss is 0.04012272134423256\n",
      "epoch: 28 step: 731, loss is 0.22246885299682617\n",
      "epoch: 28 step: 732, loss is 0.21957717835903168\n",
      "epoch: 28 step: 733, loss is 0.07720912992954254\n",
      "epoch: 28 step: 734, loss is 0.08865109086036682\n",
      "epoch: 28 step: 735, loss is 0.16841062903404236\n",
      "epoch: 28 step: 736, loss is 0.22147269546985626\n",
      "epoch: 28 step: 737, loss is 0.19908267259597778\n",
      "epoch: 28 step: 738, loss is 0.17847393453121185\n",
      "epoch: 28 step: 739, loss is 0.15478605031967163\n",
      "epoch: 28 step: 740, loss is 0.1453338861465454\n",
      "epoch: 28 step: 741, loss is 0.11898443102836609\n",
      "epoch: 28 step: 742, loss is 0.15293151140213013\n",
      "epoch: 28 step: 743, loss is 0.2107829749584198\n",
      "epoch: 28 step: 744, loss is 0.17732684314250946\n",
      "epoch: 28 step: 745, loss is 0.12939704954624176\n",
      "epoch: 28 step: 746, loss is 0.1654573678970337\n",
      "epoch: 28 step: 747, loss is 0.12175152450799942\n",
      "epoch: 28 step: 748, loss is 0.1234789788722992\n",
      "epoch: 28 step: 749, loss is 0.132003515958786\n",
      "epoch: 28 step: 750, loss is 0.12341637909412384\n",
      "epoch: 28 step: 751, loss is 0.22968916594982147\n",
      "epoch: 28 step: 752, loss is 0.30225056409835815\n",
      "epoch: 28 step: 753, loss is 0.15852397680282593\n",
      "epoch: 28 step: 754, loss is 0.2640036642551422\n",
      "epoch: 28 step: 755, loss is 0.20492523908615112\n",
      "epoch: 28 step: 756, loss is 0.2157912403345108\n",
      "epoch: 28 step: 757, loss is 0.14341476559638977\n",
      "epoch: 28 step: 758, loss is 0.2580326497554779\n",
      "epoch: 28 step: 759, loss is 0.0589042492210865\n",
      "epoch: 28 step: 760, loss is 0.13184121251106262\n",
      "epoch: 28 step: 761, loss is 0.09551449120044708\n",
      "epoch: 28 step: 762, loss is 0.2980026304721832\n",
      "epoch: 28 step: 763, loss is 0.10057886689901352\n",
      "epoch: 28 step: 764, loss is 0.1921520233154297\n",
      "epoch: 28 step: 765, loss is 0.08269781619310379\n",
      "epoch: 28 step: 766, loss is 0.13798828423023224\n",
      "epoch: 28 step: 767, loss is 0.15898025035858154\n",
      "epoch: 28 step: 768, loss is 0.1519927680492401\n",
      "epoch: 28 step: 769, loss is 0.19941583275794983\n",
      "epoch: 28 step: 770, loss is 0.1348886936903\n",
      "epoch: 28 step: 771, loss is 0.12191782146692276\n",
      "epoch: 28 step: 772, loss is 0.16346246004104614\n",
      "epoch: 28 step: 773, loss is 0.21649853885173798\n",
      "epoch: 28 step: 774, loss is 0.2454279512166977\n",
      "epoch: 28 step: 775, loss is 0.2548768222332001\n",
      "epoch: 28 step: 776, loss is 0.12817685306072235\n",
      "epoch: 28 step: 777, loss is 0.18337316811084747\n",
      "epoch: 28 step: 778, loss is 0.2574777901172638\n",
      "epoch: 28 step: 779, loss is 0.2694523334503174\n",
      "epoch: 28 step: 780, loss is 0.09663843363523483\n",
      "epoch: 28 step: 781, loss is 0.2861274480819702\n",
      "epoch: 28 step: 782, loss is 0.2795434892177582\n",
      "epoch: 28 step: 783, loss is 0.2529695928096771\n",
      "epoch: 28 step: 784, loss is 0.2690070867538452\n",
      "epoch: 28 step: 785, loss is 0.2583942115306854\n",
      "epoch: 28 step: 786, loss is 0.21185855567455292\n",
      "epoch: 28 step: 787, loss is 0.156504288315773\n",
      "epoch: 28 step: 788, loss is 0.07380641251802444\n",
      "epoch: 28 step: 789, loss is 0.2563861608505249\n",
      "epoch: 28 step: 790, loss is 0.2595069706439972\n",
      "epoch: 28 step: 791, loss is 0.2875230014324188\n",
      "epoch: 28 step: 792, loss is 0.4186461567878723\n",
      "epoch: 28 step: 793, loss is 0.1050962507724762\n",
      "epoch: 28 step: 794, loss is 0.10453755408525467\n",
      "epoch: 28 step: 795, loss is 0.15232959389686584\n",
      "epoch: 28 step: 796, loss is 0.18628638982772827\n",
      "epoch: 28 step: 797, loss is 0.2233249694108963\n",
      "epoch: 28 step: 798, loss is 0.063083715736866\n",
      "epoch: 28 step: 799, loss is 0.14747530221939087\n",
      "epoch: 28 step: 800, loss is 0.09027998894453049\n",
      "epoch: 28 step: 801, loss is 0.24093621969223022\n",
      "epoch: 28 step: 802, loss is 0.15346258878707886\n",
      "epoch: 28 step: 803, loss is 0.12228517979383469\n",
      "epoch: 28 step: 804, loss is 0.21685662865638733\n",
      "epoch: 28 step: 805, loss is 0.15718457102775574\n",
      "epoch: 28 step: 806, loss is 0.08968387544155121\n",
      "epoch: 28 step: 807, loss is 0.2100457102060318\n",
      "epoch: 28 step: 808, loss is 0.2657749354839325\n",
      "epoch: 28 step: 809, loss is 0.221511110663414\n",
      "epoch: 28 step: 810, loss is 0.14853034913539886\n",
      "epoch: 28 step: 811, loss is 0.099985271692276\n",
      "epoch: 28 step: 812, loss is 0.23389357328414917\n",
      "epoch: 28 step: 813, loss is 0.11953237652778625\n",
      "epoch: 28 step: 814, loss is 0.24935588240623474\n",
      "epoch: 28 step: 815, loss is 0.2008575201034546\n",
      "epoch: 28 step: 816, loss is 0.13045135140419006\n",
      "epoch: 28 step: 817, loss is 0.47275254130363464\n",
      "epoch: 28 step: 818, loss is 0.42323386669158936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 819, loss is 0.17069116234779358\n",
      "epoch: 28 step: 820, loss is 0.11372848600149155\n",
      "epoch: 28 step: 821, loss is 0.1712290346622467\n",
      "epoch: 28 step: 822, loss is 0.20439594984054565\n",
      "epoch: 28 step: 823, loss is 0.08072435110807419\n",
      "epoch: 28 step: 824, loss is 0.13183309137821198\n",
      "epoch: 28 step: 825, loss is 0.24207830429077148\n",
      "epoch: 28 step: 826, loss is 0.18247529864311218\n",
      "epoch: 28 step: 827, loss is 0.14095140993595123\n",
      "epoch: 28 step: 828, loss is 0.17484287917613983\n",
      "epoch: 28 step: 829, loss is 0.5503209233283997\n",
      "epoch: 28 step: 830, loss is 0.18259501457214355\n",
      "epoch: 28 step: 831, loss is 0.23830842971801758\n",
      "epoch: 28 step: 832, loss is 0.13459736108779907\n",
      "epoch: 28 step: 833, loss is 0.1594267189502716\n",
      "epoch: 28 step: 834, loss is 0.19872203469276428\n",
      "epoch: 28 step: 835, loss is 0.1427837610244751\n",
      "epoch: 28 step: 836, loss is 0.19830574095249176\n",
      "epoch: 28 step: 837, loss is 0.11906394362449646\n",
      "epoch: 28 step: 838, loss is 0.16817136108875275\n",
      "epoch: 28 step: 839, loss is 0.23965182900428772\n",
      "epoch: 28 step: 840, loss is 0.10804536193609238\n",
      "epoch: 28 step: 841, loss is 0.13660475611686707\n",
      "epoch: 28 step: 842, loss is 0.2366674244403839\n",
      "epoch: 28 step: 843, loss is 0.1597537398338318\n",
      "epoch: 28 step: 844, loss is 0.18730875849723816\n",
      "epoch: 28 step: 845, loss is 0.25962719321250916\n",
      "epoch: 28 step: 846, loss is 0.2584754228591919\n",
      "epoch: 28 step: 847, loss is 0.10057737678289413\n",
      "epoch: 28 step: 848, loss is 0.25130340456962585\n",
      "epoch: 28 step: 849, loss is 0.30429139733314514\n",
      "epoch: 28 step: 850, loss is 0.22753097116947174\n",
      "epoch: 28 step: 851, loss is 0.18006013333797455\n",
      "epoch: 28 step: 852, loss is 0.17856575548648834\n",
      "epoch: 28 step: 853, loss is 0.19281572103500366\n",
      "epoch: 28 step: 854, loss is 0.30789339542388916\n",
      "epoch: 28 step: 855, loss is 0.14427517354488373\n",
      "epoch: 28 step: 856, loss is 0.08058281242847443\n",
      "epoch: 28 step: 857, loss is 0.15918095409870148\n",
      "epoch: 28 step: 858, loss is 0.12339477241039276\n",
      "epoch: 28 step: 859, loss is 0.07736021280288696\n",
      "epoch: 28 step: 860, loss is 0.17769967019557953\n",
      "epoch: 28 step: 861, loss is 0.11623617261648178\n",
      "epoch: 28 step: 862, loss is 0.12144102901220322\n",
      "epoch: 28 step: 863, loss is 0.2340584546327591\n",
      "epoch: 28 step: 864, loss is 0.17746011912822723\n",
      "epoch: 28 step: 865, loss is 0.16055163741111755\n",
      "epoch: 28 step: 866, loss is 0.1884431689977646\n",
      "epoch: 28 step: 867, loss is 0.14881551265716553\n",
      "epoch: 28 step: 868, loss is 0.21117274463176727\n",
      "epoch: 28 step: 869, loss is 0.16222740709781647\n",
      "epoch: 28 step: 870, loss is 0.10209047049283981\n",
      "epoch: 28 step: 871, loss is 0.20686568319797516\n",
      "epoch: 28 step: 872, loss is 0.19320067763328552\n",
      "epoch: 28 step: 873, loss is 0.16990438103675842\n",
      "epoch: 28 step: 874, loss is 0.09649097174406052\n",
      "epoch: 28 step: 875, loss is 0.1770145297050476\n",
      "epoch: 28 step: 876, loss is 0.21867620944976807\n",
      "epoch: 28 step: 877, loss is 0.19806063175201416\n",
      "epoch: 28 step: 878, loss is 0.319965660572052\n",
      "epoch: 28 step: 879, loss is 0.22930186986923218\n",
      "epoch: 28 step: 880, loss is 0.2154024988412857\n",
      "epoch: 28 step: 881, loss is 0.2214636355638504\n",
      "epoch: 28 step: 882, loss is 0.0688151940703392\n",
      "epoch: 28 step: 883, loss is 0.14696796238422394\n",
      "epoch: 28 step: 884, loss is 0.2027459591627121\n",
      "epoch: 28 step: 885, loss is 0.17678409814834595\n",
      "epoch: 28 step: 886, loss is 0.05535146966576576\n",
      "epoch: 28 step: 887, loss is 0.08216942101716995\n",
      "epoch: 28 step: 888, loss is 0.25754299759864807\n",
      "epoch: 28 step: 889, loss is 0.3481772840023041\n",
      "epoch: 28 step: 890, loss is 0.16907919943332672\n",
      "epoch: 28 step: 891, loss is 0.15803463757038116\n",
      "epoch: 28 step: 892, loss is 0.12150147557258606\n",
      "epoch: 28 step: 893, loss is 0.07176709175109863\n",
      "epoch: 28 step: 894, loss is 0.13566292822360992\n",
      "epoch: 28 step: 895, loss is 0.17341868579387665\n",
      "epoch: 28 step: 896, loss is 0.3084237277507782\n",
      "epoch: 28 step: 897, loss is 0.15104740858078003\n",
      "epoch: 28 step: 898, loss is 0.28686821460723877\n",
      "epoch: 28 step: 899, loss is 0.16425511240959167\n",
      "epoch: 28 step: 900, loss is 0.1725519746541977\n",
      "epoch: 28 step: 901, loss is 0.13332900404930115\n",
      "epoch: 28 step: 902, loss is 0.12257915735244751\n",
      "epoch: 28 step: 903, loss is 0.2982475757598877\n",
      "epoch: 28 step: 904, loss is 0.11508961021900177\n",
      "epoch: 28 step: 905, loss is 0.17125797271728516\n",
      "epoch: 28 step: 906, loss is 0.21475721895694733\n",
      "epoch: 28 step: 907, loss is 0.2070484161376953\n",
      "epoch: 28 step: 908, loss is 0.15628720819950104\n",
      "epoch: 28 step: 909, loss is 0.24487486481666565\n",
      "epoch: 28 step: 910, loss is 0.30629095435142517\n",
      "epoch: 28 step: 911, loss is 0.19355623424053192\n",
      "epoch: 28 step: 912, loss is 0.29811906814575195\n",
      "epoch: 28 step: 913, loss is 0.20905569195747375\n",
      "epoch: 28 step: 914, loss is 0.2197972536087036\n",
      "epoch: 28 step: 915, loss is 0.1847551316022873\n",
      "epoch: 28 step: 916, loss is 0.2312214970588684\n",
      "epoch: 28 step: 917, loss is 0.0978768840432167\n",
      "epoch: 28 step: 918, loss is 0.0998186320066452\n",
      "epoch: 28 step: 919, loss is 0.2302713245153427\n",
      "epoch: 28 step: 920, loss is 0.2427513301372528\n",
      "epoch: 28 step: 921, loss is 0.184080109000206\n",
      "epoch: 28 step: 922, loss is 0.17376984655857086\n",
      "epoch: 28 step: 923, loss is 0.08229025453329086\n",
      "epoch: 28 step: 924, loss is 0.17566505074501038\n",
      "epoch: 28 step: 925, loss is 0.11613955348730087\n",
      "epoch: 28 step: 926, loss is 0.07561683654785156\n",
      "epoch: 28 step: 927, loss is 0.08101563900709152\n",
      "epoch: 28 step: 928, loss is 0.19480770826339722\n",
      "epoch: 28 step: 929, loss is 0.270114928483963\n",
      "epoch: 28 step: 930, loss is 0.12892721593379974\n",
      "epoch: 28 step: 931, loss is 0.16768194735050201\n",
      "epoch: 28 step: 932, loss is 0.12754619121551514\n",
      "epoch: 28 step: 933, loss is 0.17116086184978485\n",
      "epoch: 28 step: 934, loss is 0.08536425232887268\n",
      "epoch: 28 step: 935, loss is 0.08608544617891312\n",
      "epoch: 28 step: 936, loss is 0.25595054030418396\n",
      "epoch: 28 step: 937, loss is 0.07851076871156693\n",
      "epoch: 29 step: 1, loss is 0.21704840660095215\n",
      "epoch: 29 step: 2, loss is 0.14301031827926636\n",
      "epoch: 29 step: 3, loss is 0.3920811712741852\n",
      "epoch: 29 step: 4, loss is 0.17906124889850616\n",
      "epoch: 29 step: 5, loss is 0.34613195061683655\n",
      "epoch: 29 step: 6, loss is 0.11424597352743149\n",
      "epoch: 29 step: 7, loss is 0.0965462327003479\n",
      "epoch: 29 step: 8, loss is 0.1115056648850441\n",
      "epoch: 29 step: 9, loss is 0.22916831076145172\n",
      "epoch: 29 step: 10, loss is 0.19342781603336334\n",
      "epoch: 29 step: 11, loss is 0.24832792580127716\n",
      "epoch: 29 step: 12, loss is 0.2249167561531067\n",
      "epoch: 29 step: 13, loss is 0.2096334546804428\n",
      "epoch: 29 step: 14, loss is 0.17416587471961975\n",
      "epoch: 29 step: 15, loss is 0.03940083831548691\n",
      "epoch: 29 step: 16, loss is 0.22044312953948975\n",
      "epoch: 29 step: 17, loss is 0.15603721141815186\n",
      "epoch: 29 step: 18, loss is 0.15467703342437744\n",
      "epoch: 29 step: 19, loss is 0.21986055374145508\n",
      "epoch: 29 step: 20, loss is 0.20763638615608215\n",
      "epoch: 29 step: 21, loss is 0.14665758609771729\n",
      "epoch: 29 step: 22, loss is 0.1220063641667366\n",
      "epoch: 29 step: 23, loss is 0.12313598394393921\n",
      "epoch: 29 step: 24, loss is 0.16657575964927673\n",
      "epoch: 29 step: 25, loss is 0.28617680072784424\n",
      "epoch: 29 step: 26, loss is 0.10650015622377396\n",
      "epoch: 29 step: 27, loss is 0.11258906126022339\n",
      "epoch: 29 step: 28, loss is 0.09735098481178284\n",
      "epoch: 29 step: 29, loss is 0.1172592043876648\n",
      "epoch: 29 step: 30, loss is 0.25677675008773804\n",
      "epoch: 29 step: 31, loss is 0.15511126816272736\n",
      "epoch: 29 step: 32, loss is 0.16945917904376984\n",
      "epoch: 29 step: 33, loss is 0.1729646474123001\n",
      "epoch: 29 step: 34, loss is 0.31055691838264465\n",
      "epoch: 29 step: 35, loss is 0.11728766560554504\n",
      "epoch: 29 step: 36, loss is 0.1710614413022995\n",
      "epoch: 29 step: 37, loss is 0.08833888918161392\n",
      "epoch: 29 step: 38, loss is 0.3140740394592285\n",
      "epoch: 29 step: 39, loss is 0.2527841329574585\n",
      "epoch: 29 step: 40, loss is 0.1906585842370987\n",
      "epoch: 29 step: 41, loss is 0.3049214780330658\n",
      "epoch: 29 step: 42, loss is 0.2549152076244354\n",
      "epoch: 29 step: 43, loss is 0.09646900743246078\n",
      "epoch: 29 step: 44, loss is 0.13055859506130219\n",
      "epoch: 29 step: 45, loss is 0.09756337851285934\n",
      "epoch: 29 step: 46, loss is 0.3504454791545868\n",
      "epoch: 29 step: 47, loss is 0.13373921811580658\n",
      "epoch: 29 step: 48, loss is 0.12772926688194275\n",
      "epoch: 29 step: 49, loss is 0.13039369881153107\n",
      "epoch: 29 step: 50, loss is 0.04983309283852577\n",
      "epoch: 29 step: 51, loss is 0.08810784667730331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 52, loss is 0.23473280668258667\n",
      "epoch: 29 step: 53, loss is 0.45145148038864136\n",
      "epoch: 29 step: 54, loss is 0.15709343552589417\n",
      "epoch: 29 step: 55, loss is 0.21438495814800262\n",
      "epoch: 29 step: 56, loss is 0.16006052494049072\n",
      "epoch: 29 step: 57, loss is 0.3028917610645294\n",
      "epoch: 29 step: 58, loss is 0.2650129795074463\n",
      "epoch: 29 step: 59, loss is 0.12898649275302887\n",
      "epoch: 29 step: 60, loss is 0.1862933188676834\n",
      "epoch: 29 step: 61, loss is 0.06865314394235611\n",
      "epoch: 29 step: 62, loss is 0.42110833525657654\n",
      "epoch: 29 step: 63, loss is 0.2075892686843872\n",
      "epoch: 29 step: 64, loss is 0.1879858672618866\n",
      "epoch: 29 step: 65, loss is 0.15969626605510712\n",
      "epoch: 29 step: 66, loss is 0.030393268913030624\n",
      "epoch: 29 step: 67, loss is 0.19256728887557983\n",
      "epoch: 29 step: 68, loss is 0.23352192342281342\n",
      "epoch: 29 step: 69, loss is 0.29606378078460693\n",
      "epoch: 29 step: 70, loss is 0.145423024892807\n",
      "epoch: 29 step: 71, loss is 0.25870099663734436\n",
      "epoch: 29 step: 72, loss is 0.28134649991989136\n",
      "epoch: 29 step: 73, loss is 0.24351921677589417\n",
      "epoch: 29 step: 74, loss is 0.12063048779964447\n",
      "epoch: 29 step: 75, loss is 0.16495366394519806\n",
      "epoch: 29 step: 76, loss is 0.24641236662864685\n",
      "epoch: 29 step: 77, loss is 0.14610983431339264\n",
      "epoch: 29 step: 78, loss is 0.12416205555200577\n",
      "epoch: 29 step: 79, loss is 0.14300057291984558\n",
      "epoch: 29 step: 80, loss is 0.20908816158771515\n",
      "epoch: 29 step: 81, loss is 0.09253881126642227\n",
      "epoch: 29 step: 82, loss is 0.21833640336990356\n",
      "epoch: 29 step: 83, loss is 0.12320994585752487\n",
      "epoch: 29 step: 84, loss is 0.1471565216779709\n",
      "epoch: 29 step: 85, loss is 0.16280625760555267\n",
      "epoch: 29 step: 86, loss is 0.26029810309410095\n",
      "epoch: 29 step: 87, loss is 0.1888974905014038\n",
      "epoch: 29 step: 88, loss is 0.2291952669620514\n",
      "epoch: 29 step: 89, loss is 0.2243371456861496\n",
      "epoch: 29 step: 90, loss is 0.19366341829299927\n",
      "epoch: 29 step: 91, loss is 0.1241716518998146\n",
      "epoch: 29 step: 92, loss is 0.1275613158941269\n",
      "epoch: 29 step: 93, loss is 0.3816664218902588\n",
      "epoch: 29 step: 94, loss is 0.07267484813928604\n",
      "epoch: 29 step: 95, loss is 0.24796916544437408\n",
      "epoch: 29 step: 96, loss is 0.16637061536312103\n",
      "epoch: 29 step: 97, loss is 0.11774872988462448\n",
      "epoch: 29 step: 98, loss is 0.22079527378082275\n",
      "epoch: 29 step: 99, loss is 0.13564074039459229\n",
      "epoch: 29 step: 100, loss is 0.11289507150650024\n",
      "epoch: 29 step: 101, loss is 0.19834309816360474\n",
      "epoch: 29 step: 102, loss is 0.08529053628444672\n",
      "epoch: 29 step: 103, loss is 0.3430689871311188\n",
      "epoch: 29 step: 104, loss is 0.23036712408065796\n",
      "epoch: 29 step: 105, loss is 0.09951465576887131\n",
      "epoch: 29 step: 106, loss is 0.1148325577378273\n",
      "epoch: 29 step: 107, loss is 0.34525275230407715\n",
      "epoch: 29 step: 108, loss is 0.24660882353782654\n",
      "epoch: 29 step: 109, loss is 0.1831716001033783\n",
      "epoch: 29 step: 110, loss is 0.24527481198310852\n",
      "epoch: 29 step: 111, loss is 0.3216012418270111\n",
      "epoch: 29 step: 112, loss is 0.2774850130081177\n",
      "epoch: 29 step: 113, loss is 0.32604268193244934\n",
      "epoch: 29 step: 114, loss is 0.09522116929292679\n",
      "epoch: 29 step: 115, loss is 0.12570646405220032\n",
      "epoch: 29 step: 116, loss is 0.1812635064125061\n",
      "epoch: 29 step: 117, loss is 0.17182205617427826\n",
      "epoch: 29 step: 118, loss is 0.22000528872013092\n",
      "epoch: 29 step: 119, loss is 0.11129142343997955\n",
      "epoch: 29 step: 120, loss is 0.1986846923828125\n",
      "epoch: 29 step: 121, loss is 0.11278887093067169\n",
      "epoch: 29 step: 122, loss is 0.20103508234024048\n",
      "epoch: 29 step: 123, loss is 0.15209195017814636\n",
      "epoch: 29 step: 124, loss is 0.25236815214157104\n",
      "epoch: 29 step: 125, loss is 0.18210940062999725\n",
      "epoch: 29 step: 126, loss is 0.2372252196073532\n",
      "epoch: 29 step: 127, loss is 0.2890692353248596\n",
      "epoch: 29 step: 128, loss is 0.15632839500904083\n",
      "epoch: 29 step: 129, loss is 0.23978456854820251\n",
      "epoch: 29 step: 130, loss is 0.08304458111524582\n",
      "epoch: 29 step: 131, loss is 0.23213766515254974\n",
      "epoch: 29 step: 132, loss is 0.17456167936325073\n",
      "epoch: 29 step: 133, loss is 0.38396862149238586\n",
      "epoch: 29 step: 134, loss is 0.16509877145290375\n",
      "epoch: 29 step: 135, loss is 0.2120523899793625\n",
      "epoch: 29 step: 136, loss is 0.1496833860874176\n",
      "epoch: 29 step: 137, loss is 0.19524788856506348\n",
      "epoch: 29 step: 138, loss is 0.09662226587533951\n",
      "epoch: 29 step: 139, loss is 0.1133422702550888\n",
      "epoch: 29 step: 140, loss is 0.11189474910497665\n",
      "epoch: 29 step: 141, loss is 0.10103502124547958\n",
      "epoch: 29 step: 142, loss is 0.11145806312561035\n",
      "epoch: 29 step: 143, loss is 0.26072537899017334\n",
      "epoch: 29 step: 144, loss is 0.15085841715335846\n",
      "epoch: 29 step: 145, loss is 0.2830963432788849\n",
      "epoch: 29 step: 146, loss is 0.19256135821342468\n",
      "epoch: 29 step: 147, loss is 0.29663002490997314\n",
      "epoch: 29 step: 148, loss is 0.28133919835090637\n",
      "epoch: 29 step: 149, loss is 0.16445182263851166\n",
      "epoch: 29 step: 150, loss is 0.18411195278167725\n",
      "epoch: 29 step: 151, loss is 0.15222500264644623\n",
      "epoch: 29 step: 152, loss is 0.3948057293891907\n",
      "epoch: 29 step: 153, loss is 0.2521057426929474\n",
      "epoch: 29 step: 154, loss is 0.10756675153970718\n",
      "epoch: 29 step: 155, loss is 0.1563451737165451\n",
      "epoch: 29 step: 156, loss is 0.2715734839439392\n",
      "epoch: 29 step: 157, loss is 0.1428203284740448\n",
      "epoch: 29 step: 158, loss is 0.042050108313560486\n",
      "epoch: 29 step: 159, loss is 0.23816803097724915\n",
      "epoch: 29 step: 160, loss is 0.1402202695608139\n",
      "epoch: 29 step: 161, loss is 0.24122416973114014\n",
      "epoch: 29 step: 162, loss is 0.18430329859256744\n",
      "epoch: 29 step: 163, loss is 0.2868555188179016\n",
      "epoch: 29 step: 164, loss is 0.26671522855758667\n",
      "epoch: 29 step: 165, loss is 0.3601694107055664\n",
      "epoch: 29 step: 166, loss is 0.07974376529455185\n",
      "epoch: 29 step: 167, loss is 0.31592413783073425\n",
      "epoch: 29 step: 168, loss is 0.12332893162965775\n",
      "epoch: 29 step: 169, loss is 0.24388828873634338\n",
      "epoch: 29 step: 170, loss is 0.21625931560993195\n",
      "epoch: 29 step: 171, loss is 0.38571077585220337\n",
      "epoch: 29 step: 172, loss is 0.32156747579574585\n",
      "epoch: 29 step: 173, loss is 0.07893143594264984\n",
      "epoch: 29 step: 174, loss is 0.15817159414291382\n",
      "epoch: 29 step: 175, loss is 0.1772572547197342\n",
      "epoch: 29 step: 176, loss is 0.15067432820796967\n",
      "epoch: 29 step: 177, loss is 0.22428469359874725\n",
      "epoch: 29 step: 178, loss is 0.1391141265630722\n",
      "epoch: 29 step: 179, loss is 0.20889121294021606\n",
      "epoch: 29 step: 180, loss is 0.12709201872348785\n",
      "epoch: 29 step: 181, loss is 0.25162073969841003\n",
      "epoch: 29 step: 182, loss is 0.18747200071811676\n",
      "epoch: 29 step: 183, loss is 0.22689203917980194\n",
      "epoch: 29 step: 184, loss is 0.24045206606388092\n",
      "epoch: 29 step: 185, loss is 0.15606915950775146\n",
      "epoch: 29 step: 186, loss is 0.2841561436653137\n",
      "epoch: 29 step: 187, loss is 0.21909117698669434\n",
      "epoch: 29 step: 188, loss is 0.19950656592845917\n",
      "epoch: 29 step: 189, loss is 0.17078249156475067\n",
      "epoch: 29 step: 190, loss is 0.16154971718788147\n",
      "epoch: 29 step: 191, loss is 0.07240597158670425\n",
      "epoch: 29 step: 192, loss is 0.1408403217792511\n",
      "epoch: 29 step: 193, loss is 0.1811360865831375\n",
      "epoch: 29 step: 194, loss is 0.22967633605003357\n",
      "epoch: 29 step: 195, loss is 0.17130272090435028\n",
      "epoch: 29 step: 196, loss is 0.19326335191726685\n",
      "epoch: 29 step: 197, loss is 0.19463664293289185\n",
      "epoch: 29 step: 198, loss is 0.30989235639572144\n",
      "epoch: 29 step: 199, loss is 0.10301633924245834\n",
      "epoch: 29 step: 200, loss is 0.13372404873371124\n",
      "epoch: 29 step: 201, loss is 0.18718698620796204\n",
      "epoch: 29 step: 202, loss is 0.2402632087469101\n",
      "epoch: 29 step: 203, loss is 0.14569327235221863\n",
      "epoch: 29 step: 204, loss is 0.23386745154857635\n",
      "epoch: 29 step: 205, loss is 0.08675599843263626\n",
      "epoch: 29 step: 206, loss is 0.22685420513153076\n",
      "epoch: 29 step: 207, loss is 0.08890842646360397\n",
      "epoch: 29 step: 208, loss is 0.1152544617652893\n",
      "epoch: 29 step: 209, loss is 0.17337919771671295\n",
      "epoch: 29 step: 210, loss is 0.08357781171798706\n",
      "epoch: 29 step: 211, loss is 0.191414475440979\n",
      "epoch: 29 step: 212, loss is 0.09170718491077423\n",
      "epoch: 29 step: 213, loss is 0.15318237245082855\n",
      "epoch: 29 step: 214, loss is 0.19868458807468414\n",
      "epoch: 29 step: 215, loss is 0.20240309834480286\n",
      "epoch: 29 step: 216, loss is 0.09339381009340286\n",
      "epoch: 29 step: 217, loss is 0.16924840211868286\n",
      "epoch: 29 step: 218, loss is 0.1525830626487732\n",
      "epoch: 29 step: 219, loss is 0.11891452968120575\n",
      "epoch: 29 step: 220, loss is 0.14911819994449615\n",
      "epoch: 29 step: 221, loss is 0.18548789620399475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 222, loss is 0.20835529267787933\n",
      "epoch: 29 step: 223, loss is 0.14540991187095642\n",
      "epoch: 29 step: 224, loss is 0.10605456680059433\n",
      "epoch: 29 step: 225, loss is 0.1960834115743637\n",
      "epoch: 29 step: 226, loss is 0.1995401233434677\n",
      "epoch: 29 step: 227, loss is 0.09049076586961746\n",
      "epoch: 29 step: 228, loss is 0.20742182433605194\n",
      "epoch: 29 step: 229, loss is 0.22411756217479706\n",
      "epoch: 29 step: 230, loss is 0.09418772906064987\n",
      "epoch: 29 step: 231, loss is 0.3158789277076721\n",
      "epoch: 29 step: 232, loss is 0.0685841292142868\n",
      "epoch: 29 step: 233, loss is 0.13084912300109863\n",
      "epoch: 29 step: 234, loss is 0.12591375410556793\n",
      "epoch: 29 step: 235, loss is 0.17423155903816223\n",
      "epoch: 29 step: 236, loss is 0.09990929067134857\n",
      "epoch: 29 step: 237, loss is 0.15168894827365875\n",
      "epoch: 29 step: 238, loss is 0.161089688539505\n",
      "epoch: 29 step: 239, loss is 0.1261214315891266\n",
      "epoch: 29 step: 240, loss is 0.08757910132408142\n",
      "epoch: 29 step: 241, loss is 0.24751882255077362\n",
      "epoch: 29 step: 242, loss is 0.137008398771286\n",
      "epoch: 29 step: 243, loss is 0.09124656021595001\n",
      "epoch: 29 step: 244, loss is 0.20804636180400848\n",
      "epoch: 29 step: 245, loss is 0.10984157025814056\n",
      "epoch: 29 step: 246, loss is 0.11357798427343369\n",
      "epoch: 29 step: 247, loss is 0.23969019949436188\n",
      "epoch: 29 step: 248, loss is 0.2690959572792053\n",
      "epoch: 29 step: 249, loss is 0.12743042409420013\n",
      "epoch: 29 step: 250, loss is 0.16819782555103302\n",
      "epoch: 29 step: 251, loss is 0.09051822870969772\n",
      "epoch: 29 step: 252, loss is 0.06054036691784859\n",
      "epoch: 29 step: 253, loss is 0.10903201252222061\n",
      "epoch: 29 step: 254, loss is 0.39049646258354187\n",
      "epoch: 29 step: 255, loss is 0.09633100032806396\n",
      "epoch: 29 step: 256, loss is 0.18658992648124695\n",
      "epoch: 29 step: 257, loss is 0.1340668946504593\n",
      "epoch: 29 step: 258, loss is 0.04697740450501442\n",
      "epoch: 29 step: 259, loss is 0.1770874410867691\n",
      "epoch: 29 step: 260, loss is 0.15218207240104675\n",
      "epoch: 29 step: 261, loss is 0.11784063279628754\n",
      "epoch: 29 step: 262, loss is 0.2085437774658203\n",
      "epoch: 29 step: 263, loss is 0.19471050798892975\n",
      "epoch: 29 step: 264, loss is 0.12328559160232544\n",
      "epoch: 29 step: 265, loss is 0.15385399758815765\n",
      "epoch: 29 step: 266, loss is 0.1765558272600174\n",
      "epoch: 29 step: 267, loss is 0.19729648530483246\n",
      "epoch: 29 step: 268, loss is 0.2554870843887329\n",
      "epoch: 29 step: 269, loss is 0.1479741483926773\n",
      "epoch: 29 step: 270, loss is 0.103713259100914\n",
      "epoch: 29 step: 271, loss is 0.16265769302845\n",
      "epoch: 29 step: 272, loss is 0.18245020508766174\n",
      "epoch: 29 step: 273, loss is 0.15861505270004272\n",
      "epoch: 29 step: 274, loss is 0.15140953660011292\n",
      "epoch: 29 step: 275, loss is 0.10321535170078278\n",
      "epoch: 29 step: 276, loss is 0.2282189577817917\n",
      "epoch: 29 step: 277, loss is 0.14373306930065155\n",
      "epoch: 29 step: 278, loss is 0.10863819718360901\n",
      "epoch: 29 step: 279, loss is 0.35429227352142334\n",
      "epoch: 29 step: 280, loss is 0.14862728118896484\n",
      "epoch: 29 step: 281, loss is 0.19472377002239227\n",
      "epoch: 29 step: 282, loss is 0.12612465023994446\n",
      "epoch: 29 step: 283, loss is 0.24219879508018494\n",
      "epoch: 29 step: 284, loss is 0.296538770198822\n",
      "epoch: 29 step: 285, loss is 0.16832834482192993\n",
      "epoch: 29 step: 286, loss is 0.24852818250656128\n",
      "epoch: 29 step: 287, loss is 0.1626783311367035\n",
      "epoch: 29 step: 288, loss is 0.12611457705497742\n",
      "epoch: 29 step: 289, loss is 0.3781484067440033\n",
      "epoch: 29 step: 290, loss is 0.0989253968000412\n",
      "epoch: 29 step: 291, loss is 0.08892304450273514\n",
      "epoch: 29 step: 292, loss is 0.14794868230819702\n",
      "epoch: 29 step: 293, loss is 0.20716676115989685\n",
      "epoch: 29 step: 294, loss is 0.10354243218898773\n",
      "epoch: 29 step: 295, loss is 0.16675950586795807\n",
      "epoch: 29 step: 296, loss is 0.19997525215148926\n",
      "epoch: 29 step: 297, loss is 0.21696461737155914\n",
      "epoch: 29 step: 298, loss is 0.29046252369880676\n",
      "epoch: 29 step: 299, loss is 0.12879912555217743\n",
      "epoch: 29 step: 300, loss is 0.13931319117546082\n",
      "epoch: 29 step: 301, loss is 0.0718679130077362\n",
      "epoch: 29 step: 302, loss is 0.1710696518421173\n",
      "epoch: 29 step: 303, loss is 0.15943267941474915\n",
      "epoch: 29 step: 304, loss is 0.1925015151500702\n",
      "epoch: 29 step: 305, loss is 0.11491062492132187\n",
      "epoch: 29 step: 306, loss is 0.11798281222581863\n",
      "epoch: 29 step: 307, loss is 0.15577182173728943\n",
      "epoch: 29 step: 308, loss is 0.20907005667686462\n",
      "epoch: 29 step: 309, loss is 0.1558152139186859\n",
      "epoch: 29 step: 310, loss is 0.29666852951049805\n",
      "epoch: 29 step: 311, loss is 0.130083829164505\n",
      "epoch: 29 step: 312, loss is 0.1214594691991806\n",
      "epoch: 29 step: 313, loss is 0.19611312448978424\n",
      "epoch: 29 step: 314, loss is 0.19336532056331635\n",
      "epoch: 29 step: 315, loss is 0.11168985068798065\n",
      "epoch: 29 step: 316, loss is 0.2328610122203827\n",
      "epoch: 29 step: 317, loss is 0.2399284392595291\n",
      "epoch: 29 step: 318, loss is 0.13812460005283356\n",
      "epoch: 29 step: 319, loss is 0.2580433189868927\n",
      "epoch: 29 step: 320, loss is 0.3582985997200012\n",
      "epoch: 29 step: 321, loss is 0.07474974542856216\n",
      "epoch: 29 step: 322, loss is 0.29636138677597046\n",
      "epoch: 29 step: 323, loss is 0.15237775444984436\n",
      "epoch: 29 step: 324, loss is 0.08112742006778717\n",
      "epoch: 29 step: 325, loss is 0.15174439549446106\n",
      "epoch: 29 step: 326, loss is 0.11022492498159409\n",
      "epoch: 29 step: 327, loss is 0.1546216458082199\n",
      "epoch: 29 step: 328, loss is 0.3256262540817261\n",
      "epoch: 29 step: 329, loss is 0.17299944162368774\n",
      "epoch: 29 step: 330, loss is 0.20813848078250885\n",
      "epoch: 29 step: 331, loss is 0.14975854754447937\n",
      "epoch: 29 step: 332, loss is 0.28518396615982056\n",
      "epoch: 29 step: 333, loss is 0.19317315518856049\n",
      "epoch: 29 step: 334, loss is 0.10133004933595657\n",
      "epoch: 29 step: 335, loss is 0.13524679839611053\n",
      "epoch: 29 step: 336, loss is 0.17012222111225128\n",
      "epoch: 29 step: 337, loss is 0.13997089862823486\n",
      "epoch: 29 step: 338, loss is 0.4047641456127167\n",
      "epoch: 29 step: 339, loss is 0.2107672095298767\n",
      "epoch: 29 step: 340, loss is 0.24863578379154205\n",
      "epoch: 29 step: 341, loss is 0.10017801076173782\n",
      "epoch: 29 step: 342, loss is 0.16875964403152466\n",
      "epoch: 29 step: 343, loss is 0.1430080384016037\n",
      "epoch: 29 step: 344, loss is 0.38209739327430725\n",
      "epoch: 29 step: 345, loss is 0.05706682801246643\n",
      "epoch: 29 step: 346, loss is 0.20087213814258575\n",
      "epoch: 29 step: 347, loss is 0.10500820726156235\n",
      "epoch: 29 step: 348, loss is 0.2572402358055115\n",
      "epoch: 29 step: 349, loss is 0.10390494763851166\n",
      "epoch: 29 step: 350, loss is 0.16604769229888916\n",
      "epoch: 29 step: 351, loss is 0.1258973628282547\n",
      "epoch: 29 step: 352, loss is 0.2238921970129013\n",
      "epoch: 29 step: 353, loss is 0.22060413658618927\n",
      "epoch: 29 step: 354, loss is 0.11628088355064392\n",
      "epoch: 29 step: 355, loss is 0.23645195364952087\n",
      "epoch: 29 step: 356, loss is 0.10992113500833511\n",
      "epoch: 29 step: 357, loss is 0.0952683612704277\n",
      "epoch: 29 step: 358, loss is 0.11989792436361313\n",
      "epoch: 29 step: 359, loss is 0.11656737327575684\n",
      "epoch: 29 step: 360, loss is 0.24849557876586914\n",
      "epoch: 29 step: 361, loss is 0.18321895599365234\n",
      "epoch: 29 step: 362, loss is 0.2865704596042633\n",
      "epoch: 29 step: 363, loss is 0.194977805018425\n",
      "epoch: 29 step: 364, loss is 0.27637845277786255\n",
      "epoch: 29 step: 365, loss is 0.2293984442949295\n",
      "epoch: 29 step: 366, loss is 0.16804127395153046\n",
      "epoch: 29 step: 367, loss is 0.21328067779541016\n",
      "epoch: 29 step: 368, loss is 0.1100524291396141\n",
      "epoch: 29 step: 369, loss is 0.08833518624305725\n",
      "epoch: 29 step: 370, loss is 0.3161875903606415\n",
      "epoch: 29 step: 371, loss is 0.29591917991638184\n",
      "epoch: 29 step: 372, loss is 0.1640186905860901\n",
      "epoch: 29 step: 373, loss is 0.14005061984062195\n",
      "epoch: 29 step: 374, loss is 0.16835442185401917\n",
      "epoch: 29 step: 375, loss is 0.21690569818019867\n",
      "epoch: 29 step: 376, loss is 0.153135284781456\n",
      "epoch: 29 step: 377, loss is 0.11845703423023224\n",
      "epoch: 29 step: 378, loss is 0.18314391374588013\n",
      "epoch: 29 step: 379, loss is 0.28956374526023865\n",
      "epoch: 29 step: 380, loss is 0.17770135402679443\n",
      "epoch: 29 step: 381, loss is 0.13755464553833008\n",
      "epoch: 29 step: 382, loss is 0.1866081953048706\n",
      "epoch: 29 step: 383, loss is 0.2782307267189026\n",
      "epoch: 29 step: 384, loss is 0.37572816014289856\n",
      "epoch: 29 step: 385, loss is 0.49856245517730713\n",
      "epoch: 29 step: 386, loss is 0.15826335549354553\n",
      "epoch: 29 step: 387, loss is 0.16289736330509186\n",
      "epoch: 29 step: 388, loss is 0.19768230617046356\n",
      "epoch: 29 step: 389, loss is 0.06753592193126678\n",
      "epoch: 29 step: 390, loss is 0.36694198846817017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 391, loss is 0.19862397015094757\n",
      "epoch: 29 step: 392, loss is 0.23486876487731934\n",
      "epoch: 29 step: 393, loss is 0.2199545055627823\n",
      "epoch: 29 step: 394, loss is 0.24897874891757965\n",
      "epoch: 29 step: 395, loss is 0.16108766198158264\n",
      "epoch: 29 step: 396, loss is 0.08257444947957993\n",
      "epoch: 29 step: 397, loss is 0.07266468554735184\n",
      "epoch: 29 step: 398, loss is 0.1932956874370575\n",
      "epoch: 29 step: 399, loss is 0.13284772634506226\n",
      "epoch: 29 step: 400, loss is 0.13195930421352386\n",
      "epoch: 29 step: 401, loss is 0.26557597517967224\n",
      "epoch: 29 step: 402, loss is 0.11277078092098236\n",
      "epoch: 29 step: 403, loss is 0.1799292117357254\n",
      "epoch: 29 step: 404, loss is 0.15565067529678345\n",
      "epoch: 29 step: 405, loss is 0.2159615010023117\n",
      "epoch: 29 step: 406, loss is 0.19074217975139618\n",
      "epoch: 29 step: 407, loss is 0.12632453441619873\n",
      "epoch: 29 step: 408, loss is 0.1892567276954651\n",
      "epoch: 29 step: 409, loss is 0.2607806622982025\n",
      "epoch: 29 step: 410, loss is 0.16382598876953125\n",
      "epoch: 29 step: 411, loss is 0.22567346692085266\n",
      "epoch: 29 step: 412, loss is 0.28807902336120605\n",
      "epoch: 29 step: 413, loss is 0.21603327989578247\n",
      "epoch: 29 step: 414, loss is 0.20291845500469208\n",
      "epoch: 29 step: 415, loss is 0.136783629655838\n",
      "epoch: 29 step: 416, loss is 0.20462819933891296\n",
      "epoch: 29 step: 417, loss is 0.23963011801242828\n",
      "epoch: 29 step: 418, loss is 0.14474236965179443\n",
      "epoch: 29 step: 419, loss is 0.143575057387352\n",
      "epoch: 29 step: 420, loss is 0.14003995060920715\n",
      "epoch: 29 step: 421, loss is 0.08359470218420029\n",
      "epoch: 29 step: 422, loss is 0.24835482239723206\n",
      "epoch: 29 step: 423, loss is 0.11046498268842697\n",
      "epoch: 29 step: 424, loss is 0.2585403323173523\n",
      "epoch: 29 step: 425, loss is 0.2874247133731842\n",
      "epoch: 29 step: 426, loss is 0.23850783705711365\n",
      "epoch: 29 step: 427, loss is 0.24991533160209656\n",
      "epoch: 29 step: 428, loss is 0.3118375837802887\n",
      "epoch: 29 step: 429, loss is 0.17897628247737885\n",
      "epoch: 29 step: 430, loss is 0.11050084233283997\n",
      "epoch: 29 step: 431, loss is 0.11617579311132431\n",
      "epoch: 29 step: 432, loss is 0.11834360659122467\n",
      "epoch: 29 step: 433, loss is 0.07580067962408066\n",
      "epoch: 29 step: 434, loss is 0.1402525007724762\n",
      "epoch: 29 step: 435, loss is 0.18984831869602203\n",
      "epoch: 29 step: 436, loss is 0.20363344252109528\n",
      "epoch: 29 step: 437, loss is 0.16460050642490387\n",
      "epoch: 29 step: 438, loss is 0.14012333750724792\n",
      "epoch: 29 step: 439, loss is 0.19271238148212433\n",
      "epoch: 29 step: 440, loss is 0.18030327558517456\n",
      "epoch: 29 step: 441, loss is 0.1548381894826889\n",
      "epoch: 29 step: 442, loss is 0.23152874410152435\n",
      "epoch: 29 step: 443, loss is 0.2716551721096039\n",
      "epoch: 29 step: 444, loss is 0.09012682735919952\n",
      "epoch: 29 step: 445, loss is 0.1526092141866684\n",
      "epoch: 29 step: 446, loss is 0.1136874184012413\n",
      "epoch: 29 step: 447, loss is 0.2223702222108841\n",
      "epoch: 29 step: 448, loss is 0.1754312515258789\n",
      "epoch: 29 step: 449, loss is 0.1835682988166809\n",
      "epoch: 29 step: 450, loss is 0.12610577046871185\n",
      "epoch: 29 step: 451, loss is 0.11010803282260895\n",
      "epoch: 29 step: 452, loss is 0.13814441859722137\n",
      "epoch: 29 step: 453, loss is 0.2317706197500229\n",
      "epoch: 29 step: 454, loss is 0.18729203939437866\n",
      "epoch: 29 step: 455, loss is 0.16971759498119354\n",
      "epoch: 29 step: 456, loss is 0.1600424200296402\n",
      "epoch: 29 step: 457, loss is 0.09548579156398773\n",
      "epoch: 29 step: 458, loss is 0.16312570869922638\n",
      "epoch: 29 step: 459, loss is 0.1856069564819336\n",
      "epoch: 29 step: 460, loss is 0.19572530686855316\n",
      "epoch: 29 step: 461, loss is 0.11497575789690018\n",
      "epoch: 29 step: 462, loss is 0.2297048270702362\n",
      "epoch: 29 step: 463, loss is 0.14495845139026642\n",
      "epoch: 29 step: 464, loss is 0.10625007748603821\n",
      "epoch: 29 step: 465, loss is 0.0748923048377037\n",
      "epoch: 29 step: 466, loss is 0.09484416991472244\n",
      "epoch: 29 step: 467, loss is 0.09187682718038559\n",
      "epoch: 29 step: 468, loss is 0.18216688930988312\n",
      "epoch: 29 step: 469, loss is 0.16215546429157257\n",
      "epoch: 29 step: 470, loss is 0.19771166145801544\n",
      "epoch: 29 step: 471, loss is 0.14530588686466217\n",
      "epoch: 29 step: 472, loss is 0.1380767971277237\n",
      "epoch: 29 step: 473, loss is 0.24489456415176392\n",
      "epoch: 29 step: 474, loss is 0.07940880209207535\n",
      "epoch: 29 step: 475, loss is 0.12731918692588806\n",
      "epoch: 29 step: 476, loss is 0.03989406302571297\n",
      "epoch: 29 step: 477, loss is 0.19951589405536652\n",
      "epoch: 29 step: 478, loss is 0.3466421067714691\n",
      "epoch: 29 step: 479, loss is 0.19404220581054688\n",
      "epoch: 29 step: 480, loss is 0.3205879032611847\n",
      "epoch: 29 step: 481, loss is 0.18919861316680908\n",
      "epoch: 29 step: 482, loss is 0.1208324283361435\n",
      "epoch: 29 step: 483, loss is 0.13610303401947021\n",
      "epoch: 29 step: 484, loss is 0.23639275133609772\n",
      "epoch: 29 step: 485, loss is 0.1911081075668335\n",
      "epoch: 29 step: 486, loss is 0.1755477637052536\n",
      "epoch: 29 step: 487, loss is 0.12448575347661972\n",
      "epoch: 29 step: 488, loss is 0.12838760018348694\n",
      "epoch: 29 step: 489, loss is 0.2091725468635559\n",
      "epoch: 29 step: 490, loss is 0.21446706354618073\n",
      "epoch: 29 step: 491, loss is 0.26920071244239807\n",
      "epoch: 29 step: 492, loss is 0.07177956402301788\n",
      "epoch: 29 step: 493, loss is 0.37533414363861084\n",
      "epoch: 29 step: 494, loss is 0.09499300271272659\n",
      "epoch: 29 step: 495, loss is 0.24932141602039337\n",
      "epoch: 29 step: 496, loss is 0.20295830070972443\n",
      "epoch: 29 step: 497, loss is 0.15004754066467285\n",
      "epoch: 29 step: 498, loss is 0.1105373427271843\n",
      "epoch: 29 step: 499, loss is 0.1777588427066803\n",
      "epoch: 29 step: 500, loss is 0.1400134116411209\n",
      "epoch: 29 step: 501, loss is 0.20908673107624054\n",
      "epoch: 29 step: 502, loss is 0.15528230369091034\n",
      "epoch: 29 step: 503, loss is 0.12781867384910583\n",
      "epoch: 29 step: 504, loss is 0.2559104859828949\n",
      "epoch: 29 step: 505, loss is 0.10133693367242813\n",
      "epoch: 29 step: 506, loss is 0.058470115065574646\n",
      "epoch: 29 step: 507, loss is 0.2706361413002014\n",
      "epoch: 29 step: 508, loss is 0.12448068708181381\n",
      "epoch: 29 step: 509, loss is 0.06014406308531761\n",
      "epoch: 29 step: 510, loss is 0.22352302074432373\n",
      "epoch: 29 step: 511, loss is 0.2536257803440094\n",
      "epoch: 29 step: 512, loss is 0.20624315738677979\n",
      "epoch: 29 step: 513, loss is 0.22314895689487457\n",
      "epoch: 29 step: 514, loss is 0.15142959356307983\n",
      "epoch: 29 step: 515, loss is 0.21854467689990997\n",
      "epoch: 29 step: 516, loss is 0.08253034949302673\n",
      "epoch: 29 step: 517, loss is 0.1401437371969223\n",
      "epoch: 29 step: 518, loss is 0.24583700299263\n",
      "epoch: 29 step: 519, loss is 0.4404117166996002\n",
      "epoch: 29 step: 520, loss is 0.13916663825511932\n",
      "epoch: 29 step: 521, loss is 0.10473940521478653\n",
      "epoch: 29 step: 522, loss is 0.21085847914218903\n",
      "epoch: 29 step: 523, loss is 0.11403539776802063\n",
      "epoch: 29 step: 524, loss is 0.2213287204504013\n",
      "epoch: 29 step: 525, loss is 0.193857803940773\n",
      "epoch: 29 step: 526, loss is 0.3273177444934845\n",
      "epoch: 29 step: 527, loss is 0.10614971816539764\n",
      "epoch: 29 step: 528, loss is 0.08302712440490723\n",
      "epoch: 29 step: 529, loss is 0.2409982532262802\n",
      "epoch: 29 step: 530, loss is 0.17020700871944427\n",
      "epoch: 29 step: 531, loss is 0.10985963046550751\n",
      "epoch: 29 step: 532, loss is 0.13532032072544098\n",
      "epoch: 29 step: 533, loss is 0.13026973605155945\n",
      "epoch: 29 step: 534, loss is 0.19149640202522278\n",
      "epoch: 29 step: 535, loss is 0.23657362163066864\n",
      "epoch: 29 step: 536, loss is 0.15672805905342102\n",
      "epoch: 29 step: 537, loss is 0.1470143049955368\n",
      "epoch: 29 step: 538, loss is 0.08371637761592865\n",
      "epoch: 29 step: 539, loss is 0.19921132922172546\n",
      "epoch: 29 step: 540, loss is 0.42210623621940613\n",
      "epoch: 29 step: 541, loss is 0.23965416848659515\n",
      "epoch: 29 step: 542, loss is 0.3784237802028656\n",
      "epoch: 29 step: 543, loss is 0.0883103683590889\n",
      "epoch: 29 step: 544, loss is 0.2746625542640686\n",
      "epoch: 29 step: 545, loss is 0.05359255149960518\n",
      "epoch: 29 step: 546, loss is 0.08881299942731857\n",
      "epoch: 29 step: 547, loss is 0.21232180297374725\n",
      "epoch: 29 step: 548, loss is 0.1259153038263321\n",
      "epoch: 29 step: 549, loss is 0.14846171438694\n",
      "epoch: 29 step: 550, loss is 0.15120035409927368\n",
      "epoch: 29 step: 551, loss is 0.243621826171875\n",
      "epoch: 29 step: 552, loss is 0.21868276596069336\n",
      "epoch: 29 step: 553, loss is 0.17320923507213593\n",
      "epoch: 29 step: 554, loss is 0.38023507595062256\n",
      "epoch: 29 step: 555, loss is 0.23517994582653046\n",
      "epoch: 29 step: 556, loss is 0.16549654304981232\n",
      "epoch: 29 step: 557, loss is 0.31129375100135803\n",
      "epoch: 29 step: 558, loss is 0.24789434671401978\n",
      "epoch: 29 step: 559, loss is 0.19522757828235626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 560, loss is 0.16242238879203796\n",
      "epoch: 29 step: 561, loss is 0.17299625277519226\n",
      "epoch: 29 step: 562, loss is 0.17357514798641205\n",
      "epoch: 29 step: 563, loss is 0.1732015162706375\n",
      "epoch: 29 step: 564, loss is 0.09922537952661514\n",
      "epoch: 29 step: 565, loss is 0.12132004648447037\n",
      "epoch: 29 step: 566, loss is 0.34008580446243286\n",
      "epoch: 29 step: 567, loss is 0.17489409446716309\n",
      "epoch: 29 step: 568, loss is 0.26822176575660706\n",
      "epoch: 29 step: 569, loss is 0.13104434311389923\n",
      "epoch: 29 step: 570, loss is 0.1386292576789856\n",
      "epoch: 29 step: 571, loss is 0.1515514850616455\n",
      "epoch: 29 step: 572, loss is 0.2923068106174469\n",
      "epoch: 29 step: 573, loss is 0.14182020723819733\n",
      "epoch: 29 step: 574, loss is 0.15340346097946167\n",
      "epoch: 29 step: 575, loss is 0.22193722426891327\n",
      "epoch: 29 step: 576, loss is 0.11308539658784866\n",
      "epoch: 29 step: 577, loss is 0.10322178155183792\n",
      "epoch: 29 step: 578, loss is 0.20151196420192719\n",
      "epoch: 29 step: 579, loss is 0.19724926352500916\n",
      "epoch: 29 step: 580, loss is 0.1945745348930359\n",
      "epoch: 29 step: 581, loss is 0.12454984337091446\n",
      "epoch: 29 step: 582, loss is 0.17254668474197388\n",
      "epoch: 29 step: 583, loss is 0.20555049180984497\n",
      "epoch: 29 step: 584, loss is 0.12426850944757462\n",
      "epoch: 29 step: 585, loss is 0.2718978226184845\n",
      "epoch: 29 step: 586, loss is 0.3597855269908905\n",
      "epoch: 29 step: 587, loss is 0.1514621376991272\n",
      "epoch: 29 step: 588, loss is 0.17209365963935852\n",
      "epoch: 29 step: 589, loss is 0.2708718776702881\n",
      "epoch: 29 step: 590, loss is 0.3223422169685364\n",
      "epoch: 29 step: 591, loss is 0.25868672132492065\n",
      "epoch: 29 step: 592, loss is 0.13200467824935913\n",
      "epoch: 29 step: 593, loss is 0.2961428165435791\n",
      "epoch: 29 step: 594, loss is 0.26267650723457336\n",
      "epoch: 29 step: 595, loss is 0.09720155596733093\n",
      "epoch: 29 step: 596, loss is 0.21888993680477142\n",
      "epoch: 29 step: 597, loss is 0.14274907112121582\n",
      "epoch: 29 step: 598, loss is 0.2052801251411438\n",
      "epoch: 29 step: 599, loss is 0.12013059854507446\n",
      "epoch: 29 step: 600, loss is 0.21632374823093414\n",
      "epoch: 29 step: 601, loss is 0.17794384062290192\n",
      "epoch: 29 step: 602, loss is 0.2801329493522644\n",
      "epoch: 29 step: 603, loss is 0.046944521367549896\n",
      "epoch: 29 step: 604, loss is 0.12394412606954575\n",
      "epoch: 29 step: 605, loss is 0.16707447171211243\n",
      "epoch: 29 step: 606, loss is 0.11525354534387589\n",
      "epoch: 29 step: 607, loss is 0.2666621208190918\n",
      "epoch: 29 step: 608, loss is 0.2638712227344513\n",
      "epoch: 29 step: 609, loss is 0.3066910207271576\n",
      "epoch: 29 step: 610, loss is 0.2751207649707794\n",
      "epoch: 29 step: 611, loss is 0.13892701268196106\n",
      "epoch: 29 step: 612, loss is 0.13783837854862213\n",
      "epoch: 29 step: 613, loss is 0.0689278170466423\n",
      "epoch: 29 step: 614, loss is 0.20134106278419495\n",
      "epoch: 29 step: 615, loss is 0.14373308420181274\n",
      "epoch: 29 step: 616, loss is 0.37059730291366577\n",
      "epoch: 29 step: 617, loss is 0.24682100117206573\n",
      "epoch: 29 step: 618, loss is 0.6633057594299316\n",
      "epoch: 29 step: 619, loss is 0.2912892699241638\n",
      "epoch: 29 step: 620, loss is 0.08073138445615768\n",
      "epoch: 29 step: 621, loss is 0.1936335414648056\n",
      "epoch: 29 step: 622, loss is 0.08380517363548279\n",
      "epoch: 29 step: 623, loss is 0.12698785960674286\n",
      "epoch: 29 step: 624, loss is 0.3738746643066406\n",
      "epoch: 29 step: 625, loss is 0.0953788086771965\n",
      "epoch: 29 step: 626, loss is 0.12269110977649689\n",
      "epoch: 29 step: 627, loss is 0.2335749715566635\n",
      "epoch: 29 step: 628, loss is 0.19233134388923645\n",
      "epoch: 29 step: 629, loss is 0.48390957713127136\n",
      "epoch: 29 step: 630, loss is 0.2571525275707245\n",
      "epoch: 29 step: 631, loss is 0.3053995668888092\n",
      "epoch: 29 step: 632, loss is 0.18872089684009552\n",
      "epoch: 29 step: 633, loss is 0.12140379846096039\n",
      "epoch: 29 step: 634, loss is 0.17447274923324585\n",
      "epoch: 29 step: 635, loss is 0.27429088950157166\n",
      "epoch: 29 step: 636, loss is 0.16882160305976868\n",
      "epoch: 29 step: 637, loss is 0.3239474594593048\n",
      "epoch: 29 step: 638, loss is 0.3086530864238739\n",
      "epoch: 29 step: 639, loss is 0.2578904926776886\n",
      "epoch: 29 step: 640, loss is 0.1606740951538086\n",
      "epoch: 29 step: 641, loss is 0.12690769135951996\n",
      "epoch: 29 step: 642, loss is 0.1830572485923767\n",
      "epoch: 29 step: 643, loss is 0.1891268789768219\n",
      "epoch: 29 step: 644, loss is 0.25042980909347534\n",
      "epoch: 29 step: 645, loss is 0.1672886610031128\n",
      "epoch: 29 step: 646, loss is 0.16849838197231293\n",
      "epoch: 29 step: 647, loss is 0.07127691805362701\n",
      "epoch: 29 step: 648, loss is 0.1860503852367401\n",
      "epoch: 29 step: 649, loss is 0.2042495608329773\n",
      "epoch: 29 step: 650, loss is 0.1624670773744583\n",
      "epoch: 29 step: 651, loss is 0.275117963552475\n",
      "epoch: 29 step: 652, loss is 0.09666387736797333\n",
      "epoch: 29 step: 653, loss is 0.1860053539276123\n",
      "epoch: 29 step: 654, loss is 0.07507387548685074\n",
      "epoch: 29 step: 655, loss is 0.20713602006435394\n",
      "epoch: 29 step: 656, loss is 0.1592746078968048\n",
      "epoch: 29 step: 657, loss is 0.11680950224399567\n",
      "epoch: 29 step: 658, loss is 0.21551980078220367\n",
      "epoch: 29 step: 659, loss is 0.15458419919013977\n",
      "epoch: 29 step: 660, loss is 0.269641637802124\n",
      "epoch: 29 step: 661, loss is 0.1632416993379593\n",
      "epoch: 29 step: 662, loss is 0.12773291766643524\n",
      "epoch: 29 step: 663, loss is 0.15255790948867798\n",
      "epoch: 29 step: 664, loss is 0.2769968509674072\n",
      "epoch: 29 step: 665, loss is 0.19296064972877502\n",
      "epoch: 29 step: 666, loss is 0.187126025557518\n",
      "epoch: 29 step: 667, loss is 0.22522248327732086\n",
      "epoch: 29 step: 668, loss is 0.08400893211364746\n",
      "epoch: 29 step: 669, loss is 0.21144267916679382\n",
      "epoch: 29 step: 670, loss is 0.2544361650943756\n",
      "epoch: 29 step: 671, loss is 0.22282864153385162\n",
      "epoch: 29 step: 672, loss is 0.14593419432640076\n",
      "epoch: 29 step: 673, loss is 0.16796812415122986\n",
      "epoch: 29 step: 674, loss is 0.12888766825199127\n",
      "epoch: 29 step: 675, loss is 0.3025875687599182\n",
      "epoch: 29 step: 676, loss is 0.20529022812843323\n",
      "epoch: 29 step: 677, loss is 0.11704949289560318\n",
      "epoch: 29 step: 678, loss is 0.24184781312942505\n",
      "epoch: 29 step: 679, loss is 0.16693632304668427\n",
      "epoch: 29 step: 680, loss is 0.17237181961536407\n",
      "epoch: 29 step: 681, loss is 0.10802951455116272\n",
      "epoch: 29 step: 682, loss is 0.1754925698041916\n",
      "epoch: 29 step: 683, loss is 0.27048105001449585\n",
      "epoch: 29 step: 684, loss is 0.2633720934391022\n",
      "epoch: 29 step: 685, loss is 0.18564343452453613\n",
      "epoch: 29 step: 686, loss is 0.07079805433750153\n",
      "epoch: 29 step: 687, loss is 0.2600858211517334\n",
      "epoch: 29 step: 688, loss is 0.2655498683452606\n",
      "epoch: 29 step: 689, loss is 0.11060485243797302\n",
      "epoch: 29 step: 690, loss is 0.09572775661945343\n",
      "epoch: 29 step: 691, loss is 0.17872317135334015\n",
      "epoch: 29 step: 692, loss is 0.19715477526187897\n",
      "epoch: 29 step: 693, loss is 0.2752242386341095\n",
      "epoch: 29 step: 694, loss is 0.23115748167037964\n",
      "epoch: 29 step: 695, loss is 0.3003932535648346\n",
      "epoch: 29 step: 696, loss is 0.22078579664230347\n",
      "epoch: 29 step: 697, loss is 0.13549001514911652\n",
      "epoch: 29 step: 698, loss is 0.18044699728488922\n",
      "epoch: 29 step: 699, loss is 0.1476643979549408\n",
      "epoch: 29 step: 700, loss is 0.10275020450353622\n",
      "epoch: 29 step: 701, loss is 0.17822447419166565\n",
      "epoch: 29 step: 702, loss is 0.10961556434631348\n",
      "epoch: 29 step: 703, loss is 0.1362786442041397\n",
      "epoch: 29 step: 704, loss is 0.159647136926651\n",
      "epoch: 29 step: 705, loss is 0.09164540469646454\n",
      "epoch: 29 step: 706, loss is 0.10094130039215088\n",
      "epoch: 29 step: 707, loss is 0.16490896046161652\n",
      "epoch: 29 step: 708, loss is 0.17287345230579376\n",
      "epoch: 29 step: 709, loss is 0.28790315985679626\n",
      "epoch: 29 step: 710, loss is 0.1789253056049347\n",
      "epoch: 29 step: 711, loss is 0.14914891123771667\n",
      "epoch: 29 step: 712, loss is 0.28232887387275696\n",
      "epoch: 29 step: 713, loss is 0.06806428730487823\n",
      "epoch: 29 step: 714, loss is 0.16642087697982788\n",
      "epoch: 29 step: 715, loss is 0.16638512909412384\n",
      "epoch: 29 step: 716, loss is 0.08550379425287247\n",
      "epoch: 29 step: 717, loss is 0.21312925219535828\n",
      "epoch: 29 step: 718, loss is 0.15321259200572968\n",
      "epoch: 29 step: 719, loss is 0.16555927693843842\n",
      "epoch: 29 step: 720, loss is 0.13795709609985352\n",
      "epoch: 29 step: 721, loss is 0.16954073309898376\n",
      "epoch: 29 step: 722, loss is 0.1693211942911148\n",
      "epoch: 29 step: 723, loss is 0.14710070192813873\n",
      "epoch: 29 step: 724, loss is 0.2544631361961365\n",
      "epoch: 29 step: 725, loss is 0.14480093121528625\n",
      "epoch: 29 step: 726, loss is 0.09431000798940659\n",
      "epoch: 29 step: 727, loss is 0.11081387102603912\n",
      "epoch: 29 step: 728, loss is 0.17203302681446075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 729, loss is 0.15490978956222534\n",
      "epoch: 29 step: 730, loss is 0.2966940104961395\n",
      "epoch: 29 step: 731, loss is 0.1147289127111435\n",
      "epoch: 29 step: 732, loss is 0.14434555172920227\n",
      "epoch: 29 step: 733, loss is 0.22466669976711273\n",
      "epoch: 29 step: 734, loss is 0.26713404059410095\n",
      "epoch: 29 step: 735, loss is 0.14175239205360413\n",
      "epoch: 29 step: 736, loss is 0.1639576405286789\n",
      "epoch: 29 step: 737, loss is 0.1524723619222641\n",
      "epoch: 29 step: 738, loss is 0.14130276441574097\n",
      "epoch: 29 step: 739, loss is 0.11989231407642365\n",
      "epoch: 29 step: 740, loss is 0.32562100887298584\n",
      "epoch: 29 step: 741, loss is 0.099363312125206\n",
      "epoch: 29 step: 742, loss is 0.11354336142539978\n",
      "epoch: 29 step: 743, loss is 0.20150873064994812\n",
      "epoch: 29 step: 744, loss is 0.23042850196361542\n",
      "epoch: 29 step: 745, loss is 0.11043336242437363\n",
      "epoch: 29 step: 746, loss is 0.1772139072418213\n",
      "epoch: 29 step: 747, loss is 0.21138063073158264\n",
      "epoch: 29 step: 748, loss is 0.1167592778801918\n",
      "epoch: 29 step: 749, loss is 0.22535210847854614\n",
      "epoch: 29 step: 750, loss is 0.1185959205031395\n",
      "epoch: 29 step: 751, loss is 0.18617333471775055\n",
      "epoch: 29 step: 752, loss is 0.0880691409111023\n",
      "epoch: 29 step: 753, loss is 0.2399761974811554\n",
      "epoch: 29 step: 754, loss is 0.1523125171661377\n",
      "epoch: 29 step: 755, loss is 0.18930335342884064\n",
      "epoch: 29 step: 756, loss is 0.06363394111394882\n",
      "epoch: 29 step: 757, loss is 0.07129829376935959\n",
      "epoch: 29 step: 758, loss is 0.19526754319667816\n",
      "epoch: 29 step: 759, loss is 0.20264890789985657\n",
      "epoch: 29 step: 760, loss is 0.07754392921924591\n",
      "epoch: 29 step: 761, loss is 0.13137118518352509\n",
      "epoch: 29 step: 762, loss is 0.11373788863420486\n",
      "epoch: 29 step: 763, loss is 0.18587031960487366\n",
      "epoch: 29 step: 764, loss is 0.20906946063041687\n",
      "epoch: 29 step: 765, loss is 0.21249611675739288\n",
      "epoch: 29 step: 766, loss is 0.18044185638427734\n",
      "epoch: 29 step: 767, loss is 0.318206250667572\n",
      "epoch: 29 step: 768, loss is 0.14373014867305756\n",
      "epoch: 29 step: 769, loss is 0.225908100605011\n",
      "epoch: 29 step: 770, loss is 0.20294128358364105\n",
      "epoch: 29 step: 771, loss is 0.3413950800895691\n",
      "epoch: 29 step: 772, loss is 0.2930442690849304\n",
      "epoch: 29 step: 773, loss is 0.14108018577098846\n",
      "epoch: 29 step: 774, loss is 0.15208718180656433\n",
      "epoch: 29 step: 775, loss is 0.2386186122894287\n",
      "epoch: 29 step: 776, loss is 0.25826403498649597\n",
      "epoch: 29 step: 777, loss is 0.29477208852767944\n",
      "epoch: 29 step: 778, loss is 0.2550428509712219\n",
      "epoch: 29 step: 779, loss is 0.11014177650213242\n",
      "epoch: 29 step: 780, loss is 0.05896501988172531\n",
      "epoch: 29 step: 781, loss is 0.12233934551477432\n",
      "epoch: 29 step: 782, loss is 0.2165808528661728\n",
      "epoch: 29 step: 783, loss is 0.28172388672828674\n",
      "epoch: 29 step: 784, loss is 0.2544713020324707\n",
      "epoch: 29 step: 785, loss is 0.15105290710926056\n",
      "epoch: 29 step: 786, loss is 0.12197525799274445\n",
      "epoch: 29 step: 787, loss is 0.20777727663516998\n",
      "epoch: 29 step: 788, loss is 0.22650711238384247\n",
      "epoch: 29 step: 789, loss is 0.19031694531440735\n",
      "epoch: 29 step: 790, loss is 0.21363410353660583\n",
      "epoch: 29 step: 791, loss is 0.15024669468402863\n",
      "epoch: 29 step: 792, loss is 0.23545509576797485\n",
      "epoch: 29 step: 793, loss is 0.21257813274860382\n",
      "epoch: 29 step: 794, loss is 0.12965048849582672\n",
      "epoch: 29 step: 795, loss is 0.1944468766450882\n",
      "epoch: 29 step: 796, loss is 0.08528263121843338\n",
      "epoch: 29 step: 797, loss is 0.10506310313940048\n",
      "epoch: 29 step: 798, loss is 0.1242574080824852\n",
      "epoch: 29 step: 799, loss is 0.26806649565696716\n",
      "epoch: 29 step: 800, loss is 0.15352390706539154\n",
      "epoch: 29 step: 801, loss is 0.19426129758358002\n",
      "epoch: 29 step: 802, loss is 0.1789640337228775\n",
      "epoch: 29 step: 803, loss is 0.3409406542778015\n",
      "epoch: 29 step: 804, loss is 0.18211768567562103\n",
      "epoch: 29 step: 805, loss is 0.13362537324428558\n",
      "epoch: 29 step: 806, loss is 0.2532467544078827\n",
      "epoch: 29 step: 807, loss is 0.2874484360218048\n",
      "epoch: 29 step: 808, loss is 0.1681642085313797\n",
      "epoch: 29 step: 809, loss is 0.11426891386508942\n",
      "epoch: 29 step: 810, loss is 0.2246275097131729\n",
      "epoch: 29 step: 811, loss is 0.13545647263526917\n",
      "epoch: 29 step: 812, loss is 0.13878929615020752\n",
      "epoch: 29 step: 813, loss is 0.04961888864636421\n",
      "epoch: 29 step: 814, loss is 0.16788247227668762\n",
      "epoch: 29 step: 815, loss is 0.15232068300247192\n",
      "epoch: 29 step: 816, loss is 0.17378008365631104\n",
      "epoch: 29 step: 817, loss is 0.23506970703601837\n",
      "epoch: 29 step: 818, loss is 0.1579149067401886\n",
      "epoch: 29 step: 819, loss is 0.18571577966213226\n",
      "epoch: 29 step: 820, loss is 0.12829738855361938\n",
      "epoch: 29 step: 821, loss is 0.37616389989852905\n",
      "epoch: 29 step: 822, loss is 0.07824823260307312\n",
      "epoch: 29 step: 823, loss is 0.0680423453450203\n",
      "epoch: 29 step: 824, loss is 0.15719455480575562\n",
      "epoch: 29 step: 825, loss is 0.2902330756187439\n",
      "epoch: 29 step: 826, loss is 0.2938050925731659\n",
      "epoch: 29 step: 827, loss is 0.1805647760629654\n",
      "epoch: 29 step: 828, loss is 0.2926478683948517\n",
      "epoch: 29 step: 829, loss is 0.16033343970775604\n",
      "epoch: 29 step: 830, loss is 0.11315140873193741\n",
      "epoch: 29 step: 831, loss is 0.12750457227230072\n",
      "epoch: 29 step: 832, loss is 0.1467430591583252\n",
      "epoch: 29 step: 833, loss is 0.05893445760011673\n",
      "epoch: 29 step: 834, loss is 0.3178390562534332\n",
      "epoch: 29 step: 835, loss is 0.2026725560426712\n",
      "epoch: 29 step: 836, loss is 0.17351286113262177\n",
      "epoch: 29 step: 837, loss is 0.2890797257423401\n",
      "epoch: 29 step: 838, loss is 0.07101484388113022\n",
      "epoch: 29 step: 839, loss is 0.12727396190166473\n",
      "epoch: 29 step: 840, loss is 0.21089932322502136\n",
      "epoch: 29 step: 841, loss is 0.2684456408023834\n",
      "epoch: 29 step: 842, loss is 0.09551388770341873\n",
      "epoch: 29 step: 843, loss is 0.22458219528198242\n",
      "epoch: 29 step: 844, loss is 0.09075020998716354\n",
      "epoch: 29 step: 845, loss is 0.22317500412464142\n",
      "epoch: 29 step: 846, loss is 0.18737539649009705\n",
      "epoch: 29 step: 847, loss is 0.16979670524597168\n",
      "epoch: 29 step: 848, loss is 0.1490573287010193\n",
      "epoch: 29 step: 849, loss is 0.055453233420848846\n",
      "epoch: 29 step: 850, loss is 0.17726384103298187\n",
      "epoch: 29 step: 851, loss is 0.2381753921508789\n",
      "epoch: 29 step: 852, loss is 0.3080945611000061\n",
      "epoch: 29 step: 853, loss is 0.29731783270835876\n",
      "epoch: 29 step: 854, loss is 0.16421706974506378\n",
      "epoch: 29 step: 855, loss is 0.14326289296150208\n",
      "epoch: 29 step: 856, loss is 0.17608849704265594\n",
      "epoch: 29 step: 857, loss is 0.10673142224550247\n",
      "epoch: 29 step: 858, loss is 0.2604491412639618\n",
      "epoch: 29 step: 859, loss is 0.20948685705661774\n",
      "epoch: 29 step: 860, loss is 0.19838541746139526\n",
      "epoch: 29 step: 861, loss is 0.13616928458213806\n",
      "epoch: 29 step: 862, loss is 0.1825399249792099\n",
      "epoch: 29 step: 863, loss is 0.16031043231487274\n",
      "epoch: 29 step: 864, loss is 0.3284182548522949\n",
      "epoch: 29 step: 865, loss is 0.07418647408485413\n",
      "epoch: 29 step: 866, loss is 0.14925885200500488\n",
      "epoch: 29 step: 867, loss is 0.17653127014636993\n",
      "epoch: 29 step: 868, loss is 0.18551820516586304\n",
      "epoch: 29 step: 869, loss is 0.15120260417461395\n",
      "epoch: 29 step: 870, loss is 0.2101377695798874\n",
      "epoch: 29 step: 871, loss is 0.2927146852016449\n",
      "epoch: 29 step: 872, loss is 0.08719958364963531\n",
      "epoch: 29 step: 873, loss is 0.21418191492557526\n",
      "epoch: 29 step: 874, loss is 0.19705361127853394\n",
      "epoch: 29 step: 875, loss is 0.16673432290554047\n",
      "epoch: 29 step: 876, loss is 0.18611684441566467\n",
      "epoch: 29 step: 877, loss is 0.09732536971569061\n",
      "epoch: 29 step: 878, loss is 0.0704779252409935\n",
      "epoch: 29 step: 879, loss is 0.2421286553144455\n",
      "epoch: 29 step: 880, loss is 0.21689099073410034\n",
      "epoch: 29 step: 881, loss is 0.13875453174114227\n",
      "epoch: 29 step: 882, loss is 0.14574113488197327\n",
      "epoch: 29 step: 883, loss is 0.13295386731624603\n",
      "epoch: 29 step: 884, loss is 0.12421626597642899\n",
      "epoch: 29 step: 885, loss is 0.16180366277694702\n",
      "epoch: 29 step: 886, loss is 0.15892116725444794\n",
      "epoch: 29 step: 887, loss is 0.2597077488899231\n",
      "epoch: 29 step: 888, loss is 0.23216581344604492\n",
      "epoch: 29 step: 889, loss is 0.14573854207992554\n",
      "epoch: 29 step: 890, loss is 0.05726219713687897\n",
      "epoch: 29 step: 891, loss is 0.1753135621547699\n",
      "epoch: 29 step: 892, loss is 0.34334293007850647\n",
      "epoch: 29 step: 893, loss is 0.1762070655822754\n",
      "epoch: 29 step: 894, loss is 0.11323253810405731\n",
      "epoch: 29 step: 895, loss is 0.1704048365354538\n",
      "epoch: 29 step: 896, loss is 0.092440165579319\n",
      "epoch: 29 step: 897, loss is 0.08903708308935165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 898, loss is 0.06072795391082764\n",
      "epoch: 29 step: 899, loss is 0.2622710168361664\n",
      "epoch: 29 step: 900, loss is 0.14470019936561584\n",
      "epoch: 29 step: 901, loss is 0.2605525255203247\n",
      "epoch: 29 step: 902, loss is 0.13769549131393433\n",
      "epoch: 29 step: 903, loss is 0.237093985080719\n",
      "epoch: 29 step: 904, loss is 0.30848434567451477\n",
      "epoch: 29 step: 905, loss is 0.1509820967912674\n",
      "epoch: 29 step: 906, loss is 0.16389252245426178\n",
      "epoch: 29 step: 907, loss is 0.10835587978363037\n",
      "epoch: 29 step: 908, loss is 0.113388791680336\n",
      "epoch: 29 step: 909, loss is 0.055745888501405716\n",
      "epoch: 29 step: 910, loss is 0.25379976630210876\n",
      "epoch: 29 step: 911, loss is 0.17552214860916138\n",
      "epoch: 29 step: 912, loss is 0.34794968366622925\n",
      "epoch: 29 step: 913, loss is 0.2086026519536972\n",
      "epoch: 29 step: 914, loss is 0.09426085650920868\n",
      "epoch: 29 step: 915, loss is 0.15353117883205414\n",
      "epoch: 29 step: 916, loss is 0.11299394071102142\n",
      "epoch: 29 step: 917, loss is 0.0377248115837574\n",
      "epoch: 29 step: 918, loss is 0.2468096762895584\n",
      "epoch: 29 step: 919, loss is 0.16286394000053406\n",
      "epoch: 29 step: 920, loss is 0.1353388875722885\n",
      "epoch: 29 step: 921, loss is 0.07904596626758575\n",
      "epoch: 29 step: 922, loss is 0.19518724083900452\n",
      "epoch: 29 step: 923, loss is 0.2184603363275528\n",
      "epoch: 29 step: 924, loss is 0.1693853735923767\n",
      "epoch: 29 step: 925, loss is 0.0733465701341629\n",
      "epoch: 29 step: 926, loss is 0.3583333492279053\n",
      "epoch: 29 step: 927, loss is 0.2710990309715271\n",
      "epoch: 29 step: 928, loss is 0.28424206376075745\n",
      "epoch: 29 step: 929, loss is 0.41578981280326843\n",
      "epoch: 29 step: 930, loss is 0.14930695295333862\n",
      "epoch: 29 step: 931, loss is 0.13119500875473022\n",
      "epoch: 29 step: 932, loss is 0.09168796986341476\n",
      "epoch: 29 step: 933, loss is 0.1273789405822754\n",
      "epoch: 29 step: 934, loss is 0.16596822440624237\n",
      "epoch: 29 step: 935, loss is 0.1837262362241745\n",
      "epoch: 29 step: 936, loss is 0.053809478878974915\n",
      "epoch: 29 step: 937, loss is 0.14285528659820557\n",
      "epoch: 30 step: 1, loss is 0.13147702813148499\n",
      "epoch: 30 step: 2, loss is 0.0617191381752491\n",
      "epoch: 30 step: 3, loss is 0.1734352856874466\n",
      "epoch: 30 step: 4, loss is 0.20656266808509827\n",
      "epoch: 30 step: 5, loss is 0.15706266462802887\n",
      "epoch: 30 step: 6, loss is 0.1648755967617035\n",
      "epoch: 30 step: 7, loss is 0.10128723829984665\n",
      "epoch: 30 step: 8, loss is 0.11412487179040909\n",
      "epoch: 30 step: 9, loss is 0.07335250824689865\n",
      "epoch: 30 step: 10, loss is 0.2770130932331085\n",
      "epoch: 30 step: 11, loss is 0.33760643005371094\n",
      "epoch: 30 step: 12, loss is 0.16067369282245636\n",
      "epoch: 30 step: 13, loss is 0.09561880677938461\n",
      "epoch: 30 step: 14, loss is 0.1195974051952362\n",
      "epoch: 30 step: 15, loss is 0.1718645989894867\n",
      "epoch: 30 step: 16, loss is 0.3317809998989105\n",
      "epoch: 30 step: 17, loss is 0.10423457622528076\n",
      "epoch: 30 step: 18, loss is 0.05523422360420227\n",
      "epoch: 30 step: 19, loss is 0.210309699177742\n",
      "epoch: 30 step: 20, loss is 0.11522413790225983\n",
      "epoch: 30 step: 21, loss is 0.2426910698413849\n",
      "epoch: 30 step: 22, loss is 0.19591960310935974\n",
      "epoch: 30 step: 23, loss is 0.22940382361412048\n",
      "epoch: 30 step: 24, loss is 0.18468225002288818\n",
      "epoch: 30 step: 25, loss is 0.174207404255867\n",
      "epoch: 30 step: 26, loss is 0.13594527542591095\n",
      "epoch: 30 step: 27, loss is 0.16998396813869476\n",
      "epoch: 30 step: 28, loss is 0.21293014287948608\n",
      "epoch: 30 step: 29, loss is 0.0923045352101326\n",
      "epoch: 30 step: 30, loss is 0.07131639122962952\n",
      "epoch: 30 step: 31, loss is 0.1420164257287979\n",
      "epoch: 30 step: 32, loss is 0.1283721625804901\n",
      "epoch: 30 step: 33, loss is 0.22309212386608124\n",
      "epoch: 30 step: 34, loss is 0.21475546061992645\n",
      "epoch: 30 step: 35, loss is 0.17282932996749878\n",
      "epoch: 30 step: 36, loss is 0.15924659371376038\n",
      "epoch: 30 step: 37, loss is 0.11735948175191879\n",
      "epoch: 30 step: 38, loss is 0.3251449763774872\n",
      "epoch: 30 step: 39, loss is 0.1524534970521927\n",
      "epoch: 30 step: 40, loss is 0.0971124991774559\n",
      "epoch: 30 step: 41, loss is 0.2230415940284729\n",
      "epoch: 30 step: 42, loss is 0.1503857523202896\n",
      "epoch: 30 step: 43, loss is 0.14836400747299194\n",
      "epoch: 30 step: 44, loss is 0.23169556260108948\n",
      "epoch: 30 step: 45, loss is 0.10609517991542816\n",
      "epoch: 30 step: 46, loss is 0.15505383908748627\n",
      "epoch: 30 step: 47, loss is 0.3403957486152649\n",
      "epoch: 30 step: 48, loss is 0.2365194708108902\n",
      "epoch: 30 step: 49, loss is 0.0717589482665062\n",
      "epoch: 30 step: 50, loss is 0.20424383878707886\n",
      "epoch: 30 step: 51, loss is 0.21139758825302124\n",
      "epoch: 30 step: 52, loss is 0.11759335547685623\n",
      "epoch: 30 step: 53, loss is 0.11911165714263916\n",
      "epoch: 30 step: 54, loss is 0.17858846485614777\n",
      "epoch: 30 step: 55, loss is 0.1640862226486206\n",
      "epoch: 30 step: 56, loss is 0.08589901775121689\n",
      "epoch: 30 step: 57, loss is 0.21205765008926392\n",
      "epoch: 30 step: 58, loss is 0.1473124921321869\n",
      "epoch: 30 step: 59, loss is 0.22177763283252716\n",
      "epoch: 30 step: 60, loss is 0.3670566976070404\n",
      "epoch: 30 step: 61, loss is 0.26302337646484375\n",
      "epoch: 30 step: 62, loss is 0.14300906658172607\n",
      "epoch: 30 step: 63, loss is 0.16502125561237335\n",
      "epoch: 30 step: 64, loss is 0.1438358575105667\n",
      "epoch: 30 step: 65, loss is 0.13977304100990295\n",
      "epoch: 30 step: 66, loss is 0.21845978498458862\n",
      "epoch: 30 step: 67, loss is 0.14520499110221863\n",
      "epoch: 30 step: 68, loss is 0.4031049907207489\n",
      "epoch: 30 step: 69, loss is 0.1544354110956192\n",
      "epoch: 30 step: 70, loss is 0.12302352488040924\n",
      "epoch: 30 step: 71, loss is 0.16376811265945435\n",
      "epoch: 30 step: 72, loss is 0.28993070125579834\n",
      "epoch: 30 step: 73, loss is 0.22437715530395508\n",
      "epoch: 30 step: 74, loss is 0.08177155256271362\n",
      "epoch: 30 step: 75, loss is 0.10279315710067749\n",
      "epoch: 30 step: 76, loss is 0.09857551753520966\n",
      "epoch: 30 step: 77, loss is 0.1957777887582779\n",
      "epoch: 30 step: 78, loss is 0.20760373771190643\n",
      "epoch: 30 step: 79, loss is 0.1714298576116562\n",
      "epoch: 30 step: 80, loss is 0.1185050755739212\n",
      "epoch: 30 step: 81, loss is 0.14971016347408295\n",
      "epoch: 30 step: 82, loss is 0.24259638786315918\n",
      "epoch: 30 step: 83, loss is 0.2849365770816803\n",
      "epoch: 30 step: 84, loss is 0.17842668294906616\n",
      "epoch: 30 step: 85, loss is 0.15991778671741486\n",
      "epoch: 30 step: 86, loss is 0.14663083851337433\n",
      "epoch: 30 step: 87, loss is 0.1083006039261818\n",
      "epoch: 30 step: 88, loss is 0.23238690197467804\n",
      "epoch: 30 step: 89, loss is 0.08505117148160934\n",
      "epoch: 30 step: 90, loss is 0.09364086389541626\n",
      "epoch: 30 step: 91, loss is 0.1993541568517685\n",
      "epoch: 30 step: 92, loss is 0.20353195071220398\n",
      "epoch: 30 step: 93, loss is 0.11547592282295227\n",
      "epoch: 30 step: 94, loss is 0.09786076843738556\n",
      "epoch: 30 step: 95, loss is 0.27651897072792053\n",
      "epoch: 30 step: 96, loss is 0.3837452530860901\n",
      "epoch: 30 step: 97, loss is 0.08444023132324219\n",
      "epoch: 30 step: 98, loss is 0.20813384652137756\n",
      "epoch: 30 step: 99, loss is 0.18068747222423553\n",
      "epoch: 30 step: 100, loss is 0.31249263882637024\n",
      "epoch: 30 step: 101, loss is 0.12382303923368454\n",
      "epoch: 30 step: 102, loss is 0.23530027270317078\n",
      "epoch: 30 step: 103, loss is 0.15899580717086792\n",
      "epoch: 30 step: 104, loss is 0.1343681365251541\n",
      "epoch: 30 step: 105, loss is 0.10653918981552124\n",
      "epoch: 30 step: 106, loss is 0.18870550394058228\n",
      "epoch: 30 step: 107, loss is 0.20316165685653687\n",
      "epoch: 30 step: 108, loss is 0.20520605146884918\n",
      "epoch: 30 step: 109, loss is 0.1896604746580124\n",
      "epoch: 30 step: 110, loss is 0.15913337469100952\n",
      "epoch: 30 step: 111, loss is 0.344245582818985\n",
      "epoch: 30 step: 112, loss is 0.1977665275335312\n",
      "epoch: 30 step: 113, loss is 0.20459873974323273\n",
      "epoch: 30 step: 114, loss is 0.11310683190822601\n",
      "epoch: 30 step: 115, loss is 0.16223426163196564\n",
      "epoch: 30 step: 116, loss is 0.14227423071861267\n",
      "epoch: 30 step: 117, loss is 0.1383674293756485\n",
      "epoch: 30 step: 118, loss is 0.08925563842058182\n",
      "epoch: 30 step: 119, loss is 0.18481244146823883\n",
      "epoch: 30 step: 120, loss is 0.10707037895917892\n",
      "epoch: 30 step: 121, loss is 0.30263611674308777\n",
      "epoch: 30 step: 122, loss is 0.21784014999866486\n",
      "epoch: 30 step: 123, loss is 0.27558934688568115\n",
      "epoch: 30 step: 124, loss is 0.09633622318506241\n",
      "epoch: 30 step: 125, loss is 0.11012187600135803\n",
      "epoch: 30 step: 126, loss is 0.19025573134422302\n",
      "epoch: 30 step: 127, loss is 0.12405644357204437\n",
      "epoch: 30 step: 128, loss is 0.1016349047422409\n",
      "epoch: 30 step: 129, loss is 0.06315772980451584\n",
      "epoch: 30 step: 130, loss is 0.1460237205028534\n",
      "epoch: 30 step: 131, loss is 0.15517689287662506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 132, loss is 0.24195735156536102\n",
      "epoch: 30 step: 133, loss is 0.24265198409557343\n",
      "epoch: 30 step: 134, loss is 0.1108856275677681\n",
      "epoch: 30 step: 135, loss is 0.20370131731033325\n",
      "epoch: 30 step: 136, loss is 0.12101294845342636\n",
      "epoch: 30 step: 137, loss is 0.22584259510040283\n",
      "epoch: 30 step: 138, loss is 0.1915004849433899\n",
      "epoch: 30 step: 139, loss is 0.13406464457511902\n",
      "epoch: 30 step: 140, loss is 0.15826532244682312\n",
      "epoch: 30 step: 141, loss is 0.25889909267425537\n",
      "epoch: 30 step: 142, loss is 0.0792749673128128\n",
      "epoch: 30 step: 143, loss is 0.12536415457725525\n",
      "epoch: 30 step: 144, loss is 0.2699068486690521\n",
      "epoch: 30 step: 145, loss is 0.2619282007217407\n",
      "epoch: 30 step: 146, loss is 0.16648796200752258\n",
      "epoch: 30 step: 147, loss is 0.1353272795677185\n",
      "epoch: 30 step: 148, loss is 0.17451295256614685\n",
      "epoch: 30 step: 149, loss is 0.141383096575737\n",
      "epoch: 30 step: 150, loss is 0.17472994327545166\n",
      "epoch: 30 step: 151, loss is 0.20676399767398834\n",
      "epoch: 30 step: 152, loss is 0.15970543026924133\n",
      "epoch: 30 step: 153, loss is 0.02544229105114937\n",
      "epoch: 30 step: 154, loss is 0.21550504863262177\n",
      "epoch: 30 step: 155, loss is 0.21784481406211853\n",
      "epoch: 30 step: 156, loss is 0.30814796686172485\n",
      "epoch: 30 step: 157, loss is 0.10992439091205597\n",
      "epoch: 30 step: 158, loss is 0.14242605865001678\n",
      "epoch: 30 step: 159, loss is 0.18335187435150146\n",
      "epoch: 30 step: 160, loss is 0.2034119963645935\n",
      "epoch: 30 step: 161, loss is 0.19577480852603912\n",
      "epoch: 30 step: 162, loss is 0.3846414089202881\n",
      "epoch: 30 step: 163, loss is 0.13384267687797546\n",
      "epoch: 30 step: 164, loss is 0.15669843554496765\n",
      "epoch: 30 step: 165, loss is 0.09623405337333679\n",
      "epoch: 30 step: 166, loss is 0.10573922097682953\n",
      "epoch: 30 step: 167, loss is 0.1483641266822815\n",
      "epoch: 30 step: 168, loss is 0.16081973910331726\n",
      "epoch: 30 step: 169, loss is 0.22587189078330994\n",
      "epoch: 30 step: 170, loss is 0.21243777871131897\n",
      "epoch: 30 step: 171, loss is 0.11437417566776276\n",
      "epoch: 30 step: 172, loss is 0.14017359912395477\n",
      "epoch: 30 step: 173, loss is 0.3162182867527008\n",
      "epoch: 30 step: 174, loss is 0.20427550375461578\n",
      "epoch: 30 step: 175, loss is 0.2555290162563324\n",
      "epoch: 30 step: 176, loss is 0.1956082433462143\n",
      "epoch: 30 step: 177, loss is 0.22965067625045776\n",
      "epoch: 30 step: 178, loss is 0.07287780195474625\n",
      "epoch: 30 step: 179, loss is 0.2805280387401581\n",
      "epoch: 30 step: 180, loss is 0.12259025871753693\n",
      "epoch: 30 step: 181, loss is 0.16481515765190125\n",
      "epoch: 30 step: 182, loss is 0.1889955997467041\n",
      "epoch: 30 step: 183, loss is 0.09858601540327072\n",
      "epoch: 30 step: 184, loss is 0.15836220979690552\n",
      "epoch: 30 step: 185, loss is 0.26795873045921326\n",
      "epoch: 30 step: 186, loss is 0.051707204431295395\n",
      "epoch: 30 step: 187, loss is 0.3368792235851288\n",
      "epoch: 30 step: 188, loss is 0.19943326711654663\n",
      "epoch: 30 step: 189, loss is 0.10618239641189575\n",
      "epoch: 30 step: 190, loss is 0.14220713078975677\n",
      "epoch: 30 step: 191, loss is 0.10268155485391617\n",
      "epoch: 30 step: 192, loss is 0.16475529968738556\n",
      "epoch: 30 step: 193, loss is 0.186857208609581\n",
      "epoch: 30 step: 194, loss is 0.19076421856880188\n",
      "epoch: 30 step: 195, loss is 0.3203965723514557\n",
      "epoch: 30 step: 196, loss is 0.16250133514404297\n",
      "epoch: 30 step: 197, loss is 0.42303428053855896\n",
      "epoch: 30 step: 198, loss is 0.10496839135885239\n",
      "epoch: 30 step: 199, loss is 0.16776886582374573\n",
      "epoch: 30 step: 200, loss is 0.11441636085510254\n",
      "epoch: 30 step: 201, loss is 0.25907930731773376\n",
      "epoch: 30 step: 202, loss is 0.10537602007389069\n",
      "epoch: 30 step: 203, loss is 0.2635818421840668\n",
      "epoch: 30 step: 204, loss is 0.19004948437213898\n",
      "epoch: 30 step: 205, loss is 0.11858740448951721\n",
      "epoch: 30 step: 206, loss is 0.1419154852628708\n",
      "epoch: 30 step: 207, loss is 0.25760143995285034\n",
      "epoch: 30 step: 208, loss is 0.14136166870594025\n",
      "epoch: 30 step: 209, loss is 0.10852675884962082\n",
      "epoch: 30 step: 210, loss is 0.12753047049045563\n",
      "epoch: 30 step: 211, loss is 0.17691601812839508\n",
      "epoch: 30 step: 212, loss is 0.21488067507743835\n",
      "epoch: 30 step: 213, loss is 0.17795759439468384\n",
      "epoch: 30 step: 214, loss is 0.14461612701416016\n",
      "epoch: 30 step: 215, loss is 0.0855373740196228\n",
      "epoch: 30 step: 216, loss is 0.26824042201042175\n",
      "epoch: 30 step: 217, loss is 0.22658663988113403\n",
      "epoch: 30 step: 218, loss is 0.13106122612953186\n",
      "epoch: 30 step: 219, loss is 0.5050631761550903\n",
      "epoch: 30 step: 220, loss is 0.16864141821861267\n",
      "epoch: 30 step: 221, loss is 0.15862710773944855\n",
      "epoch: 30 step: 222, loss is 0.09237655997276306\n",
      "epoch: 30 step: 223, loss is 0.1372804343700409\n",
      "epoch: 30 step: 224, loss is 0.10002397000789642\n",
      "epoch: 30 step: 225, loss is 0.12519240379333496\n",
      "epoch: 30 step: 226, loss is 0.11863935738801956\n",
      "epoch: 30 step: 227, loss is 0.15458591282367706\n",
      "epoch: 30 step: 228, loss is 0.2575805187225342\n",
      "epoch: 30 step: 229, loss is 0.14798788726329803\n",
      "epoch: 30 step: 230, loss is 0.2347012758255005\n",
      "epoch: 30 step: 231, loss is 0.06917671859264374\n",
      "epoch: 30 step: 232, loss is 0.20481660962104797\n",
      "epoch: 30 step: 233, loss is 0.2833188474178314\n",
      "epoch: 30 step: 234, loss is 0.20852768421173096\n",
      "epoch: 30 step: 235, loss is 0.24144713580608368\n",
      "epoch: 30 step: 236, loss is 0.13664783537387848\n",
      "epoch: 30 step: 237, loss is 0.08549241721630096\n",
      "epoch: 30 step: 238, loss is 0.17356142401695251\n",
      "epoch: 30 step: 239, loss is 0.20474176108837128\n",
      "epoch: 30 step: 240, loss is 0.23313020169734955\n",
      "epoch: 30 step: 241, loss is 0.2432982474565506\n",
      "epoch: 30 step: 242, loss is 0.23563550412654877\n",
      "epoch: 30 step: 243, loss is 0.07533957809209824\n",
      "epoch: 30 step: 244, loss is 0.14422303438186646\n",
      "epoch: 30 step: 245, loss is 0.2634132504463196\n",
      "epoch: 30 step: 246, loss is 0.19169461727142334\n",
      "epoch: 30 step: 247, loss is 0.26518672704696655\n",
      "epoch: 30 step: 248, loss is 0.16950733959674835\n",
      "epoch: 30 step: 249, loss is 0.1739938110113144\n",
      "epoch: 30 step: 250, loss is 0.1344042867422104\n",
      "epoch: 30 step: 251, loss is 0.2501522898674011\n",
      "epoch: 30 step: 252, loss is 0.17125184834003448\n",
      "epoch: 30 step: 253, loss is 0.13700519502162933\n",
      "epoch: 30 step: 254, loss is 0.2831222414970398\n",
      "epoch: 30 step: 255, loss is 0.24362337589263916\n",
      "epoch: 30 step: 256, loss is 0.18918797373771667\n",
      "epoch: 30 step: 257, loss is 0.13296882808208466\n",
      "epoch: 30 step: 258, loss is 0.22487807273864746\n",
      "epoch: 30 step: 259, loss is 0.15782175958156586\n",
      "epoch: 30 step: 260, loss is 0.11626996845006943\n",
      "epoch: 30 step: 261, loss is 0.2529536485671997\n",
      "epoch: 30 step: 262, loss is 0.1702602356672287\n",
      "epoch: 30 step: 263, loss is 0.10362028330564499\n",
      "epoch: 30 step: 264, loss is 0.1820524036884308\n",
      "epoch: 30 step: 265, loss is 0.1826269030570984\n",
      "epoch: 30 step: 266, loss is 0.22273984551429749\n",
      "epoch: 30 step: 267, loss is 0.13478311896324158\n",
      "epoch: 30 step: 268, loss is 0.19051706790924072\n",
      "epoch: 30 step: 269, loss is 0.23735444247722626\n",
      "epoch: 30 step: 270, loss is 0.19768282771110535\n",
      "epoch: 30 step: 271, loss is 0.18565692007541656\n",
      "epoch: 30 step: 272, loss is 0.10997029393911362\n",
      "epoch: 30 step: 273, loss is 0.17271843552589417\n",
      "epoch: 30 step: 274, loss is 0.15893414616584778\n",
      "epoch: 30 step: 275, loss is 0.3786384165287018\n",
      "epoch: 30 step: 276, loss is 0.21861737966537476\n",
      "epoch: 30 step: 277, loss is 0.19688034057617188\n",
      "epoch: 30 step: 278, loss is 0.17932851612567902\n",
      "epoch: 30 step: 279, loss is 0.2143135368824005\n",
      "epoch: 30 step: 280, loss is 0.19507254660129547\n",
      "epoch: 30 step: 281, loss is 0.12225411832332611\n",
      "epoch: 30 step: 282, loss is 0.3298153877258301\n",
      "epoch: 30 step: 283, loss is 0.15116667747497559\n",
      "epoch: 30 step: 284, loss is 0.21226628124713898\n",
      "epoch: 30 step: 285, loss is 0.35304003953933716\n",
      "epoch: 30 step: 286, loss is 0.08510370552539825\n",
      "epoch: 30 step: 287, loss is 0.184830904006958\n",
      "epoch: 30 step: 288, loss is 0.12459283322095871\n",
      "epoch: 30 step: 289, loss is 0.13925644755363464\n",
      "epoch: 30 step: 290, loss is 0.20013605058193207\n",
      "epoch: 30 step: 291, loss is 0.34682852029800415\n",
      "epoch: 30 step: 292, loss is 0.12265976518392563\n",
      "epoch: 30 step: 293, loss is 0.10435207933187485\n",
      "epoch: 30 step: 294, loss is 0.22444434463977814\n",
      "epoch: 30 step: 295, loss is 0.10615292191505432\n",
      "epoch: 30 step: 296, loss is 0.20900925993919373\n",
      "epoch: 30 step: 297, loss is 0.15991772711277008\n",
      "epoch: 30 step: 298, loss is 0.10393677651882172\n",
      "epoch: 30 step: 299, loss is 0.24889928102493286\n",
      "epoch: 30 step: 300, loss is 0.1847802847623825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 301, loss is 0.06543045490980148\n",
      "epoch: 30 step: 302, loss is 0.3479171395301819\n",
      "epoch: 30 step: 303, loss is 0.20122405886650085\n",
      "epoch: 30 step: 304, loss is 0.19912339746952057\n",
      "epoch: 30 step: 305, loss is 0.24487744271755219\n",
      "epoch: 30 step: 306, loss is 0.1631651669740677\n",
      "epoch: 30 step: 307, loss is 0.26557087898254395\n",
      "epoch: 30 step: 308, loss is 0.1447194367647171\n",
      "epoch: 30 step: 309, loss is 0.12081962078809738\n",
      "epoch: 30 step: 310, loss is 0.0678950846195221\n",
      "epoch: 30 step: 311, loss is 0.40182438492774963\n",
      "epoch: 30 step: 312, loss is 0.30873215198516846\n",
      "epoch: 30 step: 313, loss is 0.23193980753421783\n",
      "epoch: 30 step: 314, loss is 0.13281477987766266\n",
      "epoch: 30 step: 315, loss is 0.31518658995628357\n",
      "epoch: 30 step: 316, loss is 0.2510995864868164\n",
      "epoch: 30 step: 317, loss is 0.11819493025541306\n",
      "epoch: 30 step: 318, loss is 0.1855703890323639\n",
      "epoch: 30 step: 319, loss is 0.224757120013237\n",
      "epoch: 30 step: 320, loss is 0.19722336530685425\n",
      "epoch: 30 step: 321, loss is 0.23208050429821014\n",
      "epoch: 30 step: 322, loss is 0.1383684128522873\n",
      "epoch: 30 step: 323, loss is 0.1426139622926712\n",
      "epoch: 30 step: 324, loss is 0.344180166721344\n",
      "epoch: 30 step: 325, loss is 0.09073538333177567\n",
      "epoch: 30 step: 326, loss is 0.12916801869869232\n",
      "epoch: 30 step: 327, loss is 0.1984277069568634\n",
      "epoch: 30 step: 328, loss is 0.1460038125514984\n",
      "epoch: 30 step: 329, loss is 0.12191160023212433\n",
      "epoch: 30 step: 330, loss is 0.2839682400226593\n",
      "epoch: 30 step: 331, loss is 0.11920063197612762\n",
      "epoch: 30 step: 332, loss is 0.05774444714188576\n",
      "epoch: 30 step: 333, loss is 0.2678956687450409\n",
      "epoch: 30 step: 334, loss is 0.3021608889102936\n",
      "epoch: 30 step: 335, loss is 0.10705593228340149\n",
      "epoch: 30 step: 336, loss is 0.30603864789009094\n",
      "epoch: 30 step: 337, loss is 0.20268504321575165\n",
      "epoch: 30 step: 338, loss is 0.17216715216636658\n",
      "epoch: 30 step: 339, loss is 0.06801079213619232\n",
      "epoch: 30 step: 340, loss is 0.2064947634935379\n",
      "epoch: 30 step: 341, loss is 0.10570932924747467\n",
      "epoch: 30 step: 342, loss is 0.2097131460905075\n",
      "epoch: 30 step: 343, loss is 0.22227197885513306\n",
      "epoch: 30 step: 344, loss is 0.22412770986557007\n",
      "epoch: 30 step: 345, loss is 0.24199363589286804\n",
      "epoch: 30 step: 346, loss is 0.10830459743738174\n",
      "epoch: 30 step: 347, loss is 0.11595463752746582\n",
      "epoch: 30 step: 348, loss is 0.1812727153301239\n",
      "epoch: 30 step: 349, loss is 0.18577267229557037\n",
      "epoch: 30 step: 350, loss is 0.1588388979434967\n",
      "epoch: 30 step: 351, loss is 0.24523451924324036\n",
      "epoch: 30 step: 352, loss is 0.19029465317726135\n",
      "epoch: 30 step: 353, loss is 0.11976683139801025\n",
      "epoch: 30 step: 354, loss is 0.26715824007987976\n",
      "epoch: 30 step: 355, loss is 0.11721658706665039\n",
      "epoch: 30 step: 356, loss is 0.14399929344654083\n",
      "epoch: 30 step: 357, loss is 0.12448131293058395\n",
      "epoch: 30 step: 358, loss is 0.2388273924589157\n",
      "epoch: 30 step: 359, loss is 0.14216740429401398\n",
      "epoch: 30 step: 360, loss is 0.12898039817810059\n",
      "epoch: 30 step: 361, loss is 0.32075071334838867\n",
      "epoch: 30 step: 362, loss is 0.09044191986322403\n",
      "epoch: 30 step: 363, loss is 0.19237355887889862\n",
      "epoch: 30 step: 364, loss is 0.21723148226737976\n",
      "epoch: 30 step: 365, loss is 0.21460646390914917\n",
      "epoch: 30 step: 366, loss is 0.33822354674339294\n",
      "epoch: 30 step: 367, loss is 0.22476768493652344\n",
      "epoch: 30 step: 368, loss is 0.2265012115240097\n",
      "epoch: 30 step: 369, loss is 0.16920959949493408\n",
      "epoch: 30 step: 370, loss is 0.06535698473453522\n",
      "epoch: 30 step: 371, loss is 0.13786672055721283\n",
      "epoch: 30 step: 372, loss is 0.06422802805900574\n",
      "epoch: 30 step: 373, loss is 0.24699193239212036\n",
      "epoch: 30 step: 374, loss is 0.14246058464050293\n",
      "epoch: 30 step: 375, loss is 0.27570784091949463\n",
      "epoch: 30 step: 376, loss is 0.24639727175235748\n",
      "epoch: 30 step: 377, loss is 0.1432456076145172\n",
      "epoch: 30 step: 378, loss is 0.2551560401916504\n",
      "epoch: 30 step: 379, loss is 0.06973512470722198\n",
      "epoch: 30 step: 380, loss is 0.05444894731044769\n",
      "epoch: 30 step: 381, loss is 0.2298675775527954\n",
      "epoch: 30 step: 382, loss is 0.2323811799287796\n",
      "epoch: 30 step: 383, loss is 0.12160445004701614\n",
      "epoch: 30 step: 384, loss is 0.2675670385360718\n",
      "epoch: 30 step: 385, loss is 0.1921505481004715\n",
      "epoch: 30 step: 386, loss is 0.1486607789993286\n",
      "epoch: 30 step: 387, loss is 0.1092473566532135\n",
      "epoch: 30 step: 388, loss is 0.2550261318683624\n",
      "epoch: 30 step: 389, loss is 0.17729605734348297\n",
      "epoch: 30 step: 390, loss is 0.10297545790672302\n",
      "epoch: 30 step: 391, loss is 0.30162596702575684\n",
      "epoch: 30 step: 392, loss is 0.09842832386493683\n",
      "epoch: 30 step: 393, loss is 0.07500080019235611\n",
      "epoch: 30 step: 394, loss is 0.1974201202392578\n",
      "epoch: 30 step: 395, loss is 0.14672258496284485\n",
      "epoch: 30 step: 396, loss is 0.17246375977993011\n",
      "epoch: 30 step: 397, loss is 0.21382007002830505\n",
      "epoch: 30 step: 398, loss is 0.25021126866340637\n",
      "epoch: 30 step: 399, loss is 0.07576226443052292\n",
      "epoch: 30 step: 400, loss is 0.1936209499835968\n",
      "epoch: 30 step: 401, loss is 0.11524025350809097\n",
      "epoch: 30 step: 402, loss is 0.11553122103214264\n",
      "epoch: 30 step: 403, loss is 0.2968880236148834\n",
      "epoch: 30 step: 404, loss is 0.16848225891590118\n",
      "epoch: 30 step: 405, loss is 0.2196677029132843\n",
      "epoch: 30 step: 406, loss is 0.13947272300720215\n",
      "epoch: 30 step: 407, loss is 0.07661688327789307\n",
      "epoch: 30 step: 408, loss is 0.20296430587768555\n",
      "epoch: 30 step: 409, loss is 0.1771901249885559\n",
      "epoch: 30 step: 410, loss is 0.2182030826807022\n",
      "epoch: 30 step: 411, loss is 0.2017335593700409\n",
      "epoch: 30 step: 412, loss is 0.12852779030799866\n",
      "epoch: 30 step: 413, loss is 0.19102096557617188\n",
      "epoch: 30 step: 414, loss is 0.10088382661342621\n",
      "epoch: 30 step: 415, loss is 0.16309966146945953\n",
      "epoch: 30 step: 416, loss is 0.08659787476062775\n",
      "epoch: 30 step: 417, loss is 0.32539671659469604\n",
      "epoch: 30 step: 418, loss is 0.14027726650238037\n",
      "epoch: 30 step: 419, loss is 0.1740868091583252\n",
      "epoch: 30 step: 420, loss is 0.17812757194042206\n",
      "epoch: 30 step: 421, loss is 0.09004925936460495\n",
      "epoch: 30 step: 422, loss is 0.17289811372756958\n",
      "epoch: 30 step: 423, loss is 0.1693931370973587\n",
      "epoch: 30 step: 424, loss is 0.2460140883922577\n",
      "epoch: 30 step: 425, loss is 0.12486345320940018\n",
      "epoch: 30 step: 426, loss is 0.20124639570713043\n",
      "epoch: 30 step: 427, loss is 0.05463872849941254\n",
      "epoch: 30 step: 428, loss is 0.16739745438098907\n",
      "epoch: 30 step: 429, loss is 0.12975050508975983\n",
      "epoch: 30 step: 430, loss is 0.2299492508172989\n",
      "epoch: 30 step: 431, loss is 0.21244384348392487\n",
      "epoch: 30 step: 432, loss is 0.18844641745090485\n",
      "epoch: 30 step: 433, loss is 0.2170284241437912\n",
      "epoch: 30 step: 434, loss is 0.06376369297504425\n",
      "epoch: 30 step: 435, loss is 0.20667126774787903\n",
      "epoch: 30 step: 436, loss is 0.16614870727062225\n",
      "epoch: 30 step: 437, loss is 0.08116050809621811\n",
      "epoch: 30 step: 438, loss is 0.1382497400045395\n",
      "epoch: 30 step: 439, loss is 0.12998364865779877\n",
      "epoch: 30 step: 440, loss is 0.16368316113948822\n",
      "epoch: 30 step: 441, loss is 0.12700550258159637\n",
      "epoch: 30 step: 442, loss is 0.2405449002981186\n",
      "epoch: 30 step: 443, loss is 0.19913877546787262\n",
      "epoch: 30 step: 444, loss is 0.1879705935716629\n",
      "epoch: 30 step: 445, loss is 0.37689587473869324\n",
      "epoch: 30 step: 446, loss is 0.10514944791793823\n",
      "epoch: 30 step: 447, loss is 0.1800774186849594\n",
      "epoch: 30 step: 448, loss is 0.25027090311050415\n",
      "epoch: 30 step: 449, loss is 0.06794213503599167\n",
      "epoch: 30 step: 450, loss is 0.2792498469352722\n",
      "epoch: 30 step: 451, loss is 0.23806045949459076\n",
      "epoch: 30 step: 452, loss is 0.2147923856973648\n",
      "epoch: 30 step: 453, loss is 0.17373022437095642\n",
      "epoch: 30 step: 454, loss is 0.1428842693567276\n",
      "epoch: 30 step: 455, loss is 0.2538972795009613\n",
      "epoch: 30 step: 456, loss is 0.14802579581737518\n",
      "epoch: 30 step: 457, loss is 0.1089501604437828\n",
      "epoch: 30 step: 458, loss is 0.20511333644390106\n",
      "epoch: 30 step: 459, loss is 0.18421831727027893\n",
      "epoch: 30 step: 460, loss is 0.157334566116333\n",
      "epoch: 30 step: 461, loss is 0.1318139135837555\n",
      "epoch: 30 step: 462, loss is 0.09763031452894211\n",
      "epoch: 30 step: 463, loss is 0.13147075474262238\n",
      "epoch: 30 step: 464, loss is 0.19943347573280334\n",
      "epoch: 30 step: 465, loss is 0.10104136168956757\n",
      "epoch: 30 step: 466, loss is 0.23114655911922455\n",
      "epoch: 30 step: 467, loss is 0.2631963789463043\n",
      "epoch: 30 step: 468, loss is 0.28152602910995483\n",
      "epoch: 30 step: 469, loss is 0.10203269124031067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 470, loss is 0.16969309747219086\n",
      "epoch: 30 step: 471, loss is 0.32558760046958923\n",
      "epoch: 30 step: 472, loss is 0.1272761970758438\n",
      "epoch: 30 step: 473, loss is 0.2571641206741333\n",
      "epoch: 30 step: 474, loss is 0.1395614892244339\n",
      "epoch: 30 step: 475, loss is 0.12096191942691803\n",
      "epoch: 30 step: 476, loss is 0.27604585886001587\n",
      "epoch: 30 step: 477, loss is 0.29039090871810913\n",
      "epoch: 30 step: 478, loss is 0.22297491133213043\n",
      "epoch: 30 step: 479, loss is 0.2545216381549835\n",
      "epoch: 30 step: 480, loss is 0.19148540496826172\n",
      "epoch: 30 step: 481, loss is 0.23026323318481445\n",
      "epoch: 30 step: 482, loss is 0.09782706946134567\n",
      "epoch: 30 step: 483, loss is 0.08878938853740692\n",
      "epoch: 30 step: 484, loss is 0.06936423480510712\n",
      "epoch: 30 step: 485, loss is 0.33720654249191284\n",
      "epoch: 30 step: 486, loss is 0.26551759243011475\n",
      "epoch: 30 step: 487, loss is 0.12087727338075638\n",
      "epoch: 30 step: 488, loss is 0.1929580122232437\n",
      "epoch: 30 step: 489, loss is 0.49237754940986633\n",
      "epoch: 30 step: 490, loss is 0.16574238240718842\n",
      "epoch: 30 step: 491, loss is 0.1426265984773636\n",
      "epoch: 30 step: 492, loss is 0.2213880866765976\n",
      "epoch: 30 step: 493, loss is 0.08882930874824524\n",
      "epoch: 30 step: 494, loss is 0.4008079767227173\n",
      "epoch: 30 step: 495, loss is 0.11948425322771072\n",
      "epoch: 30 step: 496, loss is 0.26672476530075073\n",
      "epoch: 30 step: 497, loss is 0.31862083077430725\n",
      "epoch: 30 step: 498, loss is 0.17974521219730377\n",
      "epoch: 30 step: 499, loss is 0.16923180222511292\n",
      "epoch: 30 step: 500, loss is 0.06161358207464218\n",
      "epoch: 30 step: 501, loss is 0.10125792026519775\n",
      "epoch: 30 step: 502, loss is 0.17262998223304749\n",
      "epoch: 30 step: 503, loss is 0.11042249202728271\n",
      "epoch: 30 step: 504, loss is 0.12026792764663696\n",
      "epoch: 30 step: 505, loss is 0.21919465065002441\n",
      "epoch: 30 step: 506, loss is 0.2521331310272217\n",
      "epoch: 30 step: 507, loss is 0.11666979640722275\n",
      "epoch: 30 step: 508, loss is 0.2275078147649765\n",
      "epoch: 30 step: 509, loss is 0.2021583914756775\n",
      "epoch: 30 step: 510, loss is 0.13570636510849\n",
      "epoch: 30 step: 511, loss is 0.12712562084197998\n",
      "epoch: 30 step: 512, loss is 0.2978368401527405\n",
      "epoch: 30 step: 513, loss is 0.18294334411621094\n",
      "epoch: 30 step: 514, loss is 0.24433578550815582\n",
      "epoch: 30 step: 515, loss is 0.21874326467514038\n",
      "epoch: 30 step: 516, loss is 0.23160329461097717\n",
      "epoch: 30 step: 517, loss is 0.28103214502334595\n",
      "epoch: 30 step: 518, loss is 0.2024294137954712\n",
      "epoch: 30 step: 519, loss is 0.140564426779747\n",
      "epoch: 30 step: 520, loss is 0.16026249527931213\n",
      "epoch: 30 step: 521, loss is 0.19049963355064392\n",
      "epoch: 30 step: 522, loss is 0.09319104999303818\n",
      "epoch: 30 step: 523, loss is 0.11036943644285202\n",
      "epoch: 30 step: 524, loss is 0.41644802689552307\n",
      "epoch: 30 step: 525, loss is 0.21313022077083588\n",
      "epoch: 30 step: 526, loss is 0.10395042598247528\n",
      "epoch: 30 step: 527, loss is 0.2398512065410614\n",
      "epoch: 30 step: 528, loss is 0.2048708200454712\n",
      "epoch: 30 step: 529, loss is 0.17099662125110626\n",
      "epoch: 30 step: 530, loss is 0.1168353408575058\n",
      "epoch: 30 step: 531, loss is 0.09285864233970642\n",
      "epoch: 30 step: 532, loss is 0.13164688646793365\n",
      "epoch: 30 step: 533, loss is 0.2543567717075348\n",
      "epoch: 30 step: 534, loss is 0.26307469606399536\n",
      "epoch: 30 step: 535, loss is 0.14508862793445587\n",
      "epoch: 30 step: 536, loss is 0.12456829845905304\n",
      "epoch: 30 step: 537, loss is 0.1619877964258194\n",
      "epoch: 30 step: 538, loss is 0.2447216808795929\n",
      "epoch: 30 step: 539, loss is 0.21350686252117157\n",
      "epoch: 30 step: 540, loss is 0.20465081930160522\n",
      "epoch: 30 step: 541, loss is 0.21735793352127075\n",
      "epoch: 30 step: 542, loss is 0.1676960587501526\n",
      "epoch: 30 step: 543, loss is 0.18651552498340607\n",
      "epoch: 30 step: 544, loss is 0.18622072041034698\n",
      "epoch: 30 step: 545, loss is 0.1318504512310028\n",
      "epoch: 30 step: 546, loss is 0.2006901204586029\n",
      "epoch: 30 step: 547, loss is 0.1657857447862625\n",
      "epoch: 30 step: 548, loss is 0.08540675044059753\n",
      "epoch: 30 step: 549, loss is 0.1432672142982483\n",
      "epoch: 30 step: 550, loss is 0.11266759783029556\n",
      "epoch: 30 step: 551, loss is 0.210613414645195\n",
      "epoch: 30 step: 552, loss is 0.15320296585559845\n",
      "epoch: 30 step: 553, loss is 0.14119841158390045\n",
      "epoch: 30 step: 554, loss is 0.11728405207395554\n",
      "epoch: 30 step: 555, loss is 0.20500846207141876\n",
      "epoch: 30 step: 556, loss is 0.20815880596637726\n",
      "epoch: 30 step: 557, loss is 0.17314405739307404\n",
      "epoch: 30 step: 558, loss is 0.13476672768592834\n",
      "epoch: 30 step: 559, loss is 0.20993293821811676\n",
      "epoch: 30 step: 560, loss is 0.1895657181739807\n",
      "epoch: 30 step: 561, loss is 0.0771620124578476\n",
      "epoch: 30 step: 562, loss is 0.08105860650539398\n",
      "epoch: 30 step: 563, loss is 0.39865297079086304\n",
      "epoch: 30 step: 564, loss is 0.17814044654369354\n",
      "epoch: 30 step: 565, loss is 0.12712356448173523\n",
      "epoch: 30 step: 566, loss is 0.22314533591270447\n",
      "epoch: 30 step: 567, loss is 0.1624927520751953\n",
      "epoch: 30 step: 568, loss is 0.206475168466568\n",
      "epoch: 30 step: 569, loss is 0.24024629592895508\n",
      "epoch: 30 step: 570, loss is 0.3126348555088043\n",
      "epoch: 30 step: 571, loss is 0.2728259563446045\n",
      "epoch: 30 step: 572, loss is 0.1355716735124588\n",
      "epoch: 30 step: 573, loss is 0.10103295743465424\n",
      "epoch: 30 step: 574, loss is 0.1691511869430542\n",
      "epoch: 30 step: 575, loss is 0.20778895914554596\n",
      "epoch: 30 step: 576, loss is 0.14324542880058289\n",
      "epoch: 30 step: 577, loss is 0.31952887773513794\n",
      "epoch: 30 step: 578, loss is 0.0843355730175972\n",
      "epoch: 30 step: 579, loss is 0.09849504381418228\n",
      "epoch: 30 step: 580, loss is 0.2940238118171692\n",
      "epoch: 30 step: 581, loss is 0.18586933612823486\n",
      "epoch: 30 step: 582, loss is 0.19808906316757202\n",
      "epoch: 30 step: 583, loss is 0.22661319375038147\n",
      "epoch: 30 step: 584, loss is 0.2798990309238434\n",
      "epoch: 30 step: 585, loss is 0.12166862934827805\n",
      "epoch: 30 step: 586, loss is 0.15126727521419525\n",
      "epoch: 30 step: 587, loss is 0.09033152461051941\n",
      "epoch: 30 step: 588, loss is 0.23874488472938538\n",
      "epoch: 30 step: 589, loss is 0.16115207970142365\n",
      "epoch: 30 step: 590, loss is 0.06063640117645264\n",
      "epoch: 30 step: 591, loss is 0.13662509620189667\n",
      "epoch: 30 step: 592, loss is 0.15067918598651886\n",
      "epoch: 30 step: 593, loss is 0.15252724289894104\n",
      "epoch: 30 step: 594, loss is 0.175934299826622\n",
      "epoch: 30 step: 595, loss is 0.17202416062355042\n",
      "epoch: 30 step: 596, loss is 0.14120635390281677\n",
      "epoch: 30 step: 597, loss is 0.2928752303123474\n",
      "epoch: 30 step: 598, loss is 0.10247694700956345\n",
      "epoch: 30 step: 599, loss is 0.32224586606025696\n",
      "epoch: 30 step: 600, loss is 0.15043585002422333\n",
      "epoch: 30 step: 601, loss is 0.1967703253030777\n",
      "epoch: 30 step: 602, loss is 0.21512846648693085\n",
      "epoch: 30 step: 603, loss is 0.15582509338855743\n",
      "epoch: 30 step: 604, loss is 0.25769954919815063\n",
      "epoch: 30 step: 605, loss is 0.04628485441207886\n",
      "epoch: 30 step: 606, loss is 0.10950848460197449\n",
      "epoch: 30 step: 607, loss is 0.08275701850652695\n",
      "epoch: 30 step: 608, loss is 0.19149446487426758\n",
      "epoch: 30 step: 609, loss is 0.12839962542057037\n",
      "epoch: 30 step: 610, loss is 0.15055757761001587\n",
      "epoch: 30 step: 611, loss is 0.1409907042980194\n",
      "epoch: 30 step: 612, loss is 0.23529990017414093\n",
      "epoch: 30 step: 613, loss is 0.08918622136116028\n",
      "epoch: 30 step: 614, loss is 0.20897772908210754\n",
      "epoch: 30 step: 615, loss is 0.15737594664096832\n",
      "epoch: 30 step: 616, loss is 0.3008320927619934\n",
      "epoch: 30 step: 617, loss is 0.2662701904773712\n",
      "epoch: 30 step: 618, loss is 0.1811671108007431\n",
      "epoch: 30 step: 619, loss is 0.3000774085521698\n",
      "epoch: 30 step: 620, loss is 0.11391034722328186\n",
      "epoch: 30 step: 621, loss is 0.2912452518939972\n",
      "epoch: 30 step: 622, loss is 0.11471609771251678\n",
      "epoch: 30 step: 623, loss is 0.056444957852363586\n",
      "epoch: 30 step: 624, loss is 0.3318781852722168\n",
      "epoch: 30 step: 625, loss is 0.12602123618125916\n",
      "epoch: 30 step: 626, loss is 0.2140006721019745\n",
      "epoch: 30 step: 627, loss is 0.48328283429145813\n",
      "epoch: 30 step: 628, loss is 0.15218038856983185\n",
      "epoch: 30 step: 629, loss is 0.1531454175710678\n",
      "epoch: 30 step: 630, loss is 0.18413451313972473\n",
      "epoch: 30 step: 631, loss is 0.08804548531770706\n",
      "epoch: 30 step: 632, loss is 0.11871951818466187\n",
      "epoch: 30 step: 633, loss is 0.1709989756345749\n",
      "epoch: 30 step: 634, loss is 0.09023615717887878\n",
      "epoch: 30 step: 635, loss is 0.23349443078041077\n",
      "epoch: 30 step: 636, loss is 0.09639982134103775\n",
      "epoch: 30 step: 637, loss is 0.10366837680339813\n",
      "epoch: 30 step: 638, loss is 0.3342868387699127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 639, loss is 0.29848724603652954\n",
      "epoch: 30 step: 640, loss is 0.29144856333732605\n",
      "epoch: 30 step: 641, loss is 0.11730506271123886\n",
      "epoch: 30 step: 642, loss is 0.2586931884288788\n",
      "epoch: 30 step: 643, loss is 0.10290531814098358\n",
      "epoch: 30 step: 644, loss is 0.07177738845348358\n",
      "epoch: 30 step: 645, loss is 0.10894771665334702\n",
      "epoch: 30 step: 646, loss is 0.26516193151474\n",
      "epoch: 30 step: 647, loss is 0.6175287365913391\n",
      "epoch: 30 step: 648, loss is 0.12875816226005554\n",
      "epoch: 30 step: 649, loss is 0.24264751374721527\n",
      "epoch: 30 step: 650, loss is 0.1318909376859665\n",
      "epoch: 30 step: 651, loss is 0.09340154379606247\n",
      "epoch: 30 step: 652, loss is 0.1084824651479721\n",
      "epoch: 30 step: 653, loss is 0.16849872469902039\n",
      "epoch: 30 step: 654, loss is 0.1815970540046692\n",
      "epoch: 30 step: 655, loss is 0.04860140383243561\n",
      "epoch: 30 step: 656, loss is 0.13952285051345825\n",
      "epoch: 30 step: 657, loss is 0.17767944931983948\n",
      "epoch: 30 step: 658, loss is 0.24131356179714203\n",
      "epoch: 30 step: 659, loss is 0.27889835834503174\n",
      "epoch: 30 step: 660, loss is 0.1485081911087036\n",
      "epoch: 30 step: 661, loss is 0.35213637351989746\n",
      "epoch: 30 step: 662, loss is 0.17876172065734863\n",
      "epoch: 30 step: 663, loss is 0.17214572429656982\n",
      "epoch: 30 step: 664, loss is 0.12556026875972748\n",
      "epoch: 30 step: 665, loss is 0.2506639063358307\n",
      "epoch: 30 step: 666, loss is 0.3671184778213501\n",
      "epoch: 30 step: 667, loss is 0.12345941364765167\n",
      "epoch: 30 step: 668, loss is 0.16096320748329163\n",
      "epoch: 30 step: 669, loss is 0.22686751186847687\n",
      "epoch: 30 step: 670, loss is 0.21870973706245422\n",
      "epoch: 30 step: 671, loss is 0.19625155627727509\n",
      "epoch: 30 step: 672, loss is 0.13101992011070251\n",
      "epoch: 30 step: 673, loss is 0.16298596560955048\n",
      "epoch: 30 step: 674, loss is 0.26727747917175293\n",
      "epoch: 30 step: 675, loss is 0.20607352256774902\n",
      "epoch: 30 step: 676, loss is 0.12416225671768188\n",
      "epoch: 30 step: 677, loss is 0.11012238264083862\n",
      "epoch: 30 step: 678, loss is 0.223131000995636\n",
      "epoch: 30 step: 679, loss is 0.1757979840040207\n",
      "epoch: 30 step: 680, loss is 0.21407027542591095\n",
      "epoch: 30 step: 681, loss is 0.18865470588207245\n",
      "epoch: 30 step: 682, loss is 0.1396075040102005\n",
      "epoch: 30 step: 683, loss is 0.17836742103099823\n",
      "epoch: 30 step: 684, loss is 0.18745608627796173\n",
      "epoch: 30 step: 685, loss is 0.13313958048820496\n",
      "epoch: 30 step: 686, loss is 0.1978389024734497\n",
      "epoch: 30 step: 687, loss is 0.1735863983631134\n",
      "epoch: 30 step: 688, loss is 0.10487832874059677\n",
      "epoch: 30 step: 689, loss is 0.2120259553194046\n",
      "epoch: 30 step: 690, loss is 0.3446742296218872\n",
      "epoch: 30 step: 691, loss is 0.4407516419887543\n",
      "epoch: 30 step: 692, loss is 0.14100532233715057\n",
      "epoch: 30 step: 693, loss is 0.20950669050216675\n",
      "epoch: 30 step: 694, loss is 0.18854591250419617\n",
      "epoch: 30 step: 695, loss is 0.12060403823852539\n",
      "epoch: 30 step: 696, loss is 0.07931637763977051\n",
      "epoch: 30 step: 697, loss is 0.19385959208011627\n",
      "epoch: 30 step: 698, loss is 0.1296868622303009\n",
      "epoch: 30 step: 699, loss is 0.09191849082708359\n",
      "epoch: 30 step: 700, loss is 0.16982094943523407\n",
      "epoch: 30 step: 701, loss is 0.15709376335144043\n",
      "epoch: 30 step: 702, loss is 0.15353454649448395\n",
      "epoch: 30 step: 703, loss is 0.18968407809734344\n",
      "epoch: 30 step: 704, loss is 0.25832420587539673\n",
      "epoch: 30 step: 705, loss is 0.1420646756887436\n",
      "epoch: 30 step: 706, loss is 0.38732588291168213\n",
      "epoch: 30 step: 707, loss is 0.18450495600700378\n",
      "epoch: 30 step: 708, loss is 0.10595237463712692\n",
      "epoch: 30 step: 709, loss is 0.2119571417570114\n",
      "epoch: 30 step: 710, loss is 0.2032313197851181\n",
      "epoch: 30 step: 711, loss is 0.1486690193414688\n",
      "epoch: 30 step: 712, loss is 0.10081956535577774\n",
      "epoch: 30 step: 713, loss is 0.30256107449531555\n",
      "epoch: 30 step: 714, loss is 0.2192223072052002\n",
      "epoch: 30 step: 715, loss is 0.10166556388139725\n",
      "epoch: 30 step: 716, loss is 0.2229299694299698\n",
      "epoch: 30 step: 717, loss is 0.19701188802719116\n",
      "epoch: 30 step: 718, loss is 0.25263118743896484\n",
      "epoch: 30 step: 719, loss is 0.23370800912380219\n",
      "epoch: 30 step: 720, loss is 0.34967705607414246\n",
      "epoch: 30 step: 721, loss is 0.11665206402540207\n",
      "epoch: 30 step: 722, loss is 0.09820263087749481\n",
      "epoch: 30 step: 723, loss is 0.23868682980537415\n",
      "epoch: 30 step: 724, loss is 0.22767390310764313\n",
      "epoch: 30 step: 725, loss is 0.20610128343105316\n",
      "epoch: 30 step: 726, loss is 0.22059743106365204\n",
      "epoch: 30 step: 727, loss is 0.14237448573112488\n",
      "epoch: 30 step: 728, loss is 0.14659908413887024\n",
      "epoch: 30 step: 729, loss is 0.34276992082595825\n",
      "epoch: 30 step: 730, loss is 0.14616547524929047\n",
      "epoch: 30 step: 731, loss is 0.19854438304901123\n",
      "epoch: 30 step: 732, loss is 0.06528521329164505\n",
      "epoch: 30 step: 733, loss is 0.27841344475746155\n",
      "epoch: 30 step: 734, loss is 0.21563677489757538\n",
      "epoch: 30 step: 735, loss is 0.2048112004995346\n",
      "epoch: 30 step: 736, loss is 0.12483147531747818\n",
      "epoch: 30 step: 737, loss is 0.154667466878891\n",
      "epoch: 30 step: 738, loss is 0.36467301845550537\n",
      "epoch: 30 step: 739, loss is 0.09210649132728577\n",
      "epoch: 30 step: 740, loss is 0.36477139592170715\n",
      "epoch: 30 step: 741, loss is 0.1690661609172821\n",
      "epoch: 30 step: 742, loss is 0.1087561845779419\n",
      "epoch: 30 step: 743, loss is 0.31724438071250916\n",
      "epoch: 30 step: 744, loss is 0.20370396971702576\n",
      "epoch: 30 step: 745, loss is 0.22370269894599915\n",
      "epoch: 30 step: 746, loss is 0.11479868739843369\n",
      "epoch: 30 step: 747, loss is 0.20999585092067719\n",
      "epoch: 30 step: 748, loss is 0.15100419521331787\n",
      "epoch: 30 step: 749, loss is 0.19169798493385315\n",
      "epoch: 30 step: 750, loss is 0.22610163688659668\n",
      "epoch: 30 step: 751, loss is 0.22539447247982025\n",
      "epoch: 30 step: 752, loss is 0.283528208732605\n",
      "epoch: 30 step: 753, loss is 0.06981547921895981\n",
      "epoch: 30 step: 754, loss is 0.12260327488183975\n",
      "epoch: 30 step: 755, loss is 0.18980169296264648\n",
      "epoch: 30 step: 756, loss is 0.12834550440311432\n",
      "epoch: 30 step: 757, loss is 0.1913285106420517\n",
      "epoch: 30 step: 758, loss is 0.1874386966228485\n",
      "epoch: 30 step: 759, loss is 0.18096154928207397\n",
      "epoch: 30 step: 760, loss is 0.1724601536989212\n",
      "epoch: 30 step: 761, loss is 0.1527940183877945\n",
      "epoch: 30 step: 762, loss is 0.10680809617042542\n",
      "epoch: 30 step: 763, loss is 0.20471395552158356\n",
      "epoch: 30 step: 764, loss is 0.21808159351348877\n",
      "epoch: 30 step: 765, loss is 0.21006900072097778\n",
      "epoch: 30 step: 766, loss is 0.21312281489372253\n",
      "epoch: 30 step: 767, loss is 0.18686513602733612\n",
      "epoch: 30 step: 768, loss is 0.134917214512825\n",
      "epoch: 30 step: 769, loss is 0.12646238505840302\n",
      "epoch: 30 step: 770, loss is 0.23259112238883972\n",
      "epoch: 30 step: 771, loss is 0.15361927449703217\n",
      "epoch: 30 step: 772, loss is 0.21206383407115936\n",
      "epoch: 30 step: 773, loss is 0.4075506031513214\n",
      "epoch: 30 step: 774, loss is 0.07426591217517853\n",
      "epoch: 30 step: 775, loss is 0.3515574038028717\n",
      "epoch: 30 step: 776, loss is 0.14456689357757568\n",
      "epoch: 30 step: 777, loss is 0.1622634083032608\n",
      "epoch: 30 step: 778, loss is 0.18812085688114166\n",
      "epoch: 30 step: 779, loss is 0.2316928654909134\n",
      "epoch: 30 step: 780, loss is 0.20678402483463287\n",
      "epoch: 30 step: 781, loss is 0.16867543756961823\n",
      "epoch: 30 step: 782, loss is 0.24154846370220184\n",
      "epoch: 30 step: 783, loss is 0.16548174619674683\n",
      "epoch: 30 step: 784, loss is 0.18605414032936096\n",
      "epoch: 30 step: 785, loss is 0.1343475580215454\n",
      "epoch: 30 step: 786, loss is 0.24952848255634308\n",
      "epoch: 30 step: 787, loss is 0.07850302755832672\n",
      "epoch: 30 step: 788, loss is 0.13031156361103058\n",
      "epoch: 30 step: 789, loss is 0.17584745585918427\n",
      "epoch: 30 step: 790, loss is 0.17932003736495972\n",
      "epoch: 30 step: 791, loss is 0.08289213478565216\n",
      "epoch: 30 step: 792, loss is 0.042021188884973526\n",
      "epoch: 30 step: 793, loss is 0.2671459913253784\n",
      "epoch: 30 step: 794, loss is 0.16773606836795807\n",
      "epoch: 30 step: 795, loss is 0.2004271149635315\n",
      "epoch: 30 step: 796, loss is 0.05098113790154457\n",
      "epoch: 30 step: 797, loss is 0.09158310294151306\n",
      "epoch: 30 step: 798, loss is 0.09375239163637161\n",
      "epoch: 30 step: 799, loss is 0.29894930124282837\n",
      "epoch: 30 step: 800, loss is 0.18137440085411072\n",
      "epoch: 30 step: 801, loss is 0.18940407037734985\n",
      "epoch: 30 step: 802, loss is 0.176014706492424\n",
      "epoch: 30 step: 803, loss is 0.31276950240135193\n",
      "epoch: 30 step: 804, loss is 0.27045968174934387\n",
      "epoch: 30 step: 805, loss is 0.34625548124313354\n",
      "epoch: 30 step: 806, loss is 0.16233839094638824\n",
      "epoch: 30 step: 807, loss is 0.15558989346027374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 808, loss is 0.15616655349731445\n",
      "epoch: 30 step: 809, loss is 0.09150039404630661\n",
      "epoch: 30 step: 810, loss is 0.08175072818994522\n",
      "epoch: 30 step: 811, loss is 0.12727351486682892\n",
      "epoch: 30 step: 812, loss is 0.13065160810947418\n",
      "epoch: 30 step: 813, loss is 0.195331409573555\n",
      "epoch: 30 step: 814, loss is 0.11336877942085266\n",
      "epoch: 30 step: 815, loss is 0.059319861233234406\n",
      "epoch: 30 step: 816, loss is 0.1643156111240387\n",
      "epoch: 30 step: 817, loss is 0.16097499430179596\n",
      "epoch: 30 step: 818, loss is 0.1798807829618454\n",
      "epoch: 30 step: 819, loss is 0.09549110382795334\n",
      "epoch: 30 step: 820, loss is 0.24602602422237396\n",
      "epoch: 30 step: 821, loss is 0.11077821999788284\n",
      "epoch: 30 step: 822, loss is 0.2537757158279419\n",
      "epoch: 30 step: 823, loss is 0.18531133234500885\n",
      "epoch: 30 step: 824, loss is 0.22618046402931213\n",
      "epoch: 30 step: 825, loss is 0.19398938119411469\n",
      "epoch: 30 step: 826, loss is 0.07294584810733795\n",
      "epoch: 30 step: 827, loss is 0.11266694962978363\n",
      "epoch: 30 step: 828, loss is 0.2109963595867157\n",
      "epoch: 30 step: 829, loss is 0.1299671232700348\n",
      "epoch: 30 step: 830, loss is 0.22211986780166626\n",
      "epoch: 30 step: 831, loss is 0.20681346952915192\n",
      "epoch: 30 step: 832, loss is 0.22485247254371643\n",
      "epoch: 30 step: 833, loss is 0.1613946110010147\n",
      "epoch: 30 step: 834, loss is 0.37347835302352905\n",
      "epoch: 30 step: 835, loss is 0.11225515604019165\n",
      "epoch: 30 step: 836, loss is 0.11899679154157639\n",
      "epoch: 30 step: 837, loss is 0.189029723405838\n",
      "epoch: 30 step: 838, loss is 0.14097294211387634\n",
      "epoch: 30 step: 839, loss is 0.12312506139278412\n",
      "epoch: 30 step: 840, loss is 0.12982873618602753\n",
      "epoch: 30 step: 841, loss is 0.14548291265964508\n",
      "epoch: 30 step: 842, loss is 0.137671560049057\n",
      "epoch: 30 step: 843, loss is 0.10114740580320358\n",
      "epoch: 30 step: 844, loss is 0.10271379351615906\n",
      "epoch: 30 step: 845, loss is 0.20737504959106445\n",
      "epoch: 30 step: 846, loss is 0.2164667546749115\n",
      "epoch: 30 step: 847, loss is 0.18276719748973846\n",
      "epoch: 30 step: 848, loss is 0.1899876594543457\n",
      "epoch: 30 step: 849, loss is 0.18946892023086548\n",
      "epoch: 30 step: 850, loss is 0.14366619288921356\n",
      "epoch: 30 step: 851, loss is 0.12413989752531052\n",
      "epoch: 30 step: 852, loss is 0.09826266020536423\n",
      "epoch: 30 step: 853, loss is 0.3960976302623749\n",
      "epoch: 30 step: 854, loss is 0.2607363760471344\n",
      "epoch: 30 step: 855, loss is 0.14082948863506317\n",
      "epoch: 30 step: 856, loss is 0.11163483560085297\n",
      "epoch: 30 step: 857, loss is 0.11460905522108078\n",
      "epoch: 30 step: 858, loss is 0.07504265010356903\n",
      "epoch: 30 step: 859, loss is 0.35661643743515015\n",
      "epoch: 30 step: 860, loss is 0.18138262629508972\n",
      "epoch: 30 step: 861, loss is 0.17672127485275269\n",
      "epoch: 30 step: 862, loss is 0.2197670191526413\n",
      "epoch: 30 step: 863, loss is 0.21955081820487976\n",
      "epoch: 30 step: 864, loss is 0.17902569472789764\n",
      "epoch: 30 step: 865, loss is 0.24011875689029694\n",
      "epoch: 30 step: 866, loss is 0.13554374873638153\n",
      "epoch: 30 step: 867, loss is 0.045213986188173294\n",
      "epoch: 30 step: 868, loss is 0.18396368622779846\n",
      "epoch: 30 step: 869, loss is 0.10989989340305328\n",
      "epoch: 30 step: 870, loss is 0.2851129174232483\n",
      "epoch: 30 step: 871, loss is 0.37096261978149414\n",
      "epoch: 30 step: 872, loss is 0.2660822570323944\n",
      "epoch: 30 step: 873, loss is 0.2880004048347473\n",
      "epoch: 30 step: 874, loss is 0.10824189335107803\n",
      "epoch: 30 step: 875, loss is 0.1549316644668579\n",
      "epoch: 30 step: 876, loss is 0.2124972939491272\n",
      "epoch: 30 step: 877, loss is 0.05969345197081566\n",
      "epoch: 30 step: 878, loss is 0.2025308907032013\n",
      "epoch: 30 step: 879, loss is 0.12131237983703613\n",
      "epoch: 30 step: 880, loss is 0.3140719532966614\n",
      "epoch: 30 step: 881, loss is 0.21722276508808136\n",
      "epoch: 30 step: 882, loss is 0.142424538731575\n",
      "epoch: 30 step: 883, loss is 0.23680473864078522\n",
      "epoch: 30 step: 884, loss is 0.16909323632717133\n",
      "epoch: 30 step: 885, loss is 0.2013751119375229\n",
      "epoch: 30 step: 886, loss is 0.23652276396751404\n",
      "epoch: 30 step: 887, loss is 0.13491392135620117\n",
      "epoch: 30 step: 888, loss is 0.1493559032678604\n",
      "epoch: 30 step: 889, loss is 0.14200064539909363\n",
      "epoch: 30 step: 890, loss is 0.13789138197898865\n",
      "epoch: 30 step: 891, loss is 0.195744127035141\n",
      "epoch: 30 step: 892, loss is 0.33669885993003845\n",
      "epoch: 30 step: 893, loss is 0.1885705292224884\n",
      "epoch: 30 step: 894, loss is 0.25990089774131775\n",
      "epoch: 30 step: 895, loss is 0.14944598078727722\n",
      "epoch: 30 step: 896, loss is 0.11946278810501099\n",
      "epoch: 30 step: 897, loss is 0.18391282856464386\n",
      "epoch: 30 step: 898, loss is 0.15824246406555176\n",
      "epoch: 30 step: 899, loss is 0.24020448327064514\n",
      "epoch: 30 step: 900, loss is 0.09672752767801285\n",
      "epoch: 30 step: 901, loss is 0.13476450741291046\n",
      "epoch: 30 step: 902, loss is 0.21373705565929413\n",
      "epoch: 30 step: 903, loss is 0.11297303438186646\n",
      "epoch: 30 step: 904, loss is 0.1992584466934204\n",
      "epoch: 30 step: 905, loss is 0.4524666965007782\n",
      "epoch: 30 step: 906, loss is 0.1327398419380188\n",
      "epoch: 30 step: 907, loss is 0.13646869361400604\n",
      "epoch: 30 step: 908, loss is 0.1273433268070221\n",
      "epoch: 30 step: 909, loss is 0.21734300255775452\n",
      "epoch: 30 step: 910, loss is 0.12402408570051193\n",
      "epoch: 30 step: 911, loss is 0.11242085695266724\n",
      "epoch: 30 step: 912, loss is 0.20837391912937164\n",
      "epoch: 30 step: 913, loss is 0.2747638523578644\n",
      "epoch: 30 step: 914, loss is 0.14577427506446838\n",
      "epoch: 30 step: 915, loss is 0.2596256732940674\n",
      "epoch: 30 step: 916, loss is 0.10921099036931992\n",
      "epoch: 30 step: 917, loss is 0.13030429184436798\n",
      "epoch: 30 step: 918, loss is 0.12160443514585495\n",
      "epoch: 30 step: 919, loss is 0.13409623503684998\n",
      "epoch: 30 step: 920, loss is 0.092600017786026\n",
      "epoch: 30 step: 921, loss is 0.12630429863929749\n",
      "epoch: 30 step: 922, loss is 0.10053679347038269\n",
      "epoch: 30 step: 923, loss is 0.06853911280632019\n",
      "epoch: 30 step: 924, loss is 0.16861742734909058\n",
      "epoch: 30 step: 925, loss is 0.15141604840755463\n",
      "epoch: 30 step: 926, loss is 0.11248889565467834\n",
      "epoch: 30 step: 927, loss is 0.15662674605846405\n",
      "epoch: 30 step: 928, loss is 0.22674117982387543\n",
      "epoch: 30 step: 929, loss is 0.07065549492835999\n",
      "epoch: 30 step: 930, loss is 0.1522839367389679\n",
      "epoch: 30 step: 931, loss is 0.14483585953712463\n",
      "epoch: 30 step: 932, loss is 0.3072826564311981\n",
      "epoch: 30 step: 933, loss is 0.08732428401708603\n",
      "epoch: 30 step: 934, loss is 0.15753747522830963\n",
      "epoch: 30 step: 935, loss is 0.12040089070796967\n",
      "epoch: 30 step: 936, loss is 0.15339244902133942\n",
      "epoch: 30 step: 937, loss is 0.213296577334404\n",
      "{'acc': 0.9225761217948718}\n"
     ]
    }
   ],
   "source": [
    "# 训练有正则化的网络\n",
    "model = train(ForwardFashionRegularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(976:16928,MainProcess):2022-10-17-16:47:50.328.189 [mindspore\\dataset\\engine\\datasets_user_defined.py:656] Python multiprocessing is not supported on Windows platform.\n",
      "[WARNING] ME(976:16928,MainProcess):2022-10-17-16:47:50.362.462 [mindspore\\dataset\\engine\\datasets_user_defined.py:656] Python multiprocessing is not supported on Windows platform.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个sample预测结果： 6    真实结果： 6\n",
      "第1个sample预测结果： 8    真实结果： 8\n",
      "第2个sample预测结果： 2    真实结果： 2\n",
      "第3个sample预测结果： 7    真实结果： 7\n",
      "第4个sample预测结果： 9    真实结果： 9\n",
      "第5个sample预测结果： 2    真实结果： 2\n",
      "第6个sample预测结果： 0    真实结果： 6\n",
      "第7个sample预测结果： 2    真实结果： 2\n",
      "第8个sample预测结果： 1    真实结果： 1\n",
      "第9个sample预测结果： 7    真实结果： 7\n",
      "第10个sample预测结果： 4    真实结果： 4\n",
      "第11个sample预测结果： 6    真实结果： 6\n",
      "第12个sample预测结果： 8    真实结果： 8\n",
      "第13个sample预测结果： 0    真实结果： 0\n",
      "第14个sample预测结果： 5    真实结果： 5\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "ds_test, _ = create_dataset()\n",
    "test_ = ds_test.create_dict_iterator(output_numpy=True).__next__()\n",
    "predictions = model.predict(Tensor(test_['x']))\n",
    "predictions = predictions.asnumpy()\n",
    "for i in range(15):\n",
    "    p_np = predictions[i, :]\n",
    "    p_list = p_np.tolist()\n",
    "    print('第' + str(i) + '个sample预测结果：', p_list.index(max(p_list)), '   真实结果：', test_['y'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------定义可视化函数--------------------------------\n",
    "# 输入预测结果序列，真实标签序列，以及图片序列\n",
    "# 目标是根据预测值对错，让其标签显示为红色或者蓝色。对：标签为红色；错：标签为蓝色\n",
    "def plot_image(predictions_array, true_label, img):\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # 显示对应图片\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    # 显示预测结果的颜色，如果对上了是蓝色，否则为红色\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    # 显示对应标签的格式，样式\n",
    "    plt.xlabel('{},{:2.0f}% ({})'.format(class_names[predicted_label],\n",
    "                                         100 * np.max(predictions_array),\n",
    "                                         class_names[true_label]), color=color)\n",
    "# 将预测的结果以柱状图形状显示蓝对红错\n",
    "def plot_value_array(predictions_array, true_label):\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    this_plot = plt.bar(range(10), predictions_array, color='#777777')\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    this_plot[predicted_label].set_color('red')\n",
    "    this_plot[true_label].set_color('blue')\n",
    "\n",
    "import numpy as np\n",
    "def softmax_np(x):\n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x/np.sum(exp_x)\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1000 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 预测15个图像与标签，并展现出来\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
    "    pred_np_ = predictions[i, :]\n",
    "    pred_np_ = softmax_np(pred_np_)\n",
    "    plot_image(pred_np_, test_['y'][i], test_['x'][i, 0, ...])\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
    "    plot_value_array(pred_np_, test_['y'][i])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore1.8.1",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
